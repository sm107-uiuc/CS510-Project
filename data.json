[    {        "url": "https://www.databricks.com/blog/2021/07/29/an-experimentation-pipeline-for-extracting-topics-from-text-data-using-pyspark.html",        "title": "How to Build an Experimentation Pipeline for Extracting Topics From Text Data Using PySpark - The Databricks Blog",        "h1": "An Experimentation Pipeline for Extracting Topics From Text Data Using PySpark",        "description": "Learn how topic modeling with latent dirichlet allocation (LDA) can be performed using PySpark with Feature Store being used to streamline the process.",        "all_paragraphs": "WEBINAR May 18 / 8 AM PT\nGoodbye, Data Warehouse. Hello, Lakehouse.\nAttend to understand how a data lakehouse fits within your modern data stack.,Discover the Lakehouse for Manufacturing\nHear how Corning is making critical decisions that minimize manual inspections, lower shipping costs, and increase customer satisfaction.,The Data + AI Summit catalog is now live,Connect with validated partner solutions in just a few clicks.,See why Gartner named Databricks a Leader for the second consecutive year,This post is part of a series of posts on topic modeling. Topic modeling is the process of extracting topics from a set of text documents. This is useful for understanding or summarizing large collections of text documents.  A document can be a line of text, a paragraph or a chapter in a book. The abstraction of a document refers to a standalone unit of text over which we operate. A collection of documents is referred to as a corpus, and multiple corpus, a corpora.,In this work, we will extract topics from a corpus of documents using the open source Pyspark ML library and visualize the relevance of the words in the extracted topics using Plot.ly. While ideally, one would want to couple the data engineering and model development process, there are times when a  data scientist might want to experiment on model building with a certain dataset.  Therefore, it might be wasteful to run the entire ETL pipeline when the intent is to model experimentation. In this blog, we will showcase how to separate the ETL process from the data science experimentation step using the Databricks Feature Store to save the extracted features so that they can be reused for experimentation. This makes it easier to experiment using various topic modeling algorithms such as LDA  and perform hyperparameter optimization. It also makes the experimentation more systematic and reproducible since the Feature Store allows for versioning as well.,In this work, we have downloaded  tweets from various political figures and stored them in the JSON format. The workflow to extract topics from these tweets consists of the following steps,The general idea behind a feature store is that it acts as a central repository to store the features for different models. The Databricks Feature Store allows you to do the same thing while being integrated into the Databricks unified platform. The Feature Store encourages feature discovery, sharing and lineage tracking.  Feature Stores are built on Delta tables, which bring ACID transactions to Spark and other processing engines,,We start by loading the data using Apache Pyspark™ and extracting the necessary fields required for extracting the topics. The duplicate tweets are removed, and the tweets are then tokenized and cleaned by removing the stopwords. While further processing is not done in this work, it is highly recommended to remove links and emoticons.,The words in the corpus are vectorized by word count and the Inverse Document Frequency is then computed (IDF). These are the extracted features in this model that can then be saved and reused in the model building process. Since the feature rawFeatures, which stores the IDF values, is a Sparse Vector type and the Feature Store does not support storing arrays, we convert this column into a string so that it can be saved in the Feature Store. We cast this back to a vector while reading it from the Feature Store since we know the schema of the feature, so we can use it in our model.,We start off by creating a database to hold our feature table. A feature store client object is created for interacting with this feature store. We create the feature store by specifying at least the name of the store, the keys and the columns to be saved. In the example below, we save four columns from the data frame generated above. Since Feature Stores are Delta tables, the features can be rewritten, and the feature values are simply version controlled so they can be retrieved later, allowing for reproducible experiments.,Once the features have been saved, one does not have to rerun the ETL pipeline the next time a data scientist wants to experiment with a different model, saving a considerable amount of time and compute resources. The features can simply be reloaded from the table using fs.read_table by passing the table name and, if desired, the timestamp to retrieve a specific version of the set of features.,Since the transformed IDF values were stored as a string, we need to extract the values and cast it into a Sparse Vector format. The transformation is shown below and the data frame df_new is created, which will be fed to the topic modeling algorithm.,Once we have set up the data frame with the extracted features, the topics can be extracted using the Latent Dirichlet Allocation (LDA) algorithm from the PySpark ML library.  LDA is defined as the following:,”Latent Dirichlet Allocation (LDA) is a generative, probabilistic model for a collection of documents, which are represented as mixtures of latent topics, where each topic is characterized by a distribution over words.”,In simple terms, it means that each document is made up of a number of topics, and the proportion of these topics vary between the documents. The topics themselves are represented as a combination of words, with the distribution over the words representing their relevance to the topic. There are two hyperparameters that determine the extent of the mixture of topics. The topic concentration parameter called ‘beta’  and the document concentration parameter called ‘alpha’ is used to suggest the level of similarity between topics and documents respectively. A high alpha value will result in documents having similar topics and a low value will result in documents with fewer but different topics. At very large values of alpha, as alpha approaches infinity, all documents will consist of the same topics. Similarly, a higher value of beta will result in topics that are similar while a smaller value will result in topics that have fewer words and hence are dissimilar.,Since LDA is an unsupervised algorithm, there is no ‘ground truth’ to establish the model accuracy. The number of topics k is a hyperparameter that can often be tuned or optimized through a metric such as the model perplexity. The alpha and beta hyperparameters can be set using the parameters setDocConcentration and setTopicConcentration, respectively.,Once the model has been fit on the extracted features, we can create a topic visualization using Plot.ly.,The plot below illustrates the topic distribution as sets of bar charts, where each row corresponds to a topic. The bars in a row indicate the various words associated with a topic and their relative importance to that topic. As mentioned above, the number of topics is a hyperparameter that either requires domain-level expertise or hyperparameter tuning.,We have seen how to load a collection of JSON files of tweets and obtain relatively clean text data. The text was then vectorized so that it could be utilized by one of several machine learning algorithms for NLP). The vectorized data was then saved as features using the Databricks Feature Store so that it can enable reuse and experimentation by the data scientist. The topics were then fed to the  PySpark LDA algorithm and the extracted topics were then visualized using Plot.ly. I would encourage you to try out the notebook and experiment with this pipeline by adjusting the hyperparameters, such as the number of topics, to see how it can work for you!,© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation."    }, {        "url": "http://www.databricks.com/blog/2021/07/29/an-experimentation-pipeline-for-extracting-topics-from-text-data-using-pyspark.html",        "title": "How to Build an Experimentation Pipeline for Extracting Topics From Text Data Using PySpark - The Databricks Blog",        "h1": "An Experimentation Pipeline for Extracting Topics From Text Data Using PySpark",        "description": "Learn how topic modeling with latent dirichlet allocation (LDA) can be performed using PySpark with Feature Store being used to streamline the process.",        "all_paragraphs": "WEBINAR May 18 / 8 AM PT\nGoodbye, Data Warehouse. Hello, Lakehouse.\nAttend to understand how a data lakehouse fits within your modern data stack.,Discover the Lakehouse for Manufacturing\nHear how Corning is making critical decisions that minimize manual inspections, lower shipping costs, and increase customer satisfaction.,The Data + AI Summit catalog is now live,Connect with validated partner solutions in just a few clicks.,See why Gartner named Databricks a Leader for the second consecutive year,This post is part of a series of posts on topic modeling. Topic modeling is the process of extracting topics from a set of text documents. This is useful for understanding or summarizing large collections of text documents.  A document can be a line of text, a paragraph or a chapter in a book. The abstraction of a document refers to a standalone unit of text over which we operate. A collection of documents is referred to as a corpus, and multiple corpus, a corpora.,In this work, we will extract topics from a corpus of documents using the open source Pyspark ML library and visualize the relevance of the words in the extracted topics using Plot.ly. While ideally, one would want to couple the data engineering and model development process, there are times when a  data scientist might want to experiment on model building with a certain dataset.  Therefore, it might be wasteful to run the entire ETL pipeline when the intent is to model experimentation. In this blog, we will showcase how to separate the ETL process from the data science experimentation step using the Databricks Feature Store to save the extracted features so that they can be reused for experimentation. This makes it easier to experiment using various topic modeling algorithms such as LDA  and perform hyperparameter optimization. It also makes the experimentation more systematic and reproducible since the Feature Store allows for versioning as well.,In this work, we have downloaded  tweets from various political figures and stored them in the JSON format. The workflow to extract topics from these tweets consists of the following steps,The general idea behind a feature store is that it acts as a central repository to store the features for different models. The Databricks Feature Store allows you to do the same thing while being integrated into the Databricks unified platform. The Feature Store encourages feature discovery, sharing and lineage tracking.  Feature Stores are built on Delta tables, which bring ACID transactions to Spark and other processing engines,,We start by loading the data using Apache Pyspark™ and extracting the necessary fields required for extracting the topics. The duplicate tweets are removed, and the tweets are then tokenized and cleaned by removing the stopwords. While further processing is not done in this work, it is highly recommended to remove links and emoticons.,The words in the corpus are vectorized by word count and the Inverse Document Frequency is then computed (IDF). These are the extracted features in this model that can then be saved and reused in the model building process. Since the feature rawFeatures, which stores the IDF values, is a Sparse Vector type and the Feature Store does not support storing arrays, we convert this column into a string so that it can be saved in the Feature Store. We cast this back to a vector while reading it from the Feature Store since we know the schema of the feature, so we can use it in our model.,We start off by creating a database to hold our feature table. A feature store client object is created for interacting with this feature store. We create the feature store by specifying at least the name of the store, the keys and the columns to be saved. In the example below, we save four columns from the data frame generated above. Since Feature Stores are Delta tables, the features can be rewritten, and the feature values are simply version controlled so they can be retrieved later, allowing for reproducible experiments.,Once the features have been saved, one does not have to rerun the ETL pipeline the next time a data scientist wants to experiment with a different model, saving a considerable amount of time and compute resources. The features can simply be reloaded from the table using fs.read_table by passing the table name and, if desired, the timestamp to retrieve a specific version of the set of features.,Since the transformed IDF values were stored as a string, we need to extract the values and cast it into a Sparse Vector format. The transformation is shown below and the data frame df_new is created, which will be fed to the topic modeling algorithm.,Once we have set up the data frame with the extracted features, the topics can be extracted using the Latent Dirichlet Allocation (LDA) algorithm from the PySpark ML library.  LDA is defined as the following:,”Latent Dirichlet Allocation (LDA) is a generative, probabilistic model for a collection of documents, which are represented as mixtures of latent topics, where each topic is characterized by a distribution over words.”,In simple terms, it means that each document is made up of a number of topics, and the proportion of these topics vary between the documents. The topics themselves are represented as a combination of words, with the distribution over the words representing their relevance to the topic. There are two hyperparameters that determine the extent of the mixture of topics. The topic concentration parameter called ‘beta’  and the document concentration parameter called ‘alpha’ is used to suggest the level of similarity between topics and documents respectively. A high alpha value will result in documents having similar topics and a low value will result in documents with fewer but different topics. At very large values of alpha, as alpha approaches infinity, all documents will consist of the same topics. Similarly, a higher value of beta will result in topics that are similar while a smaller value will result in topics that have fewer words and hence are dissimilar.,Since LDA is an unsupervised algorithm, there is no ‘ground truth’ to establish the model accuracy. The number of topics k is a hyperparameter that can often be tuned or optimized through a metric such as the model perplexity. The alpha and beta hyperparameters can be set using the parameters setDocConcentration and setTopicConcentration, respectively.,Once the model has been fit on the extracted features, we can create a topic visualization using Plot.ly.,The plot below illustrates the topic distribution as sets of bar charts, where each row corresponds to a topic. The bars in a row indicate the various words associated with a topic and their relative importance to that topic. As mentioned above, the number of topics is a hyperparameter that either requires domain-level expertise or hyperparameter tuning.,We have seen how to load a collection of JSON files of tweets and obtain relatively clean text data. The text was then vectorized so that it could be utilized by one of several machine learning algorithms for NLP). The vectorized data was then saved as features using the Databricks Feature Store so that it can enable reuse and experimentation by the data scientist. The topics were then fed to the  PySpark LDA algorithm and the extracted topics were then visualized using Plot.ly. I would encourage you to try out the notebook and experiment with this pipeline by adjusting the hyperparameters, such as the number of topics, to see how it can work for you!,© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation."    },{    "url": "https://medium.com/towards-data-science/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24",                        "title": "",            "h1": "",            "description": "",        "all_paragraphs": "Data Scientist  EpochGeo is looking for a junior to mid-level Data Scientist to support one of the most innovative, exciting, and growing offices that is rooted in data rich projects in the national security space.  Are you a data scientist, AI/ML practitioner, mathematician, or similar, who loves to work with data, find patterns in it, and then turn it into actionable intelligence? Are you a data scientist who loves to draw out insights from data, and then work collaboratively with analysts and engineers to turn those insights into automated workflows? If so, please reach out to us at EpochGeo to chat.  Big data problems can be overwhelming, inaccessible, and noisy. EpochGeo is a data services firm specializing in supporting the full spectrum data cycle: beginning with scalable data storage and ending in producing impactful analytics. Our developers, analysts, and data scientists have a proven track record developing open-source innovative technology and actionable analytics to inform customers’ data driven decisions.  Your background likely includes:  1 - 5+ years of experience of data engineering, coding in python, SQL, and familiarity with Keras, Tensorflow, or PyTorch frameworks preferred. Education or experience in Data Science, Economics, or Computer Science. Familiarity of applying machine learning and statistical methods to complex problem sets that resulted in successful outcomes. Some experience querying, creating, and integrating data flows from an Elasticsearch instance to include building dashboards and presenting information via Kibana or Plotly Dash. Familiarity with natural language processing pipelines, large language models/transformers and libraries such as spaCy, NLTK, OpenNLP, or StanfordNLP. Understanding text processing, information extraction (NER and Relationship extraction), and POS Tagging. A bonus to have military and/or DoD/IC experience in any of the following disciplines: data science, computer vision, target development, geospatial, or statistical analysis. What you will be doing:  Collaborating with analyst teams to understand their problem sets and identifying workflows for automation and opportunities to integrate machine learning algorithms that can be optimized at scale against big data architectures. Conceiving and constructing working prototypes and first of their kind proof of concepts to push the art of the possible against real world problem sets. Creating and fine tuning machine learning hyperparameters or utilizing AutoML solutions to optimize machine learning algorithms to solve customer problem sets. Having an end user-first attitude and advocating on their behalf in technical exchanges Working in a dynamic environment with ever evolving requirements. Bonus:  Master’s degree in Statistics, Data Science, Mathematics, Economics, Geography, or a similar field Security+ or CISSP Additional:  Must hold a TS/SCI clearance and be willing to submit for a CI Poly Benefits Include:  100% health care premiums covered, FSA, HSA 401k: 6% match, immediate vesting 14 holidays: All 11 Federal holidays, day after Thanksgiving, 24 DEC, 31 DEC PTO: 4 weeks annually 3 week sabbatical every 3 years with the company"}]