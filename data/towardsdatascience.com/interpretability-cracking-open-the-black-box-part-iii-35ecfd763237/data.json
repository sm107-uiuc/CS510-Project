{"url": "https://towardsdatascience.com/interpretability-cracking-open-the-black-box-part-iii-35ecfd763237", "time": 1683001580.7259378, "path": "towardsdatascience.com/interpretability-cracking-open-the-black-box-part-iii-35ecfd763237/", "webpage": {"metadata": {"title": "Interpretability: Cracking open the black box \u2014 Part III | by Manu Joseph | Towards Data Science", "h1": "Interpretability: Cracking open the black box \u2014 Part III", "description": "Previously, we looked at the pitfalls with the default \u201c feature importance \u201c in tree based models, talked about permutation importance, LOOC importance, and Partial Dependence Plots. Now let\u2019s\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/interpretability-cracking-open-the-black-box-part-iii-35ecfd763237?source=friends_link&sk=feefa4d113a26383994c93ce3db41f41", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "anchor_text": "looked at the pitfalls", "paragraph_index": 1}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Github", "paragraph_index": 8}, {"url": "https://pypi.org/project/lime/", "anchor_text": "installable package", "paragraph_index": 8}, {"url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "anchor_text": "Part II", "paragraph_index": 20}, {"url": "https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html", "anchor_text": "LIME ON TEXT DATA \u2014 MULTI-LABEL CLASSIFICATION", "paragraph_index": 52}, {"url": "https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb", "anchor_text": "LIME ON IMAGE CLASSIFICATION \u2014 INCEPTION_V3 \u2014 KERAS", "paragraph_index": 53}, {"url": "https://github.com/slundberg/shap/blob/master/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.ipynb", "anchor_text": "GRADIENT EXPLAINER \u2014 SHAP \u2014 INTERMEDIATE LAYER IN VGG16 IN IMAGENET", "paragraph_index": 54}, {"url": "https://github.com/manujosephv/interpretability_blog", "anchor_text": "Github", "paragraph_index": 57}], "all_paragraphs": ["Is the paywall bothering you? Click here to go around it.", "Previously, we looked at the pitfalls with the default \u201c feature importance \u201c in tree based models, talked about permutation importance, LOOC importance, and Partial Dependence Plots. Now let\u2019s switch lanes and look at a few model agnostic techniques which takes a bottom-up way of explaining predictions. Instead of looking at the model and trying to come up with global explanations like feature importance, these set of methods look at each single prediction and then try to explain them.", "As the name suggests, this is a model agnostic technique to generate local explanations to the model. The core idea behind the technique is quite intuitive. Suppose we have a complex classifier, with a highly non-linear decision boundary. But if we zoom in and look at a single prediction, the behaviour of the model in that locality can be explained by a simple interpretable model (mostly linear).", "LIME[2] uses a local surrogate model trained on perturbations of the data point we are investigating for explanations. This ensures that even though the explanation does not have global fidelity(faithfulness to original model) it have local fidelity. The paper[2] also recognizes that there is an interpretability vs fidelity trade-off and proposed a formal framework to express the framework.", "\u03be(x) is the explanation, L(f, g, \u03c0\u2093) is the inverse of local fidelity (or how unfaithful is g in approximating f in the locality), and \u03a9(g) is the complexity of the local model, g. In order to ensure both local fidelity and interpretability, we need to minimize the unfaithfulness (or maximize the local fidelity), keeping in mind that the complexity should be low enough for humans to understand.", "Even though we can use any interpretable model as the local surrogate, the paper uses a Lasso regression to induce sparsity in explanations. The authors of the paper have restricted their explorations to the fidelity of the model and kept the complexity as user input. In case of a Lasso Regression, it is the number of features for which the explanation should be attributed.", "One additional aspect they have explored and proposed a solution to (one which has not got a lot of popularity) is the challenge of providing a global explanation using a set of individual instances. They call it \u201cSubmodular Pick for Explaining Models\u201d. It is essentially a greedy optimization which tries to pick a few instances from the whole lot which maximizes something they call \u201cnon-redundant coverage\u201d. Non-redundant coverage makes sure that the optimization is not picking instances with similar explanations.", "The advantages of the technique are:", "The implementation by the paper authors is available in Github as well as an installable package in pip. Before we take a look at how we would implement those, let\u2019s discuss a few quirks in the implementation which you should know before running with it (focus on tabular explainer).", "The main steps are as follows", "The key things you need to keep in mind are:", "Now, let\u2019s continue with the same dataset we have been working with in the last part and see LIME in action.", "For variety, let\u2019s look at another example. One which the model misclassified.", "As mentioned earlier, there is another technique mentioned in the paper called \u201csubmodular pick\u201d to find a handful of explanations which try to explain most of the cases. Let\u2019s try to get that as well. This particular part of the python library is not so stable and the example notebooks provided was giving me errors. But after spending some time reading through the source code, I figured out a way out of the errors.", "There are two charts where we have aggregated the explanations across the 500 points we sampled from out test set(we can run it on all test data points, but chose to do sampling only cause of computation).", "The first chart aggregates the effect of the feature across >50k and <50k cases and ignores the sign when calculating the mean. This gives you an idea of what features were important in the larger sense.", "The second chart splits the inference across the two labels and looks at them separately. This chart lets us understand which feature was more important in predicting a particular class.", "Along with these, the submodular pick also(in fact this is the main purpose of the module) a set of n data points from the dataset, which best explains the model. We can look at it like a representative sample of the different cases in the dataset. So if we need to explain a few cases from the model to someone, this gives you those cases which will cover the most ground.", "From the looks of it, this looks like a very good technique, isn\u2019t it? But it is not without its problems.", "The biggest problem here is the correct definition of the neighbourhood, especially in tabular data. For images and text, it is more straightforward. Since the authors of the paper left kernel width as a hyperparameter, choosing the right one is left to the user. But how do you tune the parameter when you don\u2019t have a ground truth? You\u2019ll just have to try different widths, look at the explanations, and see if it makes sense. Tweak them again. But at what point are we crossing the line into tweaking the parameters to get the explanations we want?", "Another main problem is similar to the problem we have with permutation importance( Part II). When sampling for the points in the locality, the current implementation of LIME uses a gaussian distribution, which ignores the relationship between the features. This can create the same \u2018unlikely\u2019 data points on which the explanation is learnt.", "And finally, the choice of a linear interpretable model for local explanations may not hold true for all the cases. If the decision boundary is too non-linear, the linear model might not explain it well(local fidelity may be high).", "Before we discuss how Shapely Values can be used for machine learning model explanation, let\u2019s try to understand what they are. And for that, we have to take a brief detour into Game Theory.", "Game Theory is one of the most fascinating branches of mathematics which deals with mathematical models of strategic interaction among rational decision-makers. When we say game, we do not mean just chess or, for that matter, monopoly. Game can be generalized into any situation where two or more players/parties are involved in a decision or series of decisions to better their position. When you look at it that way, it\u2019s application extends to war strategies, economic strategies, poker game, pricing strategies, negotiations and contracts, the list is endless.", "But since our topic of focus is not Game Theory, we will just go over some major terms so that you\u2019ll be able to follow the discussion. The parties participating in a Game are called Players. The different actions these players can take are called choices. If there are a finite set of choices for each player, there are also a finite set of combinations of choices of each player. So if each player plays a choice, it will result in an outcome and if we quantify those outcomes, it\u2019s called a payoff. And if we list all the combinations and the payoffs associated with it, it\u2019s called payoff matrix.", "There are two paradigms in Game Theory \u2014 Non-cooperative, Cooperative games. And Shapely values is an important concept in cooperative games. Let\u2019s try to understand through an example.", "Alice, Bob, and Celine share a meal. The bill came to be 90, but they didn\u2019t want to go dutch. So to figure out what they each owe, they went to the same restaurant multiple times in different combinations and recorded how much the bill was.", "Now with this information, we do a small mental experiment. Suppose A goes to the restaurant, then B shows up and C shows up. So, for each person who joins, we can have the extra cash(marginal contribution) each person has to put in. We start with 80 (which is what A would have paid if he ate alone). Now when B joined, we look at the payoff when A and B ate together \u2014 also, 80. So the additional contribution B brought to the coalition is 0. And when C joined, the total payoff is 90. That makes the marginal contribution of C 10. So, the contribution when A, B, C joined in that order is (80,0,10). Now we repeat this experiment for all combinations of the three friends.", "Now that we have all possible orders of arriving, we have the marginal contributions of all the players in all situations. And the expected marginal contribution of each player is the average of their marginal contribution across all combinations. For eg. the marginal contribution of A would be, (80+80+56+16+5+70)/6 = 51.17. And if we calculate the expected marginal contributions of each of the player and add them together, we will get 90- which is the total payoff if all three ate together.", "You must be wondering what all these has to do with machine learning and interpretability. A lot. If we think about it, a machine learning prediction is like a game, where the different features( players), play together to bring an outcome( prediction). And since the features work together, with interactions between them, to make the prediction, this becomes a case of cooperative games. This is right up the alley of Shapely Values.", "But there is just one problem. Calculating all possible coalitions and their outcomes quickly becomes infeasible as the features increase. Therefore, in 2013, Erik \u0160trumbelj et al. proposed an approximation using Monte-Carlo sampling. And in this construct, the payoff is modelled as the difference in predictions of different Monte-Carlo samples from the mean prediction.", "where f is the blackbox machine learning model we are trying to explain, x is the instance we are trying to explain, j is the feature for which we are trying to find the expected marginal contribution, x\u1d50\u208b\u2c7c and x\u1d50\u208a\u2c7c are two instances of x which we have permuted randomly by sampling another point from the dataset itself, and M is the number of samples we draw from the training set.", "Let\u2019s look at a few desirable mathematical properties of Shapely values which makes it very desirable in interpretability application. Shapely Values is the only attribution method which satisfies the properties Efficiency, Symmetry, Dummy, and Additivity. And satisfying these together is considered to be the definition of a fair payout.", "While all of the properties make this a desirable way of feature attribution, one, in particular, has a far-reaching effect \u2014 Additivity. This means that for an ensemble model like a RandomForest or Gradient Boosting, this property guarantees that if we calculate the Shapely Values of the features for each tree individually and average them, you\u2019ll get the Shapely values for the ensemble. This property can be extended to other ensemble techniques like model stacking or model averaging as well.", "We will not be reviewing the algorithm and see the implementation of Shapely Values for two reasons:", "SHAP (SHapely Additive exPlanations) puts forward a unified approach to interpreting Model Predictions. Scott Lundberg et.al proposes a framework which unifies six previously existing feature attribution methods (including LIME and DeepLIFT) and they present their framework as an additive feature attribution model.", "They show that each of these methods can be formulated as the equation above and the Shapely Values can be calculated easily, which bring with it a few guarantees. Even though the paper mentions slightly different properties than the Shapely Values, in principle they are the same. This provides a strong theoretical foundation to the techniques(like LIME) once adapted to this framework of estimating the Shapely values. In the paper, the authors have proposed a novel model-agnostic way of approximating the Shapely Values called Kernel SHAP(LIME + Shapely Values) and some model-specific methods like DeepSHAP(which is the adaptation of DeepLIFT, a method for estimating feature importance for neural networks). In addition to it, they have also shown that for linear models, the Shapely Values can be approximated directly from the model\u2019s weight coefficients, if we assume feature independence. And in 2018, Scott Lundberg et.al[6] proposed another extension to the framework which accurately calculates the Shapely values of tree based ensembles like RandomForest or Gradient Boosting.", "Even though it\u2019s not super intuitive from the equation below, LIME is also an additive feature attribution method. And for an additive feature explanation method, Scott Lundberg et.al showed that the only solution that satisfies the desired properties is the Shapely Values. And that solution depends on the Loss function L, weighting kernel \u03c0\u2093 and regularization term \u03a9.", "If you remember, when we discussed LIME, I mentioned that one of the disadvantages is that it left the kernel function and kernel distance as hyperparameters and they are chosen using a heuristic. Kernel SHAP does away with that uncertainty by proposing a Shapely Kernel and a corresponding loss function which makes sure the solution to the equation above will result in Shapely values and enjoys the mathematical guarantees along with it.", "Tree SHAP, as mentioned before[6], is a fast algorithm which computes the exact Shapely Values for decision tree based models. In comparison, Kernel SHAP only approximates the Shapely values and is much more expensive to compute.", "Let\u2019s try to get some intuition of how it is calculated, without going into a lot of mathematics(Those of you who are mathematically inclined, the paper is linked in the references, Have a blast!).", "We will talk about how the algorithm works for a single tree first. If you remember the discussion about Shapely values, you will remember that to accurately calculate we need the predictions conditioned on all subsets of feature vector of an instance. So let the feature vector of the instance we are trying to explain be x and the subset of feature for which we need the expected prediction be S.", "Below is an artificial Decision Tree which uses just three features, age, hours_worked, and marital_status to make the prediction about earning potential.", "And now that you have the expected predictions of all subsets in one Decision Tree, you repeat that for all the trees in an ensemble. And remember the additivity property of Shapely values? It lets you aggregate them across the trees in an ensemble by calculating an average of Shapely values across all the trees.", "But, now the problem is that these expected values have to be calculated for all possible subsets of features in all the trees and for all features. The authors of the paper proposed an algorithm, where we are able to push all possible subsets of features down the tree at the same time. The algorithm is quite complicated and I refer you to the paper linked in references to know the details.", "We will only be looking at TreeSHAP in this section for two reasons:", "These lines of code calculate the Shapely values. Even though the algorithm is fast, this will still take some time.", "Now let\u2019s look at individual explanations. We will take the same cases as LIME. There are multiple ways of plotting the individual explanations in SHAP library \u2014 Force Plots and Decision Plots. Both are very intuitive to understand the different features playing together to arrive at the prediction. If the number of features is too large, Decision Plots hold a slight advantage in interpreting.", "Now, we will check the second example.", "The SHAP library also provides with easy ways to aggregate and plot the Shapely values for a set of points(in our case the test set) to have a global explanation for the model.", "As always, there are disadvantages that we should be aware of to effectively use the technique. If you were following along to find the perfect technique for explainability, I\u2019m sorry to disappoint you. Nothing in life is perfect. So, let\u2019s dive into the shortcomings.", "Some of the techniques we discussed are also applicable to Text and Image data. Although we will not be going in deep, I\u2019ll link to some notebooks which shows you how to do it.", "LIME ON TEXT DATA \u2014 MULTI-LABEL CLASSIFICATION", "LIME ON IMAGE CLASSIFICATION \u2014 INCEPTION_V3 \u2014 KERAS", "GRADIENT EXPLAINER \u2014 SHAP \u2014 INTERMEDIATE LAYER IN VGG16 IN IMAGENET", "We have come to the end of our journey through the world of explainability. Explainability and Interpretability are catalysts for business adoption of Machine Learning(including Deep Learning) and the onus is on us practitioners to make sure these aspects get addressed with reasonable effectiveness. It\u2019ll be a long time before humans trust machines blindly and till then we will have to supplant the excellent performance with some kind of explainability to develop trust.", "If this series of blog posts enabled you to answer at least one question about your model I consider my endeavor a success.", "Full Code is available in my Github", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F35ecfd763237&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----35ecfd763237--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@manujosephv?source=post_page-----35ecfd763237--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5----35ecfd763237---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/interpretability-cracking-open-the-black-box-part-iii-35ecfd763237?source=friends_link&sk=feefa4d113a26383994c93ce3db41f41", "anchor_text": "here"}, {"url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "anchor_text": "looked at the pitfalls"}, {"url": "https://github.com/thomasp85/lime", "anchor_text": "R"}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Python"}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Github"}, {"url": "https://pypi.org/project/lime/", "anchor_text": "installable package"}, {"url": "https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html", "anchor_text": "forward selection"}, {"url": "https://plot.ly/~manujosephv/43/", "anchor_text": "Click for full interactive plot"}, {"url": "https://plot.ly/~manujosephv/49/", "anchor_text": "Click for full interactive plot"}, {"url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "anchor_text": "Part II"}, {"url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "anchor_text": "last blog post"}, {"url": "https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html", "anchor_text": "LIME ON TEXT DATA \u2014 MULTI-LABEL CLASSIFICATION"}, {"url": "https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb", "anchor_text": "LIME ON IMAGE CLASSIFICATION \u2014 INCEPTION_V3 \u2014 KERAS"}, {"url": "https://github.com/slundberg/shap/blob/master/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.ipynb", "anchor_text": "DEEP EXPLAINER \u2014 SHAP -MNIST"}, {"url": "https://github.com/slundberg/shap/blob/master/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.ipynb", "anchor_text": "GRADIENT EXPLAINER \u2014 SHAP \u2014 INTERMEDIATE LAYER IN VGG16 IN IMAGENET"}, {"url": "https://github.com/manujosephv/interpretability_blog", "anchor_text": "Github"}, {"url": "https://link.medium.com/M0YZX7zbA1", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/interpretability-cracking-open-the-black-box-part-ii-e3f932b03a56", "anchor_text": "Part II"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "Interpretable Machine Learning: A Guide for making black box models explainable"}, {"url": "https://arxiv.org/abs/1602.04938", "anchor_text": "arXiv:1602.04938"}, {"url": "https://www.semanticscholar.org/paper/Explaining-prediction-models-and-individual-with-%C5%A0trumbelj-Kononenko/8fd17bf36bc22477bb2237c2be6e3212b753969d", "anchor_text": "Explaining prediction models and individual predictions with feature contributions"}, {"url": "https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "\u201cA unified approach to interpreting model predictions.\u201d"}, {"url": "https://arxiv.org/abs/1802.03888", "anchor_text": "Consistent individualized feature attribution for tree ensembles"}, {"url": "https://deep-and-shallow.com/2019/11/24/interpretability-cracking-open-the-black-box-part-iii/", "anchor_text": "http://deep-and-shallow.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----35ecfd763237---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/xai?source=post_page-----35ecfd763237---------------xai-----------------", "anchor_text": "Xai"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----35ecfd763237---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----35ecfd763237---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----35ecfd763237---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----35ecfd763237---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F35ecfd763237&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----35ecfd763237---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----35ecfd763237--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----35ecfd763237--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----35ecfd763237--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----35ecfd763237--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----35ecfd763237--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/@manujosephv/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "183 Followers"}, {"url": "https://www.linkedin.com/in/manujosephv/", "anchor_text": "https://www.linkedin.com/in/manujosephv/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7cea8b947fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-cracking-open-the-black-box-part-iii-35ecfd763237&newsletterV3=c8dcc7fb5ce5&newsletterV3Id=7cea8b947fdd&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}