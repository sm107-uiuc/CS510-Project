{"url": "https://towardsdatascience.com/listening-to-the-pixels-6d929b76eeb7", "time": 1683013525.791913, "path": "towardsdatascience.com/listening-to-the-pixels-6d929b76eeb7/", "webpage": {"metadata": {"title": "Listening to the Pixels. Self-supervised Deep Learning for\u2026 | by Rishab Sharma | Towards Data Science", "h1": "Listening to the Pixels", "description": "Human perception is multidimensional and a balanced combination of hearing, vision, smell, touch, and taste. Recently, many pieces of research have tried to step forward on the road of improving\u2026"}, "outgoing_paragraph_urls": [{"url": "https://neptune.ai/blog/self-supervised-learning", "anchor_text": "Neptune.ai blog", "paragraph_index": 27}, {"url": "http://person8.ai", "anchor_text": "person8.ai", "paragraph_index": 28}, {"url": "http://rishab.co", "anchor_text": "rishab.co", "paragraph_index": 28}], "all_paragraphs": ["Human perception is multidimensional and a balanced combination of hearing, vision, smell, touch, and taste. Recently, many pieces of research have tried to step forward on the road of improving machine perception by transitioning from single-modality learning to multimodality learning. If you are wondering what modality is, it is the classification of a single independent channel of sensory input/output between a computer and a human (like vision is one modality and audio is another). In this blog, we will talk about the use of audio and visual information (representing the two most important perceptual modalities in our daily life) to make our machine perception smarter without using any labeled data (self-supervision).", "When you hear a person\u2019s voice you know, can you recall their face? Or can you recall a person\u2019s voice on seeing their face? This shows how humans can \u2018hear faces\u2019 and \u2018see voices\u2019 by cultivating a mental picture or an acoustic memory of the person. The problem is, can you teach a machine to do that?", "Our world generates a rich source of auditory and visual signals. The visual signals are a result of light reflections, whereas the sounds originate from object motions and vibrations of the surrounding air. Often correlated at the time of naturally occurring events, these two modalities combine to jointly affect human perception. In response to this perceptual input, humans show a remarkable ability to connect and integrate signals from these two modalities. As a matter of fact, the interplay among senses is one of the most ancient schemes that explains how the human brain\u2019s sensory organization works to understand the complex interactions of the physical dimension. Inspired by our capability of interpreting sound sources from how objects move visually, we can create learning-models that learn to perform this interpretation on its own.", "While auditory scene analysis is majorly studied in the fields of environmental sound source separation and recognition, the natural synchronization between sound and vision can provide a rich self-supervisory signal for grounding auditory signals into the visual signals, which is all we need for self-supervision to show it\u2019s magic.", "In this blog, we will learn how to leverage this cross-modal context as a self-supervisory signal to extract information beyond the limits established by individual modalities. We will acknowledge the importance of temporal features that are based on significant changes in each modality and design a probabilistic formalism that can identify temporal coincidences between these features to yield visual localization and cross-modal association.", "The most intuitive solution which will come to our mind is to design a probabilistic formalism that can exploit the inherent coherence of audio-visual signals from large quantities of unlabelled videos to learn sound localization and separation. This can be done by making a computational model that can learn the relationship between visuals and sounds in an unsupervised way by recognizing objects from the sounds they make, to localize them in images, and to separate the audio component coming from each object. With such inspiration in mind, many researchers have developed models that can effectively do sound localization and sound recognition. We will also work our way to one such solution that can do sound source separation and its visual localization by distinguishing the components of sound and their association with the corresponding objects. The solution we will work on is two-fold. First, we will use a simple architecture that will rely on static visual information to learn the cross-modal context. Next, we will take a step further to include the motion cues of the video into our solution. The motion signals are of crucial importance for learning the audio-visual correspondences. This fact can be more clearly understood by taking a simple case of sound production from two similar-looking objects. Consider a case of two artists playing violin duet. This case constructs an impossible situation for humans to separate the melody from harmony by analyzing the single picture. However, if we observe the movements of the artists for a while and try to match these motion cues with the musical beats, we can probably conjecture according to this motion-beat observation. This case illustrates the importance of the temporal repetition of the motion for solving the complex multi-modal reasoning of sound source separation even for humans. Our aim is to computationally mimic this ability to reason the synergy between audio, visual, and motion signals.", "Computational models of this relationship can be utilized as a fundamental unit for many applications like combining videos with automatically generated ambient sound for better immersion in VR or for enabling equal accessibility by linking sound with visual signals for visually impaired people.", "For our initial approach, we will construct a three-component network as suggested in [1] for processing video frames and audio signals separately, followed by their features\u2019 combined processing in an audio synthesizer network.", "The first component, the Video Analysis Network (VAN) takes the video frames as input and extracts the appearance features. For the feature extraction part, we will use a dilated Resnet-18 model with an input size of TxHxWx3, and an output stride of 16 followed by a temporal max-pooling layer to output a K channel feature map. In the code snippet below, you can find the PyTorch code for a VAN.", "The second component, the Audio Analysis Network (AAN) takes the sound mixture as input and applies the Short-Time Fourier Transform (STFT) with a log-frequency scale to obtain a sound spectrogram. Then the obtained spectrogram is fed to a U-Net that yields K feature maps representing different components of input audio mixture. In the code snippet below, you can find the PyTorch code for an AAN.", "The third component, the Audio Synthesizer Network (ASN) takes the extracted pixel-level appearance features and audio features as input and predicts a vision-based spectrogram binary mask. The number of predicted binary masks depends on the number of sound sources to separate in the input mixture. These binary masks are then multiplied with the input spectrogram to separate each sound component, followed by a magnitude adjustment of the prediction with the phase of the input to get the final waveform. The final waveform is then processed with an inverse STFT to retrieve the final audio components. In the code snippet below, you can find the PyTorch code for an ASN.", "Now as I mentioned earlier, this solution might not be enough for separating sound coming from visually similar objects, as the appearance features may get fooled during the synthesizer phase. Therefore we would need another network for analyzing the motion of the sound-producing objects. This additional module is proposed by Zhao et. al [2].", "The fourth component, the Motion Analysis Network (MAN) takes the video frames as input and predicts a dense trajectory feature map in three major steps. In the first step, we can use a dense optical flow estimator like PWC-Net (lightweight design and fast speed) to extract the dense optical flow vectors for the input frames. In the next step, the network will use the extracted dense optical flow to predict the dense trajectories. To understand this in basic terms, let's assume a pixel\u2019s spatial location to be I_t = (x_t,y_t) and the dense optical flow to be \u03c9_t = (u_t, v_t) at a time \u201ct\u201d. Then for time \u201ct+1\u201d, the estimated position will be I_t+1 = (x_t+1, y_t+1) = (x_t, y_t) + \u03c9|(x_t, y_t). The concatenation of these estimated coordinates (I_t, I_t+1, I_t+2\u2026) is the full trajectory for a pixel. In the third step, the estimated dense trajectories are fed to a CNN model to extract the deep features of these trajectories. The choice of CNN is not fixed and can be arbitrary. Zhao et. al [] proposes to use an I3D model, which is well known for capturing spatiotemporal features. I3D has a compact design that inflates 2D CNN into 3D to bootstrap 3D filters from pre-trained 2D filters.", "The question that still remains unanswered is how to incorporate these trajectory features in our initial model framework. To do so, first, we have to fuse these features with the appearance features that were generated as a part of the first component (VAN). A simple way to do this fusion is to extract an attention map from the appearance features convoluting them to a single channel and activating them the values with a sigmoid function to get a spatial attention map. This attention map can then be multiplied with the trajectory features to focus only on important trajectories, followed by the concatenation of both appearance and trajectory features. After this step, either we can use these features directly in place of the old appearance features or we can do an alignment of the visual and sound features in time by applying Feature-wise Linear Modulation (FiLM) on sound features and use fuse them to act as an input to the Audio-U-Net decoder (as suggested by Zhao et. al). In the second case (using FILM) we would no longer need the audio synthesizer network and we can rewrite the U-Net decoder to directly predict the binary masks.", "In this blog section, we will discuss two major training frameworks that are necessary for training a model to learn the cross-modal context in a self-supervised way.", "The mix and separate training framework is a procedure that artificially creates a complex auditory scene for the model under training. MSF enforces the model to analyze some randomly generated complex auditory scenes and frames a situation for it to separate and ground the mixed sounds. The generated data is not directly available in the training data, thus MSF creates an auto data augmentation situation. MSF leverages the fact that audio signals are additive and thus we can mix sounds from different video samples to generate a complex auditory signal for the model input. On the other hand, this framework also creates a self-supervised learning objective for the model. The objective is to separate and restore the sounds back to their original waveform that was intact to each source before the addition by using the visual input associated with the sound mixture. For the mix and separate framework, we randomly sample N video clips from the training set and in a simple case, mix the sound components of any two of them and serve the model with audio mixture input and their respective frames. It is important to note that although the framed training targets are clear in the training process, the process still is unsupervised as we do not use any data labels and data sampling assumptions.", "By definition, curriculum learning is a type of learning in which the training samples start out with only easy examples of a task and then gradually increase the task difficulty. CL is a kind of smart sampling technique that can replace the random sampling nature of the MSF. Inspired by the observation that models trained on a single class of instruments suffer from overfitting due to class imbalance, we can use a multi-stage training curriculum that can start by sampling easily separable sound sources. Such kind of curriculum will help in bootstrapping the model with good weight initialization for better convergence on the difficult tasks.", "Note: The learning targets (spectrogram masks) can be both binary and ratios. In the case of binary nature masks, we would use a per-pixel sigmoid cross-entropy loss. Otherwise, in the case of ratio nature masks, we would use a per-pixel L1 loss for training. Also due to possible interference, the values of the ground truth mask do not necessarily stay within the range [0, 1].", "In deep learning applications, we often tend to rely on the network to learn the mathematical model on its own but if we peek under the hood, we will observe numerous interesting mathematical facts.", "In the case of a cross-modal association, we assume that each modality will generate a significant event (onset). If the generated onset coincides in time repeatedly (movement of the guitar strings make a sound), then they are assumed to be co-related. In mathematical terms, we can say that if the coincidences are more, the likelihood of cross-modal correspondence is also more. On the other hand, if the onset coincidences are low, so is the cross-modal correspondence likelihood.", "To understand the process as a likelihood matching algorithm, we must assume that all the onsets of each modality are independent and mutually exclusive. Let us consider the video onset to be a binary with notation V_on and the audio onset binary to be A_on (I am using binary values, just for the sake of explanation). Now if we pre-train our network on an optimization function (likelihood function) of nature,", "that increases as the coincidences increase, we can explain the likelihood maximization for the cross-modal association better. Assuming that the onsets are random variables that are statically independent of each other and follow the probability law, we can say that L = \u220f(P^(onset_match) \u2715 (1-P)^(onset_mismatch)) or for an instance L(i) = P^(onset_match) \u2715 (1-P)^(onset_mismatch). Now we can take a log and rewrite it as:", "Finally, we can also state the onset_match when both are V_on and A_on are either {1, 1}, or {0,0}. Thus showing that onset_match = V_on \u2715 A_on + (1-V_on)\u2715(1-A_on). Therefore we can finally state that, when our network optimizes for cross-modal correspondence modeling, it will indirectly be equivalent to the matching likelihood of features from the cross-modal sources.", "Note: Due to the limitation of expressing complex mathematical equations in the blog paragraphs, I have simplified the notations to be easily formattable in paragraph format. \u201c^\u201d stands for power, \u201cT\u201d for matrix transpose, and \u201cI\u201d for the identity matrix.", "In this blog, we discussed how we can make a system that can learn from unlabeled videos to separate auditory signals and also locate them in the visual input. We started with a simple architecture and showed how the initial system can be enhanced to model the cross-modal context more accurately even when the sound sources are visually similar. In the end, I would conclude on a note that the desire to understand the world from the human perspective has drawn the attention of the deep learning community on the topic of audio-visual learning, and these type of learning will not only help in solving many existing problems but will also lay the foundations for the future development of self-supervised learning and it\u2019s applications on real-world problems.", "My blogs are a reflection of what I worked on and simply convey my understanding of these topics. My interpretation of deep learning can be different from that of yours, but my interpretation can only be as inerrant as I am.", "[1] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. The European Conference on Computer Vision (ECCV), September 2018.", "[5] If you want to learn more about self-supervised learning, deep dive into the topic on the Neptune.ai blog", "Co-Founder person8.ai and Visual Computing Scientist rishab.co"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6d929b76eeb7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@kraken2309?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Rishab Sharma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee7035809dea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=post_page-ee7035809dea----6d929b76eeb7---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d929b76eeb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=-----6d929b76eeb7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d929b76eeb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=-----6d929b76eeb7---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.pexels.com/@reyna?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Roy Reyna"}, {"url": "https://www.pexels.com/photo/man-lying-down-on-ground-while-playing-guitar-1835686/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://www.semanticscholar.org/paper/Deep-Audio-Visual-Learning%3A-A-Survey-Zhu-Luo/7cabfa7362ebb57c77380caa57aa17fd7195605c", "anchor_text": "source"}, {"url": "https://www.semanticscholar.org/paper/The-Sound-of-Motions-Zhao-Gan/c880de441a41c351955ad0bf8f712eeee500ac67", "anchor_text": "source"}, {"url": "https://www.semanticscholar.org/paper/The-Sound-of-Pixels-Zhao-Gan/fe018f22600d07cbd0452a070e03708886470015", "anchor_text": "source"}, {"url": "https://www.semanticscholar.org/paper/The-Sound-of-Motions-Zhao-Gan/c880de441a41c351955ad0bf8f712eeee500ac67", "anchor_text": "source"}, {"url": "https://www.semanticscholar.org/paper/The-Sound-of-Pixels-Zhao-Gan/fe018f22600d07cbd0452a070e03708886470015", "anchor_text": "source"}, {"url": "https://media.giphy.com/media/fxIk8gAC4fHir5s8Su/giphy.gif", "anchor_text": "source"}, {"url": "https://www.semanticscholar.org/paper/The-Sound-of-Motions-Zhao-Gan/c880de441a41c351955ad0bf8f712eeee500ac67", "anchor_text": "source"}, {"url": "https://neptune.ai/blog/self-supervised-learning", "anchor_text": "Neptune.ai blog"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6d929b76eeb7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6d929b76eeb7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6d929b76eeb7---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/self-supervised-learning?source=post_page-----6d929b76eeb7---------------self_supervised_learning-----------------", "anchor_text": "Self Supervised Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----6d929b76eeb7---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d929b76eeb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=-----6d929b76eeb7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d929b76eeb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=-----6d929b76eeb7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d929b76eeb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee7035809dea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=post_page-ee7035809dea----6d929b76eeb7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F357292164ee7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&newsletterV3=ee7035809dea&newsletterV3Id=357292164ee7&user=Rishab+Sharma&userId=ee7035809dea&source=-----6d929b76eeb7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Written by Rishab Sharma"}, {"url": "https://medium.com/@kraken2309/followers?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "122 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://person8.ai", "anchor_text": "person8.ai"}, {"url": "http://rishab.co", "anchor_text": "rishab.co"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee7035809dea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&user=Rishab+Sharma&userId=ee7035809dea&source=post_page-ee7035809dea----6d929b76eeb7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F357292164ee7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flistening-to-the-pixels-6d929b76eeb7&newsletterV3=ee7035809dea&newsletterV3Id=357292164ee7&user=Rishab+Sharma&userId=ee7035809dea&source=-----6d929b76eeb7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-for-3d-synthesis-2dd57e2001f?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Rishab Sharma"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-learning-for-3d-synthesis-2dd57e2001f?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Deep Learning for 3D SynthesisTransformers vs Convolutional Deep Networks for synthesizing 3D Data"}, {"url": "https://towardsdatascience.com/deep-learning-for-3d-synthesis-2dd57e2001f?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "12 min read\u00b7Jan 14, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2dd57e2001f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-3d-synthesis-2dd57e2001f&user=Rishab+Sharma&userId=ee7035809dea&source=-----2dd57e2001f----0-----------------clap_footer----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-for-3d-synthesis-2dd57e2001f?source=author_recirc-----6d929b76eeb7----0---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2dd57e2001f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-3d-synthesis-2dd57e2001f&source=-----6d929b76eeb7----0-----------------bookmark_preview----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6d929b76eeb7----1---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----6d929b76eeb7----1-----------------bookmark_preview----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6d929b76eeb7----2---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----6d929b76eeb7----2-----------------bookmark_preview----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neural-hallucinations-13c645e2fd23?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Rishab Sharma"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/neural-hallucinations-13c645e2fd23?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "Neural HallucinationsHow Neural Networks hallucinate missing pixels for Image Inpainting"}, {"url": "https://towardsdatascience.com/neural-hallucinations-13c645e2fd23?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": "12 min read\u00b7Oct 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13c645e2fd23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-hallucinations-13c645e2fd23&user=Rishab+Sharma&userId=ee7035809dea&source=-----13c645e2fd23----3-----------------clap_footer----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neural-hallucinations-13c645e2fd23?source=author_recirc-----6d929b76eeb7----3---------------------c9a28982_73e0_4d3a_b34a_6ece8d968e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13c645e2fd23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-hallucinations-13c645e2fd23&source=-----6d929b76eeb7----3-----------------bookmark_preview----c9a28982_73e0_4d3a_b34a_6ece8d968e90-------", "anchor_text": ""}, {"url": "https://medium.com/@kraken2309?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "See all from Rishab Sharma"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----6d929b76eeb7----0-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----6d929b76eeb7----1-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6d929b76eeb7----0---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----6d929b76eeb7----0-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6d929b76eeb7----1---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----6d929b76eeb7----1-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----2-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----6d929b76eeb7----2---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----6d929b76eeb7----2-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "Understanding NeRFsA massive breakthrough in scene representation"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": "\u00b711 min read\u00b73 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----2a082e13c6eb----3-----------------clap_footer----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----6d929b76eeb7----3---------------------47c1288e_1368_4298_89e8_d08141a1d74b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----6d929b76eeb7----3-----------------bookmark_preview----47c1288e_1368_4298_89e8_d08141a1d74b-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----6d929b76eeb7--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}