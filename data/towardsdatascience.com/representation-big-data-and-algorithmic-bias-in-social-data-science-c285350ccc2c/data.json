{"url": "https://towardsdatascience.com/representation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c", "time": 1683017292.7998059, "path": "towardsdatascience.com/representation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c/", "webpage": {"metadata": {"title": "Representation and Bias. The buzz-phrase of our time\u2026 | by Parvathi A. Subbiah | Towards Data Science", "h1": "Representation and Bias", "description": "The buzz-phrase of our time, \u201cArtificial intelligence,\u201d has inspired all sorts of musings: from visions of utopic \u2014 and dystopic \u2014 futures freed from human labour, to more focused critiques on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.technologyreview.com/2020/07/31/1005824/decolonial-ai-for-everyone/", "anchor_text": "MIT Technological Review", "paragraph_index": 4}, {"url": "https://www.vox.com/recode/2019/10/17/20917285/google-pixel-4-facial-recognition-tech-black-people-reset-podcast", "anchor_text": "gathering consent for", "paragraph_index": 5}, {"url": "https://cdt.org/ai-machine-learning/", "anchor_text": "Center for Democracy and Technology", "paragraph_index": 6}, {"url": "https://doi.org/10.1080/1369118X.2012.678878", "anchor_text": "boyd and Crawford", "paragraph_index": 9}, {"url": "https://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html", "anchor_text": "According to Dan Hurley", "paragraph_index": 13}, {"url": "http://chronomics.com", "anchor_text": "Chronomics", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/2007.04068.pdf", "anchor_text": "shows", "paragraph_index": 17}, {"url": "https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency", "anchor_text": "as Rebeca Heilweil suggests", "paragraph_index": 20}, {"url": "https://cdt.org/area-of-focus/privacy-data/ai-machine-learning/", "anchor_text": "As the Centre for Democracy and Technology notes", "paragraph_index": 22}], "all_paragraphs": ["The buzz-phrase of our time, \u201cArtificial intelligence,\u201d has inspired all sorts of musings: from visions of utopic \u2014 and dystopic \u2014 futures freed from human labour, to more focused critiques on algorithmic discrimination. The reality is the \u2018good, bad and the ugly\u2019 of AI drives some of the most fundamental philosophical and societal questions we face today \u2014 and consequently much of our evolving tech legislation and policy.", "Algorithms have become ever faster at processing unfathomable amounts of information, as computing power continues to increase. Algorithms have taken over such diverse aspects of our lives it is hard to keep track of the decision-making and \u2018intelligent\u2019 technologies they enable: search engine results, ad content, creditworthiness, recommendation for movies we might like, eligibility for a job, Uber routes and drivers, genetic predispositions \u2014 they even help doctors make diagnoses and compose music in a particular style. The list goes on.", "A lot has been written about the opportunity-cost of our now data-centric world, yet we are all still largely oblivious to when and how these algorithms are actually employed. Either their functioning is proprietary to corporations; or \u2014 and this is especially true for very complex decision-making algorithms like neural networks or deep learning algorithms \u2014 even the developers themselves cannot explain why certain inputs yield certain outcomes. Under the guise of being \u2018objective\u2019, \u2018neutral,\u2019 or at least free from the quandaries of human bias, algorithms have become an ever-present aspect of our reality, many times for good reason. Other many times, less so.", "Most of the data that we generate from our digital interactions, which is then used to predict aspects of our behaviour, is used explicitly to expand profit margins: the Cambridge Analytica \u2014 Facebook scandal, is an obvious case in point. Horror stories related to algorithms abound: r\u00e9sum\u00e9 screening tools that learned to be sexist (having been fed a history of male-dominant r\u00e9sum\u00e9s); facial recognition technologies that are unacceptably bad at recognising people with dark-skin; even an app that claims it can detect terrorists or paedophiles based on their faces, Faception. Other, wildly less harmful stories, include algorithms that have persistently shown tampon ads to men on YouTube.", "We are, perhaps blissfully, unaware of the extent to which algorithms can curtail some of our civil and political rights by exacerbating the already fraught inequalities built into our societies and institutions. Algorithms, especially machine learning algorithms (where a system \u2018learns\u2019 to identify a number of patterns to make predictions based on a training dataset), tend to replicate the racial injustices of the society they are trying to predict for, as Hao argues for the MIT Technological Review.", "Algorithms in and of themselves are not inherently unfair, of course; they follow code and are statistical-based tools. The bias stems from the datasets \u2014 collated by human hands \u2014 on which those algorithms are trained. These are often flawed and of low-quality, i.e. incomplete, skewed or unrepresentative of the population they are trying to make predictions for. Even more comprehensive datasets can be insufficient to address all the issues: they can be collected without recourse to privacy or ethical considerations (as was the case when Google was caught sending teams out to collect facial scans, without being upfront about what they were collecting data for, or what they were gathering consent for).", "Researchers at NYU and the AI Now Institute point out that predictive-analytics algorithms can also be fed what they call \u201cdirty data:\u201d \u2018robust\u2019 data collated from the historical practices of the criminal and/or policing systems, but which have been at many times notoriously egregious, racially-unjust, and violating of civil rights. The researchers specifically examine predictive policing systems in thirteen jurisdictions in the US, put in place either to allocate police resources or to try to forecast criminal activity. They conclude that any attempt to deploy these automated systems must be addressed cautiously, and more importantly, alongside mechanisms that allow the public to know about, critique and reject them if necessary. As the Center for Democracy and Technology points out, algorithms that try to predict violent crime have very low accuracy rates anyway: the probability that a violent crime will occur is statistically low, and the data from which the algorithm can learn represents a very small proportion of the population.", "We are then clearly faced with a question of representation. Who and what do these \u2018Big Data\u2019 sets stand for? What are they used for? What does it mean when we make, what are in the end, statistical assumptions about individuals to predict a recidivism score, or whether they should be hired for a job? Especially if these decisions are made by algorithms we do not fully understand, or not open to scrutiny? What we are asking, in the end, are also fundamental questions about ethics, equality and therefore also democracy.", "Who gets to ask the important questions?", "In an article published in the journal Information, Communication and Society, danah boyd and Kate Crawford, called for a critical examination of \u2018Big Data\u2019 approaches to social research \u2014 and their assumptions \u2014 back in 2012. As they pointedly note, algorithms and ideas about \u2018Big Data\u2019 have changed the ways we think about what research entails, even what counts as knowledge; they represent what Leslie Burkholder (back in 1992) called a computational turn in thinking and research more broadly. boyd and Crawford define the idea of \u2018Big Data\u2019 as a \u201ccultural, technological and scholarly phenomenon,\u201d that partly rests on the myth that large amounts of data offer a \u201chigher form of intelligence\u201d and knowledge that \u201cwas previously impossible\u201d to obtain.", "They make the case that the fear and hope that surround our ideas about AI\u2019s possibilities, obscure the more subtle, but also more pertinent and practical issues that arise when using these methods in social science research. They note that there is poor access to historical data on Twitter and Facebook: research questions, therefore, can and are becoming limited to the present, or at least the immediate past. Also, that only the social media companies have unrestricted access to all of the data. Although some of it can be purchased, the logistics involved in the transaction, reinforces educational inequalities: top-tier universities (mostly located in what we understand to be the Global North) are asking the questions they consider to be important, as only they can afford to do so. They also note that it is also only those trained and versed in computational methods, scraping, and APIs at these elite institutions that seem placed to answer them \u2014 at the moment at least, these scientists have also tended to be male.", "Of course, some algorithms using large datasets have been shown to work efficiently, or at least as intended. There\u2019s an entire field in biology, bioinformatics, dedicated to gaining insights from inordinately large biological datasets using computational and statistical methods. The field has been highly successful at finding associations at the molecular level, including between specific regions of DNA (genes) and predisposition to disease. Scientists have discovered (and genetic testing companies have profited from) one of the main genes associated with the risk of developing breast cancer: BRCA2, and a plethora of other genes associated with specific phenotypes (that can tell you things as mundane as whether you will hate the taste of cilantro).", "We could, no doubt simplistically, conclude that most of the trouble with AI stems from applying it to social science questions that might have no \u2018right\u2019 answers. Where science studies \u2018objects\u2019, the humanities, studies \u2018subjects\u2019, we are often told. The reality is, as always, infinitely more complex.", "The case of Alleghany County in Pittsburgh comes to mind: the first county in the US to implement a predictive-analytics algorithm to offer a \u2018second opinion\u2019 on incoming calls on allegations of child abuse to the police department. The algorithm\u2019s job is to decide whether such a call requires an intervention. According to Dan Hurley, science journalist for The New York Times, it takes into account over 100 \u2018criteria\u2019 or data points from psychiatric services, jails, welfare benefits, drug and alcohol treatments, among many others. It is taxing and time-consuming for a call screener to find even a fraction of this information; the algorithm returns a decision in seconds. The data used to train the algorithm is (expectedly) biased against African-Americans \u2014 Erin Dalton, deputy director of human services at the county tells Dan Hurley for the interview: black children are already being \u201cover-surveilled\u201d in the system. Yet for Dalton and Walter Smith Jr., the county\u2019s Office of Children, Youth and Families deputy director, who is black, the algorithm produces less bias, than human screeners would. It is also used only to decide what to investigate \u2014 not which children to remove from their respective homes.", "Importantly, the Alleghany algorithm, developed by two social scientists, Rhema Vaithianathan and Emily Putnam-Hornstein, is property of the county \u2014 unlike other similar algorithms developed across the US that are trade secret of private companies and cannot be audited. Alleghany\u2019s workings are open to scrutiny by academics and specialists, parents, and even former foster care children at public meetings that were held before its adoption.", "Biology in particular is a science that shares some of the pitfalls involved in collating \u2018Big Data.\u2019 In an interview for this piece, Dr. Daniel Martin Herranz, Chief Scientific Officer at the biotech start-up Chronomics \u2014 a company that uses machine learning algorithms to calculate biological age (vis-a-vis chronological age) from markers in your DNA \u2014 explains one of the least discussed biases in the literature on algorithmic discrimination: that \u201cgenetic datasets are skewed towards Caucasian populations, both because the price of genetic and epigenetic tests means certain groups are less likely to be able to afford them, and because the countries that have compiled wide-scale sequencing of their populations are majority white.\u201d", "Dr. Martin-Herranz tells me that what this has meant is that \u201chealth technologies developed from these data are based on personalised genetic information biased towards white populations.\u201d He adds: \u201cThere are now several projects trying to correct for these biases, that is, that are trying to proactively sequence non-Caucasian genetic backgrounds. But there is still much work to be done.\u201d", "There are, indeed, several ways in which Dr. Martin-Herranz\u2019s example of \u2018coloniality\u2019 \u2014 modern global inequality that reflects an extension of power relations between coloniser and colonised countries \u2014 manifests itself in AI more broadly. Structural racism is just one aspect of this, as a recently published paper written by Shakir Mohamed, a researcher at Google\u2019s Deep Mind and his colleagues William Isaac and Marie-Therese Png, shows. To collate the datasets that power natural language processing algorithms that work with, for instance, Google\u2019s Assistant, there is a laborious manual labelling of the datasets required. This work is actually known as \u2018ghost work\u2019, in that it is rarely human work that is openly acknowledged. Ghost work is mostly done by poorly paid workers \u2014 many times working unpaid overtime \u2014 from former UK and US colonies where English is broadly spoken.", "Latin America, Central Asia and Africa have also largely been left out of the ethical discussions surrounding AI governance, countries in these areas are falling further behind legislation in these areas, and the global north continues to benefit from norms conceived around issues that concern their populations. Hao, for the MIT Technological Review, also reminds us that Cambridge Analytica beta-tested its algorithms on the 2015 Nigerian Elections, before the US and UK elections in 2016. Her example points to what Ruha Benjamin, a sociologist at Princeton, tells us in an episode of the podcast \u2018Reset\u2019: that \u201cracialized groups have been targeted and included in harmful experimentations\u201d throughout history. Benjamin adds: \u201cscientists and doctors have gone after the most vulnerable populations in order to hone various technologies and techniques.\u201d In AI, this has not fundamentally changed. It is one reason why scholars examining issues of algorithmic bias point at the importance of letting those with less resources initiate their own AI projects, as a way to mitigate paternalistic approaches that make assumptions regarding these populations and their needs.", "Clearly, we need to extend awareness of the types of biases that can be implicit in data collection, and the predictions that these data will then yield.", "The answer lies not simply in auditing these systems: as Rebeca Heilweil suggests \u2014 and as the example in my research shows \u2014 you can only correct for the biases you are aware of. There are surely other ideological biases implicit in the NRC lexicon that need to be disinterred. Simply because an algorithm is tested for bias against women, for instance, does not mean it necessarily performs fairly for women of colour or transgender people.", "Understanding these nuances requires knowledge of complex social theory and intersectionality (how interconnected systems of discrimination, for instance, sexuality and race, manifest themselves) \u2014 knowledge that those who work with and design AI systems might need to be more intimately acquainted with. Mohamed, Isaac and Png note that their examples aren\u2019t entirely comprehensive, but what they underline is the overarching advantage that understanding ideas about coloniality and race theory brings to the table. There is clearly a need to collaborate on AI governance and policy in an inter-disciplinary fashion; as much as there is a need to value the insight gained by in-depth contextual \u2018small data\u2019 research in social science, and their contribution to social theory.", "AI in public policy is a mammoth of an ethical conundrum \u2014 without even thinking about the issues of privacy involved (another elephant in the room). As the Centre for Democracy and Technology notes in their entry on Artificial Intelligence and Machine Learning, \u201cwhat matters is that automated systems give users enough information to assess how much trust they should place in its digital decisions.\u201d", "Openness and transparency must become fundamental pillars of AI implementation \u2014 especially where algorithms are handling serious, and potentially detrimental, decisions that pertain to public institutions such as the justice, criminal or police systems. I can critique the NRC lexicon because it is an open academic project, as is the Alleghany algorithm. Ideally, any algorithm that decides on our behalf, or that affects important areas of our life, public or otherwise \u2014 in banking, or to decide over our job eligibility \u2014 should be open to audit and in no way profit from our unawareness. Finally, for all this to fall into place, it is clear we need to continue to make AI democratisation, ethics and governance, an issue of public interest and debate.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD Politics and Sociology | University of Cambridge | Gates Scholar | Social_Data_Science"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc285350ccc2c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://alesubbiah.medium.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": ""}, {"url": "https://alesubbiah.medium.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Parvathi A. Subbiah"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff64a9d9ade50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&user=Parvathi+A.+Subbiah&userId=f64a9d9ade50&source=post_page-f64a9d9ade50----c285350ccc2c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/fairness-and-bias", "anchor_text": "Fairness and Bias"}, {"url": "https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral", "anchor_text": "NASA"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.technologyreview.com/2020/07/31/1005824/decolonial-ai-for-everyone/", "anchor_text": "MIT Technological Review"}, {"url": "https://www.vox.com/recode/2019/10/17/20917285/google-pixel-4-facial-recognition-tech-black-people-reset-podcast", "anchor_text": "gathering consent for"}, {"url": "https://cdt.org/ai-machine-learning/", "anchor_text": "Center for Democracy and Technology"}, {"url": "https://unsplash.com/@emilymorter?utm_source=medium&utm_medium=referral", "anchor_text": "Emily Morter"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://doi.org/10.1080/1369118X.2012.678878", "anchor_text": "boyd and Crawford"}, {"url": "https://unsplash.com/@gmalhotra?utm_source=medium&utm_medium=referral", "anchor_text": "Gayatri Malhotra"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html", "anchor_text": "According to Dan Hurley"}, {"url": "https://unsplash.com/@nci?utm_source=medium&utm_medium=referral", "anchor_text": "National Cancer Institute"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://chronomics.com", "anchor_text": "Chronomics"}, {"url": "https://unsplash.com/@britishlibrary?utm_source=medium&utm_medium=referral", "anchor_text": "British Library"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/2007.04068.pdf", "anchor_text": "shows"}, {"url": "https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral", "anchor_text": "Joshua Sortino"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency", "anchor_text": "as Rebeca Heilweil suggests"}, {"url": "https://cdt.org/area-of-focus/privacy-data/ai-machine-learning/", "anchor_text": "As the Centre for Democracy and Technology notes"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c285350ccc2c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/social-science?source=post_page-----c285350ccc2c---------------social_science-----------------", "anchor_text": "Social Science"}, {"url": "https://medium.com/tag/algorithmic-bias?source=post_page-----c285350ccc2c---------------algorithmic_bias-----------------", "anchor_text": "Algorithmic Bias"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----c285350ccc2c---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/fairness-and-bias?source=post_page-----c285350ccc2c---------------fairness_and_bias-----------------", "anchor_text": "Fairness And Bias"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&user=Parvathi+A.+Subbiah&userId=f64a9d9ade50&source=-----c285350ccc2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&user=Parvathi+A.+Subbiah&userId=f64a9d9ade50&source=-----c285350ccc2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc285350ccc2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c285350ccc2c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c285350ccc2c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c285350ccc2c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c285350ccc2c--------------------------------", "anchor_text": ""}, {"url": "https://alesubbiah.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://alesubbiah.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Parvathi A. Subbiah"}, {"url": "https://alesubbiah.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff64a9d9ade50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&user=Parvathi+A.+Subbiah&userId=f64a9d9ade50&source=post_page-f64a9d9ade50--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff64a9d9ade50%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frepresentation-big-data-and-algorithmic-bias-in-social-data-science-c285350ccc2c&user=Parvathi+A.+Subbiah&userId=f64a9d9ade50&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}