{"url": "https://towardsdatascience.com/a-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1", "time": 1683010425.3067389, "path": "towardsdatascience.com/a-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1/", "webpage": {"metadata": {"title": "A visual exploration of Multi-Armed Bandit experiments | by Paul Stubley | Towards Data Science", "h1": "A visual exploration of Multi-Armed Bandit experiments", "description": "What if we don't have time for an A/B test? How can we explore multiple offers/updates/variants, while minimising the negative impact of poorly performing variants as quickly as possible. Multi-Armed Bandits allow us to shift from exploration to exploitation rapidly, though at the cost of statistical significance."}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/paul-stubley", "anchor_text": "LinkedIn", "paragraph_index": 33}, {"url": "https://github.com/paul-stubley/portfolio/tree/master/multi_armed_bandit", "anchor_text": "GitHub", "paragraph_index": 33}], "all_paragraphs": ["Working in a Marketing Data Science department has one constant: the choice between experimentation and exploitation is never easy. This is especially true when you want to run a limited-time offer, with little pre-existing knowledge about how different versions of the offer will perform.", "Given time, you could run a multi-variate test (an A/B/C/\u2026 test) to determine which is the best version, but by that point it may be too late to benefit from that knowledge.", "Is there a way we can create multiple versions of an offer, but lean more heavily towards the best performing one as the offer runs, showing fewer and fewer people the worse-performing versions? Yes!", "Enter the Multi-Armed Bandit (see image above). The beauty of the Multi-Armed Bandit approach is that, regardless of the length of time you have available to run the test, you can make sure you are making the best decision in the long term, while also minimising the impact of poorly performing variants as quickly as possible.", "Imagine we wanted to run an online offer, and had the 5 variants of our offer shown below. Each has an intrinsic conversion rate \u2014 the proportion of customers who will go on to make a purchase after seeing the offer \u2014 but in reality we wouldn\u2019t know these conversion rates ahead of time (if we did, we\u2019d just run the best performing one straight off!).", "A traditional A/B(/C/D/E) test set up here would randomly allocate visitors to our website into one of the variants for the duration of the experiment. We could visualise these random allocations like this, where each dash is a visitor allocated to that variant. We can see that they are being randomly allocated all the way through the test.", "As the number of visitors in each variant increases, we get a better read on the underlying conversion rate for that variant (as the sample size increasing reduces the standard error on the estimate of the population mean). We\u2019ll see this below.", "Let\u2019s focus on a single variant for the moment. Each person who visits the site can either make a purchase or not. The more people who visit, the larger our sample size and the better our estimate of the conversion rate for that variant, which makes the resulting probability density function more narrow around our estimate.", "Above we\u2019re using a beta-distribution to estimate the population conversion from the counts of purchases and no-purchases.", "Back to the multi-armed bandit. As we continue serving our offer variants to visitors, we start getting an estimate for which variant is performing better. We can use this estimate to adjust our visitor-variant allocation. There are many ways to do this, I\u2019ll explore two below. We\u2019re wanting to get to an allocation which is conceptually similar to this:", "Notice how in the Multi-Armed-Bandit setup, a much larger proportion of visitors are allocated to the red-variant, which is the best performing. This is precisely our goal.", "The thinking behind a greedy allocation is that the best variant should just get all the visitors. Hold on a minute though, if we start assigning all visitors to one variant we no longer get any information about the other variants, what if we jump-the-gun and start assigning everyone to a sub-optimal variant?", "That is where the \u03b5 (epsilon) comes in. A small percentage of the time, \u03b5, we still allocate visitors randomly, just to check we haven\u2019t made a mistake. The majority of the time, 1-\u03b5, we assign visitors to the current best performing variant.", "After each new visitor, the current estimate for the conversion-rate of each variant is calculated, and if a better variant is found it will start being allocated the majority of the visitors until a new best variant is found.", "Going back to our 5 variants, we can plot our allocation (as we did before), and also our cumulative allocation (as a percentage of the total). Below we see that it takes ~50 visitors before our greedy allocation finds that the red variant is the best, but by the end of the offer, ~80% of the visitors have been allocated to that variant. This would give us an overall conversion rate of ~28%, instead of the 16% we would have achieved had we randomly allocated throughout (these are calculated as weighted averages of the variant conversion rates). That would be a +75% increase in sales from our offer!", "It is worthwhile to note that the greedy allocation can get \u201cstuck\u201d allocating to the wrong variant for a time, as it does here with the orange variant early-on, until the \u03b5 random allocations provide enough information about the other variants for the red variant to take over. One way I got round this was to start with a large value of \u03b5, so that there was more exploration at the beginning, reducing over time to 5% to exploit the better information. If this is of interest, I\u2019ve linked my analysis below.", "Similarly to above, we can visualise the progression of the allocation, by plotting an estimate for the conversion rate of each distribution, as we continue to allocate visitors. Each probability distribution (on the left) is only improved if another visitor gets added to that variant. As a reminder, for each visitor:", "It is important to note that when allocating using \u03b5-greedy , we care only about the sample mean, not the distribution. We\u2019ll see an alternative soon that explicitly uses the distribution spread too.", "Given that there are downsides to allocating greedily, is there a way that we can avoid it altogether? Instead allocating according to both our estimate of the conversion-rate and our level of confidence in each estimate? Yes! And one of the methods, Thompson sampling, is what\u2019s up next.", "Using the previous figure to help explain, can we think of a sampling method that takes into account both the mean of each variant\u2019s probability distribution and our uncertainty in them?", "One such method is known as Thompson Sampling. It works by calculating the beta-distribution of each variant based off the knowledge we have of them up to now (this is exactly what\u2019s shown in the left chart above), but instead of taking the mean of each distribution to decide the next allocation (as in \u03b5-greedy), we randomly sample from each distribution to do so.", "That way, if we have had few visitors allocated to a certain variant so far \u2192 the beta-distribution will be wide \u2192 there will be a chance of randomly sampling a high value from that distribution \u2192 there will be more allocations to that variant \u2192 over time we will increase our confidence (and make the beta-distribution more narrow).", "Similarly, if we have had many visitors allocated to a certain variant so far \u2192 the beta-distribution will be narrow \u2192 we have confidence whether we can expect high- or low-conversion rate from that variant \u2192 we know whether to over- or under-allocate visitors to that distribution.", "In this way, we completely avoid the need for a greedy algorithm which could get stuck on a sub-optimal variant!", "As before, I\u2019ll show the allocation and distributions of a Thompson-Sampled Multi-Armed Bandit approach.", "It is interesting that in this one simulation, \u03b5-greedy outperforms Thompson Sampling in how may visitors it allocates to the correct variant (85% compared to 72%), however we should keep in mind two factors:", "One way to show this second point visually is by plotting the ratio of effective conversion rate (which we introduced above as the weighted average conversion rate between the variants) against the best-case-scenario conversion rate (the conversion rate of the best performing variant) through time. This would be 1 if we just allocated everyone to the red variant, and would be ~0.53 for random allocation (in this specific case).", "We plot this now for many simulations for both Thompson and \u03b5-greedy.", "We can see that, with this set of variants, \u03b5-greedy does tend to a high effective conversion rate a little more quickly than Thompson most of the time, but can get stuck with a lower effective conversion rate sometimes. Thompson-sampled allocation on the other hand, has a much more consistent result (all the simulation lines follow similar paths). If the variants have intrinsic conversion rates that are closer to each other, it may happen that \u03b5-greedy gets stuck on a sub-optimal variant a higher proportion of the time, whereas Thompson Sampling would not fall foul of this.", "The charts above are closely related to a concept known as regret. This is measured as the loss you encounter from not assigning all visitors to the best variant from the outset, and so is in units of your output metric (e.g. \u201cI lost out on 50 sales because I didn\u2019t just run the red variant from the start\u201d, my regret would be 50). Above I chose to show the effective conversion rate instead, as this is scaled to ~1 as you run longer and longer tests, rather than continuing to increase, as regret would.", "In summary, we\u2019ve introduced the idea of Multi-Armed Bandits, where we shift the priority away from exploration (as in traditional A/B or multi-variate tests) towards exploitation. We have visually explored two of the possible allocation methods for MABs, \u03b5-greedy and Thompson Sampling. We\u2019ve also seen that Thompson Sampling uses both our best-estimate of the sample mean (for our metric of interest), and our confidence in that estimate, to allocate visitors to variants. In this way, we can have confidence that we are getting a good effective conversion rate over our entire campaign.", "The downside compared to traditional multi-variate tests is that we lose statistical significance of our worse-performing variants, as we deliberately reduce the number of visitors allocated to those variants, and so reduce their sample-size over the length of the test.", "So next time you have to run a limited-time campaign, or a product change, with a heavy focus on business results and less on statistical significance, why not consider a Multi-Armed Bandit approach?", "If you would like to contact me, you can do so on LinkedIn. If you\u2019d like to see the code and analysis (including the derivations behind each distribution), check it out on GitHub. The animations were made using Matplotlib and imagemagick.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data and Decision Scientist, PhD, seeking next opportunity in Vancouver, BC"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F278f1f15c4f1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@paulgstubley?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulgstubley?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Paul Stubley"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feac5ae7ecac5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&user=Paul+Stubley&userId=eac5ae7ecac5&source=post_page-eac5ae7ecac5----278f1f15c4f1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.amazon.com%2FLego-Minifigures-Series-6-Bandit%2Fdp%2FB006X73DGC&psig=AOvVaw3KYBg5lw0-FmCLQ8p1e1Kj&ust=1594117070106000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPipuv-yuOoCFQAAAAAdAAAAABAD", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/paul-stubley", "anchor_text": "Paul Stubley - Data & Decision Scientist \u2014Seeking next opportunity in VancouverI am an experienced Data Scientist with a demonstrated history of high-impact work, both individually and managing a teamwww.linkedin.com"}, {"url": "https://www.linkedin.com/in/paul-stubley", "anchor_text": "LinkedIn"}, {"url": "https://github.com/paul-stubley/portfolio/tree/master/multi_armed_bandit", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/a-b-testing?source=post_page-----278f1f15c4f1---------------a_b_testing-----------------", "anchor_text": "A B Testing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----278f1f15c4f1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/visualization?source=post_page-----278f1f15c4f1---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/tag/marketing?source=post_page-----278f1f15c4f1---------------marketing-----------------", "anchor_text": "Marketing"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----278f1f15c4f1---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&user=Paul+Stubley&userId=eac5ae7ecac5&source=-----278f1f15c4f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&user=Paul+Stubley&userId=eac5ae7ecac5&source=-----278f1f15c4f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F278f1f15c4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----278f1f15c4f1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----278f1f15c4f1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulgstubley?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulgstubley?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Stubley"}, {"url": "https://medium.com/@paulgstubley/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "46 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feac5ae7ecac5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&user=Paul+Stubley&userId=eac5ae7ecac5&source=post_page-eac5ae7ecac5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Feac5ae7ecac5%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-exploration-of-multi-armed-bandit-experiments-278f1f15c4f1&user=Paul+Stubley&userId=eac5ae7ecac5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}