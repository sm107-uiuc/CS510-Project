{"url": "https://towardsdatascience.com/building-data-pipelines-using-r-d9883cbc15c6", "time": 1683018415.022161, "path": "towardsdatascience.com/building-data-pipelines-using-r-d9883cbc15c6/", "webpage": {"metadata": {"title": "Building Data Pipelines using R. An example of how to consume data files\u2026 | by Ivo Bernardo | Towards Data Science", "h1": "Building Data Pipelines using R", "description": "If you work as a data analyst, the probability that you\u2019ve came across a dataset that caused you a lot of trouble due to it\u2019s size or complexity is high. Most data analysts today rely on a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.udemy.com/course/r-for-absolute-beginners/?referralCode=F839A741D06F0200F312", "anchor_text": "R Programming Course on Udemy", "paragraph_index": 6}, {"url": "https://readxl.tidyverse.org/", "anchor_text": "https://readxl.tidyverse.org/", "paragraph_index": 10}, {"url": "https://www.udemy.com/course/r-for-absolute-beginners/?couponCode=LEARN_R", "anchor_text": "R Programming course available on the Udemy platform", "paragraph_index": 46}, {"url": "http://daredata.engineering", "anchor_text": "daredata.engineering", "paragraph_index": 48}, {"url": "http://www.udemy.com/user/ivo-bernardo/", "anchor_text": "www.udemy.com/user/ivo-bernardo/", "paragraph_index": 48}], "all_paragraphs": ["If you work as a data analyst, the probability that you\u2019ve came across a dataset that caused you a lot of trouble due to it\u2019s size or complexity is high. Most data analysts today rely on a combination of several visualization and spreadsheet tools that help them make sense of the data around them, but the \u201ccurse of scattered files\u201d still stands \u2014particularly in big companies.", "But, as we leave behind the first two decades of the millenium, we witness a huge growth in the creation of new data sources \u2014 not only data analysts need to make sense of data that is produced within the organization (and with organizations trying to be more data-savyy, the amount of data produced and stored grows exponentially) but sometimes they are asked to make sense of external data, extraneous to the company. This diversity asks for new ways to approach new problems that are not solvable with old tools.", "Sure, Microsoft Excel is one of the best tools to analyze data due to it\u2019s democratization and usability, but as soon as you pass a certain amount of rows your ability to gather insights and make sense of raw data gets pretty limited. Analyzing large files with Excel has two main problems:", "It\u2019s not that Excel is a bad tool (I really think it is great), it\u2019s just that it\u2019s not tailored for large files \u2014 and yet, most people, particularly in large companies, still rely on it to do every kind of data tasks. Data visualization and self service Business Intelligence tools have been tackling this problem in some way \u2014 but to have the tools need you are overly dependent on budget or politics.", "Luckily, the adoption of open source tools surged during the past decade (particularly in the data analytics space where R and Python shine) as, regardless of having a huge community, they have a smooth learning curve and are pretty accessible for most people.", "Being able to code in these systems may enable analysts to improve their overall productivity and capacity of analyzing data \u2014 but, sometimes they get frustrated when they can\u2019t load spreadsheet files properly into R or Python and end up sticking with spreadsheet software.", "In this article we are going to explore how we can build and think of functions that prepare our spreadsheets files for analysis \u2014 the flow of the pipeline is the same that I teach on my R Programming Course on Udemy", "We are going to use the FBI Crime Data Set that has this general look:", "As we can see, we have several columns that convey almost the same information such as nominal values and rates. If we try to ingest this data directly into R we are probably going to get some weird stuff due to some additional information we have on the columns (references, for example) as any tool will have a difficult time to target the \u201creal data\u201d in this spreadsheet (and remember that Excel should have this information as it is a user-facing tool).", "As an example, the footnote will probably be included as a row somewhere in your data \u2014 this makes your data really hard to analyze \u2014 but you can fix most of these problems with a bit of data wrangling skills!", "To load this Excel file into R we will rely on the readxl (https://readxl.tidyverse.org/) R library of the tidyverse package:", "Looking at the head of the file that we just loaded:", "Ugh, look at that! Our first two rows are probably useless and the third row contains our column names. We also have some problems with some Years (look at row index #8 with the year 20015 \u2014 this happened as the superscript 5 was read as a normal number).", "The bottom of the table does not look good, also:", "Some of the descriptions and metadata we had on the excel sheet ended up being read as rows \u2014 this metadata is useful for someone that is looking to the Excel file but not for someone that is trying to analyze this data consistently on a data analysis tool.", "These are common data wrangling problems that data scientists and analysts face each time they want to analyze a new data set \u2014 particularly from a less structured source.", "We have some work on our hands, so let\u2019s get this done!", "This one is pretty easy \u2014 our column names are on the third row, so we can just set them up using the colnames function:", "Our general table gets this look:", "To build interesting and robust data pipelines we have to come up with systematic rules that can convey future changes in the possible files that we will be passing through our pipeline.", "We could definitely subset the years by using a vector to subset the first column in the dataset and looking at the values 1997, 1998\u2026 2016 (the latest year we have on this file) but what if someone gives us the file with data until 2017? or a file that contains 1996?", "If we had our years hard-coded into the pipeline, we would be building hard-coded rules \u2014 these are never a good option because they only apply to this file and this file only (or a file with exactly the same structure).", "With a new file that would contain data for more than 20 years (the ones that are available in our table) your pipeline would not be able to filter them \u2014 so by looking at the file below, can you come up with some rule that make it possible to subset the table, no matter the number of rows with yearly information?", "One nice rule we can come up with is to only get the rows that have a value that can be converted to numeric in the first column. This is an example of a good type of a \u201cdata pipeline rule\u201d as it prevents errors or loss of information in the future.", "If we try to convert the first column of this dataset into a numeric type, elements that are not numbers will be returned as NA\u2019s (Not available) \u2014 we can then filter those NA\u2019s out, efficiently retaining the rows that we are interested in\u2014 we will rely on sapply to convert our first column into numeric \u2014 we will create a new column:", "Our new converted_index column will have the following look:", "Notice how the NA\u2019s were introduced by coercion when R couldn\u2019t convert our column to numeric .", "Filtering these rows out will give us the following table \u2014 short summary of the first 11 rows and first 9 columns:", "How did we filter these NA\u2019s out? We were able to achieve that with the following code:", "With this rule we select every row of our dataframe where our last column is not NA \u2014 as our converted_index column was created inside the pipeline it would also be safe to do the following (which yields the same result):", "In this case, having the column converted_index hard-coded would not be problematic \u2014 as the converted index was created inside our pipeline and the rules that created that column are flexible to the input, there\u2019s less risk.", "We still need to get rid of the weird values like 20015 \u2014 this happened because R read the superscript 5 that points to a reference in the excel file (and that makes sense, in that system) as a number.", "Luckily, the Year Column is still a character so we are able to apply substring directly and only retrieve the first 4 \u201cletters\u201d (they are numbers but R treats them as a string because of the type of the data) of each year:", "Notice how I am rewriting the Year column with only the first 4 digits of each Year \u2014 the extra digits that were wrongly assumed by R as part of the Year are left out, as we want.", "We are able to get the following data frame:", "As we have seen columns convey the same information (rate is just a division of the original column by a million inhabitants) \u2014 as we have done with row filtering, we can also do some automated column filtering. Can you think of a rule that will be able to select the non-rate columns in the table, without relying on hard-coded rules?", "Let\u2019s inspect the names of our columns with:", "Looking closely, every column that has rate in its name should be removed from our table. Instead of hardcoding the names or indexes of the columns we can rely on the grepl function. Using:", "This function will returns us a vector with TRUE and FALSE \u2014 TRUE in the indexes where the column name contains \u201crate\u201d and FALSE otherwise:", "We can now rely on this vector to filter out these columns of our data frame \u2014 using indexers. We will also rewrite our original object:", "We\u2019re almost done! Let\u2019s look at our crime_data_filter structure:", "Most of our columns are still characters(a problem that migrated since the beginning of the pipeline as we had blank cells in the excel that made R assume this was a character column ) even if we only see numbers in it!", "Let\u2019s convert these columns to numeric \u2014 we can\u2019t apply as.numeric directly to our object (we could only do this to Vectors, Matrixes or Arrays) \u2014 we need to rely on our apply family of functions!", "As sapply returns a list, we will wrap this function on a data.frame function to get a data frame.", "Now that we have a really clean table and able to be analyzed \u2014 we can encapsulate every instruction we have done in a function that can be reused:", "In conclusion, knowing how to build data pipelines is an essential task of analyzing data in systems that rely on code \u2014 particularly in a world where more and more professionals are acessing data using R and Python, it\u2019s incredibly important to understand how to build error-proof rules in reading data files.", "This lecture is taken from my R Programming course available on the Udemy platform \u2014 the course is suitable for beginners and people that want to learn the fundamentals of R Programming. The course also contains more than 50 coding exercises that enables you to practice as you learn new concepts.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write about data science and analytics | Partner @ dd.eng (daredata.engineering) | Instructor @ Udemy \u2014 (www.udemy.com/user/ivo-bernardo/)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9883cbc15c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ivopbernardo.medium.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": ""}, {"url": "https://ivopbernardo.medium.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Ivo Bernardo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74eec53531c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&user=Ivo+Bernardo&userId=74eec53531c0&source=post_page-74eec53531c0----d9883cbc15c6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://stocksnap.io/author/4440", "anchor_text": "Negative Space"}, {"url": "https://stocksnap.io/", "anchor_text": "StockSnap"}, {"url": "https://support.microsoft.com/en-us/office/excel-specifications-and-limits-1672b34d-7043-467e-8e27-269d656771c3", "anchor_text": "https://support.microsoft.com/en-us/office/excel-specifications-and-limits-1672b34d-7043-467e-8e27-269d656771c3"}, {"url": "https://www.udemy.com/course/r-for-absolute-beginners/?referralCode=F839A741D06F0200F312", "anchor_text": "R Programming Course on Udemy"}, {"url": "https://readxl.tidyverse.org/", "anchor_text": "https://readxl.tidyverse.org/"}, {"url": "https://www.udemy.com/course/r-for-absolute-beginners/?couponCode=LEARN_R", "anchor_text": "R Programming course available on the Udemy platform"}, {"url": "https://medium.com/tag/r?source=post_page-----d9883cbc15c6---------------r-----------------", "anchor_text": "R"}, {"url": "https://medium.com/tag/r-programming?source=post_page-----d9883cbc15c6---------------r_programming-----------------", "anchor_text": "R Programming"}, {"url": "https://medium.com/tag/r-programming-language?source=post_page-----d9883cbc15c6---------------r_programming_language-----------------", "anchor_text": "R Programming Language"}, {"url": "https://medium.com/tag/data-pipeline?source=post_page-----d9883cbc15c6---------------data_pipeline-----------------", "anchor_text": "Data Pipeline"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&user=Ivo+Bernardo&userId=74eec53531c0&source=-----d9883cbc15c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&user=Ivo+Bernardo&userId=74eec53531c0&source=-----d9883cbc15c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd9883cbc15c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d9883cbc15c6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d9883cbc15c6--------------------------------", "anchor_text": ""}, {"url": "https://ivopbernardo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ivopbernardo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ivo Bernardo"}, {"url": "https://ivopbernardo.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "673 Followers"}, {"url": "http://daredata.engineering", "anchor_text": "daredata.engineering"}, {"url": "http://www.udemy.com/user/ivo-bernardo/", "anchor_text": "www.udemy.com/user/ivo-bernardo/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74eec53531c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&user=Ivo+Bernardo&userId=74eec53531c0&source=post_page-74eec53531c0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcbb9b31567f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-using-r-d9883cbc15c6&newsletterV3=74eec53531c0&newsletterV3Id=cbb9b31567f5&user=Ivo+Bernardo&userId=74eec53531c0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}