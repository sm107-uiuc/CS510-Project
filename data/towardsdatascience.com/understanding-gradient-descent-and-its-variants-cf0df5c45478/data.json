{"url": "https://towardsdatascience.com/understanding-gradient-descent-and-its-variants-cf0df5c45478", "time": 1683002380.564299, "path": "towardsdatascience.com/understanding-gradient-descent-and-its-variants-cf0df5c45478/", "webpage": {"metadata": {"title": "Understanding Gradient Descent And Its Variants | by Richmond Alake | Towards Data Science", "h1": "Understanding Gradient Descent And Its Variants", "description": "Machine learning models are fantastic; they can recognize objects in videos; they can automatically generate captions for images and accurately classify pictures of cats and dogs (sometimes). This\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "Mean Squared Error", "paragraph_index": 6}], "all_paragraphs": ["Machine learning models are fantastic; they can recognize objects in videos; they can automatically generate captions for images and accurately classify pictures of cats and dogs (sometimes).", "This article will provide a surface level understanding of what happens underneath the hood of Machine learning models. More specifically, we will be exploring the \u2018backbone algorithms\u2019 that enable these machine learning models to learn.", "The \u2018backbone algorithms\u2019 are called Optimization algorithms. Below are some definitions of keywords you will encounter within this article, and optimization algorithm is amongst the provided descriptions.", "At the bottom of this article is a link to some standard machine learning terms and their definitions. Think of it as a Christmas gift \ud83d\ude0a.", "The optimization algorithm we will be exploring in this article is Gradient Descent.", "Gradient Descent is a very common optimization algorithm, and most likely, the first optimization algorithm a lot of Machine learning engineers and Data scientists are introduced to.", "Let's paint a picture. We have a cost function, and we need to find the optimum solution to solve the cost function. Here comes gradient descent, an algorithm that works by making changes to the values of the parameters that are within the model, all in the purpose of minimizing the cost function. An example of a cost function is Mean Squared Error.", "Gradient descent intrinsic functionality works by finding the direction to take towards a local minimum based on the calculated gradient obtained from the error function with respect to the parameters at a particular data point.", "It might help to understand gradient descent with some imagery and visualization.", "Let\u2019s use a graph that contains a bowl-shaped curve, and a ball, placed on the top left side of the curve. The ball represents a certain point (value) in the parameter space that is initially randomly chosen, and the curve represents the cost values plotted against a range of parameter values. The goal is to reach the parameter value that provides the lowest cost value.", "On the x-axis of the plot is a value representing the cost, and on the y-axis is a value denoted by \u2018X\u2019 that represents the range of parameter values that we are utilizing to solve the cost function.", "The minimum (singular)/minima(plural) is a point within the slope where the optimum value that minimizes the cost function exists, and gradient descent is the algorithm that guides our ball towards the minimum at several steps (iterations).", "To solve the cost function, we are looking for the lowest point of the curve, and this is the point where the gradient is zero or close to zero.", "Quick note: Cost function curve aren\u2019t always necessarily a nice bowl shape with one local minima. In the example used in the image above there is only one input paramter(1 dimensional paramter space) to the cost fucntion, but in practice, the parameter space tends to have more dimensions.", "We understand how gradient descent works and can now apply it to our training data. The application of the gradient descent algorithm to training data comes in various forms. One form is called Batch Gradient Descent (BGD).", "In the image above, we take steps towards a local minimum. In BGD, we actually utilize every training data at our disposal to decide as to which direction and by how much we move towards a minimum. We use all our training data at each step.", "For extensive training data, the training process can be prolonged but can be computationally efficient as we do not make any changes to our model parameters as often as other variants of gradient descent.", "Although BGD is not memory efficient, as you can imagine that we require all our datasets available in memory when training a model.", "On the other side of the coin to BGD, we have Stochastic Gradient Descent (SGD).", "As opposed to iterating through every data within our training set and then making a step towards a local minimum, SGD works by actually picking a single data point from the training set and computing the gradient, based on this single data point.", "You can probably tell that between BGD and SGD, SGD is the faster algorithm since you are computing the gradient based on a single instance of the data as opposed to the entire dataset, but at what cost.", "Updates made within the parameter space during gradient descent can be noisy when using SGD. The noisiness characteristic of SGD is a result of its random nature that occurs when selecting data points from the training set to compute gradients from at each step.", "To accommodate for the noisiness nature of SGD and ensure we reach an optimum parameter value, we have to iterate over the training data a certain number of times and ensure that at the beginning of the gradient descent process, the training data is shuffled.", "Noise leads to ambiguous parameter values to solve the cost function, although given enough time, SGD will approach a local minimum. The noisiness and random nature of SGD are also beneficial. It\u2019s useful for when the algorithm gets \u2018stuck\u2019 in a local minimum that isn\u2019t the global minimum.", "In comparison to BGD, SGD has the benefit of escaping local minimums and finding the global minimum due to its random and erratic nature when allocating parameter values at each step.", "But BGD parameter values are closer to the global minimum and optimal in comparison to SGD. There is a trade-off between speed and optimality when faced with selecting between both variants of the gradient descent algorithms.", "How about an approach that leverages the good characteristics of both SGD and BGD.", "Mini Batch Gradient Descent computes the gradient based on randomly selected data within the training set just like SGD but does not include the entire dataset when computing gradients, so it\u2019s also not quite BGD. You could say it\u2019s a hybrid.", "Mini Batch GD uses a small number of data when computing gradients; in comparison to BGD, it\u2019s faster, but when compared to SGD, it\u2019s still slower.", "The advantage of Mini Batch GD to SGD is the reduction in noise within the parameter space. This means that utilizing Mini Batch GD, means optimum parameter values are more reachable.", "Phew, that wasn\u2019t all too bad and moving forward, you can look online for some resources on some code implementation for each of the algorithms.", "I should be posting a medium article outlining how to implement some of the Gradient Descent algorithms mentioned in this article shortly.", "Follow me for more articles similar to this.", "And as promised, below is a link to definitions of some common machine learning terminologies.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Content Creator with 1M+ views\u2014 Computer Vision Engineer. Interested in gaining and sharing knowledge on Technology and Finance"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf0df5c45478&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://richmondalake.medium.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Richmond Alake"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88797ba3f2f6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&user=Richmond+Alake&userId=88797ba3f2f6&source=post_page-88797ba3f2f6----cf0df5c45478---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/dogs-vs-cats", "anchor_text": "https://www.kaggle.com/c/dogs-vs-cats"}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "Mean Squared Error"}, {"url": "https://towardsdatascience.com/30-pocket-sized-terms-for-machine-learning-beginners-5e381ed93055", "anchor_text": "30 pocket-sized terms for Machine Learning beginnersA list of useful machine learning terminologies that you\u2019ll encounter in your Data Science or Machine learning career.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cf0df5c45478---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cf0df5c45478---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----cf0df5c45478---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cf0df5c45478---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----cf0df5c45478---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&user=Richmond+Alake&userId=88797ba3f2f6&source=-----cf0df5c45478---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&user=Richmond+Alake&userId=88797ba3f2f6&source=-----cf0df5c45478---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcf0df5c45478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cf0df5c45478---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf0df5c45478--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cf0df5c45478--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cf0df5c45478--------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Richmond Alake"}, {"url": "https://richmondalake.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "7.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88797ba3f2f6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&user=Richmond+Alake&userId=88797ba3f2f6&source=post_page-88797ba3f2f6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F366f35b0b39b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-its-variants-cf0df5c45478&newsletterV3=88797ba3f2f6&newsletterV3Id=366f35b0b39b&user=Richmond+Alake&userId=88797ba3f2f6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}