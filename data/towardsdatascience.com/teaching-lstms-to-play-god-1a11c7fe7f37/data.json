{"url": "https://towardsdatascience.com/teaching-lstms-to-play-god-1a11c7fe7f37", "time": 1683001967.172019, "path": "towardsdatascience.com/teaching-lstms-to-play-god-1a11c7fe7f37/", "webpage": {"metadata": {"title": "Teaching LSTMs to play God. A beginner-friendly introduction to\u2026 | by Rwiddhi Chakraborty | Towards Data Science", "h1": "Teaching LSTMs to play God", "description": "At the risk of this becoming another one of those \u201cGenerate text with an RNN\u201d tutorials, and at the thought of Shakespeare and Lewis Carrol cringing in their graves at the kinds of texts these\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.gutenberg.org/files/10/10-h/10-h.htm", "anchor_text": "here", "paragraph_index": 4}, {"url": "http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/", "anchor_text": "this", "paragraph_index": 12}, {"url": "https://r2rt.com/styles-of-truncated-backpropagation.html", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally", "anchor_text": "here", "paragraph_index": 44}, {"url": "https://www.artsy.net/article/artsy-editorial-hard-painting-made-computer-human", "anchor_text": "paintings", "paragraph_index": 45}], "all_paragraphs": ["At the risk of this becoming another one of those \u201cGenerate text with an RNN\u201d tutorials, and at the thought of Shakespeare and Lewis Carrol cringing in their graves at the kinds of texts these vanilla models normally generate, I chose to take a different route, but at the same time decided to take a dig at this.", "The Holy Bible is the biggest selling and the most influential book in human history. Vast treasure troves of human thought, exposition, and creation have been centred around this book.", "Which begs the question \u2014 Can a recurrent neural network be trained well to generate sermons?", "In this toy example, we will walk through relatively simple lines of code to attempt this. Our expectations aren\u2019t high, as we\u2019ll be using a very simple architecture. The goal of this post is to learn how to do text generation with RNNs in tensorflow. I do assume some background in training neural networks in tensorflow before, only to the point that I won\u2019t explain how the loss function or the softmax is implemented, for example.", "Since nothing in deep learning proceeds without data, let\u2019s get some. Project Gutenberg has some marvellous books for free public use [how ironic since the Gutenberg Bible was the first printed text with movable type]. We\u2019ll get the King James Version here.", "The structure of the Bible is relatively simpler than other books. Even then, we may encounter some weird characters that we don\u2019t want to be fed into our model. Let\u2019s have a look at the data:", "We don\u2019t want our model to learn characters like \u2018\\\u2019, \u2018{\u2019, \u2018\\n\u2019. In fact, let\u2019s look at the unique characters in the text:", "The numbers are important since they indicate the verses, the other punctuations are okay too, but we can live without the following:", "That\u2019s better. There are 48 unique characters in this text.", "We need to do two small things before we can proceed with the model. These characters cannot exist as they are. We need an integer encoding of them to be able to actually feed these as input arrays. Similarly, down the line, when we predict characters, we need a way to decode the integers obtained to characters again. So we can create two dictionaries, one that holds a one to one mapping of a character to an integer, and vice versa. Let\u2019s do that:", "Nothing much to explain till here, check that you have the following mappings:", "A lot of deep learning training happens to involve making a decision regarding the size of data, the shape, the structure, and so on. The text is too big to be fed at once, and most problems in the real world involve sizes orders bigger than the text we\u2019re processing at the moment. Training in batches isn\u2019t a nice-to-have. It\u2019s necessary.", "In particular, RNNs train using backpropagation through time [BPTT], which is basically the traditional backpropagation unrolled over each time step. A nice primer is this.", "Since we can\u2019t apply BPTT on the entire text, we apply it on the batches we generate. How do we generate these batches? It\u2019s mostly a design question, but I\u2019ve implemented the following procedure: [You can skip this part if you\u2019re interested in just the code].", "(1) Divide the entire text into 16 blocks.", "(2) Each block contains sequences of characters. We choose 256 as our sequence size.", "(3) Each batch i we create contains the ith sequence from each block. This. means each batch contains 16 sequences, each of size 256. This means Batch 1 has the first sequence of 256 characters from Block 1, Block 2,\u2026., Block 16. The same for Batch 2, Batch 3, \u2026, and so on. How many batches do we have? For n characters in total, it\u2019s standard middle school math to see \u2014", "n = batch_size * sequence_size * no_of_batches", "Of course, this won\u2019t always be wholly divisible, it depends on the four integer values chosen. For example, in our case, n = 4233042, batch_size = 16, sequence_size = 256, no_of_batches = 1034 but if you see carefully, the last batch cannot have sequence sizes of 256, but a smaller value [take out a pen and paper and try to figure out what this value will be], because we\u2019ve run out of characters when we get to the last batch.", "We can just drop this last batch to avoid shape mismatch issues later. We now have no_of_batches = 1033 instead, with all the arrays nicely shaped at (16, 256).", "Okay, in summary, 1033 batches, each batch containing 16 sequences, each sequence 256 characters long.", "By way, this entire process I\u2019ve described has a name \u2014 Truncated Backpropagation Through Time. Lots of details here.", "Here\u2019s the code to do all the stuff I\u2019ve rambled on about:", "The next question is, how do we create the input and target values? This is simple. Consider the following example:", "The target for this would be:", "Every character i in the target vector is the (i + 1)th character in the input vector. Notice how Y is a unit dimension smaller than X. This is because when you reach the last character of X, there\u2019s nothing left to predict. Thus, we can simply remove the last character in X. This small observation is important, our final shapes of both X and Y will be (16, 255):", "Done! We\u2019re now ready to build our model.", "We will choose a simple architecture \u2014 Two hidden layers, one MultiRNNCell, each LSTM cell contains 256 hidden units, and a softmax output layer of k units, where k is the number of unique characters in our data [Makes sense, right?].", "This should be pretty self-explanatory, except for the part where I\u2019ve added one hot encoding for the inputs and labels. Notice that this transforms the shapes (16, 256) to (16, 256, k), k = 48 in our case. I choose a small number of epochs to check whether the training losses behave as they should \u2014 decrease gradually. You can always play around with these hyperparameters later on.", "Notice that you need to feed the final state at each time step t as the initial state for (t + 1). This is crucial.", "We get the following loss curve:", "Time for the real fun. Let\u2019s get this simple model to generate some text:", "We\u2019ll provide a start sequence, and ask the model to predict 256 characters after the start. Since our softmax returns probabilities of selecting each of the characters, we have flexibility in determining which character to choose. Always choosing the maximum probability characters makes the model repeat itself, behaving like an infinite loop that prints the same value over and over again. Instead, we sort the probabilities, take the five biggest values, renormalise, and then randomly select from among these five. This introduces a stochasticity that produces better results:", "Let\u2019s look at what sermons our LSTMs spit out:", "and god and the land that the lord,at there are will i will shall tentsouss which the lord, and shall bath and all the lord, which are the wilderness.2:22for they shall cause the people: and with that with helpet was soul of his fields: and it nations, which they", "and god hath seen minister unto them.11:36 they that forsake them as a child of the congregation: the lord hath done it, and wine it: for the lord your god hath done.1:2 but i answered thee for me; the diviting of the lord god of heart thou sayest by thy light in", "and god with the light.11:11 and i said unto him, this marmer the land wourd in me, why have we have not believed you, and in the midst of the enemy;2:12 but thus saith, master, i will set them all, and send to thee, and setthem inthe way of the congregation, that", "Almost all of it is semantically nonsense, but note that the model did not have any information of what words or letters or numbers are, what punctuations are, what structure is, what language is, for that matter. Trained for 5 epochs, we can see some interesting results. It has learnt that a lot of text begins with the digits, for example. It does add the right structure of the digits and colons before a new sermon. It has also learnt to put in punctuation here and there, and mostly gets words right, as far as the vocabulary is concerned.", "Let\u2019s try a different start sequence, just for fun. Here\u2019s what our model came up with for \u2018jerusalem\u2019:", "jerusalem: but the levite, whom itshall be searated and shall put it into curse.5:16 i have set up a parable of thee, but is the land, and on them: and there is no bringing the service ofhim.61:11 they are as the holy ghost, of whom the word oftheir fathers.21:26 t", "jerusalem, and the throne, and the the three house of the horse, and the priest\u2019s servant tola which all things are come to the saying.12:4 for the waters shall be on the sepulling of theshout of the house, of the lord, their charges of them.12:1 and they which are", "jerusalem, and they also the father.1:10 and there was strength against the lord, and the sameone,what is the word of the lord, and the hangings of idols: and the saying will i serve their own labour.1:11 i have death in this take upon the flesh from this land in s", "Interesting, it has learnt pretty much that Jerusalem is a unique entity that isn\u2019t joined with other characters.", "Try this out yourself for different start sequences!", "Improvements can be brought about in many ways. Tune the hyperparameters, specifically try increasing the epochs, slightly decreasing the learning rate, increasing the number of hidden units, and all combinations of these. One interesting hyperparameter that can be introduced is temperature, which determines how conservative/diverse the model choices are in picking the next characters. It\u2019s explained nicely here.", "I conclude with a question \u2014 If in enough time, enough data, and a smart model, we can generate sermons that are indistinguishable from human-written ones [as we have done for paintings], can we program God? But if we could, haven\u2019t we run into a paradox? I\u2019d love a discussion on this.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a11c7fe7f37&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dejavu.chakraborty?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dejavu.chakraborty?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Rwiddhi Chakraborty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2452212abec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&user=Rwiddhi+Chakraborty&userId=c2452212abec&source=post_page-c2452212abec----1a11c7fe7f37---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Aaron Burden"}, {"url": "https://unsplash.com/s/photos/the-bible?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://www.gutenberg.org/files/10/10-h/10-h.htm", "anchor_text": "here"}, {"url": "http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/", "anchor_text": "this"}, {"url": "https://r2rt.com/styles-of-truncated-backpropagation.html", "anchor_text": "here"}, {"url": "https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally", "anchor_text": "here"}, {"url": "https://www.artsy.net/article/artsy-editorial-hard-painting-made-computer-human", "anchor_text": "paintings"}, {"url": "https://github.com/rwiddhic96/LSTMS_God", "anchor_text": "https://github.com/rwiddhic96/LSTMS_God"}, {"url": "https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html", "anchor_text": "https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "https://www.tensorflow.org/tutorials/text/text_generation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a11c7fe7f37---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1a11c7fe7f37---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----1a11c7fe7f37---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a11c7fe7f37---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----1a11c7fe7f37---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&user=Rwiddhi+Chakraborty&userId=c2452212abec&source=-----1a11c7fe7f37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&user=Rwiddhi+Chakraborty&userId=c2452212abec&source=-----1a11c7fe7f37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a11c7fe7f37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a11c7fe7f37---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a11c7fe7f37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dejavu.chakraborty?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dejavu.chakraborty?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rwiddhi Chakraborty"}, {"url": "https://medium.com/@dejavu.chakraborty/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "222 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2452212abec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&user=Rwiddhi+Chakraborty&userId=c2452212abec&source=post_page-c2452212abec--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9eca76e08de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-lstms-to-play-god-1a11c7fe7f37&newsletterV3=c2452212abec&newsletterV3Id=9eca76e08de6&user=Rwiddhi+Chakraborty&userId=c2452212abec&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}