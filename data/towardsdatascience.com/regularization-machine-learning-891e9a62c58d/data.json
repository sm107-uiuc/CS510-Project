{"url": "https://towardsdatascience.com/regularization-machine-learning-891e9a62c58d", "time": 1683011728.7962618, "path": "towardsdatascience.com/regularization-machine-learning-891e9a62c58d/", "webpage": {"metadata": {"title": "Regularization: Machine Learning. The solution to over-fitting model, its\u2026 | by Gokul S Kumar | Towards Data Science", "h1": "Regularization: Machine Learning", "description": "For understanding the concept of regularization and its link with Machine Learning, we first need to understand why do we need regularization. We all know Machine learning is about training a model\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/Gokul-S-Kumar/Regularization.git", "anchor_text": "link to the Jupyter notebook", "paragraph_index": 2}, {"url": "https://medium.com/@skumar.gokul", "anchor_text": "here.", "paragraph_index": 59}, {"url": "https://www.linkedin.com/in/gokul-s-kumar", "anchor_text": "LinkedIn", "paragraph_index": 59}], "all_paragraphs": ["For understanding the concept of regularization and its link with Machine Learning, we first need to understand why do we need regularization. We all know Machine learning is about training a model with relevant data and using the model to predict unknown data. By the word unknown, it means the data which the model has not seen yet. We have trained the model, and are getting good scores while using training data. But during the process of prediction, we found that the model is underperforming when compared to the training part. Now, this may be a case of over-fitting(about which I will be explaining below) which is causing incorrect prediction by the model. Regularizing the model, in this case, can help resolve the problem.", "We will be discussing the following topics in this article:", "Here is the link to the Jupyter notebook being used throughout this article. Please go through it side-by-side to understand the article thoroughly.", "Consider the case of linear regression. We will be generating some data to work on. Then the data will be split into training and testing data.", "This is how the generated data is gonna look like:", "We have split 20% of the total data generated into testing data. The true-fit function is the function based on which we have generated the data. Noise is added to simulate the real-world data(since in real no data is going to fit a function perfectly).", "Getting to know about under-fitting and over-fitting involves training the data on models having a hypothesis equations of different degrees. Let\u2019s begin with a model having hypothesis equation of degree 1 which will be:", "This is how the regression line would look like:", "This is an example of under-fitting as we can see that the line is not fitting the data to an extent by which we can make future predictions using the model. The training and testing accuracy and error values are as follows:", "The under-fitting problem is also called \u2018high-bias\u2019 problem. Both of these mean that the model is not fitting the training-data very well. The term bias refers to the model having a strong preconception that y is varying linearly with x, even though it does not.", "Now, we could try fitting a quadratic function to the data. The hypothesis would be like:", "The model can be visualized as:", "We can see that the model works pretty well in fitting the training data. The training and testing accuracy and MSE values are:", "The accuracy is increased when compared to the previous model. This implies a more optimal fit.", "Since increasing the degree seem to be increasing the performance of the model, let\u2019s try using a model with its hypothesis having an equation of degree 20. This will be the result:", "Oops! The model function appears to be distorted. This happens as a high degree enables it to be more flexible thus allowing it to incorporate the noise in the data while fitting. The model function is trying to pass through all the points thus looks distorted.", "We call this problem \u2018Over-fitting\u2019 and it has \u2018High-variance\u2019. The reason behind the term high-variance is if we are fitting such a high order polynomial then the hypothesis can fit almost any function i.e., the different types of the possible hypothesis are very large and too variable that we don\u2019t have enough data to constraint it to get a perfect hypothesis.", "We can decide upon the problem of overfitting by checking either the cross-validation scores or the MSE values. I have plotted the accuracy and MSE values for the above data for hypothesis equations having degrees from 1 to 40.", "We can see that after a certain degree(9 to be specific), the testing MSE seems to increase whereas the training MSE keeps on decreasing. This happens when the model is unable to predict the unseen data due to overfitting.", "Using an equation of degree 9 gives us the following model function:", "Overfitting can also occur if we have too many features, which is what happens indirectly when we increase the degree of the function. The learned hypothesis may fit the training set very well( to an extent that the value of cost will be zero), but fail to predict on new examples. The same concepts apply to Logistic regression as well, where the decision boundary is taken into account instead of the regression line.", "Most of the real-world datasets will be having a large no. of features. If we have a lot of features and very little training data then over-fitting can occur. There are 2 main methods to address overfitting:", "Consider the case of fitting a function of degree 10 the data that we saw above. The hypothesis would be like:", "If we penalize theta_10 and make it very small(almost equal to zero) then the hypothesis would be reduced to an equation of degree 9 which will be the optimal fit for the data as shown above.", "This is a general idea behind Regularization. Instead of reducing just one parameter, we will be penalizing all the parameters. This will give rise to a simpler hypothesis that is less prone to overfitting.", "For anyone who is not familiar with the math behind linear regression, please refer to my previously published story given below. It will also give you an idea about the terms and variables which are going to be used hereafter.", "The cost function of any regression model is given by:", "To add regularization to the model, the cost function is modified a little as:", "Lambda\u2019s purpose is to give a good fit for the training data while limiting the values of the parameters, thereby keeping the hypothesis relatively simple to avoid overfishing. In other words, we are adding a small bias to the over-fitting cost function(which tends to zero) to prevent it from overfishing. By adding that small amount of bias, we get a significant drop in the variance.", "Consider the case of the hypothesis of degree 10. What happens if we set the value of lambda very high, say lambda = \u00b9\u2070\u00b9\u2070. All the parameters theta_1 to theta_10 will be highly reduced to a point that the hypothesis will be:", "which when plotted will be a horizontal straight line, thus generating a poor fit to our data. So for regularization to work well, care should be taken to choose the value of lambda as well.", "The gradient descent for linear regression without regularizing is given by:", "The partial derivative of cost function will be changed due to the inclusion of regularization which gives rise to the gradient descent equation as:", "I have written theta_0 separately as it is not included in the regularization parameter\u2019s summation. This can be simplified as:", "Since alpha, lambda and m are positive:", "Hence while applying gradient descent, the value of theta_j each time would be reduced from a lower value of theta_j say, 0.99*theta_j. This helps in shrinking the value of theta after each iteration of gradient descent.", "There are mainly two types of regularization techniques:", "All that math that we discussed above is concerned with Ridge regression. Its cost function will be", "The following plots are obtained by fitting an equation of degree 20 to the data that we used above, with different values of lambda.", "We can see that for lambda=0 there is no change from the over-fit linear regression using an equation of degree 20.", "We can see that, as the value of lambda increases, the line asymptotically becomes parallel to the x-axis. This is a visual proof to the argument above where the value of lambda was large, thereby reducing the hypothesis to only theta_0. Getting the value of parameters for lambda = 10000.", "We can see that the parameters are tending to zero and having different values.", "Ridge regression is used in cases where the size of training examples is less, thus preventing the model from overfitting the training data by reducing the variance.", "Lasso regression is pretty much similar to Ridge regression. There is a slight modification in the cost function which would be:", "Instead of using the sum of squares of parameters in the regularization term, we will be using the sum of modulus of the parameters.", "The following plots are obtained by fitting an equation of degree 20 to the data that we used above, with different values of lambda.", "We can see that for lambda=0.1, the model is a straight line parallel to the x-axis and this is not fitting the data well. So it is evident that we have to reduce the value of lambda further.", "As the value of lambda increases the model becomes a straight line parallel to the x_axis. Here the parameters are equal to zero for larger values of lambda. This is the major difference between ridge and lasso regression.", "In ridge regression for large values of lambda the parameters tend to zero while in lasso regression, the parameters can be equal to zero.", "We can see that all the values are equal and approximately equal to zero. Further increasing the value of lambda won\u2019t change the value of parameters as shown below.", "As the parameters are reduced to zero the features associated with those parameters won\u2019t have any effect on the cost function. By doing this we will be executing a type of feature selection where we omit all the useless features that don\u2019t contribute much to the model.", "The values of the regularization coefficient or lambda at which the model performs the best can be obtained by cross-validation. There are inbuilt cross-validation techniques in the sklearn\u2019s ridge regressor. We can either use it directly or execute a separate cross-validation process.", "We will be using the same data used above with an equation of degree 20. We have to input an array containing different values of lambda to perform cross-validation.", "The first input of lambdas was:", "The best value of lambda and its corresponding CV score are:", "We can further zoom in on the lambda values by inputting the values around 0.0005. Similarly, the method of cross-validation can be employed using sklearn\u2019s LassoCV object.", "The method of regularization can be used to prevent over-fitting in Logistic regression as well. We will be using the Ridge classifier from sklearn to implement L2 regularization in logistic regression.", "We have seen the following in this article:", "2. This is another playlist on Youtube which explains all the Machine Learning basics simply without involving much jargon.", "Do check out my other articles regarding Data Science and Machine Learning here. Feel free to reach out for a more in-depth discussion in the comments and on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I teach robots how to learn!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F891e9a62c58d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@skumar.gokul?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skumar.gokul?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Gokul S Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb75603badb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&user=Gokul+S+Kumar&userId=eb75603badb&source=post_page-eb75603badb----891e9a62c58d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@killerfvith?utm_source=medium&utm_medium=referral", "anchor_text": "Alex wong"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/Gokul-S-Kumar/Regularization.git", "anchor_text": "link to the Jupyter notebook"}, {"url": "https://towardsdatascience.com/univariate-linear-regression-theory-and-practice-99329845e85d", "anchor_text": "Univariate Linear Regression-Theory and PracticeIntroduction: This article explains the math and execution of univariate linear regression. This will include the math\u2026towardsdatascience.com"}, {"url": "https://medium.com/@skumar.gokul", "anchor_text": "here."}, {"url": "https://www.linkedin.com/in/gokul-s-kumar", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----891e9a62c58d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----891e9a62c58d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/regression?source=post_page-----891e9a62c58d---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/regularization?source=post_page-----891e9a62c58d---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&user=Gokul+S+Kumar&userId=eb75603badb&source=-----891e9a62c58d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&user=Gokul+S+Kumar&userId=eb75603badb&source=-----891e9a62c58d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F891e9a62c58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----891e9a62c58d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----891e9a62c58d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----891e9a62c58d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----891e9a62c58d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skumar.gokul?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skumar.gokul?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gokul S Kumar"}, {"url": "https://medium.com/@skumar.gokul/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "52 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb75603badb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&user=Gokul+S+Kumar&userId=eb75603badb&source=post_page-eb75603badb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F329f05900600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-machine-learning-891e9a62c58d&newsletterV3=eb75603badb&newsletterV3Id=329f05900600&user=Gokul+S+Kumar&userId=eb75603badb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}