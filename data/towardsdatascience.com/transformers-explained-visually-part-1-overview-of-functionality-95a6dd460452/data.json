{"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "time": 1683017666.049348, "path": "towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452/", "webpage": {"metadata": {"title": "Transformers Explained Visually (Part 1): Overview of Functionality | by Ketan Doshi | Towards Data Science", "h1": "Transformers Explained Visually (Part 1): Overview of Functionality", "description": "A Gentle Guide to Transformers, how they are used for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34", "anchor_text": "next", "paragraph_index": 39}], "all_paragraphs": ["We\u2019ve been hearing a lot about Transformers and with good reason. They have taken the world of NLP by storm in the last few years. The Transformer is an architecture that uses Attention to significantly improve the performance of deep learning NLP translation models. It was first introduced in the paper Attention is all you need and was quickly established as the leading architecture for most text data applications.", "Since then, numerous projects including Google\u2019s BERT and OpenAI\u2019s GPT series have built on this foundation and published performance results that handily beat existing state-of-the-art benchmarks.", "Over a series of articles, I\u2019ll go over the basics of Transformers, its architecture, and how it works internally. We will cover the Transformer functionality in a top-down manner. In later articles, we will look under the covers to understand the operation of the system in detail. We will also do a deep dive into the workings of the multi-head attention, which is the heart of the Transformer.", "Here\u2019s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.", "And if you\u2019re interested in NLP applications in general, I have some other articles you might like.", "The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish.", "At its core, it contains a stack of Encoder layers and Decoder layers. To avoid confusion we will refer to the individual layer as an Encoder or a Decoder and will use Encoder stack or Decoder stack for a group of Encoder layers.", "The Encoder stack and the Decoder stack each have their corresponding Embedding layers for their respective inputs. Finally, there is an Output layer to generate the final output.", "All the Encoders are identical to one another. Similarly, all the Decoders are identical.", "The Encoder is a reusable module that is the defining component of all Transformer architectures. In addition to the above two layers, it also has Residual skip connections around both layers along with two LayerNorm layers.", "There are many variations of the Transformer architecture. Some Transformer architectures have no Decoder at all and rely only on the Encoder.", "The key to the Transformer\u2019s ground-breaking performance is its use of Attention.", "While processing a word, Attention enables the model to focus on other words in the input that are closely related to that word.", "eg. \u2018Ball\u2019 is closely related to \u2018blue\u2019 and \u2018holding\u2019. On the other hand, \u2018blue\u2019 is not related to \u2018boy\u2019.", "The Transformer architecture uses self-attention by relating every word in the input sequence to every other word.", "In the first sentence, the word \u2018it\u2019 refers to \u2018cat\u2019, while in the second it refers to \u2018milk. When the model processes the word \u2018it\u2019, self-attention gives the model more information about its meaning so that it can associate \u2018it\u2019 with the correct word.", "To enable it to handle more nuances about the intent and semantics of the sentence, Transformers include multiple attention scores for each word.", "eg. While processing the word \u2018it\u2019, the first score highlights \u2018cat\u2019, while the second score highlights \u2018hungry\u2019. So when it decodes the word \u2018it\u2019, by translating it into a different language, for instance, it will incorporate some aspect of both \u2018cat\u2019 and \u2018hungry\u2019 into the translated word.", "The Transformer works slightly differently during Training and while doing Inference.", "Let\u2019s first look at the flow of data during Training. Training data consists of two parts:", "The Transformer\u2019s goal is to learn how to output the target sequence, by using both the input and target sequence.", "The Transformer processes the data like this:", "During Inference, we have only the input sequence and don\u2019t have the target sequence to pass as input to the Decoder. The goal of the Transformer is to produce the target sequence from the input sequence alone.", "So, like in a Seq2Seq model, we generate the output in a loop and feed the output sequence from the previous timestep to the Decoder in the next timestep until we come across an end-of-sentence token.", "The difference from the Seq2Seq model is that, at each timestep, we re-feed the entire output sequence generated thus far, rather than just the last word.", "The flow of data during Inference is:", "The approach of feeding the target sequence to the Decoder during training is known as Teacher Forcing. Why do we do this and what does that term mean?", "During training, we could have used the same approach that is used during inference. In other words, run the Transformer in a loop, take the last word from the output sequence, append it to the Decoder input and feed it to the Decoder for the next iteration. Finally, when the end-of-sentence token is predicted, the Loss function would compare the generated output sequence to the target sequence in order to train the network.", "Not only would this looping cause training to take much longer, but it also makes it harder to train the model. The model would have to predict the second word based on a potentially erroneous first predicted word, and so on.", "Instead, by feeding the target sequence to the Decoder, we are giving it a hint, so to speak, just like a Teacher would. Even though it predicted an erroneous first word, it can instead use the correct first word to predict the second word so that those errors don\u2019t keep compounding.", "In addition, the Transformer is able to output all the words in parallel without looping, which greatly speeds up training.", "Transformers are very versatile and are used for most NLP tasks such as language models and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.", "There are different flavors of the Transformer architecture for different problems. The basic Encoder Layer is used as a common building block for these architectures, with different application-specific \u2018heads\u2019 depending on the problem being solved.", "A Sentiment Analysis application, for instance, would take a text document as input. A Classification head takes the Transformer\u2019s output and generates predictions of the class labels such as a positive or negative sentiment.", "A Language Model architecture would take the initial part of an input sequence such as a text sentence as input, and generate new text by predicting sentences that would follow. A Language Model head takes the Transformer\u2019s output and generates a probability for every word in the vocabulary. The highest probability word becomes the predicted output for the next word in the sentence.", "RNNs and their cousins, LSTMs and GRUs, were the de facto architecture for all NLP applications until Transformers came along and dethroned them.", "RNN-based sequence-to-sequence models performed well, and when the Attention mechanism was first introduced, it was used to enhance their performance.", "As an aside, with CNNs, all of the outputs can be computed in parallel, which makes convolutions much faster. However, they also have limitations in dealing with long-range dependencies:", "The Transformer architecture addresses both of these limitations. It got rid of RNNs altogether and relied exclusively on the benefits of Attention.", "Now that we have a high-level idea of what a Transformer is, we can go deeper into its internal functionality in the next article to understand the details of how it works.", "And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F95a6dd460452&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47----95a6dd460452---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34", "anchor_text": "How it works"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853", "anchor_text": "Multi-head Attention"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3", "anchor_text": "Why Attention Boosts Performance"}, {"url": "https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24", "anchor_text": "Beam Search"}, {"url": "https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b", "anchor_text": "Bleu Score"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34", "anchor_text": "next"}, {"url": "https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504", "anchor_text": "Audio Deep Learning Made Simple (Part 1): State-of-the-Art TechniquesA Gentle Guide to the world of disruptive deep learning audio applications and architectures. And why we all need to\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc", "anchor_text": "Leveraging Geolocation Data for Machine Learning: Essential TechniquesA Gentle Guide to Feature Engineering and Visualization with Geospatial data, in Plain Englishtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/image-captions-with-deep-learning-state-of-the-art-architectures-3290573712db", "anchor_text": "Image Captions with Deep Learning: State-of-the-Art ArchitecturesA Gentle Guide to Image Feature Encoders, Sequence Decoders, Attention, and Multi-modal Architectures, in Plain Englishtowardsdatascience.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----95a6dd460452---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----95a6dd460452---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----95a6dd460452---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----95a6dd460452---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----95a6dd460452---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----95a6dd460452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----95a6dd460452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F95a6dd460452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----95a6dd460452---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----95a6dd460452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----95a6dd460452--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----95a6dd460452--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----95a6dd460452--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----95a6dd460452--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://ketanhdoshi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fae94feabe1c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-1-overview-of-functionality-95a6dd460452&newsletterV3=54f9ca55ed47&newsletterV3Id=ae94feabe1c9&user=Ketan+Doshi&userId=54f9ca55ed47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}