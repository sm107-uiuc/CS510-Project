{"url": "https://towardsdatascience.com/did-you-know-this-in-spark-sql-a7398bfcc41e", "time": 1683015992.920753, "path": "towardsdatascience.com/did-you-know-this-in-spark-sql-a7398bfcc41e/", "webpage": {"metadata": {"title": "Did you know this in Spark SQL?. 8 non-obvious features in Spark SQL\u2026 | by David Vrba | Towards Data Science", "h1": "Did you know this in Spark SQL?", "description": "The DataFrame API of Spark SQL is user friendly because it allows expressing even quite complex transformations in high-level terms. It is quite rich and mature especially now in Spark 3.0. There are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/docs/latest/api/sql/index.html#array_sort", "anchor_text": "SQL", "paragraph_index": 3}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat", "anchor_text": "concat", "paragraph_index": 5}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf", "anchor_text": "documentation", "paragraph_index": 23}], "all_paragraphs": ["The DataFrame API of Spark SQL is user friendly because it allows expressing even quite complex transformations in high-level terms. It is quite rich and mature especially now in Spark 3.0. There are however some situations in which you might find its behavior unexpected or at least not very intuitive. This might get frustrating especially if you find out that your production pipeline produced results that you didn\u2019t expect.", "In this article, we will go over some features of Spark which are not so obvious at first sight, and by knowing them you can avoid silly mistakes. In some examples, we will also see nice optimization tricks that can become handy depending on your transformations. For the code examples, we will use Python API in Spark 3.0.", "Both of these two functions can be used for sorting the arrays, however, there is a difference in usage and null handling. While array_sort can only sort your data in ascending order, the sort_array takes a second argument in which you can specify whether your data should be sorted in descending or ascending order. The array_sort will place null elements at the end of the array which will also sort_array do when sorting in descending order. But when using sort_array in ascending (default) order the null elements will be placed at the beginning.", "And there is one more option on how to use the function array_sort, namely directly in SQL (or as a SQL expression as argument of the expr() function) where it takes a second argument which is a comparator function (supported since Spark 3.0). With this function, you can define how the elements should be compared to create the order. This actually brings quite powerful flexibility by which you can for example sort an array of structs and define by which struct field it should be sorted. Le\u2019ts see this example where we sort explicitly by the second struct field:", "Here you can see that the comparison function expressed in SQL takes two arguments left and right which are elements of the array and it defines how they should be compared (namely according to the second field f2).", "The concat function can be used for concatenating strings, but also for joining arrays. The less obvious thing is that the function is null-intolerant, which means that if any argument is null, then also the output becomes null. So for example when joining two arrays, we can easily lose the data from one array if the other one is null unless we handle it explicitly, for instance, by using coalesce:", "Aggregation function collect_list which can be used to create an array of elements after grouping by some key is not deterministic because the order of elements in the resulting array depends on the order of rows which may not be deterministic after the shuffle.", "It is also good to know that non-deterministic functions are treated with special care by the optimizer, for example, the optimizer will not push filters through it as you can see in the following query:", "As you can see from the plan, the Filter is the last transformation, so Spark will first compute the aggregation and after that, it will filter out some groups (here we filter out group where user_id is null). It would be however more efficient if the data was first reduced by the filter and then aggregated which will indeed happen with deterministic functions such as count:", "Here the Filter was pushed closer to the source because the aggregation function count is deterministic.", "Besides collect_list, there are also other non-deterministic functions, for example, collect_set, first, last, input_file_name, spark_partition_id, or rand to name some.", "There is a variety of aggregation and analytical functions that can be called over a so-called window defined as follows:", "This window can be also sorted by calling orderBy(key) and a frame can be specified by rowsBetween or rangeBetween. This frame determines over which rows will the function be called inside the window. Some functions require also to sort the window (for example row_count) but for some functions the sort is optional. The point is that the sort can change the frame which might not be intuitive. Consider the example with sum function:", "As you can see, sorting the window will change the frame to be from the beginning up to the current row, so the summing will produce a cumulative sum instead of a total sum. However, if we don\u2019t use the sort, the default frame will be the entire window and summing will produce the total sum.", "Not completely, but if your cached data is based on this table to which someone just appended data (or has it overwritten) then the data will be scanned and cached again once you call another action. Let\u2019s see this example:", "So the unexpected thing here is, that calling the same computation against cached DataFrame can potentially lead to a different result if someone appended the table in the meantime.", "In Spark, there are two types of operations, transformations, and actions, the former are lazy while the latter will materialize the query and run a job. The show() function is an action so it runs a job, the confusing part can be however that sometimes it runs more jobs. Why is this happening? A typical example can be this query:", "Now depending on the properties of your data, the situation can look for instance as in the following image, which is a screenshot from the Spark UI, where we can see that Spark run five jobs before it returned the result:", "Also notice that number of tasks in each of these jobs is different. The first job (with job id = 10) had only one task! The next job run with four tasks, then 20, 100, and finally a job with 75 tasks was the last one. By the way, the cluster on which this was executed had 32 cores available, so 32 tasks could run in parallel.", "The idea behind executing the query in multiple jobs is to avoid processing all input data. The show function returns only 20 rows by default (this can be changed by passing the n argument), so perhaps we can process only one partition of the data to return 20 rows. This is why Spark first runs a job with only one task that will process just one partition of the data hoping to find 20 rows that are required for the output. If Spark doesn\u2019t find these 20 rows, it will launch another job that will process another four partitions (that\u2019s why the second job has four tasks) and so the situation repeats, and Spark in each further job increases number of partitions that are processed until it finds the 20 required rows or all partitions are processed.", "This is an interesting optimization that makes sense especially if your dataset is very large (contains many partitions) and your query is not very selective, so Spark can process really only the first few partitions to find the 20 rows. On the other hand, if your query is very selective and you are for example looking for a single row (which may not even exist) in a very big data set, it might be more useful to use the collect function which will use the full potential of the cluster from the beginning and process all data in one job because, in the end, all partitions will have to be processed anyway (if the record doesn\u2019t exist).", "It is a known fact that user-defined functions (UDFs) are better to be avoided if they are not necessary because they bring some performance penalty (how big the penalty is, depends on whether the UDF is implemented in scala/java or python). What is not so obvious though, is that if the UDF is used, it might be executed more times than expected and so the overhead becomes even bigger. This can be however avoided and thus the overall penalty will be mitigated. Let\u2019s see a simple example:", "In this example, we create a simple UDF that we use to add a new column to the DataFrame and next we filter based on this column. If we check the query plan by calling explain we will see:", "As you can see the BatchEvalPython operator is in the plan twice which means that Spark will execute the UDF twice. Obviously, this is not the most optimal execution plan, especially if the UDF becomes a bottleneck, which is often the case. Fortunately, there is a nice trick how to make Spark call the UDF only once and it is by making the function non-deterministic (see in documentation):", "Now, checking the query plan reveals that the UDF is called only once:", "This is because Spark now thinks that the function is not deterministic, so calling it twice wouldn\u2019t be safe since it could return a different result each time it is called. It is also good to understand that by doing this we are putting some constraints on the optimizer, which will now handle it in a similar way as other non-deterministic expressions, for example, filters will not be pushed through as we have seen with the collect_list function above.", "Not literally. But let me explain what I mean by this. Imagine a situation in which you want to join two tables which are bucketed and you also need to call a UDF on one of the columns. The bucketing will allow you to do the join without a shuffle, but you need to call the transformations in the correct order. Consider the following query:", "Here we are joining two tables which are bucketed on the user_id column to the same number of buckets and we apply a UDF (add_one) on one of the columns from dfA. The plan looks as follows:", "Here as you can see everything is fine because the plan has no Exchange operator and the execution will be shuffle-free, this is exactly what we need and it is because Spark is aware of the distribution of the data and can use it for the join.", "On the other hand, let\u2019s see what happens if we apply the UDF first and do the join after it:", "Now the situation changed and we have two Exchange operators in the plan, which means that Spark will now shuffle both DataFrames before the join. It is because calling the UDF removed the information about the data distribution and Spark now doesn\u2019t know that the data is actually distributed well and it will have to shuffle it to make sure the partitioning is correct. So calling the UDF doesn\u2019t really destroy the distribution but it removes the information about it, so Spark will have to assume that the data is distributed randomly.", "In this article, we went over some examples of Spark features which may not be so obvious, or which are easy to forget when composing the queries. Improper use of some of them can lead to a bug in your code, for example, if you forget that sorting a window will change your frame, or that some functions will produce null if some of the input arguments are nulls (like the concat function). In some examples, we have also seen simple optimization tricks such as that making a UDF non-deterministic can avoid executing it twice, or calling the UDF after the join can save you from the shuffle if the tables are bucketed (or distributed according to some specific partitioning).", "We have also seen that using SQL functions is sometimes more flexible than the DSL functions, one particular example of this is the array_sort which in SQL allows you to specify the comparator function to achieve custom sorting logic.", "Senior ML Engineer at Sociabakers and Apache Spark trainer and consultant. I lecture Spark trainings, workshops and give public talks related to Spark."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa7398bfcc41e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://medium.com/@vrba.dave?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----a7398bfcc41e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7398bfcc41e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=-----a7398bfcc41e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7398bfcc41e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=-----a7398bfcc41e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://spark.apache.org/docs/latest/api/sql/index.html#array_sort", "anchor_text": "SQL"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat", "anchor_text": "concat"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf", "anchor_text": "documentation"}, {"url": "https://medium.com/tag/spark?source=post_page-----a7398bfcc41e---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----a7398bfcc41e---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a7398bfcc41e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----a7398bfcc41e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----a7398bfcc41e---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7398bfcc41e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=-----a7398bfcc41e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7398bfcc41e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=-----a7398bfcc41e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7398bfcc41e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----a7398bfcc41e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83cdb92c0d8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&newsletterV3=b7f216c64e33&newsletterV3Id=83cdb92c0d8c&user=David+Vrba&userId=b7f216c64e33&source=-----a7398bfcc41e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Written by David Vrba"}, {"url": "https://medium.com/@vrba.dave/followers?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----a7398bfcc41e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83cdb92c0d8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdid-you-know-this-in-spark-sql-a7398bfcc41e&newsletterV3=b7f216c64e33&newsletterV3Id=83cdb92c0d8c&user=David+Vrba&userId=b7f216c64e33&source=-----a7398bfcc41e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "David Vrba"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Best practices for caching in Spark SQLDeep dive into data persistence in Spark."}, {"url": "https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "\u00b710 min read\u00b7Jul 20, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb22fb0f02d34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbest-practices-for-caching-in-spark-sql-b22fb0f02d34&user=David+Vrba&userId=b7f216c64e33&source=-----b22fb0f02d34----0-----------------clap_footer----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?source=author_recirc-----a7398bfcc41e----0---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb22fb0f02d34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbest-practices-for-caching-in-spark-sql-b22fb0f02d34&source=-----a7398bfcc41e----0-----------------bookmark_preview----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a7398bfcc41e----1---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----a7398bfcc41e----1-----------------bookmark_preview----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a7398bfcc41e----2---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----a7398bfcc41e----2-----------------bookmark_preview----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "David Vrba"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "Best Practices for Bucketing in Spark SQLThe ultimate guide to bucketing in Spark."}, {"url": "https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": "\u00b721 min read\u00b7Apr 25, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea9f23f7dd53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbest-practices-for-bucketing-in-spark-sql-ea9f23f7dd53&user=David+Vrba&userId=b7f216c64e33&source=-----ea9f23f7dd53----3-----------------clap_footer----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53?source=author_recirc-----a7398bfcc41e----3---------------------cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea9f23f7dd53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbest-practices-for-bucketing-in-spark-sql-ea9f23f7dd53&source=-----a7398bfcc41e----3-----------------bookmark_preview----cfeb5f74_c689_49ca_9c4f_e6284d90ad74-------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "See all from David Vrba"}, {"url": "https://towardsdatascience.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://pierpaoloippolito28.medium.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://pierpaoloippolito28.medium.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Pier Paolo Ippolito"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Apache Spark Optimization TechniquesA review of some of the most common Spark performance problems and how to address them"}, {"url": "https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa7f20a9a2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-optimization-techniques-fa7f20a9a2cf&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=-----fa7f20a9a2cf----0-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa7f20a9a2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-optimization-techniques-fa7f20a9a2cf&source=-----a7398bfcc41e----0-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://julianwest155.medium.com/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://julianwest155.medium.com/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Julian West"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Unit testing PySpark code using PytestWhen it comes to writing unit-tests for PySpark pipelines, writing focussed, fast, isolated and concise tests can be a challenge."}, {"url": "https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b710 min read\u00b7Jan 16"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5ab2fd54415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funit-testing-pyspark-code-using-pytest-b5ab2fd54415&user=Julian+West&userId=257ac04bf615&source=-----b5ab2fd54415----1-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ab2fd54415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funit-testing-pyspark-code-using-pytest-b5ab2fd54415&source=-----a7398bfcc41e----1-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Anuj Syal"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "12 Must-Have Skills to become a Data EngineerThe Essential Skills for a Successful Data Engineering Career"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b78 min read\u00b7Jan 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&user=Anuj+Syal&userId=df3997c527b4&source=-----35b100dbee0a----0-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----a7398bfcc41e----0---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&source=-----a7398bfcc41e----0-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://liamjhartley.medium.com/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://liamjhartley.medium.com/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Liam Hartley"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "The Data Engineering Interview GuideThe best preparation tool for your Data Engineering interview with questions and answers by a former Data Engineering Manager"}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b712 min read\u00b7Dec 27, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2F710b621f3ff1&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fthe-data-engineering-interview-guide-710b621f3ff1&user=Liam+Hartley&userId=1ad19b8961a6&source=-----710b621f3ff1----1-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----a7398bfcc41e----1---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F710b621f3ff1&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fthe-data-engineering-interview-guide-710b621f3ff1&source=-----a7398bfcc41e----1-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a7398bfcc41e----2---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----a7398bfcc41e----2-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Tony Lui"}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "Applying Custom Functions in PySparkHow to Use Spark UDFs and Row-wise RDD Operations"}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": "\u00b75 min read\u00b7Nov 10, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F337d63bf32a5&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40antoniolui%2Fapplying-custom-functions-in-pyspark-337d63bf32a5&user=Tony+Lui&userId=a2544693db86&source=-----337d63bf32a5----3-----------------clap_footer----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----a7398bfcc41e----3---------------------82f3bb93_bb77_483b_badf_93a0e8768075-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F337d63bf32a5&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40antoniolui%2Fapplying-custom-functions-in-pyspark-337d63bf32a5&source=-----a7398bfcc41e----3-----------------bookmark_preview----82f3bb93_bb77_483b_badf_93a0e8768075-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----a7398bfcc41e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}