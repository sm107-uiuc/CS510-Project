{"url": "https://towardsdatascience.com/generate-fake-donald-trump-tweets-using-python-8d83885fd5c6", "time": 1683007431.857136, "path": "towardsdatascience.com/generate-fake-donald-trump-tweets-using-python-8d83885fd5c6/", "webpage": {"metadata": {"title": "Generate Fake Donald Trump Tweets using Python | by Thomas Hikaru Clark | Towards Data Science", "h1": "Generate Fake Donald Trump Tweets using Python", "description": "Let\u2019s use machine learning to generate fake tweets that emulate Donald Trump\u2019s language and style! A lot of introductory machine learning tutorials focus on classification tasks such as MNIST, so I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos126/assignments/markov/checklist.html", "anchor_text": "assignment in Intro CS", "paragraph_index": 3}, {"url": "https://github.com/thomashikaru/faketrumptweets", "anchor_text": "here", "paragraph_index": 7}, {"url": "http://twitter.com/NHC_Atlantic", "anchor_text": "@NHC_Atlantic", "paragraph_index": 13}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "this tutorial", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9", "anchor_text": "here", "paragraph_index": 23}, {"url": "http://t.co/AhYnMlbaement", "anchor_text": "http://t.co/AhYnMlbaement", "paragraph_index": 29}, {"url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "anchor_text": "This great article", "paragraph_index": 33}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks", "paragraph_index": 35}, {"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos126/assignments/markov/index.html", "anchor_text": "Markov Model of Natural Languag", "paragraph_index": 36}], "all_paragraphs": ["Let\u2019s use machine learning to generate fake tweets that emulate Donald Trump\u2019s language and style! A lot of introductory machine learning tutorials focus on classification tasks such as MNIST, so I hope you enjoy this article about generative models.", "We will first use a relatively simple approach for this task, called a Markov Model, and we will pretty much code it from scratch (the only library I used was NumPy). Then we will take a look at a more advanced Recurrent Neural Network implementation. My hope is that first working through the simpler example of the Markov Model will make it easier to appreciate how the RNN model works.", "The Trump Twitter Archive contains over 30k tweets by Donald Trump. Visit the website to download tweets from a given date range as a CSV or JSON file. There is an option to download the metadata of each tweet (number of likes, retweets, timestamp, etc.), but for now, we just need the raw text.", "A Markov Model looks at n-grams (substrings of n characters) in the text, and tallies how often various letters follow each n-gram when it occurs in the training text. This is a rather simplistic representation of language, and it\u2019s easy enough to program that doing this in Java was an assignment in Intro CS when I was in college.", "To understand how this works in greater depth, let\u2019s say that our training text consists of a random large sample of English, using an n-gram size of 3. The Markov Model, once built, should store a dictionary where the keys are n-grams (all 3-character substrings that occurred in the text) and the values are mappings from characters to frequencies (the number of times that the given character followed the given n-gram). For example, if the model were provided with the n-gram \u201cmac\u201d, then the model would know that the probability of the next letter being \u201ch\u201d (words like \u201cmachine\u201d or \u201cmachete\u201d) is higher than the probability of the next letter being \u201cz\u201d (I cant think of any words that contain \u201cmacz\u201d). On the other hand, given the n-gram \u201cpiz\u201d, the model should give a higher probability to the next letter being \u201cz\u201d (words like \u201cpizza\u201d and \u201cpizzeria\u201d) than the next letter being \u201ch\u201d.", "Training this on a language corpus will incorporate features of the training data into the model. If you train it on Shakespeare, it will pick up Shakespearean vocabulary, expressions, and the formatting typical of a Shakespearean play). If you train it on a Twitter dataset, you will see hashtags, Twitter handles, URLs, etc.", "The larger the n-gram size, the more \u201crealistic\u201d the output text will be. Having a short small value for n is like someone who forgets how they started a sentence by the time they reach the end. At the same time, having a large value of n will make the model more likely to reproduce exact words and phrases from the original text, instead of producing new sentences.", "Let\u2019s jump right into the code for the Markov Model (the full code can be found here). First, we read in and lightly pre-process the Trump tweets. We create a single massive string by joining all the tweets together and getting rid of newlines. We will preserve capitalization, since this can often be meaningful in the context of a tweet.", "Next, we must define the MarkovModel class. Its constructor will take the n-gram size n and the training text. We then move a sliding window of size n across the text and build a dictionary where each n-gram maps to its own dictionary, which contains mappings from characters to counts. As we slide the window, we take the character immediately following the current n-gram and update the count corresponding to that character in the given n-gram\u2019s dictionary. The more times you see a \u2018h\u2019 after the n-gram \u2018mac\u2019, the higher the count of \u2018h\u2019 will be in the \u2018mac\u2019 n-gram\u2019s dictionary. Explaining this verbally is a bit confusing, so let\u2019s try it in visual form.", "The image above illustrates how the model is built from the training text. In this example, the n-gram size is 3 (the number of green characters). Each n-gram\u2019s dictionary of character-counts is updated based on the following character (highlighted in red). The visual below explains the data structure that we are trying to create. Of course, after passing through the entire training text, this will be a huge dictionary!", "The Markov Model has a get_next_char() method that takes an n-gram and chooses a next character according to the probability distribution it learned in the constructor phase. Of course, we don\u2019t want the method to always return the same character for the same n-gram, so there has to be an element of randomness, but it should follow the weights learned from the text. The NumPy function numpy.random.choice() is a big helper here.", "To generate a longer string, we repeatedly call the get_next_char() method. Each time the method returns a character, we chop off the first character from the n-gram and append the returned character.", "Here are some examples of the model\u2019s outputs (I provide it with the first n characters, and it fills in the rest).", "Hillary Clinton has been working so hard and so seriously wounded. We continue to produce the following heights above grou RT @NHC_Atlantic: As hazardous conditions arrive along the Gulf Coast. Please be sure to vote for Obama. He doesnt need any help \u2014 but I am giving to various charities.", "Hillary and totally Very importantly lifted a full hour. Tomorrow at noon. I\u2019ll be better. Join me in California in two minutes tonight by the Dems) dont have to existing corrupt! DJT Still ZERO evidence in that gas is for TT help \u2014 but we are ONE movement inquiry in so little Marco Rub", "It\u2019s cool to see that the model picks up on all sort of features of language that are not hard-coded into the system in any way. It\u2019s just looking at the statistical distribution of characters after n-grams, and yet it\u2019s able to correctly spell words (most of the time), use punctuation and capitalization, and sometimes even create pseudo-grammatical sentences.", "One disadvantage of this approach is that, especially with a small training text, this method can result in replicating long strings of content from the source text, rather than creating novel combinations. Ideally, we want the model to pick up language features like spelling and grammar but produce new strings of words, not simply regurgitate long chunks (i.e. several words at a time) of the training data. Another downside of the Markov Model is that the starting string that we provide to the generate method must be an n-gram found within the training text. If you feed it an n-gram that it has never seen before, then it won\u2019t know how to handle it. The next section will look at a more advanced model for generating text.", "Recurrent Neural Networks (RNNs) have attracted a lot of attention for their ability to solve complex machine learning problems. RNNs fundamentally differ from other types of neural networks, such as simple feed-forward networks or Convolutional Neural Networks, because the current state of the network is \u201cremembered\u201d and fed in as an input to the next time step. See the article below for some fascinating background and examples.", "The first thing we need to do is determine the vocabulary \u2014 the set of unique characters in the training text. If you include uppercase letters, lowercase letters, numerals, whitespace, and special characters, this brings us to about 90 unique characters. The text then must be converted from a string of characters to a list of numbers, where there is a one-to-one mapping between numbers and characters. The string \"abcba\" might become something like [1, 2, 3, 2, 1] .", "Each training example will consist of an input sequence and a target sequence, each of which has n letters. The target sequence is simply the input sequence with the first character chopped off and the next character added. If the original text contains the sequence \"Donald\" , then we could form an input-target pair (\"Donal\", \"onald\") . Seeing lots of these pairs will allow the model to learn what characters typically follow the sequence \"Donal\" , and hopefully any other sequence.", "I used Tensorflow and Keras to create the RNN, following the structure used in this tutorial from the Tensorflow website. The network consists of the following layers: a word embedding layer, a GRU layer, and a fully connected (\u201cDense\u201d) layer.", "The embedding layer takes the input character sequences, which we translated into numerical values, and embeds them in a high-dimensional vector space. Each character is therefore mapped first to an integer (index in the vocabulary) and then each integer is mapped to a vector (in my code, a vector with 64 dimensions).", "Why do we do this? The character embedding is important because it creates more meaningful numerical representations of the input than just using indices. Saying that the letter \u2018a\u2019 corresponds to the number 1 and the letter \u2018z\u2019 corresponds to the number 26 is totally arbitrary. Ideally, characters that appear in similar contexts should have similar vectors. The embedding layer allows the model to learn good vector representations of the vocabulary characters during training.", "A GRU is a variation on the basic RNN architecture that has been applied very effectively to many natural language tasks. See the great articles on RNNs, LSTMs, and GRUs here and here, which do a fantastic job of explaining how and why they work.", "In order to train this model, we need to attach an optimizer and loss function to our model. The loss function computes the categorical cross-entropy loss between the predicted output and the correct target sequence, as defined in our training samples we made earlier. Note that the output of the model is a sequence of probabilities (well, technically the log-odds) for each character in the vocabulary. The loss function should reward the model for assigning high log-odds to the correct character and penalize the model for assigning high log-odds to the wrong character.", "After that\u2019s done, we can set up a checkpoint directory and get started with training! I experimented with both my laptop\u2019s CPU and the free GPU provided on Google Colab. The GPU trained around 10x faster. The Tensorflow tutorial recommended training for around 30 epochs to get decent results.", "Once the model has been trained, we use a similar process as with the Markov Model to generate fake tweets. Like before, we need to provide a start string, although this can be any length and does not necessarily need to be a substring that exists in the training text. The model is evaluated repeatedly, each time generating one new character. This one new character is then fed in as input to the model, which still \u201cremembers\u201d the text it saw in the previous step. At the end, we print the concatenation of all the generated characters.", "Here are some sample outputs from the RNN (I provide the starting string and it fills in the rest):", "America has reached an agency which who has done a very big trade deal changes in the men & which will\u2026. New York has to know this book but it is about him. For are going to find any protests claim. Off the new and we given to all who participated bring USA new book Billion Dollars throw about a proud bad. Sad!", "Hillary Clinton away night on CNBCNBC'S TODY-RESSUDEMINTWe must testing on welconoe of Kail Cord on 2 million billions would have built.Russia the dumbest anger and funding.@BassuckstManniseal & @DNSome1z @CNN Find Links person is definiteed Trump Int'l Good Morning America\" http://t.co/AhYnMlbaement", "Considering how much more complicated the RNN is compared to the Markov Model, the results do not seem that impressive. It is important to keep in mind, however, that RNNs are often trained on hundreds of megabytes of data, far more than the ~30k Trump tweets in this training set. Additionally, you can potentially achieve better results by tuning hyper-parameters or adding multiple RNN layers into the model. You can experiment with different values for the sequence length, number of dimensions for the embedding, or number of GRUs.", "One downside of RNNs is that that they are quite slow to train. For many practical purposes, a GPU is necessary to achieve reasonable training times, and even then require some patience. Consider using Google Colab, which includes limited free access to GPUs, for experimenting with RNNs. You can train in the cloud and keep your machine from overheating!", "If you are trying to learn some of these topics on your own, I recommend finding a mini-project like this one that you can really dive into and enjoy. It will help you stay motivated and you\u2019ll learn more from getting hands-on with your own project than from passively reading or watching tutorials.", "In a more rigorous setting, we may have wanted to evaluate our generator using some sort of metric, but this would actually be quite challenging. Evaluating the quality of text output is much more complex than evaluating a classifier, where you just need to compare predictions to labels. This great article discusses some of the metrics used to evaluate text output in NLP.", "Thanks for reading! Feel free to leave some feedback, and best of luck in your learning!", "[1] Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks (2015).", "[3] Princeton University Department of Computer Science, Markov Model of Natural Language (2020).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8d83885fd5c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://thomashikaru.medium.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Thomas Hikaru Clark"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe806fec87c25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=post_page-e806fec87c25----8d83885fd5c6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@stereophototyp?utm_source=medium&utm_medium=referral", "anchor_text": "Sara Kurfe\u00df"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www.trumptwitterarchive.com/archive", "anchor_text": "Trump Twitter ArchiveAll 30,000+ of Trump's tweets, instantly searchablewww.trumptwitterarchive.com"}, {"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos126/assignments/markov/checklist.html", "anchor_text": "assignment in Intro CS"}, {"url": "https://github.com/thomashikaru/faketrumptweets", "anchor_text": "here"}, {"url": "http://twitter.com/NHC_Atlantic", "anchor_text": "@NHC_Atlantic"}, {"url": "https://github.com/thomashikaru/faketrumptweets", "anchor_text": "https://github.com/thomashikaru/faketrumptweets"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural NetworksThere's something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent\u2026karpathy.github.io"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "this tutorial"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9", "anchor_text": "here"}, {"url": "http://t.co/AhYnMlbaement", "anchor_text": "http://t.co/AhYnMlbaement"}, {"url": "https://github.com/thomashikaru/faketrumptweets", "anchor_text": "https://github.com/thomashikaru/faketrumptweets"}, {"url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "anchor_text": "This great article"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "Text generation with an RNN"}, {"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos126/assignments/markov/index.html", "anchor_text": "Markov Model of Natural Languag"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "Illustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanation"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9", "anchor_text": "Illustrated Guide to Recurrent Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8d83885fd5c6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8d83885fd5c6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----8d83885fd5c6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/donald-trump?source=post_page-----8d83885fd5c6---------------donald_trump-----------------", "anchor_text": "Donald Trump"}, {"url": "https://medium.com/tag/twitter?source=post_page-----8d83885fd5c6---------------twitter-----------------", "anchor_text": "Twitter"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=-----8d83885fd5c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=-----8d83885fd5c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8d83885fd5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8d83885fd5c6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8d83885fd5c6--------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Hikaru Clark"}, {"url": "https://thomashikaru.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "352 Followers"}, {"url": "https://podcasts.apple.com/us/podcast/modus-mirandi-podcast-with-thomas-hikaru-clark/id1551675175?uo=4", "anchor_text": "https://podcasts.apple.com/us/podcast/modus-mirandi-podcast-with-thomas-hikaru-clark/id1551675175?uo=4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe806fec87c25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=post_page-e806fec87c25--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F70b012cf892f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerate-fake-donald-trump-tweets-using-python-8d83885fd5c6&newsletterV3=e806fec87c25&newsletterV3Id=70b012cf892f&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}