{"url": "https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a", "time": 1683003545.927659, "path": "towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a/", "webpage": {"metadata": {"title": "Preparing the data for Transformer pre-training \u2014 a write-up | by Steven van de Graaf | Towards Data Science", "h1": "Preparing the data for Transformer pre-training \u2014 a write-up", "description": "Introduced only a little over a year ago, the best-known incarnation of the Transformer model introduced by Vaswani et al. (2017), the Bidirectional Encoder Representations from Transformers (better\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/google-research/bert", "anchor_text": "repository", "paragraph_index": 0}, {"url": "https://github.com/huggingface/transformers/", "anchor_text": "Transformers library", "paragraph_index": 0}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace", "paragraph_index": 0}, {"url": "https://www.smashwords.com/", "anchor_text": "Smashwords,", "paragraph_index": 2}, {"url": "https://dumps.wikimedia.org/", "anchor_text": "Wikipedia dumps", "paragraph_index": 3}, {"url": "https://github.com/huggingface/tokenizers", "anchor_text": "Tokenizers library", "paragraph_index": 5}, {"url": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1506.06724", "anchor_text": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "paragraph_index": 8}], "all_paragraphs": ["Introduced only a little over a year ago, the best-known incarnation of the Transformer model introduced by Vaswani et al. (2017), the Bidirectional Encoder Representations from Transformers (better known as BERT) model by Devlin et al. (2018) has since become hugely popular within the Natural Language Processing (NLP) community. This is due not only due to its state-of-the-art results on a wide array of NLP tasks, but also due to it being available publicly through their own repository, but also the increasingly popular Transformers library by HuggingFace.", "BERT and other Transformers like RoBERTa, XLM and XLNet are pre-trained on (among others) a concatenation of the Toronto BookCorpus dataset (Zhu et al., 2015) and English Wikipedia. As the former is no longer publicly available, and the latter requires significant pre-processing, this write-up presents a complete picture (and code) of everything involved in collecting, concatenating, shuffling and tokenizing the data needed required to experiment with these Transformer models yourself.", "As mentioned previously, the Toronto BookCorpus (TBC) dataset by Zhu et al. (2015) is no longer publicly available. Smashwords, the source of the dataset, is still available, however. As such, you can replicate the TBC dataset yourself following this previous write-up of mine below:", "While Wikipedia dumps are freely available, they still need to be extracted, cleaned and pre-processed before they are ready to be used for Transformer pre-training. As such, similar to the TBC dataset, you can do so yourself following another previous write-up of mine below:", "Now that you have collected both the Toronto BookCorpus dataset and English Wikipedia, we can proceed to concatenate and shuffle the two. This can be accomplished quite simply in Bash or similar using the code below:", "With the data collected, concatenated and shuffled, all that is left now is to tokenize the data. In an ideal world, we would tokenize our data on-the-fly, but for performance reasons, we pre-tokenize our data instead. For this, we use the recently released, blazingly fast Tokenizers library, also by HuggingFace via the code below:", "To tokenize the data, first download the vocab file here and then simply run the following command in your terminal: python3 tokenize_data.py concat_shuffled.txt bert-base-uncased-vocab.txt", "And that\u2019s it, you\u2019re done! You can now start experimenting with these Transformer models yourself, using the data you\u2019ve freshly prepared. \ud83e\udd17", "[3] Zhu et al., Aligning books and movies: Towards story-like visual explanations by watching movies and reading books (2015), Proceedings of the IEEE international conference on computer vision (pp. 19\u201327).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Graduate student in Artificial Intelligence @UvA_Amsterdam with multiple years of experience in Python and VBA development."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F67a9dc0cae5a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sgraaf?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Steven van de Graaf"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2853d5aeff85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=post_page-2853d5aeff85----67a9dc0cae5a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT paper"}, {"url": "https://github.com/google-research/bert", "anchor_text": "repository"}, {"url": "https://github.com/huggingface/transformers/", "anchor_text": "Transformers library"}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace"}, {"url": "https://www.smashwords.com/", "anchor_text": "Smashwords,"}, {"url": "https://towardsdatascience.com/replicating-the-toronto-bookcorpus-dataset-a-write-up-44ea7b87d091", "anchor_text": "Replicating the Toronto BookCorpus dataset \u2014 a write-upReplicating the no-longer publicly available Toronto BookCorpus dataset by collecting and pre-processing books from its\u2026towardsdatascience.com"}, {"url": "https://dumps.wikimedia.org/", "anchor_text": "Wikipedia dumps"}, {"url": "https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67", "anchor_text": "Pre-processing a Wikipedia dump for NLP model training \u2014 a write-upDownloading, extracting, cleaning and pre-processing a Wikipedia dump for NLP model (e.g. transformers like BERT\u2026towardsdatascience.com"}, {"url": "https://github.com/huggingface/tokenizers", "anchor_text": "Tokenizers library"}, {"url": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Bert: Pre-training of deep bidirectional transformers for language understanding"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/abs/1506.06724", "anchor_text": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----67a9dc0cae5a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----67a9dc0cae5a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----67a9dc0cae5a---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/data?source=post_page-----67a9dc0cae5a---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=-----67a9dc0cae5a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=-----67a9dc0cae5a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F67a9dc0cae5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----67a9dc0cae5a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----67a9dc0cae5a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steven van de Graaf"}, {"url": "https://medium.com/@sgraaf/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "41 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2853d5aeff85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=post_page-2853d5aeff85--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F27561262aa3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a&newsletterV3=2853d5aeff85&newsletterV3Id=27561262aa3f&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}