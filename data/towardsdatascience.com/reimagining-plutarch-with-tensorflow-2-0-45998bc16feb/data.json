{"url": "https://towardsdatascience.com/reimagining-plutarch-with-tensorflow-2-0-45998bc16feb", "time": 1683000484.279588, "path": "towardsdatascience.com/reimagining-plutarch-with-tensorflow-2-0-45998bc16feb/", "webpage": {"metadata": {"title": "Reimagining Plutarch with Tensorflow 2.0 | by Almis Povilaitis | Towards Data Science", "h1": "Reimagining Plutarch with Tensorflow 2.0", "description": "Plutarch\u2019s Lives of the Noble Greeks and Romans, also called Parallel Lives or just Plutarch\u2019s Lives, is a series of biographies of famous Ancient Greeks and Romans, from Theseus and Lycurgus to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Parallel_Lives", "anchor_text": "Plutarch\u2019s Lives of the Noble Greeks and Romans", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Theseus", "anchor_text": "Theseus", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Lycurgus_of_Sparta", "anchor_text": "Lycurgus", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Mark_Antony", "anchor_text": "Marcus Antonius", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68", "anchor_text": "In the recently published article", "paragraph_index": 1}, {"url": "https://colab.research.google.com/notebooks/welcome.ipynb", "anchor_text": "Google Colab", "paragraph_index": 2}, {"url": "https://github.com/mlai-demo/TextExplore", "anchor_text": "Github repository of mine", "paragraph_index": 2}, {"url": "https://www.gutenberg.org/ebooks/674", "anchor_text": "available by Project Gutenberg", "paragraph_index": 3}, {"url": "https://www.tensorflow.org/install/", "anchor_text": "upload the latest TensorFlow", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Regular_expression", "anchor_text": "regular expression", "paragraph_index": 5}, {"url": "https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb", "anchor_text": "my Github repository", "paragraph_index": 16}, {"url": "https://www.tensorflow.org/beta/tutorials/text/word_embeddings", "anchor_text": "nice tutorial provided by TensorFlow", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68", "anchor_text": "CBOW model we had previously trained", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "Long Short-Term Memory", "paragraph_index": 29}, {"url": "https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb", "anchor_text": "my Github file", "paragraph_index": 29}, {"url": "https://projector.tensorflow.org/", "anchor_text": "TensorFlow\u2019s Projector", "paragraph_index": 31}], "all_paragraphs": ["Plutarch\u2019s Lives of the Noble Greeks and Romans, also called Parallel Lives or just Plutarch\u2019s Lives, is a series of biographies of famous Ancient Greeks and Romans, from Theseus and Lycurgus to Marcus Antonius.", "In the recently published article we looked into training our own word embeddings using gensim library. Here, we\u2019ll primarily focus on the word embeddings layer leveraging the TensorFlow 2.0 platform; the intent is to better understand how the layer works and how it contributes to the success of the larger NLP models.", "To help with easy replications, I have adapted the code to Google Colab, and highlighted what is unique to the platform \u2014 otherwise the entire code can be run on your local machine using Python 3.6+ and relevant packages. The code is presented throughout the article but I will skip some supplementary or minor code \u2014 the entire code can be found in a Github repository of mine.", "The text used in this analysis has been made available by Project Gutenberg.", "On Colab, let\u2019s change the Runtime Type to GPU, then import the latest TensorFlow version \u2014 this snippet below will only work on Colab, otherwise simply use pip or conda install commands to upload the latest TensorFlow on your machine.", "We will also need the OS and regular expression libraries, and then save & print the file path for future reference:", "Let\u2019s import the text (Plutarch.txt) into the Google Colab drive \u2014 we need to keep in mind that our files there are ephemeral and we\u2019ll need to upload them every time after taking a longer break from using the platform:", "The code above is also available under the Code Snippets tab in Colab \u2014 among many other very useful ones. When executing this code we\u2019ll see Colab uploading the file and then we can click on the Colab Files tab on the left to make sure the file is there along with the Google\u2019s default Sample Data directory.", "Let\u2019s read the text and do some basic regex operations:", "Since we\u2019ll be splitting the text into sentences, new line has no meaning for our analysis. Also, while using the text tokenizer I noticed that having \u201c\\r\u201d (which signifies the carriage return) creates false unique words, such as \u201cwe\u201d and \u201cwe\\r\u201d \u2014 again, not important in our case. Hence both \u201c\\n\u201d and \u201c\\r\u201d need to go.", "As we ramp up towards actual word embeddings, let\u2019s tokenize the text into sentences:", "We will see that the text has a total of 16,989 sentences. Next, we need to calculate the number of words in the longest sentences \u2014 the reason will become evident later in the tutorial:", "It turns out the longest sentence is 370 words long. Next, let\u2019s convert the entire text into positive numbers so that we can start speaking a common language with TensorFlow:", "From the above we also find out that the text has 20241 unique words, as the tokenizer assigns only one number per same word. To standardize the lengths of all of the sentences (i.e. make the input data into a single, same shape tensor to make it processable / easier for the model\u2014 we are here to serve the machines\u2019 needs), we\u2019ll need to convert the list of numbers representing the words (sent_numeric) into an actual dictionary (word_index), and add padding. We could also combine truncating very long sentences with padding the short ones, but in this case we\u2019ll just pad up to the longest sentence\u2019s length.", "Vocabulary size (a.k.a. the number of unique words) will go up by 1, to 20,242 as a result of adding the 0 for padding. Type \u201cdata[0]\u201d (i.e. the first sentence) to see what the first sentence will look like with the padding.", "To be able to translate back and forth between the words and their numeric representations, we\u2019ll need to add the reverse word index for look-ups:", "It makes sense to double-check the word indexing and conversion \u2014 a single mistake will likely throw the whole dataset off to make it incomprehensible. Examples of cross-checking \u2014 before and after the conversion \u2014 are available in my Github repository.", "Finally, let\u2019s build and run the model. There is a nice tutorial provided by TensorFlow which we are adapting to our needs.", "But first, we can simply run the embedding layer only, which will produce an embeddings\u2019 array. I have read that such an array could be saved and used in another model \u2014 yes, it can, but outside of skipping the embedding step in the new model, I am not so sure of the utility, as the vectors generated for each word are agnostic to the problem being solved:", "We will not spend much time on the above and will rather focus on the models where embedding is just the first part.", "Let\u2019s move on and construct the new, very basic model architecture after importing relevant libraries:", "The embedding layer \u2014 which can typically be used as the first layer in a model \u2014 will convert sequences of numerically encoded unique words (as a reminder, 20,241 of them plus padding coded as a zero) into sequences of vectors, the latter being learned as the model trains . Each vector will have 100 dimensions (embedding_dim=100), hence we\u2019ll have a matrix of 20242 x 100 as a result.. The input length will be fixed to the length of the longest sentence, i.e. 370 words, as every single word is perceived by the model to have the same size due to padding. Mask_zero informs the model whether the input value 0 is a special padding value that should be masked out, which is particularly useful in recurrent layers where variable input lengths can be processed by the model.", "After training on enough meaningful data words with similar meanings will likely have similar vectors.", "Here is the model summary (the model with an additional dense layer is in the github repository):", "In the model summary we\u2019ll see that the number of parameters for the embedding layer is 2,024,200, which is 20,242 words times the embedding dimension of 100.", "The previously mentioned TensorFlow tutorial is using a reviews dataset with each of the reviews being labeled 1 or 0 depending on the positive or negative sentiment. We do not have the labeling luxury but still want to test drive this model, so will simply create an array of 0s and attach to each of the sentences; the model requires such a structure. This will not be the first or the last time that machine intelligence gets assaulted with an unsolvable task yet still obliges us with a solution. Let\u2019s train this model:", "The embeddings are trained. Before we turn to visualization, let\u2019s quickly check on gensim for word similarities. First, we need to create the vectors\u2019 file \u2014 keep it temporarily in Colab or download to the local machine:", "Finally, let\u2019s check on similarity between Pompey and Caesar, which showed high in the CBOW model we had previously trained:", "The relationship between the words is high. Also, as one would expect, Caesar shows highly similar to Rome.", "For those interested in more complex models, additional variants, including Recurrent Neural Networks (Long Short-Term Memory) are available in my Github file, but keep in mind they will train much slower than the simple model above.", "For the embeddings\u2019 visualization, it is hard to beat the TensorFlow Projector, so let\u2019s create the vector and meta (i.e. words corresponding to those vectors) files for its use:", "Import the files locally and then we can go to TensorFlow\u2019s Projector, upload the files to replace the default data, and try various options available on the site. Here is the Principal Components Analysis view of the entire vector space for the text:", "And here is just the vector space for 100 words that showed up as most similar to \u201cRome\u201d.", "In this article, we briefly looked at the role of the word embedding layer in a deep learning model. In the context of such a model, the layer supports solving a particular NLP task \u2014 e.g. text classification \u2014 and through iterations trains the word vectors to be the most conducive in minimizing the model loss. Once the model is trained, we can inspect the embedding layer output through similarity calculations and visualizations.", "The embedding layer can also be used to load pre-trained word embeddings (e.g. GloVe, BERT, FastText, ELMo), which I believe would typically be a more productive way to utilize models requiring such embeddings \u2014 in part due to the \u201cindustrial grade\u201d effort and data size required to generate them. However, in cases of specialized text and especially if the corpus on which the word embeddings can be trained is sizeable, training own embeddings can still be more effective.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data-Driven Insights & Strategy. \u2764\ufe0f Lifelong learning and Statistics / ML / AI / Data Science. All views are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F45998bc16feb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----45998bc16feb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@almis?source=post_page-----45998bc16feb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@almis?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Almis Povilaitis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3b78e6bb0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&user=Almis+Povilaitis&userId=f3b78e6bb0cf&source=post_page-f3b78e6bb0cf----45998bc16feb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Parallel_Lives", "anchor_text": "Plutarch\u2019s Lives of the Noble Greeks and Romans"}, {"url": "https://en.wikipedia.org/wiki/Theseus", "anchor_text": "Theseus"}, {"url": "https://en.wikipedia.org/wiki/Lycurgus_of_Sparta", "anchor_text": "Lycurgus"}, {"url": "https://en.wikipedia.org/wiki/Mark_Antony", "anchor_text": "Marcus Antonius"}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68", "anchor_text": "In the recently published article"}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68", "anchor_text": "Reimagining Plutarch with NLP: Part 2Plutarch\u2019s Lives of the Noble Greeks and Romans through Natural Language Processing; this part includes word2vec\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-1-24e82fc6556", "anchor_text": "Reimagining Plutarch with NLP: Part 1Plutarch\u2019s Lives of the Noble Greeks and Romans through Natural Language Processing; this part includes NLTK and word\u2026towardsdatascience.com"}, {"url": "https://colab.research.google.com/notebooks/welcome.ipynb", "anchor_text": "Google Colab"}, {"url": "https://github.com/mlai-demo/TextExplore", "anchor_text": "Github repository of mine"}, {"url": "https://www.gutenberg.org/ebooks/674", "anchor_text": "available by Project Gutenberg"}, {"url": "https://www.tensorflow.org/install/", "anchor_text": "upload the latest TensorFlow"}, {"url": "https://en.wikipedia.org/wiki/Regular_expression", "anchor_text": "regular expression"}, {"url": "https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb", "anchor_text": "my Github repository"}, {"url": "https://www.tensorflow.org/beta/tutorials/text/word_embeddings", "anchor_text": "nice tutorial provided by TensorFlow"}, {"url": "https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68", "anchor_text": "CBOW model we had previously trained"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "Long Short-Term Memory"}, {"url": "https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb", "anchor_text": "my Github file"}, {"url": "https://projector.tensorflow.org/", "anchor_text": "TensorFlow\u2019s Projector"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----45998bc16feb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----45998bc16feb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tensorflow2?source=post_page-----45998bc16feb---------------tensorflow2-----------------", "anchor_text": "Tensorflow2"}, {"url": "https://medium.com/tag/nlp?source=post_page-----45998bc16feb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----45998bc16feb---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&user=Almis+Povilaitis&userId=f3b78e6bb0cf&source=-----45998bc16feb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&user=Almis+Povilaitis&userId=f3b78e6bb0cf&source=-----45998bc16feb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F45998bc16feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----45998bc16feb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----45998bc16feb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----45998bc16feb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----45998bc16feb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----45998bc16feb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----45998bc16feb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@almis?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@almis?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Almis Povilaitis"}, {"url": "https://medium.com/@almis/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "91 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3b78e6bb0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&user=Almis+Povilaitis&userId=f3b78e6bb0cf&source=post_page-f3b78e6bb0cf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F252b6daa8a45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freimagining-plutarch-with-tensorflow-2-0-45998bc16feb&newsletterV3=f3b78e6bb0cf&newsletterV3Id=252b6daa8a45&user=Almis+Povilaitis&userId=f3b78e6bb0cf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}