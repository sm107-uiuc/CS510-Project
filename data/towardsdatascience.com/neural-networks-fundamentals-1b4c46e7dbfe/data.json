{"url": "https://towardsdatascience.com/neural-networks-fundamentals-1b4c46e7dbfe", "time": 1683000775.843617, "path": "towardsdatascience.com/neural-networks-fundamentals-1b4c46e7dbfe/", "webpage": {"metadata": {"title": "Neural Networks | Fundamentals. Here is an article in which I will try\u2026 | by Maxence Fuzellier | Towards Data Science", "h1": "Neural Networks | Fundamentals", "description": "An artificial neural network (ANN) is a series of algorithms that aim at recognizing underlying relationships in a set of data through a process that mimics the way the human brain operates. Such a\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["An artificial neural network (ANN) is a series of algorithms that aim at recognizing underlying relationships in a set of data through a process that mimics the way the human brain operates. Such a system \u201clearns\u201d to perform tasks by analysing examples, generally without being programmed with task-specific rules.", "Neural networks are organized in various layers:", "The perceptron is the first and simplest neural network model, a supervised learning algorithm invented in 1957 by Frank Rosenblatt, a notable psychologist in the field of artificial intelligence.", "This network is said to be simple because it only has two layers: an input layer and an output layer. This structure involves only one matrix of weights and all the units of the input layer are connected to the output layer\u2019s.", "The perceptron is a linear classifier for binary predictions, in other words, it can classify or separate the data into two categories.", "First of all, the simple perceptron takes n input values (x1, x2, \u2026, xn) and is also defined by n+1 constants:", "Each input value must then be multiplied by its respective weight (wixi), and the result of each of these products must be added to obtain a weighted sum. The neuron will then generate one of two possible values, determined by the fact that the result of the sum is lower or higher than the threshold \u03b8.", "The weighted sum can be transformed into a dot product of two vectors, w (weight) and x (input), where w\u22c5 x = \u2211wixi, then the inequality can be resolved by moving \u03b8 (threshold) to the other side.", "Also, remember that the bias is like a neuron in which the activation function is equal to 1, so the product of this last with its weight (threshold) means multiplying the threshold by 1. Consequently, the common notation involves replacing the threshold by a variable b (for bias), where b =\u2212\u03b8.", "Completing all these steps produces the following architecture:", "Once the weighted sum is obtained, it is necessary to apply an activation function. A simple perceptron uses the Heaviside step function to convert the resulting value into a binary output, classifying the input values as 0 or 1.", "The Heaviside step function is particularly useful in classification tasks where input data is linearly separable.", "The Multi-layer Perceptron (MLP) consists of an input layer, an output layer and one or more hidden layers. So, it is no longer a neural network like the simple perceptron, but neural networks in plural form. If the MLP has n layers, then it has n-1 weight matrices.", "Theoretically, adding sufficient number of neurons to the hidden layer may be enough to approximate any non-linear function.", "A neural network generates a prediction after passing all inputs through all layers, up to the output layer. This process is called forward propagation.", "Neural networks work the same way as a perceptron. So, in order to make sense of neural networks, the perceptron must be understood!", "The activation function, also known as the transfer function, is an essential component of the neural network. In addition to introducing the non-linearity concept into the network, it aims at converting the signal entering a unit (neuron) into an output signal (response).", "The function\u2019s name comes from its biological equivalent the \u201caction potential\u201d: the excitation threshold which, once reached, results in a neuron response.", "It is worth noting that thanks to the bias, it is possible to shift the activation function curve up or down, which means greater learning opportunities for the network.", "There are two types of activation functions: linear and non-linear.", "This is a simple function which takes the form: f(x) = x. The input passes to the output with little or no modification. This is neither more nor less than a proportionality case.", "The non-linear functions are the most commonly used and make it possible to separate data that is not linearly separable. A non-linear equation governs the correspondence between inputs and outputs.", "The sigmoid function (or logistic function) is an \u201cS\u201d curve that generates an output between 0 and 1, it is expressed as probability.", "Being a \u201csmoother\u201d version, it is preferred to the Heaviside step function but is not free of defects. Indeed, the sigmoid function is not zero-centered, so negative inputs might generate positive outputs. Moreover, its impact on neurons is relatively low, the result being often very close to 0 or 1, thus it leads to the saturation of some of them. Finally, the exponential function makes the process expensive as a computation.", "The hyperbolic tangent is a sigmoidal function like the previous one; however it usually produces better results than the logistic function due to its symmetry. Indeed, the difference is that the result of the TanH function is mapped between -1 and 1. It is generally preferred to the Sigmoid function because it is zero-centered. This function is ideal for multilayer perceptrons, especially for hidden layers.", "Apart from this aspect, the Tanh function shares the same drawbacks as the Sigmoid function.", "The ReLU function helps to solve the saturation problem of the above functions. It is the most frequently used.", "If the input is negative, the output is 0, whereas if it is positive then the output is z. This activation function significantly increases the network convergence and does not saturate.", "However, the ReLU function is not perfect either. It is possible that the neuron remains inactive if the input value is negative, consequently the weights are not updated and the network no longer learns.", "Without a non-linear activation function, an artificial neural network, no matter how many layers it has, will behave as a simple perceptron, because summing its layers would only result in another linear function.", "Is it a regression or classification problem? In the first case, is it a binary classification case? Ultimately, there is no better activation function, it depends on the task to be handled.", "To learn, the perceptron must know that it has made a mistake, as well as the answer it should have given. It is supervised learning. To do this, it is necessary to use a cost function whose purpose is to compute the error, in other words, to quantify the gap between the prediction y_hat and the expected value y. It is, therefore, necessary to minimize the cost function until the optimum: it is neural network training.", "To define the cost function J, the mean squared error can be used:", "Once the comparison between the prediction and the expected value is made, the information must be returned to the neural network, so it makes the return trip to the synapses and updates the weights. This is neither more nor less than the reverse path of the forward propagation that has been mentioned earlier. It is called backpropagation.", "As aforesaid, the purpose of a Machine Learning algorithm is to find a weight combination in order to minimize the cost function", "In a case that has more weights and thus high-dimensional spaces, there is a problem: the curse of dimensionality. A naive approach like brute force cannot be used anymore. It is therefore necessary to use a viable method in order to compute the cost function: the gradient descent, one of the most popular algorithms to perform optimization.", "Let\u2019s assume that the cost function J is convex:", "The horizontal axes represent the space of parameters, weight and bias, while the cost function J is an error surface above the horizontal axes. The blue circle is the initial cost value. All that remains is to go down, but which is the best way to go from here?", "To answer that question, some parameters have to be changed, namely weights and bias. Then, it will involve the gradient of the cost function, since the gradient vector will naturally indicate the steepest slope. It is important to know that the input values are fixed, so the weights (and bias) represent the only adjustment variables which can be controlled.", "Now, imagine that a ball is dropped inside a rounded bucket (the convex function), it just has to reach the bottom of it. This is optimization. In the case of the gradient descent, it will have to move from left to right in order to optimize its position.", "Starting from an initial position, look at the tilt angle in order to draw the tangent to this point: it means computing a derivative. If the slope is negative, the ball goes to the right, if it is positive, it goes to the left.", "But something is missing, and not the least because it is a hyperparameter: the learning rate (\u03b1). The slope indicates the direction to take, but it doesn\u2019t tell how far the ball should go in that direction. This is the learning rate\u2019s role, which determines the size of each step to reach a minimum.", "Putting everything together, the gradient descent can be defined as follows:", "The choice of the learning rate is still an open-ended question, the optimal value cannot be analytically calculated for a given model on a given dataset. Instead, a good enough learning rate must be discovered via trial and error, but note that it has to decrease as time goes by.", "Choosing the right value for the learning rate is important because it will impact the learning speed on the one hand, and the opportunity of finding the local optimum (convergence) on the other hand. This value, if mischosen, may favor two of the main causes of poor performance on predictive models:", "With a good learning rate and after a few iterations, an appropriate minimum should be found, then the ball can no longer go down.", "Finally, the best weights to optimize the network are determined.", "This is what the gradient descent is all about: knowing each weight\u2019s contribution in the total error of the network and thus converging towards an optimized weights configuration.", "There remains, however, a problem: the gradient descent needs the cost function to be convex. In other words, the curve is entirely above each of its tangents and the derivative of such a function is increasing over its interval. As illustrated earlier, it takes this form: \u222a. But what about a function that is not convex?", "This time, let\u2019s assume that the cost function J is non-convex:", "In a case like this, it is no longer enough to take the steepest slope. The error surface becomes visually more complex to understand and has specific features such as local minimums and possible saddle points.", "The risk is therefore to lead to the blocking of some iterative algorithms, drastically slowing the backpropagation and to fall on a position that is not the smallest overall value (global minimum).", "In order to overcome this problem, it is possible to use the stochastic gradient descent (SGD).", "Despite appearances, this one is faster. It provides more fluctuations of weights, which increases the chances of detecting the global minimum without stopping at a local minimum.", "As a matter of fact, it is not necessary to test and load the entire data set in memory to adjust the weights only at the end. The stochastic gradient descent will do it after each iteration, making the process lighter as a computation. Furthermore, the standard (or batch) gradient descent is deterministic: if the starting conditions (weights) are always the same, then the result will always be the same as well.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Inquiring mind with a special interest in artificial intelligence."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1b4c46e7dbfe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maxence.fzr?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxence.fzr?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Maxence Fuzellier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72665d725485&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&user=Maxence+Fuzellier&userId=72665d725485&source=post_page-72665d725485----1b4c46e7dbfe---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.shutterstock.com/g/Liu+zishan", "anchor_text": "Liu Zishan"}, {"url": "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0", "anchor_text": "Understanding Activation Functions in Neural NetworksRecently, a colleague of mine asked me a few questions like \u201cwhy do we have so many activation functions?\u201d, \u201cwhy is\u2026medium.com"}, {"url": "http://www.deeplearning.ai/ai-notes/optimization/", "anchor_text": "AI Notes: Parameter optimization in neural networks - deeplearning.aiIn machine learning, you start by defining a task and a model. The model consists of an architecture and parameters\u2026www.deeplearning.ai"}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "Understanding the Mathematics behind Gradient Descent.A simple mathematical intuition behind one of the commonly used optimisation algorithms in Machine Learning.towardsdatascience.com"}, {"url": "http://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "AI Notes: Initializing neural networks - deeplearning.aiTo build a machine learning algorithm, usually you'd define an architecture (e.g. Logistic regression, Support Vector\u2026www.deeplearning.ai"}, {"url": "https://www.deeplearning.ai/", "anchor_text": "deeplearning.ai"}, {"url": "http://cs231n.stanford.edu/syllabus.html", "anchor_text": "CS231n: Convolutional Neural Networks for Visual Recognition"}, {"url": "https://www.fast.ai/", "anchor_text": "fast.ai"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "http://neuralnetworksanddeeplearning.com/chap1.html", "anchor_text": "Neural Networks and Deep Learning"}, {"url": "https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap", "anchor_text": "Deep Learning Papers Reading Roadmap"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1b4c46e7dbfe---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1b4c46e7dbfe---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1b4c46e7dbfe---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1b4c46e7dbfe---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&user=Maxence+Fuzellier&userId=72665d725485&source=-----1b4c46e7dbfe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&user=Maxence+Fuzellier&userId=72665d725485&source=-----1b4c46e7dbfe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1b4c46e7dbfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1b4c46e7dbfe---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1b4c46e7dbfe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxence.fzr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxence.fzr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maxence Fuzellier"}, {"url": "https://medium.com/@maxence.fzr/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72665d725485&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&user=Maxence+Fuzellier&userId=72665d725485&source=post_page-72665d725485--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F72665d725485%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-fundamentals-1b4c46e7dbfe&user=Maxence+Fuzellier&userId=72665d725485&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}