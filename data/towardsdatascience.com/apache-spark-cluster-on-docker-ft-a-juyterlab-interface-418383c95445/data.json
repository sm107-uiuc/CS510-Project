{"url": "https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445", "time": 1683010893.675242, "path": "towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445/", "webpage": {"metadata": {"title": "Apache Spark Cluster on Docker (ft. a JupyterLab Interface) | by Andr\u00e9 Perez | Towards Data Science", "h1": "Apache Spark Cluster on Docker (ft. a JupyterLab Interface)", "description": "Apache Spark is arguably the most popular big data processing engine. With more than 25k stars on GitHub, the framework is an excellent starting point to learn parallel computing in distributed\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 0}, {"url": "https://github.com/apache/spark", "anchor_text": "GitHub", "paragraph_index": 0}, {"url": "https://github.com/jupyter/docker-stacks", "anchor_text": "Jupyter", "paragraph_index": 1}, {"url": "https://github.com/big-data-europe/docker-spark", "anchor_text": "projects", "paragraph_index": 1}, {"url": "http://spark.apache.org/docs/latest/spark-standalone.html", "anchor_text": "standalone mode", "paragraph_index": 5}, {"url": "https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker", "anchor_text": "GitHub", "paragraph_index": 6}, {"url": "https://almond.sh/", "anchor_text": "Almond", "paragraph_index": 7}, {"url": "https://irkernel.github.io/", "anchor_text": "IRkernel", "paragraph_index": 8}, {"url": "https://spark.apache.org/docs/latest/#downloading", "anchor_text": "Apache Spark only requirement", "paragraph_index": 12}, {"url": "https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile", "anchor_text": "Dockerfile", "paragraph_index": 13}, {"url": "https://hub.docker.com/_/openjdk?tab=tags&page=1&name=8-jre-slim", "anchor_text": "same base image", "paragraph_index": 13}, {"url": "https://packages.debian.org/stable/python/python3", "anchor_text": "latest Python release", "paragraph_index": 13}, {"url": "http://spark.apache.org/docs/latest/spark-standalone.html", "anchor_text": "standalone mode", "paragraph_index": 14}, {"url": "https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html", "anchor_text": "Apache YARN", "paragraph_index": 14}, {"url": "https://archive.apache.org/dist/spark/", "anchor_text": "Apache repository", "paragraph_index": 15}, {"url": "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala", "anchor_text": "master class", "paragraph_index": 17}, {"url": "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "anchor_text": "worker class", "paragraph_index": 19}, {"url": "https://pypi.org/project/jupyterlab/", "anchor_text": "JupyterLab", "paragraph_index": 21}, {"url": "https://pypi.org/project/pyspark/", "anchor_text": "PySpark", "paragraph_index": 21}, {"url": "https://archive.ics.uci.edu/ml/datasets/iris", "anchor_text": "UCI repository", "paragraph_index": 31}], "all_paragraphs": ["Apache Spark is arguably the most popular big data processing engine. With more than 25k stars on GitHub, the framework is an excellent starting point to learn parallel computing in distributed systems using Python, Scala and R.", "To get started, you can run Apache Spark on your machine by using one of the many great Docker distributions available out there. Jupyter offers an excellent dockerized Apache Spark with a JupyterLab interface but misses the framework distributed core by running it on a single container. Some GitHub projects offer a distributed cluster experience however lack the JupyterLab interface, undermining the usability provided by the IDE.", "I believe a comprehensive environment to learn and practice Apache Spark code must keep its distributed nature while providing an awesome user experience.", "This article is all about this belief.", "In the next sections, I will show you how to build your own cluster. By the end, you will have a fully functional Apache Spark cluster built with Docker and shipped with a Spark master node, two Spark worker nodes and a JupyterLab interface. It will also include the Apache Spark Python API (PySpark) and a simulated Hadoop distributed file system (HDFS).", "This article shows how to build an Apache Spark cluster in standalone mode using Docker as the infrastructure layer. It is shipped with the following:", "To make the cluster, we need to create, build and compose the Docker images for JupyterLab and Spark nodes. You can skip the tutorial by using the out-of-the-box distribution hosted on my GitHub.", "Update #1: On 2020\u201308\u201309 we released support for Spark Scala API through the Almond Jupyter Scala kernel. Thanks Almond for being awesome. \u2728", "Update #2: On 2020\u201308\u201319 we released support for Spark R API (SparkR) through the IRkernel Jupyter R kernel. Thanks IRkernel for being awesome. \u2728", "The cluster is composed of four main components: the JupyterLab IDE, the Spark master node and two Spark workers nodes. The user connects to the master node and submits Spark commands through the nice GUI provided by Jupyter notebooks. The master node processes the input and distributes the computing workload to workers nodes, sending back the results to the IDE. The components are connected using a localhost network and share data among each other via a shared mounted volume that simulates an HDFS.", "As mentioned, we need to create, build and compose the Docker images for JupyterLab and Spark nodes to make the cluster. We will use the following Docker image hierarchy:", "The cluster base image will download and install common software tools (Java, Python, etc.) and will create the shared directory for the HDFS. On the Spark base image, the Apache Spark application will be downloaded and configured for both the master and worker nodes. The Spark master image will configure the framework to run as a master node. Similarly, the Spark worker node will configure Apache Spark application to run as a worker node. Finally, the JupyterLab image will use the cluster base image to install and configure the IDE and PySpark, Apache Spark\u2019s Python API.", "For the base image, we will be using a Linux distribution to install Java 8 (or 11), Apache Spark only requirement. We also need to install Python 3 for PySpark support and to create the shared volume to simulate the HDFS.", "First, let\u2019s choose the Linux OS. Apache Spark official GitHub repository has a Dockerfile for Kubernetes deployment that uses a small Debian image with a built-in Java 8 runtime environment (JRE). By choosing the same base image, we solve both the OS choice and the Java installation. Then, we get the latest Python release (currently 3.7) from Debian official package repository and we create the shared volume.", "For the Spark base image, we will get and setup Apache Spark in standalone mode, its simplest deploy configuration. In this mode, we will be using its resource manager to setup containers to run either as a master or a worker node. In contrast, resources managers such as Apache YARN dynamically allocates containers as master or worker nodes according to the user workload. Furthermore, we will get an Apache Spark version with Apache Hadoop support to allow the cluster to simulate the HDFS using the shared volume created in the base cluster image.", "Let\u2019s start by downloading the Apache Spark latest version (currently 3.0.0) with Apache Hadoop support from the official Apache repository. Then, we play a bit with the downloaded package (unpack, move, etc.) and we are ready for the setup stage. Lastly, we configure four Spark variables common to both master and workers nodes:", "For the Spark master image, we will set up the Apache Spark application to run as a master node. We will configure network ports to allow the network connection with worker nodes and to expose the master web UI, a web page to monitor the master node activities. In the end, we will set up the container startup command for starting the node as a master instance.", "We start by exposing the port configured at SPARK_MASTER_PORT environment variable to allow workers to connect to the master node. Then, we expose the SPARK_MASTER_WEBUI_PORT port for letting us access the master web UI page. Finally, we set the container startup command to run Spark built-in deploy script with the master class as its argument.", "For the Spark worker image, we will set up the Apache Spark application to run as a worker node. Similar to the master node, we will configure the network port to expose the worker web UI, a web page to monitor the worker node activities, and set up the container startup command for starting the node as a worker instance.", "First, we expose the SPARK_WORKER_WEBUI_PORT port to allow access to the worker web UI page, as we did with the master node. Then, we set the container startup command to run Spark built-in deploy script with the worker class and the master network address as its arguments. This will make workers nodes connect to the master node on its startup process.", "For the JupyterLab image, we go back a bit and start again from the cluster base image. We will install and configure the IDE along with a slightly different Apache Spark distribution from the one installed on Spark nodes.", "We start by installing pip, Python\u2019s package manager, and the Python development tools to allow the installation of Python packages during the image building and at the container runtime. Then, let\u2019s get JupyterLab and PySpark from the Python Package Index (PyPI). Finally, we expose the default port to allow access to JupyterLab web interface and we set the container startup command to run the IDE application.", "The Docker images are ready, let\u2019s build them up. Note that since we used Docker arg keyword on Dockerfiles to specify software versions, we can easily change the default Apache Spark and JupyterLab versions for the cluster.", "The Docker compose file contains the recipe for our cluster. Here, we will create the JuyterLab and Spark nodes containers, expose their ports for the localhost network and connect them to the simulated HDFS.", "We start by creating the Docker volume for the simulated HDFS. Next, we create one container for each cluster component. The jupyterlab container exposes the IDE port and binds its shared workspace directory to the HDFS volume. Likewise, the spark-master container exposes its web UI port and its master-worker connection port and also binds to the HDFS volume.", "We finish by creating two Spark worker containers named spark-worker-1 and spark-worker-2. Each container exposes its web UI port (mapped at 8081 and 8082 respectively) and binds to the HDFS volume. These containers have an environment step that specifies their hardware allocation:", "By default, we are selecting one core and 512 MB of RAM for each container. Feel free to play with the hardware allocation but make sure to respect your machine limits to avoid memory issues. Also, provide enough resources for your Docker application to handle the selected values.", "To compose the cluster, run the Docker compose file:", "Once finished, check out the components web UI:", "With our cluster up and running, let\u2019s create our first PySpark application.", "Open the JupyterLab IDE and create a Python Jupyter notebook. Create a PySpark application by connecting to the Spark master node using a Spark session object with the following parameters:", "Run the cell and you will be able to see the application listed under \u201cRunning Applications\u201d at the Spark master web UI. Then we download the iris dataset from UCI repository into the simulated HDFS. At last, we read and print the data with PySpark.", "That\u2019s all folks. I hope I have helped you to learn a bit more about Apache Spark internals and how distributed applications works. Happy learning!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data & ML Engineering at SumUp"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F418383c95445&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andremarcosperez.medium.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": ""}, {"url": "https://andremarcosperez.medium.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": "Andr\u00e9 Perez"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0166d96c2d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&user=Andr%C3%A9+Perez&userId=d0166d96c2d5&source=post_page-d0166d96c2d5----418383c95445---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeztimms", "anchor_text": "Jez Timms"}, {"url": "https://unsplash.com/photos/r4lM2v9M84Q", "anchor_text": "Unsplash"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://github.com/apache/spark", "anchor_text": "GitHub"}, {"url": "https://github.com/jupyter/docker-stacks", "anchor_text": "Jupyter"}, {"url": "https://github.com/big-data-europe/docker-spark", "anchor_text": "projects"}, {"url": "http://spark.apache.org/docs/latest/spark-standalone.html", "anchor_text": "standalone mode"}, {"url": "https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker", "anchor_text": "GitHub"}, {"url": "https://almond.sh/", "anchor_text": "Almond"}, {"url": "https://irkernel.github.io/", "anchor_text": "IRkernel"}, {"url": "https://spark.apache.org/docs/latest/#downloading", "anchor_text": "Apache Spark only requirement"}, {"url": "https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile", "anchor_text": "Dockerfile"}, {"url": "https://hub.docker.com/_/openjdk?tab=tags&page=1&name=8-jre-slim", "anchor_text": "same base image"}, {"url": "https://packages.debian.org/stable/python/python3", "anchor_text": "latest Python release"}, {"url": "http://spark.apache.org/docs/latest/spark-standalone.html", "anchor_text": "standalone mode"}, {"url": "https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html", "anchor_text": "Apache YARN"}, {"url": "https://archive.apache.org/dist/spark/", "anchor_text": "Apache repository"}, {"url": "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala", "anchor_text": "master class"}, {"url": "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "anchor_text": "worker class"}, {"url": "https://pypi.org/project/jupyterlab/", "anchor_text": "JupyterLab"}, {"url": "https://pypi.org/project/pyspark/", "anchor_text": "PySpark"}, {"url": "http://localhost:8888/", "anchor_text": "localhost:8888"}, {"url": "http://localhost:8080/", "anchor_text": "localhost:8080"}, {"url": "http://localhost:8081/", "anchor_text": "localhost:8081"}, {"url": "http://localhost:8082/", "anchor_text": "localhost:8082"}, {"url": "https://spark.apache.org/docs/latest/configuration.html", "anchor_text": "Spark configuration for standalone mode"}, {"url": "https://archive.ics.uci.edu/ml/datasets/iris", "anchor_text": "UCI repository"}, {"url": "https://medium.com/tag/cluster-apps-on-docker?source=post_page-----418383c95445---------------cluster_apps_on_docker-----------------", "anchor_text": "Cluster Apps On Docker"}, {"url": "https://medium.com/tag/data-science?source=post_page-----418383c95445---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----418383c95445---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/big-data?source=post_page-----418383c95445---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/programming?source=post_page-----418383c95445---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&user=Andr%C3%A9+Perez&userId=d0166d96c2d5&source=-----418383c95445---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&user=Andr%C3%A9+Perez&userId=d0166d96c2d5&source=-----418383c95445---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F418383c95445&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----418383c95445---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----418383c95445--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----418383c95445--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----418383c95445--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----418383c95445--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----418383c95445--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----418383c95445--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----418383c95445--------------------------------", "anchor_text": ""}, {"url": "https://andremarcosperez.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andremarcosperez.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andr\u00e9 Perez"}, {"url": "https://andremarcosperez.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "102 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0166d96c2d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&user=Andr%C3%A9+Perez&userId=d0166d96c2d5&source=post_page-d0166d96c2d5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F66ac1759a46b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445&newsletterV3=d0166d96c2d5&newsletterV3Id=66ac1759a46b&user=Andr%C3%A9+Perez&userId=d0166d96c2d5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}