{"url": "https://towardsdatascience.com/neural-networks-from-hero-to-zero-afc30205df05", "time": 1683003738.7647831, "path": "towardsdatascience.com/neural-networks-from-hero-to-zero-afc30205df05/", "webpage": {"metadata": {"title": "Neural Networks: From Zero to Hero | by Victor Roman | Towards Data Science", "h1": "Neural Networks: From Zero to Hero", "description": "If you want a detailed explanation of gradient descent, I recommend that you check out this article in which there is an in-depth study of the mathematics on which neural networks are based. In\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab", "anchor_text": "this article", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab", "anchor_text": "you can check here", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba", "anchor_text": "previous article", "paragraph_index": 45}, {"url": "https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9", "anchor_text": "https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9", "paragraph_index": 73}, {"url": "https://towardsdatascience.com/deep-learning-solving-problems-with-tensorflow-3722b8eeccb1", "anchor_text": "as we did in this article", "paragraph_index": 78}, {"url": "https://medium.com/@rromanss23", "anchor_text": "here", "paragraph_index": 97}], "all_paragraphs": ["Throughout this article will be covered the following topics:", "If you want a detailed explanation of gradient descent, I recommend that you check out this article in which there is an in-depth study of the mathematics on which neural networks are based.", "In summary, gradient descent calculates the error of every sample in the training set and then updates the weights in the direction that points the gradient.", "In other words, for every epoch, we need to :", "One regular neural network may need hundreds, or even thousands, epochs to converge appropriately. Let\u2019s assume that we need 100 epochs, which is a low number.", "How much will it take to our neural network to be trained?", "This is so much time. And we were being nice by assuming we only had 100,000 samples. ImageNet, for example, consists of 1.2 million images, and it would take 2h per epoch, or in other words, 8.3 days. More than a week to see the behavior of a network.", "One way to drastically reduce the time needed to train a neural network would be to use a single sample chosen randomly every time we want to update the weights.", "This method is called Stochastic Gradient Descent (SDG). With SDG, we would simply have to calculate the predictions, errors, and backpropagation of one sample to update the weights.", "This would reduce the total training time to:", "This is a huge improvement. But this method has one very important disadvantage.", "Out of these two paths, which one do you think that is the one followed by Gradient Descent? And which one by the Stochastic Gradient Descent?", "The red path is the one that follows gradient descent. It calculates the gradient (the descending path) using all the samples of the dataset and gets consistent updates always in the direction that minimizes the error.", "The purple path is the one followed by the SGD. What\u2019s going on here? Each weight update is done to minimize the error by taking into account only one sample, so what we minimize is the error for that particular sample.", "That\u2019s why it has a more chaotic behavior and it costs more to converge, although, in return, it runs much faster, so in the time that the GD needs to run an epoch, the SGD can run thousands.", "It seems like the best option would be to have a balance with both approaches. If we take a look at the previous picture, the best path is the green line.", "To calculate this path, let\u2019s review the methods discussed up to now:", "What if instead of 1 element, we chose K elements? In this way:", "This method is known as Mini-batch Stochastic Gradient Descent and is the most popular one in practice.", "K is usually chosen to be a power of 2, as this allows you to take advantage of some optimizations that have GPUs implemented for these cases. A typical K might be K=32, but in the end, this is limited by the memory of the GPU.", "The lower the K is, the more it will resemble pure SGD, and the more epochs it will need to converge, although it is also true that it will calculate them faster.", "On the other hand, the higher the K is, the more it will resemble pure GD, and the more trouble it will have to calculate each epoch, but it will need less time to converge.", "Learning rate and batch size are two parameters directly related to the descent gradient algorithm.", "As you may know, (or if you don\u2019t, you can check here) the way to update neural\u2019s weights is through these formulas:", "What multiplies to \u2202Etotal/\u2202wn is called \u03b7, which is the learning rate. The learning rate indicates the importance we give to the error to update each weight. That is, how fast or how abrupt are the changes in the weights.", "Thus, a very high \u03b7, will make changes in the weights in huge steps from one iteration to another, which can cause to skip the minimum.", "Another possibility is to establish a very low \u03b7, which would make our network to need too many epochs to reach an acceptable minimum. Also, we would take the risk of being trapped in a worse minimum than the best we could achieve with a higher \u03b7.", "Let\u2019s talk about the minimums: what we achieve with a neural network, normally, is not the global minimum of our function, but a local minimum good enough to correctly perform the task we are developing.", "What we want is an optimal learning rate, which allows us to reduce the error as time goes by, until we reach our minimum. In the graph, this learning rate would be the red line.", "And to get our learning rate to be optimal, we can apply a decay to our learning rate. This decay decreases the learning rate with time, so when we are reaching the minimum it will be small enough to avoid skipping it.", "Thus, we avoid both long waits to converge by choosing a very low learning rate and skipping our minimum because the closer we are to it, the smaller the steps we take towards it.", "Recalling from the previous section, SGD is a Mini-batch SGD where K=1 .", "And the Mini-batch SGD the K indicates the number of samples used to update the weights each time. This is not a critical parameter and it is usually set as the maximum number of samples that can fit in our GPU.", "We have a GPU with 8GB of memory, how many samples can we fit if each image occupies 1MB?", "Well, it\u2019s not that easy! It depends on the architecture of the network. The Dense or Fully Connected layers (which are the traditional ones in which all the neurons are interconnected with all the neurons in the next layer) are the ones that have more parameters, and therefore, the ones that occupy more memory.", "We also have convolutional layers, pooling layers, dropout layers, and many other types. So in practice, it is difficult to calculate by hand the maximum number of samples we can use.", "What we do is try to set batch sizes of multiples of 2 and decrease them if we have a memory error. For example, we would start with 512, and if we have an error we would go down to 256, 128, 64, 32, 16, 8, 4, 2 and even 1.", "Depending on the architecture of your network, you may have to use K=1, and therefore SGD. Although it is often preferable to reduce the image size, for example, from 512x512 to 256x256 or 128x128 pixels, and use a larger K.", "It is very important to keep in mind that the learning rate is related to the batch size.", "If we approach to K=1, we must lower the learning rate so that the updates of the weights have less importance, since it is closer to the SGD, in other words, to calculations of the gradient with single random samples.", "So, in summary, if we use a lower batch size, it is recommended to use a lower learning rate but we would also increase the number of epochs, as the latter conditions would make our neural network to take more time to converge.", "The loss function is the one that tells us how wrong our predictions have been.", "Imagine that we have to guess how much a house costs just by looking at a picture. Our neural network would have as input the pixels of the photo and as output a number indicating the price.", "For example, let\u2019s say we want to predict the price of a house so we are training the network and this house is in our training set. When the picture passes by, a prediction is calculated, which is that it is worth 323,567$. The truth is that the house costs 600,000$, so it seems obvious that a proper loss function could be:", "Taking this into account, the most common loss functions are:", "As I wrote in a previous article focused just on regression problems, let\u2019s a look at each one of them:", "Mean Square Error or MSE, is the average of the squared difference between the real data points and the predicted outcome. This method penalizes more the bigger the distance is, and it is the standard in regression problems.", "Mean Absolute Error or MAE, is the average of the absolute difference between the real data points and the predicted outcome. If we take this as the strategy to follow, each step of the gradient descent would reduce the MAE.", "We need first to understand what is entropy. Let\u2019s try to illustrate this with a couple of examples:", "Imagine that we are playing a game: we have a bag with different colored balls, and the goal of the game is to guess which color is the one that a volunteer draws with the minimum number of questions.", "In this case, we have a blue ball, a red ball, a green ball, and an orange ball:", "Which means every ball has a 1/4 chance of getting out.", "One of the best strategies would be to first ask if the ball you have served is blue or red. If it is, we would ask if the ball is blue. If not, we would ask if it is green. So we need 2 questions.", "This time we have a bag with balls in which 1/2 are blue, 1/4 are red, 1/8 are green and 1/8 are red. Now, the optimal strategy would be to ask if it\u2019s blue first since there\u2019s a better chance of a blue one coming out. If it is, we\u2019re done. If not, we could ask if it\u2019s red, which is the next most likely class. If it is, we\u2019re done. If not, we could ask if it\u2019s green (or if it\u2019s orange).", "Now what happens is that half of the time (1/2) is blue, and it costs us 1 question to guess. 1/4 of the time is red, and it costs us 2 questions. 1/8 is green, and it costs us 3 questions, and the same if it is orange.", "Imagine now that we have a bag full of blue balls. How many questions do we need to find out what color ball they take out? None, 0.", "From these examples, we can come up with an expression that allows us to calculate the number of questions depending on the probability of the ball. Thus, a ball with probability p costs log2(1/p) questions.", "So the expected number of questions in total is:", "In the end, one way to understand entropy is the following:", "If we were to follow the optimal strategy, what is the expected number of questions that would allow us to guess the color of the ball?", "So, the more complicated the game is, the higher the entropy. In this case, Example 1 > Example 2 > Example 3.", "Okay, so now that we know what entropy is, let\u2019s see what cross-entropy is.", "Imagine that we had followed the strategy of Example 1 for Example 2:", "So, we would have had to ask two questions to find out if it\u2019s any color. If we calculate the total number of questions needed, taking into account that this time each ball has a probability, it gives us that:", "So this strategy is worse than the strategy we followed in Example 1.", "In the end, intuitively, entropy is the number of questions expected using the best possible strategy, and cross entropy is the number of questions expected when you do not use the best possible strategy.", "For this reason, what we try to do is to minimize the cross-entropy.", "One way to remember what goes where in the formula, is to remember that what we want to do is to find out how many questions are needed following our strategy, which is pi^, so inside the log2 goes pi^.", "If we didn\u2019t have activation functions we would have the following:", "We would have to y(x)=Wx+b . This is a linear combination that would be unable to even solve a problem like XOR.", "Therefore, we need a way to introduce non-linearity, and that is what the activation function does. In the following image you can see some of the most typical ones, and where they intervene in the network:", "Here you can see the most used ones:", "It is difficult to know with which of them our network will behave better, but there is one that usually gives good results almost always: the ReLU.", "Therefore, whenever we start, we will use the ReLU, and once we get some results that we consider good, we can try with the Leaky ReLU or any other that you want. Every day new ones come out, and a simple google search can lead you to some interesting ones: like SELU, for example (https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9).", "Many of these activation functions need specific methods of weight initialization, so that lay between a specific range of values and that the gradient descent works properly.", "In the case of the output layers, the softmax activation function is the one most used, since it is capable of giving a probability to each class, making all of them add up to 1.", "As this may seem a little complicated, find below the recommended recipe as a summary of all the above:", "As you have seen before, weights and biases initialization is very important to achieve the convergence of our network to an adequate minimum. So let\u2019s look at some ways to initialize the weights.", "If we follow the MNIST dataset (as we did in this article), our weight matrix would be 768 (inputs) x 10 (outputs).", "We can pre-set our weights to", "We can also initialize the weights using a uniform distribution, where an [upper_bound,lower_bound] is defined and all numbers within the range have the same probability of being chosen.", "With this instruction, we will initialize the WW weight matrix with values extracted from the range between [-0.2,0.2][-0.2,0.2] where they all have the same probability of being extracted.", "We can also do it with a normal or Gaussian distribution, which is defined as", "So we could initialize our weights with a normal distribution with \u03bc=0 and \u03c3=0.2, for example:", "Another, more advanced, method is the LeCun method, also known as \u201cEfficient backprop\u201d.", "The code for initializing W by this method using a uniform distribution would be:", "This is perhaps the most widely used method for initializing weights and biases. It\u2019s the default when using Keras.", "In this case, also the same parameters are defined as with LeCun, but the calculation of the limit varies:", "The code to initialize WW using this method would be the same as with LeCun.", "For a uniform distribution it would be:", "This method is named after Kaiming He, the first author of Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.", "Normally, this method is used when we are training very deep neural networks that use a particular type of ReLU as activation like the Parametric ReLU.", "The code in the case of the uniform is this:", "And in the case of the normal one, this one:", "The initialization of the weights is usually not a determining factor in the training of a net, but sometimes it can cause the net to not be able to train because it fails to converge.", "Therefore, the recommended advice is to use Glorot\u2019s, and if that day you feel lucky and want to see if you can improve in the accuracy, try with others.", "As always, I hope you enjoyed the post, that you are now a pro on neural networks!", "If you liked this post then you can take a look at my other posts on Data Science and Machine Learning here.", "If you want to learn more about Machine Learning, Data Science and Artificial Intelligence follow me on Medium, and stay tuned for my next posts!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Industrial Engineer and passionate about 4.0 Industry. My goal is to encourage people to learn and explore its technologies and their infinite posibilites."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fafc30205df05&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rromanss23.medium.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Victor Roman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77fd145c8783&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&user=Victor+Roman&userId=77fd145c8783&source=post_page-77fd145c8783----afc30205df05---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.vectorstock.com/royalty-free-vector/super-hero-brain-cartoon-vector-17919284", "anchor_text": "vectorstock"}, {"url": "https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab", "anchor_text": "this article"}, {"url": "https://www.fromthegenesis.com/gradient-descent/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab", "anchor_text": "you can check here"}, {"url": "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/", "anchor_text": "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"}, {"url": "https://www.quora.com/In-neural-networks-why-would-one-use-many-learning-rates-in-decreasing-steps-rather-than-one-smooth-learning-rate-decay", "anchor_text": "https://www.quora.com/In-neural-networks-why-would-one-use-many-learning-rates-in-decreasing-steps-rather-than-one-smooth-learning-rate-decay"}, {"url": "https://www.researchgate.net/publication/226939869_An_Improved_EMD_Online_Learning-Based_Model_for_Gold_Market_Forecasting/figures?lo=1", "anchor_text": "https://www.researchgate.net/publication/226939869_An_Improved_EMD_Online_Learning-Based_Model_for_Gold_Market_Forecasting/figures?lo=1"}, {"url": "https://towardsdatascience.com/useful-plots-to-diagnose-your-neural-network-521907fa2f45", "anchor_text": "https://towardsdatascience.com/useful-plots-to-diagnose-your-neural-network-521907fa2f45"}, {"url": "https://www.pinterest.es/pin/825918019139095502/", "anchor_text": "https://www.pinterest.es/pin/825918019139095502/"}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba", "anchor_text": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba"}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba", "anchor_text": "https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba"}, {"url": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy", "anchor_text": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy"}, {"url": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy", "anchor_text": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy"}, {"url": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy", "anchor_text": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy"}, {"url": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy", "anchor_text": "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy"}, {"url": "https://towardsdatascience.com/neural-representation-of-logic-gates-df044ec922bc", "anchor_text": "https://towardsdatascience.com/neural-representation-of-logic-gates-df044ec922bc"}, {"url": "http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html", "anchor_text": "http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html"}, {"url": "https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f", "anchor_text": "https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f"}, {"url": "https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044", "anchor_text": "https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044"}, {"url": "https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9", "anchor_text": "https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9"}, {"url": "https://towardsdatascience.com/deep-learning-solving-problems-with-tensorflow-3722b8eeccb1", "anchor_text": "as we did in this article"}, {"url": "https://medium.com/@rromanss23", "anchor_text": "here"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----afc30205df05---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----afc30205df05---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----afc30205df05---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----afc30205df05---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----afc30205df05---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&user=Victor+Roman&userId=77fd145c8783&source=-----afc30205df05---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&user=Victor+Roman&userId=77fd145c8783&source=-----afc30205df05---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fafc30205df05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----afc30205df05---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----afc30205df05--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----afc30205df05--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----afc30205df05--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----afc30205df05--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----afc30205df05--------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Victor Roman"}, {"url": "https://rromanss23.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77fd145c8783&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&user=Victor+Roman&userId=77fd145c8783&source=post_page-77fd145c8783--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e807bd3a047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-from-hero-to-zero-afc30205df05&newsletterV3=77fd145c8783&newsletterV3Id=7e807bd3a047&user=Victor+Roman&userId=77fd145c8783&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}