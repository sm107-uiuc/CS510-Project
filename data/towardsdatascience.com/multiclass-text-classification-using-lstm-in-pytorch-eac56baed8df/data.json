{"url": "https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df", "time": 1683005494.383641, "path": "towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df/", "webpage": {"metadata": {"title": "Multiclass Text Classification using LSTM in Pytorch | by Aakanksha NS | Towards Data Science", "h1": "Multiclass Text Classification using LSTM in Pytorch", "description": "Human language is filled with ambiguity, many-a-times the same phrase can have multiple interpretations based on the context and can even appear confusing to humans. Such challenges make natural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification", "anchor_text": "https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embedding", "paragraph_index": 8}, {"url": "https://www.linkedin.com/in/aakanksha-ns/", "anchor_text": "https://www.linkedin.com/in/aakanksha-ns/", "paragraph_index": 34}], "all_paragraphs": ["Human language is filled with ambiguity, many-a-times the same phrase can have multiple interpretations based on the context and can even appear confusing to humans. Such challenges make natural language processing an interesting but hard problem to solve. However, we\u2019ve seen a lot of advancement in NLP in the past couple of years and it\u2019s quite fascinating to explore the various techniques being used. This article aims to cover one such technique in deep learning using Pytorch: Long Short Term Memory (LSTM) models.", "Here\u2019s a link to the notebook consisting of all the code I\u2019ve used for this article: https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification", "If you\u2019re new to NLP or need an in-depth read on preprocessing and word embeddings, you can check out the following article:", "What sets language models apart from conventional neural networks is their dependency on context. Conventional feed-forward networks assume inputs to be independent of one another. For NLP, we need a mechanism to be able to use sequential information from previous inputs to determine the current output. Recurrent Neural Networks (RNNs) tackle this problem by having loops, allowing information to persist through the network.", "However, conventional RNNs have the issue of exploding and vanishing gradients and are not good at processing long sequences because they suffer from short term memory.", "Long Short Term Memory networks (LSTM) are a special kind of RNN, which are capable of learning long-term dependencies. They do so by maintaining an internal memory state called the \u201ccell state\u201d and have regulators called \u201cgates\u201d to control the flow of information inside each LSTM unit. Here\u2019s an excellent source explaining the specifics of LSTMs:", "Before we jump into the main problem, let\u2019s take a look at the basic structure of an LSTM in Pytorch, using a random input. This is a useful step to perform before getting into complex inputs because it helps us learn how to debug the model better, check if dimensions add up and ensure that our model is working as expected.", "Even though we\u2019re going to be dealing with text, since our model can only work with numbers, we convert the input into a sequence of numbers where each number represents a particular word (more on this in the next section).", "We first pass the input (3x8) through an embedding layer, because word embeddings are better at capturing context and are spatially more efficient than one-hot vector representations.", "In Pytorch, we can use the nn.Embedding module to create this layer, which takes the vocabulary size and desired word-vector length as input. You can optionally provide a padding index, to indicate the index of the padding element in the embedding matrix.", "In the following example, our vocabulary consists of 100 words, so our input to the embedding layer can only be from 0\u2013100, and it returns us a 100x7 embedding matrix, with the 0th index representing our padding element.", "We pass the embedding layer\u2019s output into an LSTM layer (created using nn.LSTM), which takes as input the word-vector length, length of the hidden state vector and number of layers. Additionally, if the first element in our input\u2019s shape has the batch size, we can specify batch_first = True", "The LSTM layer outputs three things:", "We can verify that after passing through all layers, our output has the expected dimensions:", "Let\u2019s now look at an application of LSTMs.", "Problem Statement: Given an item\u2019s review comment, predict the rating ( takes integer values from 1 to 5, 1 being worst and 5 being best)", "Dataset: I\u2019ve used the following dataset from Kaggle:", "We usually take accuracy as our metric for most classification problems, however, ratings are ordered. If the actual value is 5 but the model predicts a 4, it is not considered as bad as predicting a 1. Hence, instead of going with accuracy, we choose RMSE \u2014 root mean squared error as our North Star metric. Also, rating prediction is a pretty hard problem, even for humans, so a prediction of being off by just 1 point or lesser is considered pretty good.", "As mentioned earlier, we need to convert our text into a numerical form that can be fed to our model as input. I\u2019ve used spacy for tokenization after removing punctuation, special characters, and lower casing the text:", "We count the number of occurrences of each token in our corpus and get rid of the ones that don\u2019t occur too frequently:", "We lost about 6000 words! This is expected because our corpus is quite small, less than 25k reviews, the chance of having repeated words is quite small.", "We then create a vocabulary to index mapping and encode our review text using this mapping. I\u2019ve chosen the maximum length of any review to be 70 words because the average length of reviews was around 60.", "The dataset is quite straightforward because we\u2019ve already stored our encodings in the input dataframe. We also output the length of the input sequence in each case, because we can have LSTMs that take variable-length sequences.", "The training loop is pretty standard. I\u2019ve used Adam optimizer and cross-entropy loss.", "I\u2019ve used three variations for the model:", "This pretty much has the same structure as the basic LSTM we saw earlier, with the addition of a dropout layer to prevent overfitting. Since we have a classification problem, we have a final linear layer with 5 outputs. This implementation actually works the best among the classification LSTMs, with an accuracy of about 64% and a root-mean-squared-error of only 0.817", "2. LSTM with variable input size:", "We can modify our model a bit to make it accept variable-length inputs. This ends up increasing the training time though, because of the pack_padded_sequence function call which returns a padded batch of variable-length sequences.", "3. LSTM with fixed input size and fixed pre-trained Glove word-vectors:", "Instead of training our own word embeddings, we can use pre-trained Glove word vectors that have been trained on a massive corpus and probably have better context captured. For our problem, however, this doesn\u2019t seem to help much.", "Since ratings have an order, and a prediction of 3.6 might be better than rounding off to 4 in many cases, it is helpful to explore this as a regression problem. Not surprisingly, this approach gives us the lowest error of just 0.799 because we don\u2019t have just integer predictions anymore.", "The only change to our model is that instead of the final layer having 5 outputs, we have just one. The training loop changes a bit too, we use MSE loss and we don\u2019t need to take the argmax anymore to get the final prediction.", "LSTM appears to be theoretically involved, but its Pytorch implementation is pretty straightforward. Also, while looking at any problem, it is very important to choose the right metric, in our case if we\u2019d gone for accuracy, the model seems to be doing a very bad job, but the RMSE shows that it is off by less than 1 rating point, which is comparable to human performance!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ Snap Inc. | MSDS University of San Francisco | CSE NIT Calicut https://www.linkedin.com/in/aakanksha-ns/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feac56baed8df&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eac56baed8df--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakanksha.ns?source=post_page-----eac56baed8df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Aakanksha NS"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff30e60797a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&user=Aakanksha+NS&userId=f30e60797a83&source=post_page-f30e60797a83----eac56baed8df---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification", "anchor_text": "https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification"}, {"url": "https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05", "anchor_text": "Getting Started with Natural Language Processing (NLP)using simple Python librariestowardsdatascience.com"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM NetworksPosted on August 27, 2015 Humans don't start their thinking from scratch every second. As you read this essay, you\u2026colah.github.io"}, {"url": "https://www.researchgate.net/publication/329362532_Designing_neural_network_based_decoders_for_surface_codes", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embedding"}, {"url": "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews", "anchor_text": "Women's E-Commerce Clothing Reviews23,000 Customer Reviews and Ratingswww.kaggle.com"}, {"url": "https://www.usfca.edu/data-institute/certificates/deep-learning-part-one", "anchor_text": "https://www.usfca.edu/data-institute/certificates/deep-learning-part-one"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"}, {"url": "http://web.stanford.edu/class/cs224n/", "anchor_text": "http://web.stanford.edu/class/cs224n/"}, {"url": "https://medium.com/tag/nlp?source=post_page-----eac56baed8df---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eac56baed8df---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----eac56baed8df---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/lstm?source=post_page-----eac56baed8df---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----eac56baed8df---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&user=Aakanksha+NS&userId=f30e60797a83&source=-----eac56baed8df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&user=Aakanksha+NS&userId=f30e60797a83&source=-----eac56baed8df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feac56baed8df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eac56baed8df---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eac56baed8df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eac56baed8df--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eac56baed8df--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eac56baed8df--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eac56baed8df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakanksha NS"}, {"url": "https://medium.com/@aakanksha.ns/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "322 Followers"}, {"url": "https://www.linkedin.com/in/aakanksha-ns/", "anchor_text": "https://www.linkedin.com/in/aakanksha-ns/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff30e60797a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&user=Aakanksha+NS&userId=f30e60797a83&source=post_page-f30e60797a83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb925b3bc1777&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-text-classification-using-lstm-in-pytorch-eac56baed8df&newsletterV3=f30e60797a83&newsletterV3Id=b925b3bc1777&user=Aakanksha+NS&userId=f30e60797a83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}