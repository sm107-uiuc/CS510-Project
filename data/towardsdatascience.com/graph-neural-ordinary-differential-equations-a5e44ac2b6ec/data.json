{"url": "https://towardsdatascience.com/graph-neural-ordinary-differential-equations-a5e44ac2b6ec", "time": 1683001503.397194, "path": "towardsdatascience.com/graph-neural-ordinary-differential-equations-a5e44ac2b6ec/", "webpage": {"metadata": {"title": "Graph Neural Ordinary Differential Equations | by Michael Poli | Towards Data Science", "h1": "Graph Neural Ordinary Differential Equations", "description": "Multi \u2014 agent systems are prevalent across a variety of scientific fields: from physics to robotics, game-theory, finance and molecular biology, among others. Often, closed \u2014 form analytic\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/N-body_simulation", "anchor_text": "physics", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Swarm_robotics", "anchor_text": "robotics", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Auction_theory", "anchor_text": "game-theory", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Initial_value_problem", "anchor_text": "initial value problem", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "paper", "paragraph_index": 8}, {"url": "https://github.com/Zymrael/gde", "anchor_text": "github repository", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)", "anchor_text": "graphs", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1901.00596", "anchor_text": "survey on GNNs", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "background section in our paper", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Well-posed_problem", "anchor_text": "Well-posedness", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1911.07532.pdf", "anchor_text": "paper", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations", "anchor_text": "numerical methods", "paragraph_index": 12}, {"url": "https://tkipf.github.io/graph-convolutional-networks/", "anchor_text": "Graph Convolutional Networks", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods", "anchor_text": "Runge-Kutta", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Dormand%E2%80%93Prince_method", "anchor_text": "Dormand-Prince", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Hybrid_system", "anchor_text": "hybrid dynamical systems", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Hybrid_automaton", "anchor_text": "hybrid automata", "paragraph_index": 22}, {"url": "https://dot.ca.gov/programs/traffic-operations/mobility-performance-reports", "anchor_text": "PeMS traffic dataset", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Root-mean-square_deviation#Normalized_root-mean-square_deviation", "anchor_text": "normalized RMSE", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Mean_absolute_percentage_error", "anchor_text": "mean absolute percentage error", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "paper", "paragraph_index": 25}, {"url": "https://github.com/Zymrael/gde", "anchor_text": "github repository", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "citing us", "paragraph_index": 29}], "all_paragraphs": ["Multi \u2014 agent systems are prevalent across a variety of scientific fields: from physics to robotics, game-theory, finance and molecular biology, among others. Often, closed \u2014 form analytic formulations are not available and forecasting or decision making tasks have to rely on noisy, irregularly sampled observations.", "This class of systems offers a crystal clear example of inductive relational biases. Introducing inductive biases in statistics or machine learning is a well known approach to improving sample efficiency and generalization performance. From the choice of objective function, to the design of ad \u2014 hoc deep learning architectures suited to the specific problem at hand, biases are common and effective.", "Relational inductive biases [1] represent a special class of biases, concerned with relationship between entities. Graphical models, probabilistic or otherwise, are a traditional class of models specialized in imposing relational biases in the form of prior structures on entities. These graph structures are useful in different ways; namely reducing computational complexity by introducing conditional independence assumptions and increasing sample efficiency by encoding prior knowledge in graph \u2014 form.", "Graph neural networks (GNNs) are the deep learning counterpart to graphical models. They are usually utilized when the target problem structure can be encoded as a graph or in settings where prior knowledge about relationships among input entities can itself be described as a graph. GNNs have shown remarkable results in various application areas such as node classification [2], graph classification and forecasting [3][4] as well as generative tasks [5].", "A different but equally important class of inductive biases is concerned with the class of systems from which the data is collected. Although deep learning has traditionally been a field dominated by discrete models, recent advances propose a treatment of neural networks as models equipped with a continuum of layers [6]. This view allows a reformulation of the forward pass as the solution of the initial value problem of an ordinary differential equation (ODE). This assumption allows direct modeling of ODEs and enhances the performance of neural networks on tasks involving continuous time processes.", "Our work is aimed at bridging the gap between geometric deep learning and continuous models. Graph neural ordinary differential equations (GDEs) cast common tasks on graph \u2014 structured data into a system \u2014 theoretic framework:", "GDEs provide flexibility due to their structure defined by a continuum of GNN layers and can therefore accommodate irregularly sampled sequential data.", "The primary purpose of GDEs is to offer a data \u2014 driven approach to the modeling of structured systems, particularly when the dynamics are nonlinear and therefore challenging to approach with classical or analytical methods.", "What follows is an introduction to GDEs. We refer to the full paper for additional details and derivations. A github repository with introductory examples in the form of commented Jupyter notebooks is currently under development. We encourage requests/suggestions of additional applications of GDEs: our plan it to eventually include working examples with GDE variants of all major graph neural network (GNN) architectures, deployed across various settings (forecasting, control\u2026)", "GDEs, like GNNs, operate on graphs. We refer to the excellent survey on GNNs as well as the background section in our paper for a more detailed introduction on notation and basic definitions. The GDE introduction to follow is in distilled form and only two basic facts about graphs are immediately necessary:", "A Graph neural ordinary Differential Equation (GDE) is defined as follows:", "where H is the node feature matrix. The above defines a vector field for H, parametrized by function F which can be any known graph neural network (GNN) layer. In other words, F utilizes connectivity information of graph G, as well as its node features, to specify the evolution of H in S. Here S is the depth domain of the model; S is continuous, differently from GNNs with depth domain specified by subsets of the natural numbers, and represents the integration domain of the ordinary differential equation defined by F. GDEs can be trained in a variety of ways, much like standard Neural ODEs [6]. Well-posedness of the system is discussed in full in the paper.", "The general GDE formulation carries with it several implications. In the case of general Neural ODEs, it has been observed that the choice of discretization scheme can describe previously known discrete multi \u2014 step variants of ResNets [7]. The continuous, dynamical system point of view of deep learning is therefore not limited to the modeling of differential equations and can guide discovery of novel general purpose models by leveraging a rich literature of numerical methods.", "Compared to ResNets, GNNs are relatively younger as a model class. The literature of multi \u2014 step or variants with complicated, fractal \u2014 like residual connections is therefore not as developed; discovery of new GNNs variants can be guided by applying various discretization schemes to GDEs, instead of starting from scratch.", "We show that GDEs can be utilized as high \u2014 performance general purpose models by performing a series of experiments on a semi \u2014 supervised node classification task on Cora, Pubmed and Citeseer. These datasets contain static graphs, where adjacency matrix A remains fixed, and are thus far removed from the dynamical system setting where GDEs shine. We evaluate performance of Graph Convolutional ordinary Differential Equations (GCDEs), defined as:", "as well as their fully \u2014 discretized counterpart Graph Convolutional Networks (GCN) [8]. We include well \u2014 known Graph ATtention networks (GAT) [9] for reference:", "GCDEs are shown to be competitive with state \u2014 of \u2014 the \u2014 art models and outperform their discrete counterparts. We evaluate two versions of GCDEs: discretized with a fixed step scheme, Runge-Kutta4 (GCDE \u2014 rk4), as well as an adaptive step scheme, Dormand-Prince (GDDE \u2014 dpr5). Fixed step discretizations do not ensure that the ODE approximation remains close to the analytical solution; in this case, where solving a proper ODE is not necessary, GCDE \u2014 rk4 simply offer a computationally efficient FractalNet \u2014 like structure for GCNs that improves accuracy.", "On the other hand, training GCDEs with adaptive step solvers naturally leads to deeper models than possible with vanilla GCNs, whose layer depth greatly reduces performance. In our experiments, we successfully trained GCDE \u2014 dpr5 with up to 200 ODE function evaluations (NFE), a significantly higher amount of computation on graphs than possible with vanilla GCNs, whose layer depth greatly reduces performance. It should be noted that GDEs do not require more parameters than their discrete counterparts, due to the fact that they reutilize their parameters across function evaluations. Interestingly, adaptive step GDEs do not seem to suffer from over \u2014 smoothing of node features. The over \u2014 smoothing issue [10] prevents effective use of deep GNNs in various domains, particularly multi \u2014 agent reinforcement learning (MARL); we are currently exploring this property of GDEs and will include a more detailed analysis soon.", "A key setting for GDEs involves spatio \u2014 temporal graph data. When handling sequences of graphs, a recurrent version of GNNs is necessary [11][12]. However, much like regular recurrent neural networks (RNNs) and their variants, fixed discretizations do not allow operations on irregularly sampled data. This fact has motivated further developments in the form of RNNs with prior assumption on dynamics between arrival times [13] as well as the ODE version of RNN [14].", "In scenarios involving a temporal component, the depth domain of GDEs S coincides with the time domain and can be adapted depending on the requirements. For example, given a time window \u0394t, the prediction performed by a GDE assumes the form:", "regardless of the specific GDE architecture. GDEs represent a natural model class for autoregressive modeling of sequences of graphs and naturally lead to an extension of classical spatio \u2014 temporal architectures in the form of hybrid dynamical systems, i.e. systems characterized by interacting continuous and discrete \u2014 time dynamics. The core idea is to have a GDE smoothly steering the latent node features between two time instants and then apply some discrete operator, resulting in a jump of node features H which is then processed by an output layer.", "Given a set of time instants {(t \u2096)} and a state \u2014 graph data stream {(X \u209c , G \u209c)} the general formulation of autoregressive GDEs is:", "where F, G, K are GNN \u2014 like operators or general neural network layers and H \u207a represents the value of H after the discrete transition. The evolution of the system can be visualized by means of hybrid automata:", "Compared to standard recurrent models which are only equipped with discrete jumps, autoregressive GDEs incorporate a continuous flow of latent node features H between jumps. This feature of autoregressive GDEs allows them to track dynamical systems from irregular observations. Different combinations of F, G, K can yield continuous variants of most common spatio-temporal GNN models.", "To evaluate the effectiveness of autoregressive GDE models on forecasting tasks we perform a series of experiments on the established PeMS traffic dataset. We follow the experimental setup of [15] with an additional preprocessing step: undersampling of time series with a probability 0.7 of removal for each entry in order to simulate a challenging environment with irregular timestamps and missing values.", "To measure performance gains obtained by GDEs in settings with data generated by continuous time systems, we employ a GCDE \u2014 GRU as well as its discrete counterpart GCGRU [12] and we contextualize the results with vanilla GRU metrics. For each model under consideration we collect normalized RMSE (NRMSE) and mean absolute percentage error (MAPE). More details about the chosen metrics and data are found in our paper.", "Non \u2014 constant differences between timestamps result in a challenging forecasting task for a single model since the average prediction horizon changes drastically over the course of training and testing. For a fair comparison between models we include delta timestamp information as an additional node feature for GCGNs and GRUs.", "Since GCDE-GRUs and GCGRUs are designed to match in structure and number of parameters we can measure a performance increase of 3% in NRSME and 7% in MAPE over GCGRUs metrics. A variety of other application areas with continuous dynamics and irregular datasets could similarly benefit from adopting GDEs as modeling tools: medicine, finance or distributed control systems, to name a few. We are working on additional experiments in these domains and welcome requests, ideas or collaborations.", "As mentioned before, we are currently working on a github repository with a series of examples and applications of different types of GDEs.", "We encourage requests/suggestions of additional applications of GDEs: our plan it to eventually include working examples with GDE variants of all major graph neural network (GNN) architectures, deployed across various settings (forecasting, control\u2026). Our paper is available on arXiv as preprint: consider citing us if you find our work useful.", "[2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1993\u20132001, 2016.", "[3] Z. Cui, K. Henrickson, R. Ke, and Y. Wang. Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting. arXiv preprint arXiv:1802.07007, 2018", "[6] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems, pages 6571\u20136583, 2018.", "[7] Y. Lu, A. Zhong, Q. Li, and B. Dong. Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121, 2017.", "[15] B. Yu, H. Yin, and Z. Zhu. Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI), 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Researcher at KAIST. Working at the intersection of deep learning, dynamical systems, optimization and control."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa5e44ac2b6ec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaelpoli.medium.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": ""}, {"url": "https://michaelpoli.medium.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Michael Poli"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F912c466c26f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&user=Michael+Poli&userId=912c466c26f5&source=post_page-912c466c26f5----a5e44ac2b6ec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection", "anchor_text": "Rayleigh\u2013B\u00e9nard"}, {"url": "https://en.wikipedia.org/wiki/Finite_element_method", "anchor_text": "Finite element methods"}, {"url": "https://www.youtube.com/watch?v=OM0l2YPVMf8&t=74s.", "anchor_text": "original author"}, {"url": "https://en.wikipedia.org/wiki/N-body_simulation", "anchor_text": "physics"}, {"url": "https://en.wikipedia.org/wiki/Swarm_robotics", "anchor_text": "robotics"}, {"url": "https://en.wikipedia.org/wiki/Auction_theory", "anchor_text": "game-theory"}, {"url": "https://en.wikipedia.org/wiki/Initial_value_problem", "anchor_text": "initial value problem"}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "paper"}, {"url": "https://github.com/Zymrael/gde", "anchor_text": "github repository"}, {"url": "https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)", "anchor_text": "graphs"}, {"url": "https://arxiv.org/abs/1901.00596", "anchor_text": "survey on GNNs"}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "background section in our paper"}, {"url": "https://en.wikipedia.org/wiki/Well-posed_problem", "anchor_text": "Well-posedness"}, {"url": "https://arxiv.org/pdf/1911.07532.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations", "anchor_text": "numerical methods"}, {"url": "https://tkipf.github.io/graph-convolutional-networks/", "anchor_text": "Graph Convolutional Networks"}, {"url": "https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods", "anchor_text": "Runge-Kutta"}, {"url": "https://en.wikipedia.org/wiki/Dormand%E2%80%93Prince_method", "anchor_text": "Dormand-Prince"}, {"url": "https://en.wikipedia.org/wiki/Hybrid_system", "anchor_text": "hybrid dynamical systems"}, {"url": "https://en.wikipedia.org/wiki/Hybrid_automaton", "anchor_text": "hybrid automata"}, {"url": "https://dot.ca.gov/programs/traffic-operations/mobility-performance-reports", "anchor_text": "PeMS traffic dataset"}, {"url": "https://en.wikipedia.org/wiki/Root-mean-square_deviation#Normalized_root-mean-square_deviation", "anchor_text": "normalized RMSE"}, {"url": "https://en.wikipedia.org/wiki/Mean_absolute_percentage_error", "anchor_text": "mean absolute percentage error"}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "paper"}, {"url": "https://github.com/Zymrael/gde", "anchor_text": "github repository"}, {"url": "https://arxiv.org/abs/1911.07532", "anchor_text": "citing us"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a5e44ac2b6ec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a5e44ac2b6ec---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----a5e44ac2b6ec---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a5e44ac2b6ec---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----a5e44ac2b6ec---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&user=Michael+Poli&userId=912c466c26f5&source=-----a5e44ac2b6ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&user=Michael+Poli&userId=912c466c26f5&source=-----a5e44ac2b6ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa5e44ac2b6ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a5e44ac2b6ec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a5e44ac2b6ec--------------------------------", "anchor_text": ""}, {"url": "https://michaelpoli.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaelpoli.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Poli"}, {"url": "https://michaelpoli.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "93 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F912c466c26f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&user=Michael+Poli&userId=912c466c26f5&source=post_page-912c466c26f5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F16bb10564d7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-neural-ordinary-differential-equations-a5e44ac2b6ec&newsletterV3=912c466c26f5&newsletterV3Id=16bb10564d7a&user=Michael+Poli&userId=912c466c26f5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}