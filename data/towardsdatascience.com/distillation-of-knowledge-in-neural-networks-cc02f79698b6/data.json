{"url": "https://towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6", "time": 1683003307.805534, "path": "towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6/", "webpage": {"metadata": {"title": "Distillation of Knowledge in Neural Networks | by Mukul Malik | Towards Data Science", "h1": "Distillation of Knowledge in Neural Networks", "description": "Currently, especially in NLP, very large scale models are being trained. A large portion of those can\u2019t even fit on an average person\u2019s hardware. Plus, due to the Law of diminishing returns, a great\u2026"}, "outgoing_paragraph_urls": [{"url": "https://developers.google.com/machine-learning/glossary/#logits", "anchor_text": "Logits", "paragraph_index": 37}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "XLM", "paragraph_index": 49}], "all_paragraphs": ["Distillation of Knowledge (in machine learning) is an architecture agnostic approach for generalization of knowledge (consolidating the knowledge) within a neural network to train another neural network.", "Currently, especially in NLP, very large scale models are being trained. A large portion of those can\u2019t even fit on an average person\u2019s hardware. Plus, due to the Law of diminishing returns, a great increase in the size of model barely maps to a small increase in the accuracy.", "These models can barely run on commercial servers, let alone on a smartphone.", "Using distillation, one could reduce the size of models like BERT by 87% and still retain 96% of its performance.", "The goal of every learner is to optimize its performance on training data. This doesn\u2019t exactly translate as a generalization of knowledge within the dataset.", "Take an example of MNIST dataset. Let\u2019s pick a sample picture of number 3.", "In training data, the number 3 translates to a corresponding one-hot-vector:", "This vector simply tells that the number in that image in 3 but", "fails to explicitly mention anything about the shape of number 3. Like the shape of 3 is similar to 8.", "neural network is never explicitly being asked to learn the generalized undersdtanding of the training data. The degree of generalization is the implicit ability of the neural network.", "As a result, in a normally trained neural network, information (detected feature) within each neuron is not equally significant with respect to the desired output.", "Simply put normally trained neural networks carry a lot of dead weight in the form of neurons which never learned to generalize data and hence result in lowering the accuracy over the test data.", "Distillation enables us to train another neural network using a pre-trained network, without the dead weight of the original neural network.", "Enabling us to compress the size of the network without much loss of accuracy.", "Hence distilled models have higher accuracies than their normally trained counterparts.", "Note: Distillation of knowledge can be done from any form of a learner (logistic regression, SVM, neural networks etc) to any other form of a learner.", "Though for the simplicity of this blog, I\u2019ll be taking references of only neural networks.", "Let us take a step back and revise the goal of a neural network:", "predict the output for samples that the network had never seen during training by generalizing the knowledge within the training data.", "Taking an example of a discriminative neural network whose objective is to identify the relevant class for a given input.", "Now the neural network returns distribution of probabilities across all classes, even the wrong ones.", "This tells us a lot about the capability of the network to generalize over the concepts within the training data.", "For a decently trained neural network on MNIST following observations would be true:", "So, the neural network is able to identify that the shape of the number in that image is 3 but the neural network also suggests that the shape of 3 is quite similar to the shape of numbers 8 and 0 (all are quite curvy).", "Let\u2019s start with the obvious, train an enormously gigantic neural network (that your hardware can support) in a normal manner. We\u2019ll be referring to this network as the cumbersome network.", "Note: This cumbersome model can very well be an ensemble of multiple normally trained models.", "When the cumbersome model is not a single model but an ensemble of multiple models, the arithmetic/geometric mean of their outputs as the soft-targets.", "The new model can be trained on the same dataset as the original model or on a different dataset called the \u2018transfer set\u2019.", "Transfer-Set: Pass the data through the cumbersome model and use its output (probability distribution) as the respective truth values. It can consist of the dataset used to train the original model, new dataset or both.", "By adjusting the temperature of the soft-targets, it is possible to adjust the size of this transfer-set.", "When soft-targets have high entropy, they give much more information per-training sample than hard-targets (let\u2019s get back to that in a minute).", "Something we learned in Physics that applies here \u2018Entropy increases with temperature\u2019.", "Imagine a box of balls well stacked on top of each other. If we increase the entropy of the box by shaking it a bit, the balls don\u2019t fall out of the box but rather spread a bit.", "The resulting distribution has a shape similar to the original distribution but the magnitude of its peaks change.", "However, at higher temperatures, a certain amount of heat added to the system causes a smaller change in entropy than the same amount of heat at a lower temperature.", "Following is an example of probability distribution comparing a system with low temperature and high temperature.", "Now the formula we use for temperature for transferring knowledge from one neural network to another is:", "Logits: vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function.", "Imagine an MNIST data-point for number 3 (yes, I like the number 3) but let us concentrate on only 3 (yes yes) classes:", "Higher temperature results in a softer probability distribution over classes.", "Now, let\u2019s visualize the smoothness of probability distribution as we increase the temperature:", "The simplest form of distillation is training a model using the soft targets generated by a cumbersome model with high temperature and are distilled into another model with the same temperature.", "After the training, the temperature of the distilled model is set to 1.", "By now we have established that the distilled network can be trained on a transfer set consisting of soft targets.", "We can also leverage the truth values or the hard targets which are known for all or some of the data.", "One of the most efficient methods of doing this is by using 2 objective functions:", "Note: The magnitude of soft-targets is scaled to i/T\u00b2 times, whereas hard-targets undergo no such scaling. So we multiply the soft-targets with T\u00b2 to normalize the impact of soft-targets and hard-targets.", "HuggingFace actually provides scripts to train your own DistilBERT & DistilRoBERTa, which are 40% smaller, 60% faster while retaining 99% accuracy of the original model.", "First, we will binarize the data, i.e. tokenize the data and convert each token in an index in our model\u2019s vocabulary.", "HuggingFace follows XLM\u2019s one and smoothes the probability of masking with a factor that put more emphasis on rare words. Thus count the occurrences of each token in the data:", "Training with distillation is really simple once you have pre-processed the data:", "By distilling neural networks, we obtain a smaller model that bears a lot of similarities with the original model while being lighter, smaller and faster to run.", "Distilled models are thus an interesting option to put the large-scaled neural networks into production.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Manager Data Science at PublicisSapient (NLP & Deep Learning Specialist)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcc02f79698b6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mukulmalik?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Mukul Malik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbd3e39707a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&user=Mukul+Malik&userId=bd3e39707a79&source=post_page-bd3e39707a79----cc02f79698b6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://developers.google.com/machine-learning/glossary/#logits", "anchor_text": "Logits"}, {"url": "https://github.com/huggingface/transformers.git", "anchor_text": "https://github.com/huggingface/transformers.gi"}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "XLM"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----cc02f79698b6---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cc02f79698b6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cc02f79698b6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cc02f79698b6---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----cc02f79698b6---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&user=Mukul+Malik&userId=bd3e39707a79&source=-----cc02f79698b6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&user=Mukul+Malik&userId=bd3e39707a79&source=-----cc02f79698b6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcc02f79698b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cc02f79698b6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cc02f79698b6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cc02f79698b6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cc02f79698b6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mukul Malik"}, {"url": "https://medium.com/@mukulmalik/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "656 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbd3e39707a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&user=Mukul+Malik&userId=bd3e39707a79&source=post_page-bd3e39707a79--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3e3c8532f5d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistillation-of-knowledge-in-neural-networks-cc02f79698b6&newsletterV3=bd3e39707a79&newsletterV3Id=3e3c8532f5d8&user=Mukul+Malik&userId=bd3e39707a79&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}