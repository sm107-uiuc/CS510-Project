{"url": "https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9", "time": 1683012320.678682, "path": "towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9/", "webpage": {"metadata": {"title": "Overview of tokenization algorithms in NLP | by Ane Berasategi | Towards Data Science", "h1": "Overview of tokenization algorithms in NLP", "description": "This article is an overview of tokenization algorithms, ranging from word level, character level and subword level tokenization, with emphasis on BPE, Unigram LM, WordPiece used in BERT and SentencePiece. It is..."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer XL", "paragraph_index": 9}, {"url": "https://github.com/karpathy/char-rnn", "anchor_text": "Karpathy in 2015", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1704.01444", "anchor_text": "Radford et al. (2017)", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1610.10099", "anchor_text": "Kalchbrenner et al. (2016)", "paragraph_index": 11}, {"url": "https://www.aclweb.org/anthology/Q17-1026", "anchor_text": "Leet et al. (2017)", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers,", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "Sennrich et al. in 2015", "paragraph_index": 16}, {"url": "https://leimao.github.io/blog/Byte-Pair-Encoding/", "anchor_text": "many other articles", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1804.10959", "anchor_text": "Kudo, 2018", "paragraph_index": 18}, {"url": "https://research.google.com/pubs/archive/37842.pdf", "anchor_text": "Schuster and Nakajima, 2012", "paragraph_index": 20}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "Github", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM", "paragraph_index": 21}, {"url": "https://twitter.com/aberasategi", "anchor_text": "Twitter", "paragraph_index": 23}], "all_paragraphs": ["This article is an overview of tokenization algorithms, ranging from word level, character level and subword level tokenization, with emphasis on BPE, Unigram LM, WordPiece and SentencePiece. It is meant to be readable by both experts and beginners alike. If any concept or explanation is unclear, please contact me and I will be happy to clarify whatever is needed.", "Tokenization is one of the first steps in NLP, and it\u2019s the task of splitting a sequence of text into units with semantic meaning. These units are called tokens, and the difficulty in tokenization lies on how to get the ideal split so that all the tokens in the text have the correct meaning, and there are no left out tokens.", "In most languages, text is composed of words divided by whitespace, where individual words have a semantic meaning. We will see later what happens with languages that use symbols, where a symbols has a much more complex meaning than a word. For now we can work with English. As an example:", "\u2018Burger\u2019 is a type of food, \u2018and\u2019 is a conjunction, \u2018good\u2019 is a positive adjective, and so on. By tokenizing this way, each element has a meaning, and by joining all the meanings of each token we can understand the meaning of the whole sentence. The punctuation marks get their own tokens as well, the comma to separate clauses and the period to signal the end of the sentence. Here is an alternate tokenization:", "For the multiword unit \u2018I ate\u2019, we can just add the meanings of \u2018I\u2019 and \u2018ate\u2019, and for the subword units \u2018bur\u2019 and \u2018ger\u2019, they have no meaning separately but by joining them we arrive at the familiar word and we can understand what it means.", "But what do we do with \u2018d it\u2019? What meaning does this have? As humans and speakers of English, we can deduce that \u2018it\u2019 is a pronoun, and the letter \u2018d\u2019 belongs to a previous word. But following this tokenization, the previous word \u2018an\u2019 already has a meaning in English, the article \u2018an\u2019 very different from \u2018and\u2019. How to deal with this? You might be thinking: stick with words, and give punctuations their own tokens. This is the most common way of tokenizing, called word level tokenization.", "It consists only of splitting a sentence by the whitespace and punctuation marks. There are plenty of libraries in Python that do this, including NLTK, SpaCy, Keras, Gensim or you can do a custom Regex.", "Splitting on whitespace can also split an element which should be regarded as a single token, for example, New York. This is problematic and mostly the case with names, borrowed foreign phrases, and compounds that are sometimes written as multiple words.", "What about words like \u2018don\u2019t\u2019, or contractions like \u2018John\u2019s\u2019? Is it better to obtain the token \u2018don\u2019t\u2019 or \u2018do\u2019 and \u2018n\u2019t\u2019? What if there is a typo in the text, and burger turns into \u2018birger\u2019? We as humans can see that it was a typo, replace the word with \u2018burger\u2019 and continue, but machines can\u2019t. Should the typo affect the complete NLP pipeline?", "Another drawback of word level tokenization is the huge vocabulary size it creates. Each token is saved into a token vocabulary, and if the vocabulary is built with all the unique words found in all the input text, it creates a huge vocabulary, which produces memory and performance problems later on. A current state-of-the-art deep learning architecture, Transformer XL, has a vocabulary size of 267,735. To solve the problem of the big vocabulary size, we can think of creating tokens with characters instead of words, which is called character level tokenization.", "First introduced by Karpathy in 2015, instead of splitting a text into words, the splitting is done into characters, for example, smarter becomes s-m-a-r-t-e-r. The vocabulary size is dramatically reduced to the number of characters in the language, 26 for English plus the special characters. Misspellings or rare words are handled better because they are broken down into characters and these characters are already known in the vocabulary.", "Tokenizing sequences at the character level has shown some impressive results. Radford et al. (2017) from OpenAI showed that character level models can capture the semantic properties of text. Kalchbrenner et al. (2016) from Deepmind and Leet et al. (2017) both demonstrated translation at the character level. These are particularly impressive results as the task of translation captures the semantic understanding of the underlying text.", "Reducing the vocabulary size has a tradeoff with the sequence length. Now, each word being splitted into all its characters, the tokenized sequence is much longer than the initial text. The word \u2018smarter\u2019 is transformed into 7 different tokens. Additionally, the main goal of tokenization is not achieved, because characters, at least in English, have no semantic meaning. Only when joining characters together do they acquire a meaning. As an in-betweener between word and character tokenization, subword tokenization produces subword units, smaller than words but bigger than just characters.", "Subword level tokenization doesn\u2019t transform most common words, and decomposes rare words in meaningful subword units. If \u2018unfriendly\u2019 was labelled as a rare word, it would be decomposed into \u2018un-friend-ly\u2019 which are all meaningful units, \u2018un\u2019 meaning opposite, \u2018friend\u2019 is a noun, and \u2018ly\u2019 turns it into an adverb. The challenge here is how to make that segmentation, how do we get \u2018un-friend-ly\u2019 and not \u2018unfr-ien-dly\u2019.", "As of 2020, the state-of-the-art deep learning architectures, based on Transformers, use subword level tokenization. BERT makes the following tokenization for this example:", "Words present in the vocabulary are tokenized as words themselves, but \u2018GPU\u2019 is not found in the vocabulary and is treated as a rare word. Following an algorithm it is decided that it is segmented into \u2018gp-u\u2019. The ## before \u2018u\u2019 are to show that this subword belongs to the same word as the previous subword. BPE, Unigram LM, WordPiece and SentencePiece are the most common subword tokenization algorithms. They will be explained briefly because this is an introductory post, if you are interested in deeper descriptions, let me know and I will do more detailed posts for each of them.", "Introduced by Sennrich et al. in 2015, it merges the most frequently occurring character or character sequences iteratively. This is roughly how the algorithm works:", "BPE is a greedy and deterministic algorithm and can not provide multiple segmentations. That is, for a given text, the tokenized text is always the same. A more detailed explanation of how BPE works will be detailed in a later article, or you can also find it in many other articles.", "Unigram language modelling (Kudo, 2018) is based on the assumption that all subword occurrences are independent and therefore subword sequences are produced by the product of subword occurrence probabilities. These are the steps of the algorithm:", "Kudo argues that the unigram LM model is more flexible than BPE because it is based on a probabilistic LM and can output multiple segmentations with their probabilities. Instead of starting with a group of base symbols and learning merges with some rule, like BPE or WordPiece, it starts from a large vocabulary (for instance, all pretokenized words and the most common substrings) that it reduces progressively.", "WordPiece (Schuster and Nakajima, 2012) was initially used to solve Japanese and Korean voice problem, and is currently known for being used in BERT, but the precise tokenization algorithm and/or code has not been made public. It is similar to BPE in many ways, except that it forms a new subword based on likelihood, not on the next highest frequency pair. These are the steps of the algorithm:", "All the tokenization methods so far required some form of pretokenization, which constitutes a problem because not all languages use spaces to separate words, or some languages are made of symbols. SentencePiece is equipped to accept pretokenization for specific languages. You can find the open source software in Github. For example, XLM uses SentencePiece and adds specific pretokenizers for Chinese, Japanese and Thai.", "SentencePiece is conceptually similar to BPE, but it does not use the greedy encoding strategy, achieving higher quality tokenization. SentencePiece sees ambiguity in character grouping as a source of regularization for the model during training, which makes training much slower because there are more parameters to optimize for and discouraged Google from using it in BERT, opting for WordPiece instead.", "Historically, tokenization methods have evolved from word to character, and lately subword level. This is a quick overview of tokenization methods, I hope the text is readable and understandable. Follow me on Twitter for more NLP information, or ask me any questions there :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc41a7d5ec4f9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://anebz.medium.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": ""}, {"url": "https://anebz.medium.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Ane Berasategi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb250300b3b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&user=Ane+Berasategi&userId=b250300b3b8&source=post_page-b250300b3b8----c41a7d5ec4f9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hannahwrightdesigner?utm_source=medium&utm_medium=referral", "anchor_text": "Hannah Wright"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://anebz.eu/nlp-tokenization-algorithms", "anchor_text": "ORIGINAL POST"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer XL"}, {"url": "https://github.com/karpathy/char-rnn", "anchor_text": "Karpathy in 2015"}, {"url": "https://arxiv.org/abs/1704.01444", "anchor_text": "Radford et al. (2017)"}, {"url": "https://arxiv.org/abs/1610.10099", "anchor_text": "Kalchbrenner et al. (2016)"}, {"url": "https://www.aclweb.org/anthology/Q17-1026", "anchor_text": "Leet et al. (2017)"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers,"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "Sennrich et al. in 2015"}, {"url": "https://leimao.github.io/blog/Byte-Pair-Encoding/", "anchor_text": "many other articles"}, {"url": "https://arxiv.org/abs/1804.10959", "anchor_text": "Kudo, 2018"}, {"url": "https://research.google.com/pubs/archive/37842.pdf", "anchor_text": "Schuster and Nakajima, 2012"}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "Github"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM"}, {"url": "https://twitter.com/aberasategi", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c41a7d5ec4f9---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c41a7d5ec4f9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tokenization?source=post_page-----c41a7d5ec4f9---------------tokenization-----------------", "anchor_text": "Tokenization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c41a7d5ec4f9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/language?source=post_page-----c41a7d5ec4f9---------------language-----------------", "anchor_text": "Language"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&user=Ane+Berasategi&userId=b250300b3b8&source=-----c41a7d5ec4f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&user=Ane+Berasategi&userId=b250300b3b8&source=-----c41a7d5ec4f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc41a7d5ec4f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c41a7d5ec4f9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c41a7d5ec4f9--------------------------------", "anchor_text": ""}, {"url": "https://anebz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://anebz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ane Berasategi"}, {"url": "https://anebz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "351 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb250300b3b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&user=Ane+Berasategi&userId=b250300b3b8&source=post_page-b250300b3b8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F14aee5609f7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-nlp-tokenization-algorithms-c41a7d5ec4f9&newsletterV3=b250300b3b8&newsletterV3Id=14aee5609f7e&user=Ane+Berasategi&userId=b250300b3b8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}