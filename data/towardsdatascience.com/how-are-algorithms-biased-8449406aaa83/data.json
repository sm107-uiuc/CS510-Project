{"url": "https://towardsdatascience.com/how-are-algorithms-biased-8449406aaa83", "time": 1683010041.586518, "path": "towardsdatascience.com/how-are-algorithms-biased-8449406aaa83/", "webpage": {"metadata": {"title": "Bias In AI Algorithms. Algorithms reinforce human biases and\u2026 | by Karan Praharaj | Towards Data Science", "h1": "Bias In AI Algorithms", "description": "Algorithms do what they\u2019re taught. Unfortunately some are inadvertently taught prejudices and unethical biases by societal patterns hidden in the data. This can be dangerous."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment", "anchor_text": "Tuskegee syphilis experiment", "paragraph_index": 0}, {"url": "https://in.reuters.com/article/amazon-com-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idINKCN1MK0AH", "anchor_text": "It is not.", "paragraph_index": 9}, {"url": "https://twitter.com/jackyalcine/status/615329515909156865", "anchor_text": "pointed out", "paragraph_index": 17}, {"url": "https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/", "anchor_text": "Google Photos were classifying his black friends as \u201cgorillas.\u201d", "paragraph_index": 17}, {"url": "https://www.npr.org/2020/06/24/882683463/the-computer-got-it-wrong-how-facial-recognition-led-to-a-false-arrest-in-michig", "anchor_text": "Another disastrous episode", "paragraph_index": 19}, {"url": "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html", "anchor_text": "M.I.T.", "paragraph_index": 19}, {"url": "https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html", "anchor_text": "National Institute of Standards and Technology", "paragraph_index": 19}, {"url": "https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf", "anchor_text": "Language is always \u201csituated\u201d", "paragraph_index": 21}, {"url": "https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1", "anchor_text": "Universal Sentence Encoder", "paragraph_index": 24}, {"url": "https://karanpraharaj.github.io", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://twitter.com/IntrepidIndian", "anchor_text": "IntrepidIndian", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1904.07325.pdf", "anchor_text": "Characterizing the Variability in Face Recognition Accuracy Relative to Race", "paragraph_index": 31}, {"url": "https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf", "anchor_text": "Life of Language", "paragraph_index": 32}, {"url": "https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html", "anchor_text": "Text Embedding Models Contain Bias. Here\u2019s Why That Matters.", "paragraph_index": 33}, {"url": "http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d", "anchor_text": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "paragraph_index": 34}], "all_paragraphs": ["After the end of the Second World War, the Nuremberg trials laid bare the atrocities conducted in medical research by the Nazis. In the aftermath of the trials, the medical sciences established a set of rules \u2014 The Nuremberg Code \u2014 to control future experiments involving human subjects. The Nuremberg Code has influenced medical codes of ethics around the world, as has the exposure of experiments that had failed to follow it even three decades later, such as the infamous Tuskegee syphilis experiment.", "The direct negative impact of AI experiments and applications on users isn\u2019t quite as inhumane as that of the Tuskegee and Nazi experimentations, but in the face of an overwhelming and growing body of evidence of algorithms being biased against certain demographic cohorts, it is important that a dialogue takes place sooner or later. AI systems can be biased based on who builds them, the way they are developed, and how they\u2019re eventually deployed. This is known as algorithmic bias.", "While the data sciences have not developed a Nuremberg Code of their own yet, the social implications of research in artificial intelligence are starting to be addressed in some curricula. But even as the debates are starting to sprout up, what is still lacking is a discipline-wide discussion to grapple with questions of how to tackle societal and historical inequities that are reinforced by AI algorithms.", "We are flawed creatures. Every single decision we make involves a certain kind of bias. However, algorithms haven\u2019t proven to be much better. Ideally, we would want our algorithms to make better-informed decisions devoid of biases so as to ensure better social justice, i.e., equal opportunities for individuals and groups (such as minorities) within society to access resources, have their voices heard, and be represented in society.", "When these algorithms do the job of amplifying racial, social and gender inequality, instead of alleviating it; it becomes necessary to take stock of the ethical ramifications and potential malevolence of the technology.", "This essay was motivated by two flashpoints : the racial inequality discussion that is now raging on worldwide, and Yann LeCun\u2019s altercation with Timnit Gebru on Twitter which was caused due to a disagreement over a downsampled image of Barack Obama (left) that was depixelated to a picture of a white man (right) by a face upsampling machine learning (ML) model.", "The (rather explosive) argument was sparked by this tweet by LeCun where he says that the resulting face was that of a white man because of a bias in data that trained the algorithm. Gebru responded sharply that the harms of ML systems cannot be reduced to biased data.", "In most baseline ML algorithms, the model fits better to the attributes that that occur most frequently across various data points. For example, if you were to design an AI recruiting tool to review the r\u00e9sum\u00e9s of applicants for a software engineering position, you would first need to train it with a dataset of past candidates which contains details like \u201cexperience\u201d, \u201cqualifications\u201d, \u201cdegree(s) held\u201d, \u201cpast projects\u201d etc. For every datapoint, the algorithm of the hiring tool would need a decision or a \u201clabel\u201d, so as to \u201clearn\u201d how to make a decision for a given applicant by observing patterns in their r\u00e9sum\u00e9.", "For an industry where the gender disparity in representation is large, it is reasonable to assume that a large majority of the data points will be male applicants. And this collective imbalance in the data ends up being interpreted by the algorithm as a useful pattern in the data rather than undesirable noise which is to be ignored. Consequently, it teaches itself that male candidates are more preferable than female candidates.", "I wish that this was merely an imaginary, exaggerated example that I used to prove my point. It is not.", "LeCun wasn\u2019t wrong in his assessment because in the case of that specific model, training the model on a dataset that contains faces of black people (as opposed to one that contains mainly white faces) would not have given rise to an output as absurd as that. But the upside of the godfather of modern AI getting dragged into a spat (albeit unfairly) has meant that more researchers will now be aware of the implications of their research.", "The misunderstanding clearly seems to emanate from the interpretation of the word \u201cbias\u201d \u2014 which in any discussion about the social impact of ML/AI seems to get crushed under the burden of its own weight.", "As Sebastian Raschka puts it, \u201cthe term bias in ML is heavily overloaded\u201d. It has multiple senses that can all be mistaken for each other.", "(1) bias (as in mathematical bias unit) \u200b (2) \u201cFairness\u201d bias (also called societal bias) \u200b (3) ML bias (also known as inductive bias, which is dependent on decisions taken to build the model.) \u200b (4) bias-variance decomposition of a loss function \u200b (5) Dataset bias (usually causing 2)", "I imagine that a lot of gaps in communication could be covered by just being a little more precise when we use these terms.", "On a lighter note, never mind Obama, the model even depixelized a dog\u2019s face to a caucasian man\u2019s. It sure loves the white man.", "Learning algorithms have inductive biases going beyond the biases in data too, sure. But if the data has a little bias, it is amplified by these systems, thereby causing high biases to be learnt by the model. Simply put, creating a 100% non-biased dataset is practically impossible. Any dataset picked by humans is cherry-picked and non-exhaustive. Our social cognitive biases result in inadvertent cherry-picking of data. This biased data, when fed to a data-variant model (a model whose decisions are heavily influenced by the data it sees) encodes these societal, racial, gender, cultural and political biases and bakes them into the ML model.", "These problems are exacerbated, once they are applied to products. A couple of years ago, Jacky Alcin\u00e9 pointed out that the image recognition algorithms in Google Photos were classifying his black friends as \u201cgorillas.\u201d Google apologised for the blunder and assured to resolve the issue. However, instead of coming up with a proper solution, it simply blocked the algorithm from identifying gorillas at all.", "It might seem surprising that a company of Google\u2019s size was unable to come up with a solution to this. But this only goes to show that training an algorithm that is consistent and fair isn\u2019t an easy proposition, not least when it is not trained and tested on a diverse set of categories that represent various demographic cohorts of the population proportionately.", "Another disastrous episode of facial recognition tech getting it terribly wrong came as recently as last week when a faulty facial recognition match led to a Michigan man\u2019s arrest for a crime he did not commit. Recent studies by M.I.T. and the National Institute of Standards and Technology, or NIST, found that even though face recognition works well on white men, the results are not good enough for other demographics (the misidentification ratio can be more than 10 times worse), in part because of a lack of diversity in the images used to develop the underlying databases.", "Problems of algorithmic bias are not limited to image/video tasks and they manifest themselves in language tasks too.", "Language is always \u201csituated\u201d, i.e., it depends on external references for its understanding and the receiver(s) must be in a position to resolve these references. This therefore means that the text used to train models carries latent information about the author and the situation, albeit to varying degrees.", "Due to the situatedness of language, any language data set inevitably carries with it a demographic bias. For example, some speech to text transcription models tend to have higher error rates for African Americans, Arabs and South Asians as compared to Americans and Europeans. This is because the corpus that the speech recognition models are trained are dominated by utterances of people from western countries. This causes the system to be good at interpreting European and American accents but subpar at transcribing speech from other parts of the world.", "Another example in this space is the gender biases in existing word embeddings (which are learned through a neural networks) that show females having a higher association with \u201cless-cerebral\u201d occupations while males tend to be associated with purportedly \u201cmore-cerebral\u201d or higher paying occupations.", "In the table below, we see the gender bias scores associated with various occupations in the Universal Sentence Encoder embedding model. The occupations with positive scores are female-biased occupations and ones with negative scores are male-biased occupations.", "While it is easy for ML Researchers to hold their hands up and absolve themselves of all responsibility, it is imperative for them to acknowledge that they \u2014 knowingly or otherwise \u2014 build the base layer of AI products for a lot of companies that are devoid of AI expertise. These companies, without the knowledge of fine-tuning and tweaking models, use pre-trained models, as they are, put out on the internet by ML researchers (like GloVe, BERT, ResNet, YOLO etc).", "Deploying these models without explicitly recalibrating them to account for demographic differences is perilous and can lead to issues of exclusion and overgeneralisation of people along the way. The buck stops with the researchers who must own up responsibility for the other side of the coin.", "It is also easy to blame the data and not the algorithm. (It reminds me of the Republican stance on the second amendment debate: \u201cGuns don\u2019t kill people, people kill people.\u201d) Pinning the blame on just the data is irresponsible and akin to saying that the racist child isn\u2019t racist because he was taught the racism by his racist father.", "More than we need to improve the data, it is the algorithms that need to be made more robust, less sensitive and less prone to being biased by the data. This needs to be a responsibility for anyone who does research. In the meantime, de-bias the data.", "The guiding question for deployment of algorithms in the real world should always be \u201cwould a false answer be worse than no answer?\u201d", "You can visit my page here. My Twitter handle is @IntrepidIndian.", "2) Krishnapriya, KS., Vangara, K., King, M., Albiero, V., Bowyer, K. Characterizing the Variability in Face Recognition Accuracy Relative to Race in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.", "3) Life of Language by Martin Kay, Stanford University", "4) Text Embedding Models Contain Bias. Here\u2019s Why That Matters. by Ben Packer, Yoni Halpern, Mario Guajardo-C\u00e9spedes & Margaret Mitchell, Google AI", "5) Bolukbasi, T., Chang, KW., Zou, J., Saligrama, V., Kalai, A. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings in Advances in Neural Information Processing Systems 29, 2016.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Likes to explore how computers can be made to understand human language."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8449406aaa83&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8449406aaa83--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@karanpraharaj?source=post_page-----8449406aaa83--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanpraharaj?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Karan Praharaj"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2eee9862711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&user=Karan+Praharaj&userId=2eee9862711a&source=post_page-2eee9862711a----8449406aaa83---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment", "anchor_text": "Tuskegee syphilis experiment"}, {"url": "https://in.reuters.com/article/amazon-com-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idINKCN1MK0AH", "anchor_text": "It is not."}, {"url": "https://twitter.com/jackyalcine/status/615329515909156865", "anchor_text": "pointed out"}, {"url": "https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/", "anchor_text": "Google Photos were classifying his black friends as \u201cgorillas.\u201d"}, {"url": "https://www.npr.org/2020/06/24/882683463/the-computer-got-it-wrong-how-facial-recognition-led-to-a-false-arrest-in-michig", "anchor_text": "Another disastrous episode"}, {"url": "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html", "anchor_text": "M.I.T."}, {"url": "https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html", "anchor_text": "National Institute of Standards and Technology"}, {"url": "https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf", "anchor_text": "Language is always \u201csituated\u201d"}, {"url": "https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1", "anchor_text": "Universal Sentence Encoder"}, {"url": "https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html", "anchor_text": "https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html"}, {"url": "https://karanpraharaj.github.io", "anchor_text": "here"}, {"url": "https://twitter.com/IntrepidIndian", "anchor_text": "IntrepidIndian"}, {"url": "https://towardsdatascience.com/[", "anchor_text": "Facial Recognition Is Accurate, if You\u2019re a White Guy"}, {"url": "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html", "anchor_text": "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html"}, {"url": "https://arxiv.org/pdf/1904.07325.pdf", "anchor_text": "Characterizing the Variability in Face Recognition Accuracy Relative to Race"}, {"url": "https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf", "anchor_text": "Life of Language"}, {"url": "https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html", "anchor_text": "Text Embedding Models Contain Bias. Here\u2019s Why That Matters."}, {"url": "http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d", "anchor_text": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"}, {"url": "https://medium.com/tag/racism?source=post_page-----8449406aaa83---------------racism-----------------", "anchor_text": "Racism"}, {"url": "https://medium.com/tag/ai-bias?source=post_page-----8449406aaa83---------------ai_bias-----------------", "anchor_text": "Ai Bias"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8449406aaa83---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/fairness?source=post_page-----8449406aaa83---------------fairness-----------------", "anchor_text": "Fairness"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8449406aaa83---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&user=Karan+Praharaj&userId=2eee9862711a&source=-----8449406aaa83---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&user=Karan+Praharaj&userId=2eee9862711a&source=-----8449406aaa83---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8449406aaa83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8449406aaa83---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8449406aaa83--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8449406aaa83--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8449406aaa83--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8449406aaa83--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8449406aaa83--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanpraharaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanpraharaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Karan Praharaj"}, {"url": "https://medium.com/@karanpraharaj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2eee9862711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&user=Karan+Praharaj&userId=2eee9862711a&source=post_page-2eee9862711a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2eee9862711a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-algorithms-biased-8449406aaa83&user=Karan+Praharaj&userId=2eee9862711a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}