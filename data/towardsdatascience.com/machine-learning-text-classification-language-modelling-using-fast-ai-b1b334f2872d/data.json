{"url": "https://towardsdatascience.com/machine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d", "time": 1682994869.0545762, "path": "towardsdatascience.com/machine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d/", "webpage": {"metadata": {"title": "Machine Learning \u2014 Text Classification, Language Modelling using fast.ai | by Javaid Nabi | Towards Data Science", "h1": "Machine Learning \u2014 Text Classification, Language Modelling using fast.ai", "description": "Transfer learning is a technique where instead of training a model from scratch, we reuse a pre-trained model and then fine-tune it for another related task. It has been very successful in computer\u2026"}, "outgoing_paragraph_urls": [{"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT", "paragraph_index": 0}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI Transformer,", "paragraph_index": 0}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe", "paragraph_index": 2}, {"url": "https://course.fast.ai/", "anchor_text": "options", "paragraph_index": 9}, {"url": "https://course.fast.ai/start_floydhub.html", "anchor_text": "Floydhub", "paragraph_index": 10}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "anchor_text": "post", "paragraph_index": 11}, {"url": "https://docs.fast.ai/text.data.html#TextDataBunch", "anchor_text": "documentation", "paragraph_index": 13}, {"url": "https://docs.fast.ai/text.data.html#TextLMDataBunch", "anchor_text": "TextLMDataBunch", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "paper", "paragraph_index": 25}, {"url": "https://docs.fast.ai/basic_train.html#Learner", "anchor_text": "Learner", "paragraph_index": 39}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "this", "paragraph_index": 44}, {"url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "anchor_text": "here", "paragraph_index": 52}, {"url": "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456", "anchor_text": "here", "paragraph_index": 52}, {"url": "https://github.com/javaidnabi31/text_classification_fast_ai", "anchor_text": "here", "paragraph_index": 52}], "all_paragraphs": ["Transfer learning is a technique where instead of training a model from scratch, we reuse a pre-trained model and then fine-tune it for another related task. It has been very successful in computer vision applications. In natural language processing (NLP) transfer learning was mostly limited to the use of pre-trained word embeddings. Research in the field of using language modelling during pre-training have resulted in massive leap in state-of-the-art results for many of the NLP tasks, such as text classification, natural language inference and question-answering through various approaches such as ULMFiT, the OpenAI Transformer, ELMo and Google AI\u2019s BERT.", "In this post, we will discuss the limitations of word embedding approach in transfer learning for NLP problems and use of language model to build a text classifier using fast.ai library.", "Word embeddings algorithms word2vec and GloVe provide a mapping of words to a high-dimensional continuous vector space where different words with a similar meaning have a similar vector representation. These word embeddings, pre-trained on large amounts of unlabeled data, are used to initialize the first layer of a neural network called embedding layer, the rest of the model is then trained on data of a particular task. This kind of transfer learning in NLP problems is shallow as learning is transferred to only the first layer of the model, the rest of the network still needs to be trained from scratch. A model initialized with word embeddings needs to learn from scratch not only to disambiguate words, but also to derive meaning from a sequence of words. In order to do so, NLP models initialized with these shallow representations require a huge dataset to achieve good performance, which can result in very large computational costs as well [1].", "The core objective of a language modelling is of language understanding and it requires modeling complex language phenomena to deal with challenging language understanding problems such as translation, question answering and sentiment analysis. A language model attempts to learn the structure of natural language through hierarchical representations, and thus contains both low-level features (word representations) and high-level features (semantic meaning). A key feature of language modelling is that it is generative, meaning that it aims to predict the next word given a previous sequence of words. It is able to do this because language models are typically trained on very large datasets in an unsupervised manner, and hence the model can \u201clearn\u201d the syntactic features of language in a much deeper way than word embeddings [2].", "To predict the next word of a sentence, the model actually needs to know quite a lot about the language and quite a lot of world knowledge. Here is an an example:", "As you can see, there\u2019s not enough information here to decide what the next word probably is. But with a neural net, you absolutely can, provided you train a neural net to predict the next word of a sentence then you actually have a lot of information [3].", "fastai library is focused on using pre-trained Language Models and fine-tuning them, done in below three steps:", "Before we proceed further, we need to setup environment for fast.ai.", "conda install -c pytorch -c fastai fastai pytorch", "In case you want to try out some ready to run options:", "I used Colab for initial learning but faced lot of \u2018disconnected\u2019 issues. Kaggle is another good option as well. Floydhub worked smoothly but after free credits, you need to pay for the usage. Having setup your environment let us proceed for some action.", "Let us build a text classifier to classify the sentiments of IMDB movie dataset using the techniques discussed so far. IMDb dataset contains set of 25,000 movie reviews for training, and 25,000 for testing. We already have the IMDb data downloaded and saved in a csv format from the previous post. Let us load the data into a dataframe,", "We are using 5000 [2500 each label] from training and 3000[1500 each label] from validation examples. I am using smaller set to finish the training faster, please use full dataset for improved performance.", "Creating a dataset from your raw texts is very simple. The library provides very easy to use API\u2019s [4], depending on how our data is structured, to create a data class TextDataBunch for text processing, from_csv, from_folder,from_df refer to documentation for more details. Here we\u2019ll use the method from_df of the TextLMDataBunch to create a language model specific data bunch:", "This does all the necessary preprocessing behind the scene. Let us see how the data is encoded by fast.ai", "We can see a number of tags are applied to words as shown above. This is to retain all the information which can be used to gather an understanding of the new task\u2019s vocabulary. All punctuation, hashtags and special characters are also retained. The text is encoded with various tokens like below:", "Vocabulary: List of unique possible tokens is called the vocabulary. Listing below first 20 unique tokens in order of frequency:", "Numericalization: Finally it is easier for machine to deal with the numbers so replace the tokens with the location of the token in the vocab:", "The default vocab size is set to 60,000 words and min count for a word to be added to vocab is 2, to avoid getting the weight matrix huge.", "Save and Load: We can save the data bunch after the pre-processing is done. We can load as well whenever we need.", "Fast.ai has a pre-trained Wikitext model, consisting of a pre-processed subset of 103 million tokens extracted from Wikipedia. It\u2019s a model that understands a lot about language and a lot about what language describes. Next step is to fine-tune this model and do transfer learning to create a new language model that\u2019s specifically good at predicting the next word of movie reviews.", "This is the first stage of training, where we use the pre-trained language model weights and fine-tune it with the training data of IMDb movie reviews. When we create a learner, we have to pass in two things:", "drop_mult , a hyper-parameter ,used for regularization, sets the amount of dropout. If the model is over-fitting increase it, if under-fitting, you can decrease the number.", "How to fine-tune the pre-trained model on our movie review data? Learning rate hyper-parameter is one of the most important parameters to train a model. Fast.ai provides a convenient utility (learn.lr_find) to search through a range of learning rates to find the optimum one for our dataset. Learning rate finder will increase the learning rate after each mini-batch. Eventually, the learning rate is too high that loss will get worse. Now look at the plot of learning rate against loss and determine the lowest point (around 1e-1 for the plot below) and go back by one magnitude and choose that as a learning rate (something around 1e-2).", "We start training the model with learning rate 1e-2 using fit_one_cycle.", "fast.ai library uses latest techniques from deep learning research and one cycle learning is from one of the recent paper and turned out to be both more accurate and faster than any previous approach. First argument \u20181\u2019 is number of epoch runs . We get an accuracy of 29% after running just one epoch.", "It trained last layers and basically left most of the model exactly as it was. But what we really want is to train the whole model. Normally after we fine-tune the last layers, the next thing we do is we go unfreeze (unfreeze the whole model for training) and train the whole thing.", "Accuracy = 0.3 means the model is guessing the next word of the movie review correctly about a third of the time. That sounds like a pretty high number. So it\u2019s a good sign that my language model is doing pretty well.", "To evaluate our language model, we can now run learn.predict and pass in the start of a sentence and specify the number of words we want it to guess.", "That is pretty decent response and looks like correct grammar. After fine-tuning we get a model that\u2019s good at understanding movie reviews and we can fine-tune that with transfer learning to classify movie reviews to be positive or negative. Let us save the encoding of the model to be used later for classification.", "The part of the model that has the understanding of the sentence is called the encoder. So we save it to later use it during the classification stage.", "Now we\u2019re ready to create our classifier. Step one, is to create a data bunch, TextClasDataBunch, passing the vocab from the language model to make sure that this data bunch is going to have exactly the same vocab. Batch size bs to be used is according to the GPU memory you have available, for a 16GB GPU around bs=64 will work fine. You can find whatever batch size fits on your card and use it accordingly.", "Finally we will create a text classifier learner. Load in our pre train model, the encoding part we saved earlier \u2018fine_enc\u2019.", "Again, we follow the same procedure to find the learning rate and train the model.", "The learning rate around 2e-2 seems right, so let us train the classifier:", "Wow 85% accuracy in 16 minutes of training and just using 5K training and 3K validation samples. This is the power of transfer learning.", "Loss Plot: Let us plot the loss while training the model:", "The loss curve seems going down smoothly and has not reached saturation point yet. fastai calculates the exponentially weighted moving average of the losses thus makes it easier to read these charts [by making the curve smoother] at the same time it might be a batch or two behind where they should be.", "Let us understand the techniques fast.ai uses underneath for such impressive results.", "Discriminative learning rates: Applying different learning rate to layers as you go from layer to layer. When fitting a model you can pass a list of learning rates which will apply a different rate to each layer group. When working with a Learner on which you've called split, you can set hyper-parameters in four ways:", "If we chose to set it in way 1, we must specify a number of values exactly equal to the number of layer groups. If we chose to set it in way 2, the chosen value will be repeated for all layer groups. If you pass slice(start,end) then the first group's learning rate is start, the last is end, and the remaining are evenly spaced.", "If you pass just slice(end) then the last group's learning rate is end, and all the other groups are end/10. For instance (for our learner that has 3 layer groups):", "The bottom of the slice and the top of the slice is the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. As you go from layer to layer, we decrease the learning rate. The lowest levels are given smaller learning rates so as not to disturb the weights much.", "What is fit_one_cycle()? It is one cycle of learning rate, start low, go up, and then go down again. Let us plot the learning rate per batch using plot_lr", "When we call fit_one_cycle, we are actually passing in a maximum learning rate. The left side plot shows learning rate change vs the batches. The learning starts slow and it increases about half the time and then it decreases about half the time. As you get close to the final answer you need to anneal your learning rate to hone in on it. The motivation behind this is that during the middle of learning when learning rate is higher, the learning rate works as regularization method and keep network from over-fitting. This helps the network to avoid steep areas of loss and land better flatter minima. Please refer to this paper by Leslie smith which talks in great detail about neural network hyper-parameter tuning and you can find most of these ideas implemented in fastai.", "There is one more argument(moms=(0.8,0.7))\ufe63momentums equals 0.8,0.7. Basically for training recurrent neural networks (RNNs), it really helps to decrease the momentum a little bit.The right side above is the momentum plot. Every time our learning rate is small, our momentum is high. Why is that? Because as you are learning small learning rate, but you keep going in the same direction, you may as well go faster (higher momentum). But as you are learning high learning rate, but you keep going in the same direction, you may overshoot the target, so momentum should be slowed. This trick can help you train 10 times faster.", "To improve the accuracy further, fast.ai provides some more tricks;freeze_to. Don't unfreeze the whole thing but to unfreeze one layer at a time. The below approach works very well and gives incredible results.", "We reached an accuracy of 90%. The training loss is still higher than the validation loss so we are not over-fitting yet, there is still scope of improving accuracy by running more epochs.", "In classification problems it is very useful to use something called a confusion matrix which shows you for each label, how many times was it predicted correctly. Confusion-matrix is good technique to summarize the performance of a classification algorithm.", "We use ClassificationInterpretationclass to do the job for us.", "Let us use our classifier and predict some movie review:", "This is predicted as \u20180\u2019, negative review. Fantastic!!!", "We have previously done sentiment classification of the IMDb movie review using classical machine learning approach here and then using word embedding approach here. The language modelling approach using fast.ai is the simplest and most powerful tool, I have come across. The library provides very easy to use methods and with a few lines of code, you can achieve state-of-the-art results. Please refer to jupyter notebook here.", "We discussed briefly the use of transfer learning in NLP problems. We explored the fast.ai library and different hyper-parameter tuning techniques in detail. We created a language model and later applied it on the text classification problem. I hope you enjoyed this post and learned something new and useful.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb1b334f2872d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@javaid.nabi?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@javaid.nabi?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Javaid Nabi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70c04bf6660e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&user=Javaid+Nabi&userId=70c04bf6660e&source=post_page-70c04bf6660e----b1b334f2872d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT"}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI Transformer,"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe"}, {"url": "https://course.fast.ai/", "anchor_text": "options"}, {"url": "https://course.fast.ai/start_colab.html", "anchor_text": "Colab"}, {"url": "https://course.fast.ai/start_kaggle.html", "anchor_text": "Kaggle Kernels"}, {"url": "https://course.fast.ai/start_floydhub.html", "anchor_text": "Floydhub"}, {"url": "https://course.fast.ai/start_floydhub.html", "anchor_text": "Floydhub"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB"}, {"url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "anchor_text": "post"}, {"url": "https://docs.fast.ai/text.data.html#TextDataBunch", "anchor_text": "documentation"}, {"url": "https://docs.fast.ai/text.data.html#TextLMDataBunch", "anchor_text": "TextLMDataBunch"}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "paper"}, {"url": "https://docs.fast.ai/basic_train.html#Learner", "anchor_text": "Learner"}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "this"}, {"url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456", "anchor_text": "here"}, {"url": "https://github.com/javaidnabi31/text_classification_fast_ai", "anchor_text": "here"}, {"url": "http://ruder.io/nlp-imagenet/", "anchor_text": "http://ruder.io/nlp-imagenet/"}, {"url": "https://towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde", "anchor_text": "https://towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde"}, {"url": "https://course.fast.ai/", "anchor_text": "https://course.fast.ai/"}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "https://docs.fast.ai/text.html"}, {"url": "https://towardsdatascience.com/transfer-learning-946518f95666", "anchor_text": "https://towardsdatascience.com/transfer-learning-946518f95666"}, {"url": "https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6", "anchor_text": "https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6"}, {"url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "anchor_text": "https://blog.floydhub.com/ten-techniques-from-fast-ai/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b1b334f2872d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b1b334f2872d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/text-processing?source=post_page-----b1b334f2872d---------------text_processing-----------------", "anchor_text": "Text Processing"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----b1b334f2872d---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/language-modeling?source=post_page-----b1b334f2872d---------------language_modeling-----------------", "anchor_text": "Language Modeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&user=Javaid+Nabi&userId=70c04bf6660e&source=-----b1b334f2872d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&user=Javaid+Nabi&userId=70c04bf6660e&source=-----b1b334f2872d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb1b334f2872d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b1b334f2872d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b1b334f2872d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b1b334f2872d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b1b334f2872d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@javaid.nabi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@javaid.nabi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Javaid Nabi"}, {"url": "https://medium.com/@javaid.nabi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70c04bf6660e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&user=Javaid+Nabi&userId=70c04bf6660e&source=post_page-70c04bf6660e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F146b1e2d6450&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d&newsletterV3=70c04bf6660e&newsletterV3Id=146b1e2d6450&user=Javaid+Nabi&userId=70c04bf6660e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}