{"url": "https://towardsdatascience.com/tensorflow-and-transformers-df6fceaf57cc", "time": 1683016764.513964, "path": "towardsdatascience.com/tensorflow-and-transformers-df6fceaf57cc/", "webpage": {"metadata": {"title": "How to Use Transformers in TensorFlow | Towards Data Science", "h1": "TensorFlow and Transformers", "description": "This guide covers how to build an incredibly powerful language classifier using the transformers library with Google's BERT, in TensorFlow with Python."}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/models", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://huggingface.co/exbert/", "anchor_text": "exBERT lite", "paragraph_index": 13}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/build-better-pipelines-with-tensorflow-dataset-328932b16d56", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/why-adamw-matters-736223f31b5d", "anchor_text": "this article", "paragraph_index": 45}, {"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here", "paragraph_index": 53}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter", "paragraph_index": 54}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs", "paragraph_index": 56}], "all_paragraphs": ["Transformers are, without a doubt, one of the biggest advances in NLP in the past decade. They have (quite fittingly) transformed the landscape of language-based ML.", "Despite this, there are no built-in implementations of transformer models in the core TensorFlow or PyTorch frameworks. To use them, you either need to apply for the relevant Ph.D. program, and we\u2019ll see you in three years \u2014 or you pip install transformers.", "Although this is simplifying the process a little \u2014 in reality, it really is incredibly easy to get up and running with some of the most cutting-edge models out there (think BERT and GPT-2).", "When using Huggingface\u2019s transformers library, we have the option of implementing it via TensorFlow or PyTorch. We will be covering everything you need to know to get started with the TensorFlow flavor in this article.", "If you prefer video, I\u2019ve covered the full build process here too:", "HuggingFace is a company building and maintaining the hugely popular Transformers library. We can easily hit the ground running with the majority of the big, most cutting-edge transformer models available today through this library.", "To download and begin working with some of the biggest models out there, including (but not limited to) \u2014 BERT, RoBERTa, GPT, GPT-2, XLNet, and HuggingFace\u2019s own DistilBERT and DistilGPT-2 \u2014 it takes no more than three lines of code, which look like this:", "Not only can we access all of these models with incredible ease, but we can even take advantage of prebuilt transformers for question and answering, sentiment analysis, text summarization, and much more.", "It\u2019s a hugely popular library, built by an incredibly talented team, democratizing some of the most powerful models at the cutting-edge of NLP.", "HuggingFace has built an incredible ecosystem that provides an insanely large number of ready-to-use transformers, the full list of which we can find here. Both community-built and HuggingFace-built models are available.", "We can filter for models via the Tags dropdown. We can search based on the framework they are built with, such as PyTorch/TensorFlow \u2014 it\u2019s use-case such as classification, QnA, etc.", "We will be using TensorFlow, and we can see a list of the most popular models using this filter. Uncased/cased refers to whether the model will identify a difference between lowercase and uppercase characters \u2014 which can be important in understanding text sentiment.", "The bert-base-cased model fits our use-case, and we will be implementing this model (later) using the instructions provided on the model page.", "As a brief but interesting side-note, HuggingFace, alongside IBM Research and Harvard NLP, have built an incredibly fascinating tool for visualizing the attention in several transformer models with exBERT lite.", "TensorFlow support in the transformers library came later than that for PyTorch, meaning the majority of articles you read on the topic will show you how to integrate HuggingFace and PyTorch \u2014 but not TensorFlow.", "Of-course, the steps are slightly different \u2014 but at a high-level, the process is the same:", "We will cover each of these steps \u2014 but focusing primarily on steps 2\u20134.", "First, we need to prepare our data for our transformer model. What is done here will depend very much on the data and use-case. We will be taking some clean data and processing it with BERT as a classifier for sentiment analysis.", "We can use the IMDB movie review dataset, which provides us with sentiment ratings from 0 (terrible) to 4 (amazing).", "You can get the dataset here \u2014 or via the Kaggle API:", "Because there are a lot of sentence fragments, these can easily pollute the validation set with near-matches to that in the training set. So, I removed them using drop_duplicates, keeping the first record of each unique SentenceId (the full review, meaning we drop all review segments).", "All we need to do at this point is one-hot encode our sentiment labels, like so:", "We will be using the HuggingFace transformers library to source our transformer models. A smaller transformer model available to us is DistilBERT \u2014 a smaller version of BERT with ~40% of the parameters while maintaining ~95% of the accuracy.", "DistilBERT is a good option for anyone working with less compute. Just switch out bert-base-cased for distilbert-base-cased below. We initialize the BERT tokenizer and model like so:", "It really is as easy as that!", "We have our input data and tokenizer ready, so now we can encode our input data into two arrays \u2014 (1) the input IDs and (2) the attention mask.", "Input IDs are simply a set of integers that represent a word, \u201chello\u201d could be 0, \u201cworld\u201d might be 1. But, BERT uses a predefined set of mappings \u2014 hence why we loaded our tokenizer using the .from_pretrained method.", "There are several special tokens that are used by BERT. These are:", "Once we have our sequences, we can encode them withtokenizer.encode_plus:", "Where SEQ_LEN=50, the output of the input IDs will look like this:", "Next up is the attention mask. This mask is simply an array of 0s and 1s where each 1 represents a valid word/input ID, and a 0 represents padding. The encode_plus method outputs both input IDs and the attention mask tensors inside a dictionary:", "BERT's attention layers consume this mask and apply attention operations to word embedding that corresponds to a 1 while ignoring those matching up with a 0.", "The effect of this is that we only apply attention to real words, while additional padding tokens are ignored.", "We have our encoded inputs IDs and attention masks, and the initialized BERT model \u2014 now, we need to add the additional layers required for inputting the input ID and attention mask arrays and the layers required for classifying the BERT output into sentiment ratings.", "For every BERT-based transformer model, we need two input layers that match our sequence length. We encoded our inputs to a length of 50 tokens \u2014 so we use an input shape of (50,) here:", "Both our input IDs and attention mask arrays contain integers only, so we specify dtype='int32'. More importantly, the layer names must match the key-value pairs in our data inputs, for which we have two options:", "We won\u2019t cover option (2) any further than this here \u2014 but if you\u2019re working with a lot of data, it\u2019s a much better option (which you can read about here).", "Next up are our classification layers. These will take the output from our BERT model and produce one of our three sentiment labels \u2014 there are a lot of ways to do this, but we will keep it simple:", "Here we pull the outputs from distilbert and use a MaxPooling layer to convert the tensor from 3D to 2D \u2014 alternatively, use a 3D network (like convolutional or recurrent neural nets) followed by MaxPooling.", "When we put all of this together we get the following:", "If you are wanting to train the transformer parameters further, the final line is not necessary! We choose not to as BERT is already an incredibly well built and fine-tuned model. It would take a very long time to train, so for the likely minuscule performance increase \u2014 there\u2019s little justification.", "Our model summary shows the two input layers, BERT, and our final classification layers. We have a total of 108M+ parameters, of which just 100K are trainable because we froze the BERT parameters.", "Here we are returning to the standard TensorFlow build process.", "First, we use the optimizer we all know and love. Next, we use category cross-entry and categorical accuracy for our loss and single metric. Because we have one-hot encoded our outputs, we use Categorical.", "Finally, we compile our model with the .compile method \u2014 we are now ready to begin training!", "Note: If training BERT layers too, try Adam optimizer with weight decay \u2014 which can help reduce overfitting and improve generalization [1]. I would recommend this article for understanding why.", "We train as per usual using the fit method. If not using a tf.data.Dataset object we must explicitly state our multiple inputs using a dictionary. All this requires is:", "Training will take a long time with most GPUs \u2014for those of you on CPU only, good luck!", "If you are stuck on CPU, try out Google Colab \u2014 it\u2019s a free, cloud-based notebook service provided by Google. Colab includes a GPU as standard \u2014 albeit not a particularly powerful one (but it is free).", "Here is a full version of the code:", "After several hours training on my measly 940MX we get a validation accuracy of 94.9% \u2014 with more training epochs, the model looks like will easily improve on this too:", "These are pretty great results for such a simple output network. Further fine-tuning, the addition of CNNs, LSTMs, or other more expressive networks may improve our results even further.", "Alternatively, (although I found this to be detrimental) we can even use BERTs pre-pooled output tensors by swapping out last_hidden_state with pooler_output \u2014 but that is for another time.", "I hope you\u2019ve enjoyed this article on integrating TF2 and HuggingFace\u2019s transformers library. If you\u2019d like more, I post programming/ML tutorials on YouTube here!", "If you have any questions, let me know via Twitter or in the comments below.", "*All images are by the author except where stated otherwise", "Freelance ML engineer learning and writing about everything. I post a lot on YT https://www.youtube.com/c/jamesbriggs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdf6fceaf57cc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://jamescalam.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "James Briggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----df6fceaf57cc---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf6fceaf57cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=-----df6fceaf57cc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf6fceaf57cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=-----df6fceaf57cc---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@introspectivedsgn?utm_source=medium&utm_medium=referral", "anchor_text": "Erik Mclean"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://huggingface.co/models", "anchor_text": "here"}, {"url": "https://huggingface.co/models", "anchor_text": "HuggingFace.co"}, {"url": "https://huggingface.co/models", "anchor_text": "HuggingFace.co"}, {"url": "https://huggingface.co/exbert/", "anchor_text": "exBERT lite"}, {"url": "https://huggingface.co/exbert/", "anchor_text": "exBERT lite"}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset", "anchor_text": "tf.data.Dataset"}, {"url": "https://towardsdatascience.com/build-better-pipelines-with-tensorflow-dataset-328932b16d56", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/why-adamw-matters-736223f31b5d", "anchor_text": "this article"}, {"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here"}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter"}, {"url": "https://arxiv.org/abs/1711.05101", "anchor_text": "Decoupled Weight Decay Regularization"}, {"url": "https://bit.ly/nlp-transformers", "anchor_text": "\ud83e\udd16 NLP With Transformers Course"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----df6fceaf57cc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----df6fceaf57cc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----df6fceaf57cc---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----df6fceaf57cc---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----df6fceaf57cc---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf6fceaf57cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=-----df6fceaf57cc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf6fceaf57cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=-----df6fceaf57cc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf6fceaf57cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----df6fceaf57cc---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F75e31c56d187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&newsletterV3=b9d77a4ca1d1&newsletterV3Id=75e31c56d187&user=James+Briggs&userId=b9d77a4ca1d1&source=-----df6fceaf57cc---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Written by James Briggs"}, {"url": "https://jamescalam.medium.com/followers?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "9.6K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----df6fceaf57cc---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F75e31c56d187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensorflow-and-transformers-df6fceaf57cc&newsletterV3=b9d77a4ca1d1&newsletterV3Id=75e31c56d187&user=James+Briggs&userId=b9d77a4ca1d1&source=-----df6fceaf57cc---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "James Briggs"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "The Right Way to Build an API with PythonAll you need to know on API development in Flask"}, {"url": "https://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "\u00b77 min read\u00b7Sep 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd08ab285f8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-right-way-to-build-an-api-with-python-cd08ab285f8f&user=James+Briggs&userId=b9d77a4ca1d1&source=-----cd08ab285f8f----0-----------------clap_footer----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f?source=author_recirc-----df6fceaf57cc----0---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd08ab285f8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-right-way-to-build-an-api-with-python-cd08ab285f8f&source=-----df6fceaf57cc----0-----------------bookmark_preview----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----df6fceaf57cc----1---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----df6fceaf57cc----1-----------------bookmark_preview----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----df6fceaf57cc----2---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----df6fceaf57cc----2-----------------bookmark_preview----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "James Briggs"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "BERT For Measuring Text Similarity"}, {"url": "https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": "\u00b75 min read\u00b7May 5, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feec91c6bf9e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-for-measuring-text-similarity-eec91c6bf9e1&user=James+Briggs&userId=b9d77a4ca1d1&source=-----eec91c6bf9e1----3-----------------clap_footer----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=author_recirc-----df6fceaf57cc----3---------------------d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feec91c6bf9e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-for-measuring-text-similarity-eec91c6bf9e1&source=-----df6fceaf57cc----3-----------------bookmark_preview----d1c5f436_54ed_499e_a5ff_79b54c07d0d0-------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "See all from James Briggs"}, {"url": "https://towardsdatascience.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----df6fceaf57cc----0-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Temporal Fusion Transformer: Time Series Forecasting with Deep Learning \u2014 Complete TutorialCreate accurate & interpretable predictions"}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b712 min read\u00b7Nov 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd32c1e51cd91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----d32c1e51cd91----1-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd32c1e51cd91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91&source=-----df6fceaf57cc----1-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----df6fceaf57cc----0---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----df6fceaf57cc----0-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----1-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----df6fceaf57cc----1---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----df6fceaf57cc----1-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----df6fceaf57cc----2---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----df6fceaf57cc----2-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----3-----------------clap_footer----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----df6fceaf57cc----3---------------------4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----df6fceaf57cc----3-----------------bookmark_preview----4e94936c_1bba_4b6b_84d4_5e5ddd81225f-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----df6fceaf57cc--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}