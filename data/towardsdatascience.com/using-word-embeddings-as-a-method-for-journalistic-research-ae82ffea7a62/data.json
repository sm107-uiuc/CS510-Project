{"url": "https://towardsdatascience.com/using-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62", "time": 1683004494.597673, "path": "towardsdatascience.com/using-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62/", "webpage": {"metadata": {"title": "Word2Vec: Using Word Embeddings as a Method for Journalistic Research | Towards Data Science", "h1": "Using Word Embeddings for Journalistic Research", "description": "Like most other areas, journalism is going digital. I am part of the data and digital investigations team at S\u00fcddeutsche Zeitung in Germany, where we try to use modern technical means to carry out\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Bundestag", "anchor_text": "Bundestag", "paragraph_index": 2}, {"url": "https://www.bundestag.de/en/documents/minutes_neu", "anchor_text": "The resulting documents", "paragraph_index": 6}, {"url": "https://transacl.org/ojs/index.php/tacl/article/view/1202/286", "anchor_text": "Antoniak et. al. (2018)", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)", "anchor_text": "hyperparameters", "paragraph_index": 18}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "word2vec", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "Mikolov et al. 2013a", "paragraph_index": 21}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "Rehurek and Sojka, 2010", "paragraph_index": 21}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Jayesh Bapu Ahire", "paragraph_index": 27}, {"url": "http://vectors.nlpl.eu/explore/embeddings/en/calculator/", "anchor_text": "Semantic Calculator by the Nordic Language Processing Laboratory", "paragraph_index": 35}, {"url": "https://github.com/sueddeutsche/political-german-word-embeddings", "anchor_text": "I published the word embedding models for the latest electoral term 2017\u20132019 on Github", "paragraph_index": 52}, {"url": "https://books.google.com/ngrams", "anchor_text": "Google Books ngram Corpus", "paragraph_index": 54}, {"url": "https://www.ims.uni-stuttgart.de/en/institute/team/Schlechtweg-00003/", "anchor_text": "Dominik Schlechtweg", "paragraph_index": 57}, {"url": "http://www.schulteimwalde.de/", "anchor_text": "Professor Sabine Schulte im Walde", "paragraph_index": 57}, {"url": "https://www.aclweb.org/anthology/P19-1072/", "anchor_text": "Schlechtweg et. al.", "paragraph_index": 58}, {"url": "https://mitpress.mit.edu/books/artificial-unintelligence", "anchor_text": "Meredith Broussard", "paragraph_index": 77}, {"url": "https://github.com/sueddeutsche/political-german-word-embeddings", "anchor_text": "I published 30 word embedding models for the current election term (2017\u20132019) on Github. Feel free to use them and cite me", "paragraph_index": 81}], "all_paragraphs": ["Like most other areas, journalism is going digital. I am part of the data and digital investigations team at S\u00fcddeutsche Zeitung in Germany, where we try to use modern technical means to carry out journalistic research.", "In my latest project, I combined policy analysis and machine learning. I wanted to know: How has political language and the use of certain words changed over time?", "70 years have passed since the Federal Republic of Germany was founded in May 1949. During all this time, each of the more than 4200 sittings in the federal parliament, called Bundestag, was meticulously recorded: every speech, every comment, every acclamation is written down. A trove of data that preserves Germany\u2019s recent history, far-reaching developments and major political conflicts.", "Of course, a computer does not understand language in the same way that humans do. Instead, the mathematical systems of computational linguistics reveal the meaning of a word from its context. \u201cYou shall know a word by the company it keeps\u201d, linguist John Ruppert Firth famously said in 1957, long before computers were used to find contexts of content. And yet the principle still applies in the 21st century, when algorithms can take over this task.", "The method I chose to evaluate texts automatically is called word embeddings. We interact with this technology all the time, for example when doing a simple Google search, or using translation software. I applied these methods of the scientific field of computational linguistics to political rhetoric.", "In every paper I read about Word Embeddings there were phrases like: \u201cpromising new field\u201d or \u201clots of research\u201d. And yes, both turned out to be true\u2026", "Like in other countries around the world, the German parliament keeps transcripts on everything that the members of parliament say and do. The resulting documents are the corpora for my analysis.", "I am interested in changes over time, so I do not create one large corpus over all the years, but each legislative term has its own corpus. So in total, there are 19 different corpora.", "As you can see from the table, individual corpora are not very large. This leads to the first major decision on how to apply the methodology of word embeddings to the 19 rather small corpora of parliamentary transcripts.", "Most research papers refer to projects with much more data. This was sobering because the methods cannot simply be applied to my case. But I came across a table in Antoniak et. al. (2018) that really cheered me up. It distinguishes two different approaches: downstream-centered, a general approach for big corpora, and a corpus-centered approach for specific domains like politics.", "This distinction determines the method of analysis, as Antoniak and Mimno write:", "\u201cUnlike the downstream-centered approach, the corpus-centered approach is based on direct human analysis of nearest neighbors to embedding vectors, and the training corpus is not simply an off-the-shelf convenience but rather the central object of study.\u201d", "One method used by computational linguistics for extracting the meaning of a word from its context is called word embedding. Each word in the corpus is embedded in the model by context and semantics. With regard to the transcripts from the German parliament, that means: When calculating a model of word embeddings the context and semantics of the language of the members of parliament are embedded inside the model.", "Every unique word is represented by a series of numbers aka dimensions, which are the result of a counting and weighing process during learning the context. This process is called \u201ctraining the model\u201d.", "The result looks for example like this:", "The power of the learning machine is hidden in these, at first sight, unspectacular words and number sequences.", "The set of vectors carries the following information:", "The more dimensions, the more meaning may be encoded in a vector, theoretically. But in reality, it is limited by the amount of text you feed the algorithm. Finding the optimal length can be a trial and error evaluation process. Here, I am using 300 dimensions. It is a default setting used by many researchers and I stick to their experience.", "Users of word embeddings are free to decide how many dimensions should be calculated. Such setting options are called hyperparameters in machine learning. Their adjusting can heavily influence the results and a user is repeatedly confronted with making such decisions when implementing a machine learning model.", "Keep in mind that the meaning embedded in a model only holds for the specific domain. In this case: the speeches in the German parliament in the last 70 years.", "So, an algorithm takes a text corpus as input and returns calculated word vectors. But how does this happen?", "There are several possible algorithms that can solve the task of calculating word embeddings. For this project, I have chosen the well documented word2vec algorithm. It was introduced at Google in 2013 (Mikolov et al. 2013a). A number of scientific papers tested word embedding algorithms and came to the conclusion that word2vec is the best choice. I used Python\u2019s implementation called gensim (Rehurek and Sojka, 2010).", "At first, the vocabulary is built: every unique word from the input text is extracted and stored. Word2vec learns from a text by checking which words occur together in a certain context. A context is defined as a window around the word under examination. The size of the window is a further hyperparameter to be set.", "Now we want to train the vector of dog with a window size of five words: The word marked in yellow is picked from the vocabulary and trained on every possible pair of words inside the window: The, fluffy, barked, as. Words that are immediate neighbors are weighted higher. For example, fluffy + barked are given more weight than The + as.", "Next, we go one word further and look at barked. The window moves forward by one word and the neighboring words are weighted.", "In this way the window iterates over all parliamentary speeches since 1949.", "All occurring word pairs are the input for the training of a one-dimensional neural network. Word2vec is designed to calculate a model, that can predict how likely one word occurs with another. To solve this task there is a hidden layer with as many neurons as the are dimensions for the vectors.", "\u201cThe word vector is the model\u2019s attempt to learn a good numerical representation of the word in order to minimize the loss (error) of its predictions\u201d, writes Jayesh Bapu Ahire.", "Finally, when the training process has finished, the output layer is stripped off. The word vectors are the result of the trained neurons. In the field of machine learning, this is called a fake task: You can train a model on a task, that you have enough data for and then toss the last layer.", "If you want to go deeper into word vectors and neural networks, I recommend these blog posts (in ascending nerdiness):", "During the training process, the model does not only recognize the context of words but conceptually learns grammatical categories like number, case or gender. These relations are encoded in the dimensions of the model.", "A notional example: Imagine the vectors \u201cfemale chancellor\u201d (Bundeskanzlerin), \u201cmale chancellor\u201d (Bundeskanzler), \u201cwoman\u201d (Frau) and \u201cman\u201d (Mann) with four dimensions and made-up numbers.", "Consider the second dimension from the left: The vectors \u201cfemale chancellor\u201d (Bundeskanzlerin) and \u201cwoman\u201d (Frau) have high values and \u201cmale chancellor\u201d (Bundeskanzler) and \u201cman\u201d (Mann) have low values. This dimension might be labeled with femininity. The third dimension from the left is the other way round and hints towards masculinity.", "Instead of four dimensions, there are 300 in the Bundestag model \u2014 without the labels, of course. And instead of just four words, there are mare than 61.000 for the latest election term alone. This results in 18.3 million data points \u2014 way too complex for our brain to imagine. But despite it is not conceivable, it is possible to calculate with the vectors like back in high school.", "A popular example when describing this feature of Word2Vec is: \u201cKing \u2014 Man + Woman = Queen\u201c. Applied to the data from the German parliament the mathematical operation can be performed with the words: \u201cfemale chancellor \u2014 woman + man = male chancellor\u201d.", "A really nice application for calculating with different corpora yourself is the Semantic Calculator by the Nordic Language Processing Laboratory.", "Calculating with vectors is fun. But explorations into the wide vector space might be too much to stretch our readers\u2019 patience. Besides, it won\u2019t answer the main question: How are political debates changing?", "The main idea to discover a shift in political speeches is comparing how much one legislative term differs from the previous one. Therefore, the following measure turns out to be useful: the cosine similarity. It calculates the cosine of the angle between two vectors. The result is a number between 0 and 1, where 0 means no similarity and 1 absolute similarity. The closer the cosine similarity is to 1, the more a word can be interpreted as a synonym.", "Since the Federal Republic of Germany was founded 71 years ago there have been 19 electoral terms. For each term, I calculated a model and then queried the models to extract the most similar words for a defined list of 180 words that are of special interest, for example: \u201cclimate change\u201d (Klimawandel), \u201ctransportation\u201d (Verkehr), \u201crefugees\u201d (Fl\u00fcchtlinge) or \u201cracism\u201d (Rassismus).", "Our team at S\u00fcddeutsche Zeitung defined a set of 180 relevant terms with a high frequency for a closer investigation. We didn\u2019t select this list of query words a priori. It took some time of exploring and researching in archives as well as in data to identify words that have stories to tell.", "An example: The table shows the most similar words for the word \u201cenvironment\u201d, in German Umwelt, for every legislative term:", "It is evident that during the first 20 years the most similar word is changing every term and had a different meaning than today. Back then \u201cenvironment\u201d (Umwelt) was often used to describe the surrounding world. This changed during the 1980ies when the Green party was elected to parliament and its focus on environmental issues heavily influenced how the word \u201cenvironment\u201d (Umwelt) was used. The synonyms changed first to \u201clivelihood\u201d (Lebensgrundlage) and further to \u201cnature conservation\u201d (Naturschutz). In recent times the most similar word is \u201cbiodiversity\u201d (Artenvielfalt).", "This is really powerful: word embeddings can recreate political debates. But they have one huge disadvantage: They are unstable, especially for relatively small corpora like mine[1, 2]. Every time you write a new model with the same data and the same settings the result of a query for the embedding of a certain term will vary from model to model. Antoniak et. al. [1] studied this phenomenon in their paper:", "\u201cWe find that there is considerable variability in embeddings that may not be obvious to users of these methods. Rankings of most similar words are not reliable, and both ordering and membership in such lists are liable to change significantly. Some uncertainty is expected, and there is no clear criterion for \u2018acceptable\u2019 levels of variance, but we argue that the amount of variation we observe is sufficient to call the whole method into question.\u201d", "The table shows the 15 most similar words to \u201cenvironment\u201d from the term 1983 to 1987. The six columns represent the different models. The values are the cosine similarity between \u201cenvironment\u201d and the corresponding word in the row. An empty cell means that this model did not recognize the word to be among the top ten similar words.", "Since word embedding models are unstable they don\u2019t deliver the same results over different models. For example, the second model doesn\u2019t recognize \u201cliving space\u201d, in German Lebensraum, as a synonym for \u201cenvironment\u201d, but the five other models do.", "When models deliver varying results, which one should I trust? None. Such unstable results of a single model are an unacceptable basis for journalistic research. I tackle this problem by training 30 models instead of just one for every electoral term. In total, there are 570 models for the 19 election terms. I tested it: I deleted all the models and recomputed them. My queries produced the same results with the new models. So the analysis is reproducible.", "When looking up similar words of a politically relevant term, I extracted the ten most similar ones from all 30 models trained on one election term. The next step was a counting process: How often does a word appear as a synonym per term?", "This procedure of querying multiple models and counting occurrences ensures that we only examine those words and accompanying embeddings that appear recurrently in the models and can be seen as stable results.", "To visualize the results we use heatmaps. The following chart shows the most common synonyms for \u201cenvironment\u201d.", "The twelve words on the left side are sorted by the overall appearance since 1949. Here, we plot values lower than 30, too, as they indicate the rising or falling of a word in the discourse of \u201cenvironment\u201d.", "The heatmaps are a great tool to inspect the whole time span and to see changes over time. But they are not sufficient to evaluate an electoral term in detail. To this end we used lists of similar words sorted by the mean cosine similarity over the 30 models and marked stable words with a stronger font-weight.", "I published the word embedding models for the latest electoral term 2017\u20132019 on Github. Feel free to use it for your own projects in the domain of German politics. Please drop a line if you do so, I am interested in your work.", "So far we have investigated how the context of a word changes. Another approach is to focus on the word itself and look at the change of the word vector. This is what is called diachronic lexical semantic change.", "Kim et. al. [3] provided a method of doing so. They worked with the Google Books ngram Corpus and calculated one model for every year from 1900 to 2009. Two words were particularly noteworthy: \u201ccell\u201d and \u201cgay\u201d. Their meaning shifted a lot in those 100 years. The neighboring words of \u201cgay\u201d changed from \u201ccheerful\u201d, \u201cpleasant\u201d, \u201cbrilliant\u201d to \u201clesbian\u201d, \u201cbisexual\u201d and \u201clesbians\u201d, and \u201ccell\u201d from \u201ccloset\u201d, \u201cdungeon\u201d, \u201ctent\u201d to \u201cphone\u201d, \u201ccordless\u201d, \u201ccellular\u201d.", "Wouldn\u2019t it be great to get those words for German politics?", "Unfortunately, the paper is often cited to criticize its method of finding these words, for example, because they are working with unstable word embeddings.", "Luckily, I got in touch with Dominik Schlechtweg, a Ph.D. student who works together with Professor Sabine Schulte im Walde on Distributional Models of Semantic Change at the Institute for Natural Language Processing (University of Stuttgart). He patiently advised me to apply the method to my data.", "Schlechtweg et. al. [4] tested several methods to measure those changes: \u201cThe overall best-performing model is Skip-Gram with orthogonal alignment and cosine distance.\u201d", "The methodology can be applied as follows:", "Hold on, here is an example:", "A first attempt at visual reasoning:", "What you see as a blue line is the average change of the vector \u201cUmwelt\u201d (environment) from 15 models from term to term. One dot in light blue represents a single cosine distance value from one model (cosine distance = 1- cosine similarity).", "It is important to compare the blue line to the grey one, which shows the frequency of \u201cUmwelt\u201d. As you can see: if the grey line changes, the blue one does as well because the cosine similarity is directly dependent on the frequency. The more words available, the lower the similarity.", "With this background you see: There is nothing to see. The change in the word vector (blue line) is due to the frequency variation (grey line). To eliminate this statistical effect you have to find words with the same change of frequency and plot their distance from term to term as well.", "The next attempt ended up with reference words as you can see in the following chart:", "It looks much better because there is one term, the 10th, where the blue points lie above the light blue points. This indicates a real change in word vector regardless of frequency.", "The blue points represent the cosine distances for the vector \u201cUmwelt\u201d (those 15 models). The light blue points represent now the cosine distance from comparable words that share the same frequency as my word of interest in the more recent term within the model pair. The reference words change from model pair to optimally represent the frequency at that point in time.", "As I am comparing and calculating the similarity of model pairs from two different electoral terms, I need to compare the frequency change of the reference words from both electoral terms as well. The distance of the word vector over time is not compared with another course over time, but a specific point in time. That\u2019s the flaw.", "I tried to fix it by searching for those words that share the same frequency change as \u201cUmwelt\u201d. But there are no identical frequency changes, even if I let the frequencies fluctuate by a small margin. The corpora are simply too small.", "So, let\u2019s get to the next iteration. Instead of absolute changes, maybe relative frequency changes can help?", "Look at the charts above: When the frequency line graph changes, the cosine distance line graph changes as well.", "If you pick reference words with the same relative frequency change, but a higher absolute frequency, the cosine similarity is likely to be smaller, because of the inverse relation between frequency and cosine similarity.", "So, printed against the word of interest would you would reason visually changes in word vectors just because of statistical effects based on differences in frequencies.", "There exists a theoretical solution to this problem: Look for words with the same change in absolute numbers, but a higher frequency \u2014 and then go back to the initial corpora. The key is now to sample down the reference word by masking it randomly until the frequency is the same as the word of interest. Then calculate new models, align them and query the numbers of similarity for the chart. That\u2019s where I stopped.", "As we discussed this unsolved task Dominik wrote: \u201dI would also say that the field of automatic meaning change detection is not yet ready to provide standard methods for your application problem. Two years from now, things may look different.\u201d", "Three stories were published based on this research.", "I am fascinated by how computers can capture natural language. But this fascination holds the danger of technochauvinism. Technochauvinism is a term by Meredith Broussard \u2014 \u201cthe belief that technology is always the solution\u201d.", "We are so enthusiastic about assigning tasks to machines that our heads cannot solve. So we can easily overlook the fact that the results must also be questioned. We are caught up in the idea that computers always objectively produce flawless results. That\u2019s why I think it\u2019s important to communicate the uncertainties of projects like this one. If I change the hyper-parameters for the algorithm, other relationships in the data may be emphasized more strongly and potential articles may turn out differently. It\u2019s like asking a number of witnesses about a specific issue. How I formulate my questions also has an impact on the answers I get.", "Do you have answers or comments? I appreciate any feedback.", "Special thanks to Katharina Brunner and Carmen Heger for their review.", "I published 30 word embedding models for the current election term (2017\u20132019) on Github. Feel free to use them and cite me.", "[1] Antoniak, Maria; Mimno, David. Evaluating the Stability of Embedding-based Word Similarities. Transactions of the Association for Computational Linguistics, p. 107\u2013119, feb. 2018.", "[2] Johannes Hellrich and Udo Hahn. 2016. Bad company\u2013Neighborhoods in neural embedding spaces considered harmful. In Proceedings of the International Conference on Computational Linguistics 2016, pages 2785\u20132796, Osaka, Japan.", "[3] Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014. Temporal analysis of language through neural language models. In Proceedings of the ACL Workshop on Language Technologies and Computational Social Science, pages 61\u201365.", "[4] Schlechtweg, Dominik et al. \u201cA Wind of Change: Detecting and Evaluating Lexical Semantic Change Across Times and Domains.\u201d Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (2019)", "[5] Haim Dubossarsky, Daphna Weinshall, and Eitan Grossman. 2017. Outta control: Laws of semantic change and inherent biases in word representation models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1147\u20131156, Copenhagen, Denmark.", "I am a data journalist at S\u00fcddeutsche Zeitung. I program to find stories."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fae82ffea7a62&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@martina.schories?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@martina.schories?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Martina Schories"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20f0c367db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=post_page-20f0c367db9----ae82ffea7a62---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae82ffea7a62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=-----ae82ffea7a62---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae82ffea7a62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=-----ae82ffea7a62---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Bundestag", "anchor_text": "Bundestag"}, {"url": "https://www.bundestag.de/en/documents/minutes_neu", "anchor_text": "The resulting documents"}, {"url": "https://transacl.org/ojs/index.php/tacl/article/view/1202/286", "anchor_text": "Antoniak et. al. (2018)"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)", "anchor_text": "hyperparameters"}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "word2vec"}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "Mikolov et al. 2013a"}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "Rehurek and Sojka, 2010"}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Introduction to Word Vectors by Jayesh Bapu Ahire"}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Introduction to Word Vectors by Jayesh Bapu Ahire"}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Jayesh Bapu Ahire"}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Introduction to Word Vectors by Jayesh Bapu Ahire"}, {"url": "https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf", "anchor_text": "Introduction to Word Vectors by Jayesh Bapu Ahire"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "Word2Vec Tutorial \u2014 The Skip-Gram Model"}, {"url": "https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae", "anchor_text": "Word2vec Made Easy"}, {"url": "http://vectors.nlpl.eu/explore/embeddings/en/calculator/", "anchor_text": "Semantic Calculator by the Nordic Language Processing Laboratory"}, {"url": "https://github.com/sueddeutsche/political-german-word-embeddings", "anchor_text": "I published the word embedding models for the latest electoral term 2017\u20132019 on Github"}, {"url": "https://books.google.com/ngrams", "anchor_text": "Google Books ngram Corpus"}, {"url": "https://www.ims.uni-stuttgart.de/en/institute/team/Schlechtweg-00003/", "anchor_text": "Dominik Schlechtweg"}, {"url": "http://www.schulteimwalde.de/", "anchor_text": "Professor Sabine Schulte im Walde"}, {"url": "https://www.aclweb.org/anthology/P19-1072/", "anchor_text": "Schlechtweg et. al."}, {"url": "https://github.com/Garrafao/LSCDetection", "anchor_text": "aligned to the model"}, {"url": "https://projekte.sueddeutsche.de/artikel/politik/artikel-e704090/", "anchor_text": "in German"}, {"url": "https://projekte.sueddeutsche.de/artikel/politik/artikel-e953507/", "anchor_text": "in German"}, {"url": "https://projekte.sueddeutsche.de/artikel/politik/artikel-e893391/", "anchor_text": "in German"}, {"url": "https://mitpress.mit.edu/books/artificial-unintelligence", "anchor_text": "Meredith Broussard"}, {"url": "https://github.com/sueddeutsche/political-german-word-embeddings", "anchor_text": "I published 30 word embedding models for the current election term (2017\u20132019) on Github. Feel free to use them and cite me"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----ae82ffea7a62---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/data-journalism?source=post_page-----ae82ffea7a62---------------data_journalism-----------------", "anchor_text": "Data Journalism"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ae82ffea7a62---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ae82ffea7a62---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----ae82ffea7a62---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae82ffea7a62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=-----ae82ffea7a62---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae82ffea7a62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=-----ae82ffea7a62---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae82ffea7a62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@martina.schories?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20f0c367db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=post_page-20f0c367db9----ae82ffea7a62---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F837e399957d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&newsletterV3=20f0c367db9&newsletterV3Id=837e399957d0&user=Martina+Schories&userId=20f0c367db9&source=-----ae82ffea7a62---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@martina.schories?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Written by Martina Schories"}, {"url": "https://medium.com/@martina.schories/followers?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "42 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20f0c367db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&user=Martina+Schories&userId=20f0c367db9&source=post_page-20f0c367db9----ae82ffea7a62---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F837e399957d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word-embeddings-as-a-method-for-journalistic-research-ae82ffea7a62&newsletterV3=20f0c367db9&newsletterV3Id=837e399957d0&user=Martina+Schories&userId=20f0c367db9&source=-----ae82ffea7a62---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ae82ffea7a62----0---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----ae82ffea7a62----0-----------------bookmark_preview----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ae82ffea7a62----1---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ae82ffea7a62----1-----------------bookmark_preview----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----ae82ffea7a62----2---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----ae82ffea7a62----2-----------------bookmark_preview----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----ae82ffea7a62----3---------------------47daa993_a4c9_4e40_8014_e8b92923f8db-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----ae82ffea7a62----3-----------------bookmark_preview----47daa993_a4c9_4e40_8014_e8b92923f8db-------", "anchor_text": ""}, {"url": "https://medium.com/@martina.schories?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "See all from Martina Schories"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----0-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----ae82ffea7a62----0-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----1-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----ae82ffea7a62----1-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ae82ffea7a62----0---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----ae82ffea7a62----0-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ae82ffea7a62----1---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ae82ffea7a62----1-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----2-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----ae82ffea7a62----2---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----ae82ffea7a62----2-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----3-----------------clap_footer----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----ae82ffea7a62----3---------------------9ce7c344_0eb9_4830_a736_727d9a26e0fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----ae82ffea7a62----3-----------------bookmark_preview----9ce7c344_0eb9_4830_a736_727d9a26e0fd-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----ae82ffea7a62--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}