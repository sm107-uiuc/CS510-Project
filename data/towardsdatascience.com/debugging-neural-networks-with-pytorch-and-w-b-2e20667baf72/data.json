{"url": "https://towardsdatascience.com/debugging-neural-networks-with-pytorch-and-w-b-2e20667baf72", "time": 1683007361.614291, "path": "towardsdatascience.com/debugging-neural-networks-with-pytorch-and-w-b-2e20667baf72/", "webpage": {"metadata": {"title": "Debugging Neural Networks with PyTorch and W&B | by Ayush Thakur | Towards Data Science", "h1": "Debugging Neural Networks with PyTorch and W&B", "description": "In this post, we\u2019ll see what makes a neural network under perform and ways we can debug this by visualizing the gradients and other parameters associated with model training. We\u2019ll also discuss the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765", "anchor_text": "piece", "paragraph_index": 2}, {"url": "http://karpathy.github.io/2019/04/25/recipe/", "anchor_text": "A Recipe for Training Neural Networks", "paragraph_index": 4}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Overfit_Small.ipynb", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1902.07208", "anchor_text": "Transfusion: Understanding Transfer Learning for Medical Imaging", "paragraph_index": 12}, {"url": "https://www.fast.ai/2020/01/13/self_supervised/", "anchor_text": "blog", "paragraph_index": 12}, {"url": "http://cs231n.github.io/neural-networks-3/#loss", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_LRFinder.ipynb", "anchor_text": "this notebook", "paragraph_index": 17}, {"url": "https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/", "anchor_text": "blog", "paragraph_index": 21}, {"url": "https://www.pyimagesearch.com/", "anchor_text": "PyImageSearch", "paragraph_index": 21}, {"url": "https://mc.ai/how-to-find-a-descent-learning-rate-using-tensorflow-2/", "anchor_text": "blog", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=JIWXbzRXk1I&t=312s", "anchor_text": "video", "paragraph_index": 22}, {"url": "https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b", "anchor_text": "blog", "paragraph_index": 22}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Vanishing_Gradient.ipynb", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://www.youtube.com/watch?v=IJ9atfxFjOQ&t=12s", "anchor_text": "video", "paragraph_index": 30}, {"url": "http://neuralnetworksanddeeplearning.com/chap5.html", "anchor_text": "blog", "paragraph_index": 30}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Exploding_Gradient.ipynb", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://www.quora.com/Why-are-the-exploding-gradient-problems-mostly-encountered-in-recurrent-neural-networks", "anchor_text": "thread", "paragraph_index": 33}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Pytorch_wandb_Exploding_Gradient_NaN.ipynb", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Pytorch_wandb_Exploding_Gradient_NaN.ipynb", "anchor_text": "linked notebook", "paragraph_index": 35}, {"url": "https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/", "anchor_text": "blog", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "article", "paragraph_index": 40}, {"url": "https://pytorch.org/docs/stable/nn.init.html?highlight=nn%20init#torch.nn.init.zeros_", "anchor_text": "doc", "paragraph_index": 41}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Cifar10_pytorch_wandb_Wt_Init.ipynb", "anchor_text": "here", "paragraph_index": 42}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Cifar10_pytorch_wandb_Dropout_BN.ipynb", "anchor_text": "here", "paragraph_index": 46}, {"url": "https://arxiv.org/abs/1801.05134", "anchor_text": "this", "paragraph_index": 51}, {"url": "https://sayak.dev/", "anchor_text": "Sayak Paul", "paragraph_index": 55}], "all_paragraphs": ["In this post, we\u2019ll see what makes a neural network under perform and ways we can debug this by visualizing the gradients and other parameters associated with model training. We\u2019ll also discuss the problem of vanishing and exploding gradients and methods to overcome them.", "Finally, we\u2019ll see why proper weight initialization is useful, how to do it correctly, and dive into how regularization methods like dropout and batch normalization affect model performance.", "As shown in this piece, neural network bugs are really hard to catch because:", "1. The code never crashes, raises an exception, or even slows down.2. The network still trains and the loss will still go down.3. The values converge after a few hours, but to really poor results", "I highly recommend reading A Recipe for Training Neural Networks by Andrej Karparthy if you\u2019d like to dive deeper into this topic.", "There is no decisive set of steps to be followed while debugging neural networks. But here is a list of concepts that, if implemented properly, can help debug your neural networks.", "There is no decisive set of steps to be followed while debugging neural networks. But here is a list of concepts that, if implemented properly, can help debug your neural networks.", "1. Decisions about data: We must understand the nuances of data \u2014 the type of data, the way it is stored, class balances for targets and features, value scale consistency of data, etc.", "2. Data Preprocessing: We must think about data preprocessing and try to incorporate domain knowledge into it. There are usually two occasions when data preprocessing is used:", "3. Overfitting on a small dataset: If we have a small dataset of 50\u201360 data samples, the model will overfit quickly i.e., the loss will be zero in 2\u20135 epochs. To overcome this, be sure to remove any regularization from the model. If your model is not overfitting, it might be because might be your model is not architected correctly or the choice of your loss is incorrect. Maybe your output layer is activated with sigmoid while you were trying to do multi-class classification. These errors can be easy to miss error. Check out my notebook demonstrating this here.", "So how can one avoid such errors? Keep reading.", "1. Start with a small architecture: Using fancy regularizers and schedulers may be overkill. In case of an error, it\u2019s easier to debug a small network. Common errors include forgetting to pass tensors from one layer to another, have insane input to output neurons ratio, etc.", "2. Pretrained model(weights): If your model architecture is built on top of a standard backbone like VGG, Resnet, Inception, etc you can use pre-trained weights on a standard dataset \u2014 if you can, find one on the dataset that you are working with. One interesting recent paper, Transfusion: Understanding Transfer Learning for Medical Imaging shows that using even a few early layers from a pre-trained ImageNet model can improve both the speed of training and final accuracy of medical imaging models. Therefore, you should use a general-purpose pre-trained model, even if it is not in the domain of the problem you\u2019re solving. The amount of improvement from an ImageNet pre-trained model when applied to medical imaging is not that great. Thus there isn\u2019t much guarantee on a head start either. For more, I recommend reading this amazing blog post by Jeremy Howard.", "1. Choosing the right loss function: First, make sure that you are using the right loss function for the given task. For a multi-class classifier, a binary loss function will not help improve the accuracy, so categorical cross-entropy is the right choice.", "2. Determine theoretical loss: If your model started by guessing randomly (i.e. no pretrained model), check to see if the initial loss is close to your expected loss. If you\u2019re using cross-entropy loss, check to see that your initial loss is approximately -log(1/num_classes. You can get some more suggestions here.", "3. Learning Rate: This parameter determines the step size at each iteration while moving toward the minimum of a loss function. You can tweak the learning rate according to how steep or smooth your loss function is. But this can be a time and resource consuming step. Can you find the most optimal learning rate, automatically?", "Leslie N. Smith presented a very smart and simple approach to systematically find a learning rate in a short amount of time and minimal resources. All you need is a model and a training set. The model is initialized with a small learning rate and trained on a batch of data. The associated loss and learning rate is saved. The learning rate is then increased, either linearly or exponentially, and the model is updated with this learning rate.", "In this notebook, you\u2019ll find an implementation of this approach in PyTorch. I have implemented a class LRfinder. The method range_test holds the logic described above. Using wandb.log()I was able to log the learning rate and corresponding loss.", "Use this LRFinder to automatically find the optimal learning rate for your model.", "You can now head to your W&B run page and find the minima of the LR curve. Use this as your learning rate and train on the entire batch of training set.", "When the learning rate is too low the model is not able to learn anything and it remains plateaued. When the learning rate is just large enough it starts learning and you will find a sudden dip in the plot. The minima of the curve is what you are looking for as the optimal learning rate. When the learning rate is high the loss explodes i.e. sudden jump in loss.", "If you are using Keras to build your model you can make use of the learning rate finder as demonstrated in this blog by PyImageSearch. You can also refer this blog for an implementation in TensorFlow 2.0.", "1. Problem of the vanishing gradients: There was a major problem 10 years ago in training a deep neural network due to the use of sigmoid/tanh activation functions. To understand this problem the reader is expected to have an understanding of feed forward and back propagation algorithms along with gradient-based optimization. I recommend that you watch this video or read this blog for a better understanding of this problem.", "In a nutshell, when back propagation is performed, the gradient of the loss with respect to weights of each layer is calculated and it tends to get smaller as we keep on moving backwards in the network. The gradient for each layer can be computed using the chain rule of differentiation. Since the derivative of sigmoid ranges only from 0\u20130.25 numerically the gradient computed is really small and thus negligible weight updates take place. Due to this problem, the model could not converge or it would take a long time to do so.", "Suppose you are building a not so traditional neural network architecture. The easiest way to debug such a network is to visualize the gradients. If you are building your network using PyTorch W&B automatically plots gradients for each layer. Check out my notebook here.", "You can find two models, NetwithIssueand Netin the notebook. The first model uses sigmoid as an activation function for each layer. The latter uses ReLU. The last layer in both the models uses a softmaxactivation function.", "W&B provides first class support for PyTorch. To automatically log gradients and store the network topology, you can call watch and pass in your PyTorch model. If you want to log histograms of parameter values as well, you can pass log='all'argument to the watch method.", "In the W&B project page look for the gradient plot in Vanishing_Grad_1, VG_Converge and VG_solved_Relu the run page. To do so click on the run name and then click on the Gradient section.", "In this run the model was trained for 40 epochs on MNIST handwritten dataset. It eventually converged with a train-test accuracy of over 80%. You can notice a zero gradient for most of the epochs.", "2. Dead ReLU: ReLUs aren\u2019t a magic bullet since they can \u201cdie\u201d when fed with values less than zero. A large chunk of the network might stop learning if most of the neurons die within a short period of training. In such a situation, take a closer look at your initial weights or add a small initial bias to your weights. If that doesn\u2019t work, you can try to experiment with Maxout, Leaky ReLUs and ReLU6 as illustrated in the MobileNetV2 paper.", "3. Problem of exploding gradients: This problem occurs when the later layers learn slower compared to the initial layers, unlike the vanishing gradient problem where earlier layers learn slower than the later layers. This problem occurs when the gradient grows exponentially as we move backwards through the layers. Practically, when gradients explode, the gradients could become NaN because of the numerical overflow or we might see irregular oscillations in the training loss curve. In the case of vanishing gradients, the weight updates are very small while in case of exploding gradients these updates are huge because of which the local minima is missed and models do not converge. You can watch this video for a better understanding of this problem or go through this blog.", "Let\u2019s try to visualize the gradients in case of the exploding gradients. Check out this notebook here where I intentionally initialized the weights with a big value of 100, such that they would explode.", "Notice how the gradients are increasing exponentially going backward. The gradient value for conv1 is in the order of 10\u2077 while for conv2 is 10\u2075. Bad weight initialization can be one reason for this problem.", "Exploding gradients are not usually encountered in the case of CNN based architectures. They\u2019re more of a problem for Recurrent NNs. Check out this thread for more insight. Due to numerical instability caused by exploding gradient you may get NaN as your loss. This notebook here demonstrates this problem.", "There are two simple ways around this problem. They are:", "I used Gradient Clipping to overcome this problem in the linked notebook. Gradient clipping will \u2018clip\u2019 the gradients or cap them to a threshold value to prevent the gradients from getting too large. In PyTorch you can do this with one line of code.", "Here 4.0 is the threshold. This value worked for my demo use case. Check out the trainModified function in the notebook to see the implementation.", "1. Weight Initialization: This is one of the most important aspects of training a neural network. Problems like image classification, sentiment analysis or playing Go can\u2019t be solved using deterministic algorithms. You need a non deterministic algorithm to solve such problems. These algorithms use elements of randomness when making decisions during the execution of the algorithm. These algorithms make careful use of randomness. Artificial neural networks are trained using a stochastic optimization algorithm called stochastic gradient descent. Training a neural network is simply a non deterministic search for a \u2018good\u2019 solution.", "As the search process (training) unfolds, there is a risk that we are stuck in an unfavorable area of the search space. The idea of getting stuck and returning a \u2018less-good\u2019 solution is called being getting stuck in a local optima. At times vanishing/exploding gradients prevent the network from learning. To counter this weight initialization is one method of introducing careful randomness into the searching problem. This randomness is introduced in the beginning. Using mini-batches for training with shuffle=True is another method of introducing randomness during progression of search. For more clarity of the underlying concept check out this blog.", "A good initialization has many benefits. It helps the network achieve global minima for gradient based optimization algorithms (just a piece of the puzzle). It prevents vanishing/exploding gradient problems. A good initialization can speed up training time as well. This blog here explains the basic idea behind weight initialization well.", "The choice of your initialization method depends on your activation function. To learn more about initialization check out this article.", "Most initialization methods come in uniform and normal distribution flavors. Check out this PyTorch doc for more info.", "Check out my notebook here to see how you can initialize weights in PyTorch.", "Notice how the layers were initialized with kaiming_uniform. You\u2019ll notice this model overfits. By simplifying the model you can easily overcome this problem.", "2. Dropout and Batch Normalization: Dropout is a regularization technique that \u201cdrops out\u201d or \u201cdeactivates\u201d a few neurons in the neural network randomly, in order to avoid the problem of overfitting. During training some neurons in the layer after which the dropout is applied are \u201cturned off\u201d. An ensemble of neural networks with fewer parameters (simpler model) reduces overfitting. Dropout simulates this phenomenon, contrary to snapshot ensembles of networks, without additional computational expense of training and maintaining multiple models. It introduces noise into a neural network to force it to learn to generalize well enough to deal with noise.", "Batch Normalization is a technique to improve optimization. It\u2019s a good practice to normalize the input data before training on it which prevents the learning algorithm from oscillating. We can say that the output of one layer is the input to the next layer. If this output can be normalized before being used as the input the learning process can be stabilized. This dramatically reduces the number of training epochs required to train deep networks. Batch Normalization makes normalization a part of the model architecture and is performed on mini-batches while training. Batch Normalization also allows the use of much higher learning rates and for us to be less careful about initialization.", "Let\u2019s implement the above discussed concepts and see the results. Check out my notebook here to see how one can use Batch Normalization and Dropout in Pytorch. I started with a base model to set the benchmark for this study. The implemented architecture is simple and results in overfitting.", "Notice how test loss increases eventually. I then applied Dropout layers with a drop rate of 0.5 after Conv blocks. To initialize this layer in PyTorch simply call the Dropout method of torch.nn.", "Dropout prevented overfitting but the model didn\u2019t converge quickly as expected. This means that ensemble networks take longer to learn. In the context of dropout not every neuron is available while learning.", "Next up is Batch Normalization. To initialize this layer in PyTorch simply call the BatchNorm2d method of torch.nn.", "Batch Normalization took fewer steps to converge the model. Since the model was simple, overfitting could not be avoided.", "Now let\u2019s use both these layers together. If you are using BN and Dropout together follow this order (for more insight check out this paper).", "Notice that by using both Dropout and Batch Normalization overfitting was eliminated while the model converged quicker.", "When you have a large dataset, it\u2019s important to optimize well, and not as important to regularize well, so batch normalization is more important for large datasets. You can of course use both batch normalization and dropout at the same time, though Batch Normalization also acts as a regularizer, in some cases eliminating the need for Dropout.", "I hope that this blog will be helpful for everyone in the Machine Learning community. I\u2019ve tried to share some insights of my own and lots of good reading material for deeper understanding of the topics. The most important aspect of debugging neural network is to track your experiments so you can reproduce them later. Weight and Biases is really handy when it comes to tracking your experiments. With all the latest ways to visualize your experiments, it\u2019s getting easier day by day.", "I would like to thank Lavanya for the opportunity. I learned a lot in the process. Thank you Sayak Paul for the constant mentoring.", "Machine Learning Engineer at Weights and Biases"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2e20667baf72&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/production-ml", "anchor_text": "Machine Learning in Production"}, {"url": "https://mein2work.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Ayush Thakur"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F568907d77025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=post_page-568907d77025----2e20667baf72---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e20667baf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=-----2e20667baf72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e20667baf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=-----2e20667baf72---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@efekurnaz?utm_source=medium&utm_medium=referral", "anchor_text": "Efe Kurnaz"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/reports/Visualizing-and-Debugging-Neural-Networks-with-PyTorch-and-W%26B--Vmlldzo2OTUzNA?utm_source=tds&utm_medium=report&utm_campaign=report_author", "anchor_text": "here"}, {"url": "https://github.com/ayulockin/debugNNwithWandB", "anchor_text": "here"}, {"url": "https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765", "anchor_text": "piece"}, {"url": "http://karpathy.github.io/2019/04/25/recipe/", "anchor_text": "A Recipe for Training Neural Networks"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Overfit_Small.ipynb", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1902.07208", "anchor_text": "Transfusion: Understanding Transfer Learning for Medical Imaging"}, {"url": "https://www.fast.ai/2020/01/13/self_supervised/", "anchor_text": "blog"}, {"url": "http://cs231n.github.io/neural-networks-3/#loss", "anchor_text": "here"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_LRFinder.ipynb", "anchor_text": "this notebook"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets?workspace=user-ayush-thakur", "anchor_text": "here"}, {"url": "https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/", "anchor_text": "blog"}, {"url": "https://www.pyimagesearch.com/", "anchor_text": "PyImageSearch"}, {"url": "https://mc.ai/how-to-find-a-descent-learning-rate-using-tensorflow-2/", "anchor_text": "blog"}, {"url": "https://www.youtube.com/watch?v=JIWXbzRXk1I&t=312s", "anchor_text": "video"}, {"url": "https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b", "anchor_text": "blog"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Vanishing_Gradient.ipynb", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/neo43xm0", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=IJ9atfxFjOQ&t=12s", "anchor_text": "video"}, {"url": "http://neuralnetworksanddeeplearning.com/chap5.html", "anchor_text": "blog"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_Exploding_Gradient.ipynb", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/7gygoykn?workspace=user-ayush-thakur", "anchor_text": "here"}, {"url": "https://www.quora.com/Why-are-the-exploding-gradient-problems-mostly-encountered-in-recurrent-neural-networks", "anchor_text": "thread"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Pytorch_wandb_Exploding_Gradient_NaN.ipynb", "anchor_text": "here"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Pytorch_wandb_Exploding_Gradient_NaN.ipynb", "anchor_text": "linked notebook"}, {"url": "https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "here"}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "article"}, {"url": "https://pytorch.org/docs/stable/nn.init.html?highlight=nn%20init#torch.nn.init.zeros_", "anchor_text": "doc"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Cifar10_pytorch_wandb_Wt_Init.ipynb", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/2a5qddz3?workspace=user-ayush-thakur", "anchor_text": "here"}, {"url": "https://github.com/ayulockin/debugNNwithWandB/blob/master/Cifar10_pytorch_wandb_Dropout_BN.ipynb", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/dxdz06vk", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/tlf4e8fm", "anchor_text": "here"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/pntlvpic", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1801.05134", "anchor_text": "this"}, {"url": "https://app.wandb.ai/ayush-thakur/debug-neural-nets/runs/1e5c7bk2", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21", "anchor_text": "Checklist for debugging neural networks"}, {"url": "https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765", "anchor_text": "How to unit test machine learning code"}, {"url": "http://neuralnetworksanddeeplearning.com/chap5.html", "anchor_text": "Why are deep neural networks hard to train?"}, {"url": "https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/", "anchor_text": "how to avoid exploding gradients in neural networks with gradient clipping?"}, {"url": "https://www.wandb.com/articles/the-effects-of-weight-initialization-on-neural-nets", "anchor_text": "The effects of weight initialization on neural nets"}, {"url": "https://sayak.dev/", "anchor_text": "Sayak Paul"}, {"url": "https://medium.com/tag/production-ml?source=post_page-----2e20667baf72---------------production_ml-----------------", "anchor_text": "Production Ml"}, {"url": "https://medium.com/tag/wandb?source=post_page-----2e20667baf72---------------wandb-----------------", "anchor_text": "Wandb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e20667baf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=-----2e20667baf72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e20667baf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=-----2e20667baf72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e20667baf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F568907d77025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=post_page-568907d77025----2e20667baf72---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd6e372c0f477&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&newsletterV3=568907d77025&newsletterV3Id=d6e372c0f477&user=Ayush+Thakur&userId=568907d77025&source=-----2e20667baf72---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Written by Ayush Thakur"}, {"url": "https://mein2work.medium.com/followers?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "61 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F568907d77025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&user=Ayush+Thakur&userId=568907d77025&source=post_page-568907d77025----2e20667baf72---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd6e372c0f477&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-neural-networks-with-pytorch-and-w-b-2e20667baf72&newsletterV3=568907d77025&newsletterV3Id=d6e372c0f477&user=Ayush+Thakur&userId=568907d77025&source=-----2e20667baf72---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/converting-fc-layers-to-conv-layers-8a43880a44ed?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Ayush Thakur"}, {"url": "https://mein2work.medium.com/converting-fc-layers-to-conv-layers-8a43880a44ed?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Converting FC Layers to Conv LayersUnderstand and implement Fully Convolutional Network"}, {"url": "https://mein2work.medium.com/converting-fc-layers-to-conv-layers-8a43880a44ed?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "5 min read\u00b7Jul 4, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F8a43880a44ed&operation=register&redirect=https%3A%2F%2Fmein2work.medium.com%2Fconverting-fc-layers-to-conv-layers-8a43880a44ed&user=Ayush+Thakur&userId=568907d77025&source=-----8a43880a44ed----0-----------------clap_footer----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/converting-fc-layers-to-conv-layers-8a43880a44ed?source=author_recirc-----2e20667baf72----0---------------------963f7f8b_cd71_4445_848b_de952978f84c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a43880a44ed&operation=register&redirect=https%3A%2F%2Fmein2work.medium.com%2Fconverting-fc-layers-to-conv-layers-8a43880a44ed&source=-----2e20667baf72----0-----------------bookmark_preview----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2e20667baf72----1---------------------963f7f8b_cd71_4445_848b_de952978f84c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----2e20667baf72----1-----------------bookmark_preview----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2e20667baf72----2---------------------963f7f8b_cd71_4445_848b_de952978f84c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----2e20667baf72----2-----------------bookmark_preview----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Ayush Thakur"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "Interpretability in Deep Learning with W&B \u2014 CAM and GradCAMThis report will review how Grad-CAM counters the common criticism that neural networks are not interpretable."}, {"url": "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": "\u00b712 min read\u00b7May 22, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45ba5296a58a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a&user=Ayush+Thakur&userId=568907d77025&source=-----45ba5296a58a----3-----------------clap_footer----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=author_recirc-----2e20667baf72----3---------------------963f7f8b_cd71_4445_848b_de952978f84c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45ba5296a58a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a&source=-----2e20667baf72----3-----------------bookmark_preview----963f7f8b_cd71_4445_848b_de952978f84c-------", "anchor_text": ""}, {"url": "https://mein2work.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "See all from Ayush Thakur"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/@iamleonie?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/@iamleonie?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Leonie Monigatti"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "A Visual Guide to Learning Rate Schedulers in PyTorchLR decay and annealing strategies for Deep Learning in Python"}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b79 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24bbb262c863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----24bbb262c863----0-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24bbb262c863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863&source=-----2e20667baf72----0-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----2e20667baf72----1-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2e20667baf72----0---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----2e20667baf72----0-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----2e20667baf72----1---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "90"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----2e20667baf72----1-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----2-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----2e20667baf72----2---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----2e20667baf72----2-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----3-----------------clap_footer----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2e20667baf72----3---------------------fad83e30_1773_46f9_955c_235e71045322-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----2e20667baf72----3-----------------bookmark_preview----fad83e30_1773_46f9_955c_235e71045322-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2e20667baf72--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----2e20667baf72--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}