{"url": "https://towardsdatascience.com/squeeze-that-neural-network-juice-3ce28a2cca75", "time": 1683006338.039007, "path": "towardsdatascience.com/squeeze-that-neural-network-juice-3ce28a2cca75/", "webpage": {"metadata": {"title": "Squeeze These Neural Networks. Tips on making neural caipirinhas (and\u2026 | by Ygor Serpa | Towards Data Science", "h1": "Squeeze These Neural Networks", "description": "We all know how expensive neural networks can be. Training is a tedious task that takes multiple iterations (and each iteration takes several epochs (and each epoch takes several steps (and have you\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/lite/performance/post_training_float16_quant", "anchor_text": "TensorFlow Lite package can do it for you", "paragraph_index": 13}, {"url": "https://www.tensorflow.org/guide/keras/mixed_precision", "anchor_text": "enabling half-precision mode", "paragraph_index": 13}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning", "anchor_text": "easily accessible", "paragraph_index": 16}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "Keras backend", "paragraph_index": 16}, {"url": "https://github.com/BenWhetton/keras-surgeon", "anchor_text": "Keras Surgeon", "paragraph_index": 16}, {"url": "https://github.com/BenWhetton/keras-surgeon/blob/master/src/kerassurgeon/examples/lenet_mnist.py", "anchor_text": "nice example", "paragraph_index": 16}, {"url": "https://github.com/BenWhetton/keras-surgeon", "anchor_text": "Keras surgeon", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1608.08710", "anchor_text": "this work on pruning filters", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "lottery ticket hypothesis", "paragraph_index": 17}, {"url": "https://keras.io/applications/", "anchor_text": "Keras", "paragraph_index": 23}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/applications", "anchor_text": "Tensorflow", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1704.04861", "anchor_text": "reading papers", "paragraph_index": 24}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D", "anchor_text": "DepthwiseConv2D", "paragraph_index": 26}], "all_paragraphs": ["We all know how expensive neural networks can be. Training is a tedious task that takes multiple iterations (and each iteration takes several epochs (and each epoch takes several steps (and have you tuned hyper-parameters? (and one more layer maybe? (and \u2026)))))(((())))))))()))", "\u2026 If you got to the point of deploying anything, you also know that cloud inference is not free. The larger the model, the more dollars A\u0336m\u0336a\u0336z\u0336o\u0336n the neural network will squeeze out of you.", "In this article, I\u0336 \u0336t\u0336e\u0336a\u0336c\u0336h\u0336 \u0336h\u0336o\u0336w\u0336 \u0336t\u0336o\u0336 \u0336m\u0336a\u0336k\u0336e\u0336 \u0336c\u0336a\u0336i\u0336p\u0336i\u0336r\u0336i\u0336n\u0336h\u0336a\u0336s I overview a set of techniques that you can explore to make training faster, produce smaller models, and, ultimately, save money and time. All tips are practical ones, things you can apply today and get results at most tomorrow.", "Don\u2019t let the neural network squeeze your wallet. Squeeze those weights out.", "Btw, I am sorry if you came for the caipirinhas. It takes a lime cut in eight half-moons and a tablespoon (or three) of sugar crushed on a large rocks glass. Not too much, not too little. Pour one shot of cacha\u00e7a and fill the rest with ice cubes. Vodka will do, but it goes by caipivodka instead.", "Consider you have a company and that it has a neural network product that is 92% accurate. However, it uses about a hundred million parameters and takes two days to train on a multi-GPU infrastructure. Improving the model any further is especially costly, as most changes either make it worse or do not provide any improvements at all.", "If you could have a 90% accurate model that takes 20\u201350 times fewer resources, would it kill your business or make it thrive?", "Getting to 94% is hard. It takes a lot of resources, larger networks, time\u2026", "Getting to 90% is easy. If you can do 92%, doing less is trivial.", "The goal is: how much can you simplify without going below 90%?", "Current literature and practice show that it is fairly easy to downgrade models while maintaining most of the original accuracy intact. Some of the techniques are purely technical, and others need slight alterations to the network architecture. In the following, I first overview the techniques that can be applied to pre-trained models so that you can get results today and then the ones which require some retraining, which might just finish tomorrow.", "Typically, models are trained using floating-point numbers. Most coders are aware of the \u201cdouble\u201d type, a more precise float, but not as many know the \u201chalf\u201d type, the 16-bit float.", "Using the \u201chalf\u201d type halves memory consumption and computational time. Neural networks trained using half-precision can be trained with double the batch size of normal nets, basically halving the time per epoch. During inference, half-precision models are lighter to transfer and can be executed faster. Also, many companies feature especial cloud hardware for half-precision models.", "The good news is: you can quantize a pre-trained model to half-precision, and it works. The TensorFlow Lite package can do it for you. If you are training your model from scratch, you can benefit from it by enabling half-precision mode (do not forget to double the batch size!).", "Among the millions of parameters, some are large, and some are tiny. There are many techniques to spot these \u201ctiny\u201d weights and remove them from the network. Pruning alone can remove from 10 to 90% of weights while impacting less than 2% on the total accuracy.", "Pruned networks compress better and have improved inference times. You may be surprised that even MobileNet can be downsized to half of its original size. Large non-optimized models, such as InceptionNet, may have up to 85% of its original weights removed.", "The good news is: TensorFlow has a couple of weight pruning techniques easily accessible, also compatible with the Keras backend. A community solution is Keras Surgeon, which as a nice example showing how to alternate between pruning and tuning.", "Going the extra mile: Most pruning tools remove weights but leave the architecture intact. More advanced techniques can remove entire filters from convolutions and neurons from dense layers, improving training speed as well. Keras surgeon can do that. Two interesting reads are this work on pruning filters and the lottery ticket hypothesis.", "Many companies are using pre-trained models in their pipelines, not only as backbone architectures. Typically, they use \u201clarge\u201d models such as VGG-19, InceptionNet, and ResNet. Consider using an optimized model instead.", "For instance, if you are using ResNet-151, you might consider migrating to ResNet-50, or down to the MobileNetV3 architecture. If you are relying on the Faster R-CNN architecture for object detection, consider switching its backbone (or the entire thing). YOLO and YOLO fast are great optimized alternatives. It might save you thousands of dollars in inference times.", "Be Wise: Some of these choices will take you to bellow our 2% agreement. Some will hit you a 3% drop, or a 10% drop. How much money will they save? How many customers might you lose? It might still be profitable to change.", "A quick reminder that you can take YOLO or MobileNet, prune it, and quantize to half-precision. These techniques are not mutually exclusive.", "Continuing the last tip, if you are using an off-the-shelf model as a backbone, which is a trendy thing to do, consider switching to a smaller ResNet backbone or the MobileNetV3. This works wonders for transfer learning.", "Get rid of that low back pain. Keras / Tensorflow bundles many pre-trained models that can be easily used as backbones.", "The good thing about reading papers is knowing how other authors did marvelous things. The key to MobileNet\u2019s performance is the Depth-wise Separable Convolution.", "In simplified terms, the idea is to divide a 3 by 3 convolution with 128 filters into two sequential steps: a \u201cspatial\u201d convolution, that does the \u201c3 by 3\u201d part, and a \u201cdepth-wise\u201d convolution, that does the \u201c128 filters\u201d part. On average, this is nine times faster and uses orders of magnitude fewer parameters.", "The good news is: this is as simple as changing one line of code. For Keras users, just change the \u201cConv2D\u201d to \u201cDepthwiseConv2D\u201d. All parameters are the same. In my experience, this doesn\u2019t train much faster, but the inference is tremendously improved, as is the model size.", "If you already have a working model, a very efficient pruning technique is knowledge distillation. Also known as the teacher-student model.", "Run the good old model for the entire dataset and save its predictions. These are \u201csoft labels.\u201d They are not your typical one-hot encoded labels; they have some uncertainty. Now, train the smaller model on the soft labels, instead of the ground truth. This will teach the \u201cstudent\u201d to replicate the \u201cteacher\u201d instead of trying to learn it by itself.", "It seems complicated, but it is rather easy: You just have to run the old model as you normally would and pickle the results. Then, using the same code you had, ditch the dataset and read the ground truth labels from the pickle file. To avoid having the \u201cdataset\u201d and \u201cteacher\u201d as two separate things, you can just pickle the dataset too. The original teacher.", "Going the Extra Mile: More comprehensive approaches might interpolate the true labels with the teacher labels or create a mixed loss function. You can also \u201cteach\u201d the output of internal layers, besides the output one. However, this complicates things a bit.", "Most successful networks use 224x224 or 256x256 inputs. If you are using more than this, you might be wasting resources. Less is often more.", "The thing with images is they are highly correlated. The data found at the pixel (23, 23) is almost the same found at the (24, 23) or (23, 22). As resolution increases, the correlation dominates. A 4K face is not more face-y than a 256x256 one. It just has all the pores in crisp detail.", "Unless you are dealing with a needle-in-the-haystack problem, consider downsampling your inputs to a lower resolution. As long as you, human, can still recognize what there is to be recognized, the model will be fine.", "Downsampling can be training-free: If your model is fully convolutional or uses a global average pooling before turning into a fully connected network, you might get the benefits of downsampling without having to retrain it.", "If you are dealing with image-to-image problems, especially image segmentation, high are the odds that the segmentation can be done in a lower resolution.", "Let\u2019s say you are producing 256x256 segmentation maps. If you retrain your model to do it 128x128 instead, its performance might go up. Then, you just need to upscale it back to 256x256 and reevaluate the actual impact. High are the odds you will stay around the 2% accuracy of the original, but saving the initial convolution and the final deconvolution.", "The thing to note here is that segmentation borders are usually noisy and wrong, despite the overall shape being right. Doing it at a lower resolution + traditional up-scaling is an easy way to get smoother results while also saving computational time.", "The key takeaway of this article is that we are all wasting resources. Neural networks do not need to be large. If you squeeze it really hard, all that inefficiency will go away. The juicy network that remains is all you need.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Former game developer turned data scientist after falling in love with AI and all its branches."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3ce28a2cca75&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ygorserpa.medium.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----3ce28a2cca75---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thepaintedsquare?utm_source=medium&utm_medium=referral", "anchor_text": "Jessica Lewis"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@mariadasdores?utm_source=medium&utm_medium=referral", "anchor_text": "Maria das Dores"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_float16_quant", "anchor_text": "TensorFlow Lite package can do it for you"}, {"url": "https://www.tensorflow.org/guide/keras/mixed_precision", "anchor_text": "enabling half-precision mode"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning", "anchor_text": "easily accessible"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "Keras backend"}, {"url": "https://github.com/BenWhetton/keras-surgeon", "anchor_text": "Keras Surgeon"}, {"url": "https://github.com/BenWhetton/keras-surgeon/blob/master/src/kerassurgeon/examples/lenet_mnist.py", "anchor_text": "nice example"}, {"url": "https://github.com/BenWhetton/keras-surgeon", "anchor_text": "Keras surgeon"}, {"url": "https://arxiv.org/abs/1608.08710", "anchor_text": "this work on pruning filters"}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "lottery ticket hypothesis"}, {"url": "https://keras.io/applications/", "anchor_text": "Keras"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/applications", "anchor_text": "Tensorflow"}, {"url": "https://arxiv.org/abs/1704.04861", "anchor_text": "reading papers"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D", "anchor_text": "DepthwiseConv2D"}, {"url": "https://unsplash.com/@kaizennguyen?utm_source=medium&utm_medium=referral", "anchor_text": "Kaizen Nguy\u1ec5n"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/ai?source=post_page-----3ce28a2cca75---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3ce28a2cca75---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3ce28a2cca75---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3ce28a2cca75---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3ce28a2cca75---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&user=Ygor+Serpa&userId=857d8734c7da&source=-----3ce28a2cca75---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&user=Ygor+Serpa&userId=857d8734c7da&source=-----3ce28a2cca75---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3ce28a2cca75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3ce28a2cca75---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3ce28a2cca75--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://ygorserpa.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3644829c90d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsqueeze-that-neural-network-juice-3ce28a2cca75&newsletterV3=857d8734c7da&newsletterV3Id=3644829c90d9&user=Ygor+Serpa&userId=857d8734c7da&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}