{"url": "https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae", "time": 1682999770.287572, "path": "towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae/", "webpage": {"metadata": {"title": "Advanced Topics in Neural Networks | by Matthew Stewart, PhD | Towards Data Science", "h1": "Advanced Topics in Neural Networks", "description": "This article will act as an introduction to some of the more advanced topics used in neural networks and will cover several important topics still discussed in neural network research. You can access\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1710.01878.pdf", "anchor_text": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "research paper", "paragraph_index": 42}, {"url": "https://mc.ai/what-is-orthogonalization-in-machine-learning/", "anchor_text": "Mc.ai", "paragraph_index": 66}, {"url": "https://keras.io/initializers/", "anchor_text": "here", "paragraph_index": 86}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io", "paragraph_index": 91}], "all_paragraphs": ["This article will act as an introduction to some of the more advanced topics used in neural networks and will cover several important topics still discussed in neural network research.", "The topics that will be covered in this article are:", "You can access the previous articles below. The first provides a simple introduction to the topic of neural networks, to those who are unfamiliar. The second article covers more intermediary topics such as activation functions, neural architecture, and loss functions. The third article looks at more advanced aspects such as momentum, adaptive learning rates, and batch normalization. The fourth article is an in-depth tutorial looking at hyperparameter tuning in neural networks.", "Imagine that you work for a small company and have limited computational resources but are asked to develop a state-of-the-art algorithm for classifying pictures of different plant species. This is a fairly common problem for any company trying to develop commercial solutions that utilize computer vision (vision-oriented machine learning).", "In this situation, you have (atleast) two problems. The amount of data you have access too may be limited, and the computing power is also limited. Clearly, it is not feasible to hook up a 150 layer deep neural network with a few dozen GPUs and train a model for several weeks to distinguish between the easiest 1000 plant species you currently have data for in order to create your startup's algorithm for its minimum viable product.", "What can you do in this situation to make life a bit easier?", "Fortunately, several companies like Google and Microsoft have released state-of-the-art neural network architectures that are optimized for image recognition. These companies had huge resources and expertise at their disposal so it is likely that you can trust that these models would have superior performance to any in-house model you can cook up.", "There is a problem, however, which is that the data that these algorithms were trained on may not have been (and, in fact, was not) trained on the 1000 plant species that you would like to use the algorithm to distinguish between.", "But, there is a solution: transfer learning. Transfer learning is exactly that, you transfer the learning from one model to a second model which is examining data very similar to the data of the original model. In this case, the original models were used for general image recognition (typically on the CIFAR datasets), which is very close to what we are trying to do. We can then retrain the last few layers of the network on our own data in order to fine-tune the model for our specific application.", "So how does this work? We first build the architecture of the original model using Keras and then load the model weights of the trained network (you need to download these first; they usually come in .h5 format).", "The second thing we need to do is to freeze the weights of the initial layers by setting the layers to have the trainable=False parameter. The reason this works is that the initial layers of a convolutional neural network (used for processing images) contain primitive information about the image, such as interpretations of lines, edges, shapes, and other low-level features. As we go deeper into the network, the objects become more complex and high-level, which is where the network begins to differentiate more clearly between image qualities.", "The idea here is that we are not trying to teach the network to examine images, as it has already been trained well to do this by Google or Microsoft. We just need to fine-tune it for our use-case.", "So, we can set the fully connected network layers (or a subset of them) at the output to be trainable, and perhaps the final convolutional layer (if we have enough data) to be trainable, and then train the network on our data.", "This procedure has the added benefits of requiring fewer data to train the model \u2014 since the number of network trainable parameters is a fraction of the total number of parameters in the network.", "Transfer learning only works if the two datasets that are being analyzed are very similar. As such, transfer learning is most commonly used for image recognition or for speech recognition, where networks can be trained for general recognition of images or of general recognition of sounds or a given language. Training a network using transfer learning requires significantly less data and computation time than training the full network from scratch.", "Please see the \u2018Further Reading\u2019 section for open-source literature articles to research transfer learning in more detail.", "Model pruning seeks to induce sparsity in a deep neural network\u2019s various connection matrices, thereby reducing the number of nonzero-valued parameters in the model.", "The word \u2018pruning\u2019 comes from its use in decision trees where branches of the tree are pruned as a form of model regularization. Analogously, weights in a neural network that are considered unimportant or rarely fire can be removed from the network with little to no consequence. In fact, the majority of neurons have a relatively small impact on the model performance, meaning we can achieve high accuracy even when eliminating large numbers of parameters.", "Reducing the number of parameters in a network becomes increasingly important as neural architectures and datasets get larger in order to obtain reasonable execution times of models.", "The research paper \u201cTo prune, or not to prune: exploring the efficacy of pruning for model compression\u201d examined the performance of neural networks as a function of sparsity (effectively the percentage of neurons removed) and found that even when reducing 75% of the neurons in a network, the model performance was not affected significantly. Indeed, halving the number of parameters only reduced the accuracy by 0.1%.", "Pruning is typically done in convolutional neural networks, however, since the majority of parameters in convolutional models occur in the fully connected (vanilla) neural layers, most of the parameters are eliminated from this portion of the network.", "There are multiple ways of performing pruning in a deep neural network. The most common way is weight pruning. Weight pruning rank-orders the weights by their magnitude since parameters with larger weights are more likely to fire and thus more likely to be important.", "Another way is unit pruning, which sets entire columns in the weight matrix to zero, in effect deleting the corresponding output neuron. Here to achieve sparsity of k% we rank the columns of a weight matrix according to their L2-norm and delete the smallest k%.", "A third and more advanced method is to use Fischer pruning, which relies on the Fischer information. This generates a norm known as the Fischer-Rao norm which can then be used to rank-order parameters. It is suspected there is a link between the Fischer information and the redundancy of parameters, and this is why this technique seems to produce good results. You can read more about Fischer pruning here:", "To see a code implementation of pruning on the standard VGG16 network, I recommend looking at this excellent article:", "As you have likely come to realize from your own adventures with neural networks, and possibly from other articles and research literature, the learning rate is a very important part of neural network training.", "The learning rate essentially determines how \u2018fast\u2019 the network will learn, it determines the step size of the movement. A higher learning rate results in faster learning, but convergence is more difficult and often unstable. A smaller learning rate results in slower learning, and whilst convergence is possible, it may only occur after an inordinate number of epochs, which is computationally inefficient.", "Clearly, there is a sweet spot in between that is optimal for a given neural architecture \u2014 this is unique and is a function of the neural loss surface. However, since we are training the network, the neural architecture is dynamic and constantly shifting, resulting in distortion of the optimal learning rate. In addition to this temporal dependence, the sweet spot is also spatially dependent \u2014 since certain locations on the loss surface may have extremely sharp or shallow gradients \u2014 which further complicates matters.", "So how do we choose a learning rate that will give us the best results? Clearly, we could just use a constant learning rate, but if we do so we cannot complain when we achieve suboptimal results.", "The only way to achieve better results is to use a dynamic learning rate that tries to leverage the spatial and temporal variations in the optimal learning rate. This is not a trivial task, but several techniques have been developed in order to do this.", "Cyclical learning rates are fairly self-explanatory, they are oscillatory in nature \u2014 they oscillate back and forth. The main use of cyclical learning rates is to escape local extreme points, especially sharp local minima (overfitting). Saddle points are abundant in high dimensions, and convergence becomes very slow, if not impossible. Thus, if we use a purely decreasing learning rate it is easy to get stuck in a single location, especially in higher dimensions.", "Cyclic learning rates raise the learning rate periodically. This has a short term negative effect and yet helps to achieve a longer-term beneficial effect. \\", "Decreasing learning rates may still help reduce error towards the end. This can be done by using a dampened cyclical learning rate, which slowly decays over time to zero.", "Whilst this is a nice idea, it tells us nothing about what learning rate scheme we should set and the magnitude of these learning rates.", "How can we get a good learning rate estimate? This may seem complex but it is actually not too difficult.", "First, we can start with a small learning rate and increase it on every batch exponentially. Simultaneously, we can compute the loss function on a validation set. This also works for finding bounds for cyclic learning rates.", "The plot below shows the exponentially-increasing learning rate as a function of epochs.", "At the same time, we compute the loss function for each of these learning rates.", "We can see there is a cliff region in between the two extremes in which there is steadily decreasing and stable learning occurring. This is the region we want to ensure that we remain in during the course of training.", "Warm restarts are a very simple idea: restart the learning after a specified number of epochs. For example, the learning rate starts at 0.1 initially and decreases exponentially over time. After 30 iterations, the learning rate scheduler resets the learning rate to the same value as epoch 1, and then the learning rate scheduler repeats the same exponentially decay. The best estimates are recorded each time before the learning rate is reset.", "This idea is similar to the cyclical learning rate except for the learning rate graph typically looks more like a sawtooth wave rather than something symmetric and cyclic.", "It is important to note that the restarts are not from scratch, but from the last estimate, and the learning rate is increased. Thus, restarts provide most of the same benefits as cyclical learning rates, such as being able to escape extreme local minima.", "Snapshot ensembles are a wonderful idea and I suggest reading the research paper if you are interested.", "Ensemble networks are much more robust and accurate than individual networks. They constitute another type of regularization technique. The novelty is to train a single neural network but obtain M different models. The idea is to converge to M different local optima and save network parameters.", "For those of you who are seasoned data scientists, you may recall that often developing ensembled or blended models for classification purposes often provide superior results to any single model \u2014 although, there are several requirements needed for this to be ensured, such as a relatively high correlation between models. This is in some ways just an extension of that technique to neural networks.", "At the end of each training cycle, the model reaches a local minimum with respect to the training loss. Thus, before raising the learning rate, we take a \u201csnapshot\u201d of the model weights. After training M cycles, we have M model snapshots, f\u2081, f\u2082,. . . , each of which will be used in the final ensemble. In the ensemble model, we take the average of all of the snapshots and use this to obtain our results, achieving a neural network that has smoothened parameters, thus reducing the total noise and as a result, the total error.", "It is important to highlight that the total training time of the M snapshots is the same as training a model with a standard schedule. This means that we have a simple method to obtain ensembles of neural networks without any additional training cost.", "However, snapshot ensembles are not perfect, different initialization points or hyperparameter choices may be chosen, which could converge to different local minima. Although these local minima may perform similarly in terms of averaged errors, they may not make the same mistakes.", "Ensemble methods train many neural networks and then optimize through majority vote, or averaging of the prediction outputs \u2014 this is as opposed to averaging the weights and using these for prediction. The final ensemble consists of snapshots of the optimization path.", "The results below show the results of the snapshot ensemble on several common datasets used for testing neural network models.", "As is clear from the results, the snapshot ensemble performance was superior to standard models, as well as cycle ensembles and dropout models.", "Given the similarity between snapshot ensembles and Polyak averaging, I thought it best to include this. Polyak averaging is another approach to address the unstable learning issue that stimulated the use of snapshot ensembles. The message uses an average of the weights from multiple models seen towards the end of the training run.", "Suppose in t iterations you have parameters \u03b8\u00b9, \u03b8\u00b2, etc. We can then take the time average of these parameters to obtain a smoother estimator for the true parameter.", "This can be leveraged in several different ways:", "Polyak averaging guarantees strong converge in a convex setting. However, in non-convex surfaces (which is typically the case) the parameter space can differ greatly in different regions. In this case, averaging is less useful. When in this situation, it is typical to consider the exponentially decaying average instead:", "Depending on the chosen value of \u03b1, additional weight is either placed on the newest parameter values or the older parameter values, whereby the importance of the older parameters exponentially decays over time.", "Hopefully, the differences and similarities between Polyak averaging and snapshot ensembles are clear to you. They are just different ways of smoothing the random error manifestly present in the unstable learning process of neural networks.", "A good walkthrough of Polyak averaging applied to neural networks can be found here:", "Overfitting in deep neural networks is so common that you can never really truly believe that the results you get from a trained network are the optimal results. Given their prevalence, we will look at some of the ways in which we can address overfitting in neural networks.", "Point estimation is the attempt to provide the single \u201cbest\u201d prediction of some quantity of interest:", "From the frequentist perspective, \u03b8 is fixed but unknown, and the data is random. This means that our estimator is a random variable. Every random variable is susceptible to bias and variance.", "So how can we analyze this bias-variance trade-off in neural networks?", "In high dimensions, we cannot draw decision curves to inspect bias-variance. Instead, we calculate error values to infer the source of errors on the training set, as well as on the validation set. To determine bias, we need a baseline, such as human-level performance.", "The Bayes error rate is the lowest possible error rate for any classifier of a random outcome and is analogous to the irreducible error. For all intent and purpose, the Bayes error is the minimum error we could obtain with a perfect model stripped of all avoidable bias and variance.", "We can analyze and compare the errors on the training and validation sets in order to deduce the cause of the error, whether it is due to avoidable bias or avoidable variance (it is unlikely that we ever obtain the Bayes error rate).", "Orthogonalization is another technique we can use; it aims to decompose the process to adjust neural network performance. It assumes the errors come from different sources and uses a systematic approach to minimize them.", "Mc.ai has a wonderful analogy for this of the knobs used to tune radios. The designers of a radio worked hard to make sure that one particular knob controlled one particular aspect of the signal, such as the volume or the frequency. It is much easier to tune to a specific radio frequency and get the sound you want if there aren\u2019t multiple parameters varying simultaneously.", "Early stopping is also a popular regularization mechanism, but couples the bias and variance errors.", "Other more obvious ways of reducing bias are to train a bigger model (increases the explainability of the model), train longer (reach convergence), or adapt the neural architecture or perform an extensive hyperparameter search.", "Obvious ways of reducing the variance are to get more data, to use regularization mechanisms (early stopping is a popular regularization technique which couples both bias and variance errors).", "Being able to analyze your network\u2019s results and determine whether the issue is caused by biasing or variance can be an extremely helpful way to troubleshoot the network and also to improve the network performance. For example, if you are struggling with variance issues and you try to run the network for more epochs, this is clearly not going to help. However, adding more data or regularization are more prudent approaches in this situation.", "Most of you are probably familiar with dropout more than the other items I have discussed in this article so far. Dropout is easy to implement but more difficult to grasp its underlying philosophy.", "Dropout is a regularization technique for deep neural networks. It is employed at training time and eliminates the output of some units randomly. This helps to prevent the network from relying on individual neurons too much, which helps to prevent overfitting. Instead, the knowledge is spread across all of the neurons to help obtain a more robust network. At testing time, all of the neurons are present and their firing probability is scaled since there are more neurons than previously present at training time.", "Dropout can be used in combination with other regularization techniques (such as L2, batch normalization, etc.).", "Purpose: prevent the co-adaptation of feature detectors for a set of neurons, and avoid overfitting.", "Interpretation: training examples provide gradients from different, randomly sampled architectures.", "With normal dropout at test time, you have to scale activations by dropout rate p because you are not dropping out any of the neurons, so you need to match expected value at training.", "With inverted dropout, scaling is applied at the training time, but inversely. First, dropout all activations by dropout factor p, and second, scale them by inverse dropout factor 1/p.", "Inverted dropout has an advantage, that you don\u2019t have to do anything at test time, which makes inference faster.", "Due to this faster inference, most current implementations use inverted dropout:", "Weight initialization of a neural network has already been discussed in some of my other articles, but it is worth briefly reiterating here.", "In convex problems with a small learning rate, no matter what the initialization, convergence is guaranteed (although may be slow).", "However, in the non-convex regime initialization is much more important. Some parameter initializations can be unstable and may not converge. Neural networks are not well understood to have principled, mathematically nice initialization strategies, and hence heuristics are often used to select a reasonable set of starting weights from which the network is trained.", "For a fully connected layer with m inputs and n outputs, we can sample from the following uniform distribution to obtain our initial weights:", "Alternatively, the popular Xavier initialization uses the following parameters for its uniform distribution.", "Xavier initialization is derived considering that the network consists of matrix multiplications with no nonlinearites, and seems to perform well in practice.", "For a full list of weight initialization schemes, check out the Initializers section of the Keras documentation here. There are a dozen or two initializers, and it is also possible to use your own custom initializer. Please see the reference material in the Further Reading section.", "Thank you for reading, I hope you found the article insightful. If you enjoyed this, please be on the lookout for my future articles related to neural networks.", "Other advanced topics related to fully connected neural networks that I will touch on in future articles and the reader might be interested in are:", "For updates on new blog posts and extra content, sign up for my newsletter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Postdoc @Harvard | Environmental + Data Science PhD @Harvard | ML consultant @Critical Future | Blogger @TDS | Content Creator @EdX. https://mpstewart.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff27fbcc638ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----f27fbcc638ae---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c", "anchor_text": "Simple Introduction to Neural NetworksA detailed overview of neural networks with a wealth of examples and simple imagery.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/comprehensive-introduction-to-neural-network-architecture-c08c6d8e5d98", "anchor_text": "Comprehensive Introduction to Neural Network ArchitectureA detailed overview of neural architecture, activation functions, loss functions, output units.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0", "anchor_text": "Neural Network OptimizationCovering optimizers, momentum, adaptive learning rates, batch normalization, and more.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594", "anchor_text": "Simple Guide to Hyperparameter Tuning in Neural NetworksA step-by-step Jupyter notebook walkthrough on hyperparameter optimization.towardsdatascience.com"}, {"url": "https://arxiv.org/pdf/1710.01878.pdf", "anchor_text": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"url": "https://www.inference.vc/pruning-neural-networks-two-recent-papers/", "anchor_text": "Pruning Neural Networks: Two Recent PapersI wanted to briefly highlight two recent papers on pruning neural networks (disclaimer, one of them is ours): Christos\u2026www.inference.vc"}, {"url": "https://jacobgil.github.io/deeplearning/pruning-deep-learning", "anchor_text": "Pruning deep neural networks to make them fast and smallMy PyTorch implementation of [1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]\u2026jacobgil.github.io"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "research paper"}, {"url": "https://arxiv.org/pdf/1704.00109.pdf", "anchor_text": "Source"}, {"url": "https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/", "anchor_text": "How to Calculate an Ensemble of Neural Network Model Weights in Keras (Polyak Averaging)The training process of neural networks is a challenging optimization process that can often fail to converge. This can\u2026machinelearningmastery.com"}, {"url": "https://mc.ai/what-is-orthogonalization-in-machine-learning/", "anchor_text": "Mc.ai"}, {"url": "https://keras.io/initializers/", "anchor_text": "here"}, {"url": "https://mailchi.mp/6304809e49e7/matthew-stewart", "anchor_text": "Newsletter SubscriptionEnrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain\u2026mailchi.mp"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608014002135?source=post_page---------------------------", "anchor_text": "Deep learning in neural networks: An overview"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608019300231?source=post_page---------------------------", "anchor_text": "Continual lifelong learning with neural networks: A review \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608019300784?source=post_page---------------------------", "anchor_text": "Recent advances in physical reservoir computing: A review \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303332?source=post_page---------------------------", "anchor_text": "Deep learning in spiking neural networks"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303319?source=post_page---------------------------", "anchor_text": "Ensemble Neural Networks (ENN): A gradient-free stochastic method \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/0893608089900208?source=post_page---------------------------", "anchor_text": "Multilayer feedforward networks are universal approximators"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303277?source=post_page---------------------------", "anchor_text": "A comparison of deep networks with ReLU activation function and linear spline-type methods \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608097000117?source=post_page---------------------------", "anchor_text": "Networks of spiking neurons: The third generation of neural network models"}, {"url": "https://www.sciencedirect.com/science/article/pii/089360809190009T?source=post_page---------------------------", "anchor_text": "Approximation capabilities of multilayer feedforward networks"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608098001166?source=post_page---------------------------", "anchor_text": "On the momentum term in gradient descent learning algorithms"}, {"url": "https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf", "anchor_text": "Runtime Neural Pruning"}, {"url": "https://arxiv.org/abs/1512.08571", "anchor_text": "Structured Pruning of Deep Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/1509.09308.pdf", "anchor_text": "Fast Algorithms for Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/1801.07365.pdf", "anchor_text": "Learning to Prune Filters in Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/1710.01878.pdf", "anchor_text": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"url": "https://arxiv.org/abs/1611.06440", "anchor_text": "Pruning Convolutional Neural Networks for Resource Efficient Inference"}, {"url": "https://arxiv.org/abs/1712.01312", "anchor_text": "Learning Sparse Neural Networks through L0 Regularization"}, {"url": "https://arxiv.org/abs/1801.05787", "anchor_text": "Faster gaze prediction with dense networks and Fisher pruning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f27fbcc638ae---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f27fbcc638ae---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f27fbcc638ae---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f27fbcc638ae---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f27fbcc638ae---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----f27fbcc638ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----f27fbcc638ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff27fbcc638ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f27fbcc638ae---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f27fbcc638ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://medium.com/@matthew_stewart/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.5K Followers"}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F20066c159638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-topics-in-neural-networks-f27fbcc638ae&newsletterV3=b89dbc0712c4&newsletterV3Id=20066c159638&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}