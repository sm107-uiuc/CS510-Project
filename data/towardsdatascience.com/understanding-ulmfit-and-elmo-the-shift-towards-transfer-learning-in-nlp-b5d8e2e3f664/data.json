{"url": "https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664", "time": 1683001541.898551, "path": "towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664/", "webpage": {"metadata": {"title": "Understanding ULMFiT \u2014 The Shift Towards Transfer Learning in NLP | by Akhilesh Ravi | Towards Data Science", "h1": "Understanding ULMFiT \u2014 The Shift Towards Transfer Learning in NLP", "description": "An understanding of what ULMFiT is; what it does and how it works; and the radical shift that it caused in NLP"}, "outgoing_paragraph_urls": [{"url": "https://github.com/fastai/fastai/blob/master/examples/ULMFit.ipynb", "anchor_text": "How to use ULMFiT for Text Processing", "paragraph_index": 13}, {"url": "http://bit.ly/ai-n-ml-youtube", "anchor_text": "YouTube channel", "paragraph_index": 15}], "all_paragraphs": ["Natural language processing has picked up pace over the last decade and with the with the increasing ease of implementing deep learning, there have been major developments in this field. However, it was lagging behind the level of expertise attained in the field of computer vision.", "The main reason for this was that transfer learning made many computer vision tasks much simpler than they actually were \u2014 pretrained models like VGGNet\u2078 or AlexNet\u2079 could be fine-tuned to fit most computer vision tasks. These pretrained models were trained on a huge corpus like ImageNet. They had been made to capture the general features and properties of images. Thus, with some tweaking, they could be used for most tasks. Hence, models did not have to be trained from scratch for each and every computer vision task. Moreover, since they were trained on a huge corpus, the accuracy or results for many tasks was exceptional and would outperform smaller models trained from scratch for the particular tasks.", "On the other hand, models had to be trained separately and one-by-one for each NLP task. This was time-consuming and limited the scope of these models. \u201cRecent approaches (in 2017 and 2018) that concatenate embeddings derived from other tasks with the input at different layers still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.\u201d\u00b9 There was a lack of knowledge of how to properly fine-tune language models for various NLP tasks.", "In 2018, Howard and Ruder et. al.\u00b9 provided a novel method for fine-tuning of neural models for inductive transfer learning\u00b9 \u2014 given a source task in which the model is trained, the same model is to be used to obtain good performance on other tasks (NLP tasks) as well.", "Choosing the base modelThe ideal source task was seen as language modelling and was considered as the analogous to ImageNet for NLP tasks. This is because of the following reason: \u201cIt captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations, and sentiment. In contrast to tasks like machine translation (MT) and entailment, it provides data in near-unlimited quantities for most domains and languages.\u201d\u00b9 Moreover, language modelling can be trained to adapt to the particular unique features of the target task and language modelling is a component of various other NLP tasks.Generally, a good language model (LM) like the AWD-LSTM\u2077, is chosen as the base model. It is generally expected that the better the base model, the better will be the performance of the final model on various NLP tasks after fine-tuning.", "General-domain LM pretrainingThe pre-training is to be done on a large corpus of for language which effectively catches the main properties and aspects of language. This would be something like the Image-Net corpus, but, for language. This stage has to be performed only once. The resulting pretrained model can be reused for the next stages for all tasks.The pre-training is done so model already understands the general properties of language and has to be tweaked a little to suit the specific task. In fact, it was found that pre-training was especially useful for small datasets and medium-sized datasets.", "Target task LM fine-tuningThis stage is done to make the model fit the model to the specific target task. When a pretrained model is used, then, the convergence at this stage is faster. In this stage, discriminative fine-tuning and slanted triangular learning rates are used for fine-tuning the language model.", "Discriminative Fine-Tuning\u201cAs different layers capture different types of information, they should be fine-tuned to different extents.\u201d\u00b9 Thus, for each layer, a different learning rate is used. The learning of the last layer, \u03b7\u1d38 is fixed. Then, \u03b7\u02e1\u207b\u00b9 = \u03b7\u02e1/2.6 is used to obtain the rest of the learning rates.", "Slanted triangular learning ratesThe learning rates are not kept constant throughout the fine-tuning process. Initially, for some epochs, they are increased linearly with a steep slope of increase. Then, for multiple epochs, they are decreased linearly with a gradual slope. This was found out to give good performance of the model.", "Target task classifier fine-tuning\u201cFinally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks.\u201d Each block has the following1. batch normalization2. dropout3. ReLU activation for the intermediate layer4. softmax activation for the output layer to predict the classes", "Concat PoolingIn general, in text classification, the important words are only a few words and may be a small part of the entire document, especially if the documents are large. Thus, to avoid loss of information, the hidden state vector is concatenated with the max-pooled and mean-pooled form of the hidden state vector.Gradual UnfreezingWhen all layers are fine-tuned at the same time, there is a risk of catastrophic forgetting. Thus, initially all layers except the last one are frozen and the fine-tuning is done for one epoch. One-by-one the layers are unfrozen and fine-tuning is done. This is repeated till convergence.Using Bidirectional ModelEnsembling a forward and a backward LM-classifier improves the performance further.", "Advantages of ULMFiTULMFiT-based models (which have been pre-trained) perform very well even on small and medium datasets compared to models trained from scratch on the corresponding dataset. This is because they have already captured the properties of the language during pre-training.The newly proposed methods for fine-tuning the LM and the classifier prove to give better accuracy than using the traditional methods of fine-tuning a model.", "ULMFiT revolutionized the field of NLP, improving the scope of deep learning in NLP and making it possible to train models for various tasks in much less time than before. This set the base for transfer learning for NLP and paved the way for ELMo, GPT, GPT-2, BERT and XLNet.", "How to use ULMFiT for Text Processing", "Note: I had initially written this article as an assignment in my Natural Language Processing course.", "Watch videos explaining concepts in Machine Learning, AI and Data Science on my YouTube channel.", "* Many phrases, sentences and paragraphs in this article are summaries of sentences or paragraphs form [1]. Figure 3was also taken from [1].", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "UG student at Indian Institute of Technology Gandhinagar; Pursuing a dual major in CS and EE; ML, Computer Vision and Data Science Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb5d8e2e3f664&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akhileshravi.medium.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": ""}, {"url": "https://akhileshravi.medium.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Akhilesh Ravi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa50645071fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&user=Akhilesh+Ravi&userId=a50645071fd&source=post_page-a50645071fd----b5d8e2e3f664---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hiteshchoudhary", "anchor_text": "Hitesh Choudhary"}, {"url": "https://unsplash.com/photos/t1PaIbMTJIM", "anchor_text": "Unsplash"}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/ulmfit_flow_2.png", "anchor_text": "Analytics Vidya"}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "ULMFiT Research Paper"}, {"url": "https://yashuseth.files.wordpress.com/2018/06/stlr-formula2.jpg?w=413&h=303", "anchor_text": "Wordpress"}, {"url": "https://www.hu-berlin.de/en", "anchor_text": "HU-Berlin"}, {"url": "https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_5.png", "anchor_text": "GitHub"}, {"url": "http://nlp.fast.ai/images/ulmfit_imdb.png", "anchor_text": "FastAI"}, {"url": "https://github.com/fastai/fastai/blob/master/examples/ULMFit.ipynb", "anchor_text": "How to use ULMFiT for Text Processing"}, {"url": "https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/", "anchor_text": "https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/"}, {"url": "http://bit.ly/ai-n-ml-youtube", "anchor_text": "YouTube channel"}, {"url": "https://miro.medium.com/proxy/0*2t3JCdtfsV2M5S_B.png", "anchor_text": "https://miro.medium.com/proxy/0*2t3JCdtfsV2M5S_B.png"}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/ulmfit_flow_2.png", "anchor_text": "https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/ulmfit_flow_2.png"}, {"url": "https://yashuseth.files.wordpress.com/2018/06/stlr-formula2.jpg?w=413&h=303", "anchor_text": "https://yashuseth.files.wordpress.com/2018/06/stlr-formula2.jpg?w=413&h=303"}, {"url": "https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_5.png", "anchor_text": "https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_5.png"}, {"url": "http://nlp.fast.ai/images/ulmfit_imdb.png", "anchor_text": "http://nlp.fast.ai/images/ulmfit_imdb.png"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b5d8e2e3f664---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b5d8e2e3f664---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b5d8e2e3f664---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b5d8e2e3f664---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----b5d8e2e3f664---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&user=Akhilesh+Ravi&userId=a50645071fd&source=-----b5d8e2e3f664---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&user=Akhilesh+Ravi&userId=a50645071fd&source=-----b5d8e2e3f664---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb5d8e2e3f664&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b5d8e2e3f664---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b5d8e2e3f664--------------------------------", "anchor_text": ""}, {"url": "https://akhileshravi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akhileshravi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akhilesh Ravi"}, {"url": "https://akhileshravi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "29 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa50645071fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&user=Akhilesh+Ravi&userId=a50645071fd&source=post_page-a50645071fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb17b18ca135&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664&newsletterV3=a50645071fd&newsletterV3Id=b17b18ca135&user=Akhilesh+Ravi&userId=a50645071fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}