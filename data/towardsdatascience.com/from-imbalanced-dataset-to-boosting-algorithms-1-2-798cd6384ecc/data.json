{"url": "https://towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc", "time": 1683007703.228287, "path": "towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc/", "webpage": {"metadata": {"title": "From imbalanced datasets to boosting algorithms | by Linda Chen | Towards Data Science", "h1": "From imbalanced datasets to boosting algorithms", "description": "Type one is when the cost of misclassifying is a lot higher for the minority class than the majority. In other words, the primary goal of our model is to minimize False Positive predictions. Some\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/programming-journal-4-why-do-we-have-to-talk-about-type-1-error-and-type-2-error-41b3ae68bb96", "anchor_text": "Why do we have to talk about Type-1 error and Type-2 error?", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=U3X98xZ4_no&t=384s", "anchor_text": "SMOTE (Synthetic Minority Oversampling Technique)", "paragraph_index": 5}, {"url": "https://www.kaggle.com/barun2104/telecom-churn", "anchor_text": "original dataset", "paragraph_index": 8}], "all_paragraphs": ["Type one is when the cost of misclassifying is a lot higher for the minority class than the majority. In other words, the primary goal of our model is to minimize False Positive predictions. Some examples are: detecting credit card frauds, customer churn, and detecting a rare and deadly disease. To better understand how different types of false predictions translate into actual business cost, check out my post: Why do we have to talk about Type-1 error and Type-2 error?", "Type two is when we didn\u2019t collect enough data for the minority group. Our samples were not representative enough. In other words, the imbalanced dataset should have been balanced.", "In python, we have several tools in our toolbox to fix an imbalanced dataset. However, each tool is built to solve different types of problems.", "Downsampling: randomly select some points from the majority class and delete them.", "Upsampling: randomly select a point from the minority class, copy and paste it to make a new point. Repeat the process until you have the same amount of samples as the majority class.", "SMOTE: it creates more samples in the minority class. However, not by replicating the existing data points but by creating new points within the range of possibility. In other words, it creates new data points around the existing data. See the graph below for illustration. (A great video on this: SMOTE (Synthetic Minority Oversampling Technique))", "Adjusting class weight: there are two ways to do this. One is changing the parameter class_weights to class_weight = \u2018balanced\u2019 in Decision Trees and Random Forest. The algorithm will use stratified sampling to build trees rather than the default setting: random sampling.", "The second method is only applicable to Random Forest. You can manually assign a weight to different classes. I will share the code below. By assigning a heavier weight to one class, we will use a weighted Gini. A weighted Gini will make all the variables we use to make a tree more sensitive toward the minority class. For example, when we calculate our Gini impurity, the variable that can better identify the minority class will have a lower impurity score an thus, are more likely to be chosen to build a tree.", "Background: The dataset is from a telecom company. It has 3333 samples (original dataset via Kaggle). Among these samples, 85.5% of them are from the group \u201cChurn = 0\u201d with 14.5% from the group \u201cChurn = 1\u201d. The goal is to predict customer churn. I will show the performance of 4 tree algorithms \u2014 Decision Tree, Random Forest, Gradient Boosting, XG Boosting.", "The performance with the original dataset:", "The performance of using balanced class_weight for RF and Decision Tree:", "The performance of RF after manually adjusted the weights", "When it comes to each method\u2019s pros and cons, many articles say something like the following:", "Random Downsampling: leaves out some valuable information. As we all know, the more samples we have, the better because it gets us closer to reality. But downsampling reduces the samples.", "Random Upsampling: tend to overfit the existing data points of the minority group since we keep copying the existing data points.", "This drawback actually reflects from the analysis above. Comparing the numbers of False Positive, False Positive in upsampling did not reduce as much as other methods.", "SMOTE and Weighted Class: often described as the \u201cadvanced\u201d methods to combat the shortcomings of downsampling and upsampling. It is true that SMOTE, which finds a reasonable way to supply new data is a revolutionary idea. And it is also true that a Weighted Class method keeps all the information.", "But if you were the business decision-maker, looking at the results, which method would you pick?", "To answer this question: we can put things in numerical terms. Let\u2019s pick the performance of Random Forest as an example. Assume we can save all the customers after we foresee that they will churn.", "Though the cost can vary case by case depending on the industry, I am willing to say I would have picked Downsampling. One because I lose the least amount of client which is my priority goal. Another because since those 38 customers intended to stay with us, I am assuming that it shouldn\u2019t take too much effort or cost for us to \u201csave\u201d them.", "Although, of course, it will depend on our method of \u201csaving\u201d and the scale of our sample size. If we have 10 thousand clients, the consideration might be different.", "Regardless, the key insight here is \u2014 there is no better or worse method, we need to always keep the context in mind.", "Yes. I mentioned there are two reasons why an imbalanced dataset is problematic. Type 1 was the False Positive number was more important to us than overall accuracy. Type 2 was having biased in the data collection process. The churn rate problem is the first-category problem. In other words, the primary problem here is not that my dataset has biased, but that they are the natural minority. Downsampling and Weighted Class addressed our problem the best in this case because they enlarge the voice of the minority class.", "If we were facing a type-2 problem, then SMOTE should work better than the other methods because it introduces new samples in the minority class. It tries to fill the gaps in the minority class and eases out the biased.", "Upsampling and the balanced class method work the same way and therefore, the results were very similar. They were not particularly suitable for either type. The reason is that after you kept copying data from the minority dataset, it is impossible to not have the problem of overfitting.", "Pay attention to the max_depth and the numbers of False Positive. In Downsampling, if I limit the depth, I would actually have more False Positives and with lower accuracy. But in manually tuning the class weight, the more I reduce the depth, the less False Positive numbers I will have. The shortcoming is I will sacrifice the overall accuracy a lot with a 0 False Positive numbers.", "This detail really demonstrates the difference between the two methods. While using downsampling, we are still building the trees by finding the optimal variable to split. It works just like a normal random forest. And Random Forest is very good for handling overfits. When I limit the depth, I might actually run into unfit. And it is impossible to have a 0 False Positive unless we do some regularization later.", "However, in the case of adjusting class weight, it works like regularization. That said, we need to be careful with the weights that we pick.", "By adjusting the class weight, we have just shifted from bagging algorithms to a pseudo-boosting algorithm.", "Look at the data again. Did you notice the performance of Weighted Class is very similar to the performance of boosting algorithms? Also, did you notice something else? The two boosting methods, Gradient Boosting and XG Boosting did not fluctuate as much as the other two tree methods. Their two types of errors were also more balanced to start with. Why is that?", "One of the core differences between bagging and boosting algorithms are, in a bagging algorithm, it follows the rule of 1 tree 1 vote. No tree is above another regardless of any conditions. For boosting, it is not equal. Some will have more say than others on the final decision. Different boosting algorithms have different ways to decide how much say a tree will have on the final decision. But the principle is, voices are not equal. Thus, I say a weighted Random Forest is a pseudo-boosting algorithm.", "The difference is a boosting algorithm changes the weights of the samples, while a weighted Random Forest changes the weights of the classes. If using people as an analogy. A weighted Random Forest says: everyone who voted A will be counted\u00a0more. A boosting algorithm says: whoever that\u2019s more qualified can cast more votes.", "The tricky part is: how do we justify who is more qualified than the other and by how much more power? That\u2019s the problem that boosting algorithms are trying to address.", "In the context of boosting algorithms, who are the \u201cqualified people\u201d? They are the trees that produce more right results and produce them consistently.", "If Tree X always produces more right results than Tree Y, Tree X is better at identifying the minority class than Tree Y (all the trees are good at identifying the majority class when there\u2019s an imbalanced dataset). And boosting algorithms amplify these voices. That\u2019s why boosting algorithms did better than other algorithms to start with.", "Boosting algorithms ensure performance consistency by testing different trees\u2019 accuracy repeatedly. The algorithm adjusts the power of the trees after each test until the adjustments only have minor fluctuation. This is the second major difference between boosting and bagging algorithms (the first one is assigning different weights). In bagging algorithms, trees have many final exams and the final grade is a simple adding up the numbers of the pass and fail. In boosting algorithms, you have quizzes and midterms. Each test impacts your final grade.", "So with a balanced dataset, regardless of how the dataset structures, trees that produce more right results always have the dominant voice. That\u2019s why after we implemented SMOTE and upsampling, performance for gradient boosting did not change at all.", "So second key insights: when you are not sure what\u2019s the main cause of the imbalanced, use a boosting algorithm. You will be less wrong.", "Ok, so here is a glimpse into boosting algorithms. I am writing an article to explain boosting algorithms in more detail.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Share what I learned, and learn from what I shared. All about machines, humans, and the links between them. Take everything with a grain of salt."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F798cd6384ecc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://linnndachen.medium.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Linda Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0d085971d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&user=Linda+Chen&userId=b0d085971d50&source=post_page-b0d085971d50----798cd6384ecc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@enginakyurt?utm_source=medium&utm_medium=referral", "anchor_text": "engin akyurt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/programming-journal-4-why-do-we-have-to-talk-about-type-1-error-and-type-2-error-41b3ae68bb96", "anchor_text": "Why do we have to talk about Type-1 error and Type-2 error?"}, {"url": "https://www.youtube.com/watch?v=U3X98xZ4_no&t=384s", "anchor_text": "SMOTE (Synthetic Minority Oversampling Technique)"}, {"url": "https://www.kaggle.com/taliac/customer-churn-using-tree-models", "anchor_text": "My Fully Churn Analysis"}, {"url": "https://www.kaggle.com/barun2104/telecom-churn", "anchor_text": "original dataset"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees", "anchor_text": "Full analysis"}, {"url": "https://media.giphy.com/media/TKpMzTvUcqlT8HGAyG/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://unsplash.com/@sahandbabali?utm_source=medium&utm_medium=referral", "anchor_text": "Sahand Babali"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://media.giphy.com/media/CDJo4EgHwbaPS/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://media.giphy.com/media/fnDY3C9MKukcER71r7/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://cling.csd.uwo.ca/papers/cost_sensitive.pdf", "anchor_text": "Cost-Sensitive Learning and the Class Imbalance Problem"}, {"url": "https://www.kaggle.com/taliac/different-resampling-methods-for-trees/data?scriptVersionId=34283208#4.-Models-With-Resamplings", "anchor_text": "Different resampling methods for trees"}, {"url": "https://www.kaggle.com/taliac/customer-churn-using-tree-models", "anchor_text": "Customer churn using tree models"}, {"url": "https://medium.com/tag/imbalanced-data?source=post_page-----798cd6384ecc---------------imbalanced_data-----------------", "anchor_text": "Imbalanced Data"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----798cd6384ecc---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----798cd6384ecc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----798cd6384ecc---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/tag/churn?source=post_page-----798cd6384ecc---------------churn-----------------", "anchor_text": "Churn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&user=Linda+Chen&userId=b0d085971d50&source=-----798cd6384ecc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&user=Linda+Chen&userId=b0d085971d50&source=-----798cd6384ecc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F798cd6384ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----798cd6384ecc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----798cd6384ecc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----798cd6384ecc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----798cd6384ecc--------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Linda Chen"}, {"url": "https://linnndachen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "477 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0d085971d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&user=Linda+Chen&userId=b0d085971d50&source=post_page-b0d085971d50--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe27496a5a656&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc&newsletterV3=b0d085971d50&newsletterV3Id=e27496a5a656&user=Linda+Chen&userId=b0d085971d50&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}