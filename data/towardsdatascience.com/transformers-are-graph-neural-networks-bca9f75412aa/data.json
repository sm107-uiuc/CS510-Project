{"url": "https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa", "time": 1683004612.114233, "path": "towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa/", "webpage": {"metadata": {"title": "Transformers are Graph Neural Networks | by Chaitanya K. Joshi | Towards Data Science", "h1": "Transformers are Graph Neural Networks", "description": "Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications? Besides the obvious\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48", "anchor_text": "Pinterest", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1902.08730", "anchor_text": "Alibaba", "paragraph_index": 1}, {"url": "https://blog.twitter.com/en_us/topics/company/2019/Twitter-acquires-Fabula-AI.html", "anchor_text": "Twitter", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer architecture", "paragraph_index": 1}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "taken", "paragraph_index": 1}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "the", "paragraph_index": 1}, {"url": "https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/", "anchor_text": "NLP", "paragraph_index": 1}, {"url": "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/", "anchor_text": "industry", "paragraph_index": 1}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "by", "paragraph_index": 1}, {"url": "https://nv-adlr.github.io/MegatronLM", "anchor_text": "storm", "paragraph_index": 1}, {"url": "https://graphdeeplearning.github.io/project/spatial-convnets/", "anchor_text": "Graph Neural Networks (GNNs)", "paragraph_index": 2}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "RNNs", "paragraph_index": 6}, {"url": "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/", "anchor_text": "representation learning", "paragraph_index": 6}, {"url": "https://distill.pub/2016/augmented-rnns/", "anchor_text": "attention", "paragraph_index": 7}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "mechanism", "paragraph_index": 7}, {"url": "https://www.youtube.com/watch?v=iDulhoQ2pro", "anchor_text": "video overview", "paragraph_index": 9}, {"url": "https://nealjean.com/ml/neural-network-normalization/", "anchor_text": "normalization layer", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "LayerNorm", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "scale", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/2001.08361", "anchor_text": "up", "paragraph_index": 22}, {"url": "https://petar-v.com/GAT/", "anchor_text": "attention mechanism", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Syntactic_Structures", "anchor_text": "linguistic structure", "paragraph_index": 34}, {"url": "https://arxiv.org/abs/1503.00075", "anchor_text": "Tree LSTMs", "paragraph_index": 34}, {"url": "https://openai.com/blog/sparse-transformer/", "anchor_text": "sparse", "paragraph_index": 36}, {"url": "https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/", "anchor_text": "adaptive", "paragraph_index": 36}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "recurrence", "paragraph_index": 36}, {"url": "https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory", "anchor_text": "compression", "paragraph_index": 36}, {"url": "https://www.pragmatic.ml/reformer-deep-dive/", "anchor_text": "Locality Sensitive Hashing", "paragraph_index": 36}, {"url": "https://arxiv.org/abs/1911.04070", "anchor_text": "Binary Partitioning", "paragraph_index": 37}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "several", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1905.05950", "anchor_text": "interesting", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1906.04341", "anchor_text": "papers", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1909.07913", "anchor_text": "not so convinced", "paragraph_index": 39}, {"url": "https://lena-voita.github.io/posts/acl19_heads.html", "anchor_text": "these", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1905.10650", "anchor_text": "papers", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "MoNet", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "recent", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1901.10430", "anchor_text": "work", "paragraph_index": 43}, {"url": "http://jbcordonnier.com/posts/attention-cnn/", "anchor_text": "something", "paragraph_index": 43}, {"url": "https://twitter.com/ChrSzegedy/status/1232148457810538496", "anchor_text": "similar", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1906.01787", "anchor_text": "recent", "paragraph_index": 45}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "results", "paragraph_index": 45}, {"url": "https://arxiv.org/abs/2002.04745", "anchor_text": "suggest", "paragraph_index": 45}, {"url": "https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "massive carbon footprints", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "inductive biases", "paragraph_index": 48}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 49}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer", "paragraph_index": 49}, {"url": "https://ipam.wistia.com/medias/1zgl4lq6nh", "anchor_text": "an excellent talk", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "star-studded position paper", "paragraph_index": 50}, {"url": "https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html", "anchor_text": "a nice tutorial", "paragraph_index": 50}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "\ud83e\udd17 HuggingFace", "paragraph_index": 51}, {"url": "https://graphdeeplearning.github.io/publication/xu-2019-multi/", "anchor_text": "a recent paper", "paragraph_index": 52}, {"url": "https://thegradient.pub/transformers-are-graph-neural-networks/", "anchor_text": "The Gradient", "paragraph_index": 53}, {"url": "https://graphdeeplearning.github.io/post/transformers-are-gnns/", "anchor_text": "NTU Graph Deep Learning lab website", "paragraph_index": 54}, {"url": "https://mp.weixin.qq.com/s/DABEcNf1hHahlZFMttiT2g", "anchor_text": "Chinese", "paragraph_index": 54}, {"url": "https://habr.com/ru/post/491576/", "anchor_text": "Russian", "paragraph_index": 54}, {"url": "https://twitter.com/chaitjo/status/1233220586358181888?s=20", "anchor_text": "Twitter", "paragraph_index": 54}, {"url": "https://www.reddit.com/r/MachineLearning/comments/fb86mo/d_transformers_are_graph_neural_networks_blog/", "anchor_text": "Reddit", "paragraph_index": 54}, {"url": "https://news.ycombinator.com/item?id=22518263", "anchor_text": "HackerNews", "paragraph_index": 54}], "all_paragraphs": ["Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?", "Besides the obvious ones\u2013recommendation systems at Pinterest, Alibaba and Twitter\u2013a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm.", "Through this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I\u2019ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress.", "Let\u2019s start by talking about the purpose of model architectures\u2013representation learning.", "At a high level, all neural network architectures build representations of input data as vectors/embeddings, which encode useful statistical and semantic information about the data. These latent or hidden representations can then be used for performing something useful, such as classifying an image or translating a sentence. The neural network learns to build better-and-better representations by receiving feedback, usually via error/loss functions.", "For Natural Language Processing (NLP), conventionally, Recurrent Neural Networks (RNNs) build representations of each word in a sentence in a sequential manner, i.e., one word at a time. Intuitively, we can imagine an RNN layer as a conveyor belt, with the words being processed on it autoregressively from left to right. At the end, we get a hidden feature for each word in the sentence, which we pass to the next RNN layer or use for our NLP tasks of choice.", "I highly recommend Chris Olah\u2019s legendary blog for recaps on RNNs and representation learning for NLP.", "Initially introduced for machine translation, Transformers have gradually replaced RNNs in mainstream NLP. The architecture takes a fresh approach to representation learning: Doing away with recurrence entirely, Transformers build features of each word using an attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. Knowing this, the word\u2019s updated features are simply the sum of linear transformations of the features of all the words, weighted by their importance.", "Back in 2017, this idea sounded very radical, because the NLP community was so used to the sequential\u2013one-word-at-a-time\u2013style of processing text with RNNs. The title of the paper probably added fuel to the fire!", "For a recap, Yannic Kilcher made an excellent video overview.", "Let\u2019s develop intuitions about the architecture by translating the previous paragraph into the language of mathematical symbols and vectors. We update the hidden feature h of the i\u2019th word in a sentence S from layer \u2113 to layer \u2113+1 as follows:", "where j\u2208 S denotes the set of words in the sentence and Q, K, V are learnable linear weights (denoting the Query, Key and Value for the attention computation, respectively). The attention mechanism is performed parallelly for each word in the sentence to obtain their updated features in one shot\u2013another plus point for Transformers over RNNs, which update features word-by-word.", "We can understand the attention mechanism better through the following pipeline:", "Getting this dot-product attention mechanism to work proves to be tricky\u2013bad random initializations can de-stabilize the learning process. We can overcome this by parallelly performing multiple \u2018heads\u2019 of attention and concatenating the result (with each head now having separate learnable weights):", "where Q^k,K^k, V^k are the learnable weights of the k\u2019th attention head and O is a down-projection to match the dimensions of h_i^{\u2113+1} and h_i^\u2113 across layers.", "Multiple heads allow the attention mechanism to essentially \u2018hedge its bets\u2019, looking at different transformations or aspects of the hidden features from the previous layer. We\u2019ll talk more about this later.", "A key issue motivating the final Transformer architecture is that the features for words after the attention mechanism might be at different scales or magnitudes: (1) This can be due to some words having very sharp or very distributed attention weights w_{ij} when summing over the features of the other words. (2) At the individual feature/vector entries level, concatenating across multiple attention heads\u2013each of which might output values at different scales\u2013can lead to the entries of the final vector h_i^{\u2113+1} having a wide range of values. Following conventional ML wisdom, it seems reasonable to add a normalization layer into the pipeline.", "Transformers overcome issue (2) with LayerNorm, which normalizes and learns an affine transformation at the feature level. Additionally, scaling the dot-product attention by the square-root of the feature dimension helps counteract issue (1).", "Finally, the authors propose another \u2018trick\u2019 to control the scale issue: a position-wise 2-layer MLP with a special structure. After the multi-head attention, they project h_i^{\u2113+1} to a (absurdly) higher dimension by a learnable weight, where it undergoes the ReLU non-linearity, and is then projected back to its original dimension followed by another normalization:", "To be honest, I\u2019m not sure what the exact intuition behind the over-parameterized feed-forward sub-layer was and nobody seems to be asking questions about it, too! I suppose LayerNorm and scaled dot-products didn\u2019t completely solve the issues highlighted, so the big MLP is a sort of hack to re-scale the feature vectors independently of each other.", "Email me if you know more!", "The final picture of a Transformer layer looks like this:", "The Transformer architecture is also extremely amenable to very deep networks, enabling the NLP community to scale up in terms of both model parameters and, by extension, data. Residual connections between the inputs and outputs of each multi-head attention sub-layer and the feed-forward sub-layer are key for stacking Transformer layers (but omitted from the diagram for clarity).", "Let\u2019s take a step away from NLP for a moment.", "Graph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) build representations of nodes and edges in graph data. They do so through neighbourhood aggregation (or message passing), where each node gathers features from its neighbours to update its representation of the local graph structure around it. Stacking several GNN layers enables the model to propagate each node\u2019s features over the entire graph\u2013from its neighbours to the neighbours\u2019 neighbours, and so on.", "In their most basic form, GNNs update the hidden features h of node i (for example, \ud83d\ude06) at layer \u2113 via a non-linear transformation of the node\u2019s own features h_i^\u2113 added to the aggregation of features h_j^\u2113 from each neighbouring node j\u2208 N(i):", "where U, V are learnable weight matrices of the GNN layer and \u03c3 is a non-linearity such as ReLU. In the example, N(\ud83d\ude06) = { \ud83d\ude18, \ud83d\ude0e, \ud83d\ude1c, \ud83e\udd29 }.", "The summation over the neighbourhood nodes j\u2208 N(i) can be replaced by other input size-invariant aggregation functions such as simple mean/max or something more powerful, such as a weighted sum via an attention mechanism.", "Maybe a pipeline will help make the connection:", "If we were to do multiple parallel heads of neighbourhood aggregation and replace summation over the neighbours j with the attention mechanism, i.e., a weighted sum, we\u2019d get the Graph Attention Network (GAT). Add normalization and the feed-forward MLP, and voila, we have a Graph Transformer!", "To make the connection more explicit, consider a sentence as a fully-connected graph, where each word is connected to every other word. Now, we can use a GNN to build features for each node (word) in the graph (sentence), which we can then perform NLP tasks with.", "Broadly, this is what Transformers are doing: they are GNNs with multi-head attention as the neighbourhood aggregation function. Whereas standard GNNs aggregate features from their local neighbourhood nodes j\u2208 N(i), Transformers for NLP treat the entire sentence S as the local neighbourhood, aggregating features from each word j\u2208 S at each layer.", "Importantly, various problem-specific tricks\u2013such as position encodings, causal/masked aggregation, learning rate schedules and extensive pre-training\u2013are essential for the success of Transformers but seldom seem in the GNN community. At the same time, looking at Transformers from a GNN perspective could inspire us to get rid of a lot of the bells and whistles in the architecture.", "Now that we\u2019ve established a connection between Transformers and GNNs, let me throw some ideas around\u2026", "Before statistical NLP and ML, linguists like Noam Chomsky focused on developing fomal theories of linguistic structure, such as syntax trees/graphs. Tree LSTMs already tried this, but maybe Transformers/GNNs are better architectures for bringing the world of linguistic theory and statistical NLP closer?", "Another issue with fully-connected graphs is that they make learning very long-term dependencies between words difficult. This is simply due to how the number of edges in the graph scales quadratically with the number of nodes, i.e., in an n word sentence, a Transformer/GNN would be doing computations over n^2 pairs of words. Things get out of hand for very large n.", "The NLP community\u2019s perspective on the long sequences and dependencies problem is interesting: Making the attention mechanism sparse or adaptive in terms of input size, adding recurrence or compression into each layer, and using Locality Sensitive Hashing for efficient attention are all promising new ideas for better Transformers.", "It would be interesting to see ideas from the GNN community thrown into the mix, e.g., Binary Partitioning for sentence graph sparsification seems like another exciting approach.", "There have been several interesting papers from the NLP community on what Transformers might be learning. The basic premise is that performing attention on all word pairs in a sentence\u2013with the purpose of identifying which pairs are the most interesting\u2013enables Transformers to learn something like a task-specific syntax. Different heads in the multi-head attention might also be \u2018looking\u2019 at different syntactic properties.", "In graph terms, by using GNNs on full graphs, can we recover the most important edges\u2013and what they might entail\u2013from how the GNN performs neighbourhood aggregation at each layer? I\u2019m not so convinced by this view yet.", "I\u2019m more sympathetic to the optimization view of the multi-head mechanism\u2013having multiple attention heads improves learning and overcomes bad random initializations. For instance, these papers showed that Transformer heads can be \u2018pruned\u2019 or removed after training without significant performance impact.", "Multi-head neighbourhood aggregation mechanisms have also proven effective in GNNs, e.g., GAT uses the same multi-head attention and MoNet uses multiple Gaussian kernels for aggregating features. Although invented to stabilize attention mechanisms, could the multi-head trick become standard for squeezing out extra model performance?", "Conversely, GNNs with simpler aggregation functions such as sum or max do not require multiple aggregation heads for stable training. Wouldn\u2019t it be nice for Transformers if we didn\u2019t have to compute pair-wise compatibilities between each word pair in the sentence?", "Could Transformers benefit from ditching attention, altogether? Yann Dauphin and collaborators\u2019 recent work suggests an alternative ConvNet architecture. Transformers, too, might ultimately be doing something similar to ConvNets!", "Reading new Transformer papers makes me feel that training these models requires something akin to black magic when determining the best learning rate schedule, warmup strategy and decay settings. This could simply be because the models are so huge and the NLP tasks studied are so challenging.", "But recent results suggest that it could also be due to the specific permutation of normalization and residual connections within the architecture.", "At this point I\u2019m ranting, but this makes me skeptical: Do we really need multiple heads of expensive pair-wise attention, overparameterized MLP sub-layers, and complicated learning schedules?", "Do we really need massive models with massive carbon footprints?", "Shouldn\u2019t architectures with good inductive biases for the task at hand be easier to train?", "To dive deep into the Transformer architecture from an NLP perspective, check out these amazing blog posts: The Illustrated Transformer and The Annotated Transformer.", "Also, this blog isn\u2019t the first to link GNNs and Transformers: Here\u2019s an excellent talk by Arthur Szlam on the history and connection between Attention/Memory Networks, GNNs and Transformers. Similarly, DeepMind\u2019s star-studded position paper introduces the Graph Networks framework, unifying all these ideas. For a code walkthrough, the DGL team has a nice tutorial on seq2seq as a graph problem and building Transformers as GNNs.", "In our next post, we\u2019ll be doing the reverse: using GNN architectures as Transformers for NLP (based on the Transformers library by \ud83e\udd17 HuggingFace).", "Finally, we wrote a recent paper applying Transformers to sketch graphs. Do check it out!", "Transformers are GNNs was published with The Gradient in September 2020!", "The post originally appeared on the NTU Graph Deep Learning lab website, and has also been translated to Chinese and Russian. Do join the discussion on Twitter, Reddit or HackerNews!", "For attribution in academic contexts or books, please cite this work as", "Chaitanya K. Joshi, \u201cTransformers are Graph Neural Networks\u201d, The Gradient, 2020.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbca9f75412aa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chaitjo?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitjo?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Chaitanya K. Joshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7768e811155d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&user=Chaitanya+K.+Joshi&userId=7768e811155d&source=post_page-7768e811155d----bca9f75412aa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com/s/photos/transformers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48", "anchor_text": "Pinterest"}, {"url": "https://arxiv.org/abs/1902.08730", "anchor_text": "Alibaba"}, {"url": "https://blog.twitter.com/en_us/topics/company/2019/Twitter-acquires-Fabula-AI.html", "anchor_text": "Twitter"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer architecture"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "taken"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "the"}, {"url": "https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/", "anchor_text": "NLP"}, {"url": "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/", "anchor_text": "industry"}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "by"}, {"url": "https://nv-adlr.github.io/MegatronLM", "anchor_text": "storm"}, {"url": "https://graphdeeplearning.github.io/project/spatial-convnets/", "anchor_text": "Graph Neural Networks (GNNs)"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "RNNs"}, {"url": "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/", "anchor_text": "representation learning"}, {"url": "https://distill.pub/2016/augmented-rnns/", "anchor_text": "attention"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "mechanism"}, {"url": "https://www.youtube.com/watch?v=iDulhoQ2pro", "anchor_text": "video overview"}, {"url": "https://nealjean.com/ml/neural-network-normalization/", "anchor_text": "normalization layer"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "LayerNorm"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "scale"}, {"url": "https://arxiv.org/abs/2001.08361", "anchor_text": "up"}, {"url": "https://petar-v.com/GAT/", "anchor_text": "attention mechanism"}, {"url": "https://en.wikipedia.org/wiki/Syntactic_Structures", "anchor_text": "linguistic structure"}, {"url": "https://arxiv.org/abs/1503.00075", "anchor_text": "Tree LSTMs"}, {"url": "https://en.wikipedia.org/wiki/Syntactic_Structures#/media/File:Cgisf-tgg.svg", "anchor_text": "Wikipedia"}, {"url": "https://openai.com/blog/sparse-transformer/", "anchor_text": "sparse"}, {"url": "https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/", "anchor_text": "adaptive"}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "recurrence"}, {"url": "https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory", "anchor_text": "compression"}, {"url": "https://www.pragmatic.ml/reformer-deep-dive/", "anchor_text": "Locality Sensitive Hashing"}, {"url": "https://arxiv.org/abs/1911.04070", "anchor_text": "Binary Partitioning"}, {"url": "https://arxiv.org/abs/1911.04070", "anchor_text": "Ye et al., 2019"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "several"}, {"url": "https://arxiv.org/abs/1905.05950", "anchor_text": "interesting"}, {"url": "https://arxiv.org/abs/1906.04341", "anchor_text": "papers"}, {"url": "https://arxiv.org/abs/1909.07913", "anchor_text": "not so convinced"}, {"url": "https://arxiv.org/abs/1906.04341", "anchor_text": "Clark et al., 2019"}, {"url": "https://lena-voita.github.io/posts/acl19_heads.html", "anchor_text": "these"}, {"url": "https://arxiv.org/abs/1905.10650", "anchor_text": "papers"}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "MoNet"}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "recent"}, {"url": "https://arxiv.org/abs/1901.10430", "anchor_text": "work"}, {"url": "http://jbcordonnier.com/posts/attention-cnn/", "anchor_text": "something"}, {"url": "https://twitter.com/ChrSzegedy/status/1232148457810538496", "anchor_text": "similar"}, {"url": "https://arxiv.org/pdf/1901.10430.pdf", "anchor_text": "Wu et al., 2019"}, {"url": "https://arxiv.org/abs/1906.01787", "anchor_text": "recent"}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "results"}, {"url": "https://arxiv.org/abs/2002.04745", "anchor_text": "suggest"}, {"url": "https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "massive carbon footprints"}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "inductive biases"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer"}, {"url": "https://ipam.wistia.com/medias/1zgl4lq6nh", "anchor_text": "an excellent talk"}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "star-studded position paper"}, {"url": "https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html", "anchor_text": "a nice tutorial"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "\ud83e\udd17 HuggingFace"}, {"url": "https://graphdeeplearning.github.io/publication/xu-2019-multi/", "anchor_text": "a recent paper"}, {"url": "https://thegradient.pub/transformers-are-graph-neural-networks/", "anchor_text": "The Gradient"}, {"url": "https://graphdeeplearning.github.io/post/transformers-are-gnns/", "anchor_text": "NTU Graph Deep Learning lab website"}, {"url": "https://mp.weixin.qq.com/s/DABEcNf1hHahlZFMttiT2g", "anchor_text": "Chinese"}, {"url": "https://habr.com/ru/post/491576/", "anchor_text": "Russian"}, {"url": "https://twitter.com/chaitjo/status/1233220586358181888?s=20", "anchor_text": "Twitter"}, {"url": "https://www.reddit.com/r/MachineLearning/comments/fb86mo/d_transformers_are_graph_neural_networks_blog/", "anchor_text": "Reddit"}, {"url": "https://news.ycombinator.com/item?id=22518263", "anchor_text": "HackerNews"}, {"url": "https://thegradient.pub/transformers-are-gaph-neural-networks/", "anchor_text": "https://thegradient.pub/transformers-are-gaph-neural-networks/"}, {"url": "https://medium.com/tag/transformers?source=post_page-----bca9f75412aa---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bca9f75412aa---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/graph-neural-networks?source=post_page-----bca9f75412aa---------------graph_neural_networks-----------------", "anchor_text": "Graph Neural Networks"}, {"url": "https://medium.com/tag/nlp?source=post_page-----bca9f75412aa---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----bca9f75412aa---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&user=Chaitanya+K.+Joshi&userId=7768e811155d&source=-----bca9f75412aa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&user=Chaitanya+K.+Joshi&userId=7768e811155d&source=-----bca9f75412aa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbca9f75412aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bca9f75412aa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bca9f75412aa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bca9f75412aa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bca9f75412aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitjo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitjo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chaitanya K. Joshi"}, {"url": "https://medium.com/@chaitjo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "212 Followers"}, {"url": "https://www.chaitjo.com/", "anchor_text": "https://www.chaitjo.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7768e811155d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&user=Chaitanya+K.+Joshi&userId=7768e811155d&source=post_page-7768e811155d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F62cae1753758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-are-graph-neural-networks-bca9f75412aa&newsletterV3=7768e811155d&newsletterV3Id=62cae1753758&user=Chaitanya+K.+Joshi&userId=7768e811155d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}