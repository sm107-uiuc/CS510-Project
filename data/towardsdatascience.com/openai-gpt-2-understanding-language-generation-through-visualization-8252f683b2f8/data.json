{"url": "https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8", "time": 1682995285.080384, "path": "towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8/", "webpage": {"metadata": {"title": "GPT-2: Understanding Language Generation through Visualization | by Jesse Vig | Towards Data Science", "h1": "GPT-2: Understanding Language Generation through Visualization", "description": "In the eyes of most NLP researchers, 2018 was a year of great technological advancement, with new pre-trained NLP models shattering records on tasks ranging from sentiment analysis to question\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP", "paragraph_index": 0}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "pre-trained", "paragraph_index": 0}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 2}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT", "paragraph_index": 2}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "GLUE leaderboard", "paragraph_index": 2}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "Illustrated BERT", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "another paper", "paragraph_index": 5}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "GPT", "paragraph_index": 5}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer", "paragraph_index": 5}, {"url": "https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/", "anchor_text": "language modeling", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "fill-in-the-blanks", "paragraph_index": 6}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "write stories about talking unicorns", "paragraph_index": 8}, {"url": "https://www.nytimes.com/interactive/2019/06/07/technology/ai-text-disinformation.html", "anchor_text": "generate fake news", "paragraph_index": 10}, {"url": "https://blog.openai.com/better-language-models/#sample8", "anchor_text": "anti-recycling manifestos", "paragraph_index": 10}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "GitHub", "paragraph_index": 14}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer", "paragraph_index": 22}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "attention", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/N-gram", "anchor_text": "n-gram", "paragraph_index": 27}, {"url": "https://gpt2.apps.allenai.org/?text=Who%20is", "anchor_text": "generate text", "paragraph_index": 38}, {"url": "http://www.fatml.org/", "anchor_text": "here", "paragraph_index": 47}, {"url": "https://thispersondoesnotexist.com/", "anchor_text": "images", "paragraph_index": 48}, {"url": "https://youtu.be/ayPqjPekn7g", "anchor_text": "videos", "paragraph_index": 48}, {"url": "https://deepmind.com/blog/wavenet-generative-model-raw-audio/", "anchor_text": "audio", "paragraph_index": 48}, {"url": "http://thispersondoesnotexist.com", "anchor_text": "thispersondoesnotexist.com", "paragraph_index": 49}, {"url": "https://www.theverge.com/tldr/2018/4/17/17247334/ai-fake-news-video-barack-obama-jordan-peele-buzzfeed", "anchor_text": "Obama as AI puppet", "paragraph_index": 50}, {"url": "https://www.youtube.com/watch?v=r1jng79a5xc", "anchor_text": "Steve Buscemi-Jennifer Lawrence chimera", "paragraph_index": 50}, {"url": "https://en.wikipedia.org/wiki/Deepfake", "anchor_text": "deepfakes", "paragraph_index": 50}, {"url": "https://www.americaninno.com/boston/funding-boston/cambridge-voice-skins-startup-modulate-raises-2m-in-seed-funding/", "anchor_text": "voice skin", "paragraph_index": 50}, {"url": "https://twitter.com/jesse_vig", "anchor_text": "Twitter", "paragraph_index": 52}, {"url": "https://arxiv.org/abs/1904.09223", "anchor_text": "Enhanced Representation through kNowledge IntEgration", "paragraph_index": 53}, {"url": "https://arxiv.org/abs/1905.07129", "anchor_text": "Enhanced Language Representation through INformative Entities", "paragraph_index": 53}, {"url": "https://grover.allenai.org/", "anchor_text": "GROVER", "paragraph_index": 53}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "GitHub repo", "paragraph_index": 54}, {"url": "https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77", "anchor_text": "Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters", "paragraph_index": 55}, {"url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "anchor_text": "Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention", "paragraph_index": 56}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "A.I. Plays Mad Libs and the Results are Terrifying", "paragraph_index": 57}], "all_paragraphs": ["In the eyes of most NLP researchers, 2018 was a year of great technological advancement, with new pre-trained NLP models shattering records on tasks ranging from sentiment analysis to question answering.", "But for others, 2018 was the year that NLP ruined Sesame Street forever.", "First came ELMo (Embeddings from Language Models) and then BERT (Bidirectional Encoder Representations from Transformers), and now BigBird sits atop the GLUE leaderboard. My own thinking has been so corrupted by this naming convention that when I hear \u201cI\u2019ve been playing with Bert,\u201d for example, the image that pops into my head is not of the fuzzy unibrowed conehead from my childhood, but instead is something like this:", "I can\u2019t unsee that one, Illustrated BERT!", "I ask you \u2014 if Sesame Street isn\u2019t safe from NLP model branding, what is?", "But there was one model that left my childhood memories intact, an algorithm that remained nameless and faceless, referred to by its authors from OpenAI simply as \u201cthe language model\u201d or \u201cour method.\u201d Only when authors of another paper needed to compare their model to this nameless creation was it deemed worthy of a moniker. And it wasn\u2019t ERNIE or GroVeR or CookieMonster; the name described exactly what the algorithm was, and no more: GPT, the Generative Pre-trained Transformer.", "But in the same breath that it was given its name, GPT was unceremoniously knocked off the GLUE leaderboard by BERT. One reason for GPT\u2019s downfall was that it was pre-trained using traditional language modeling, i.e., predicting the next word in a sentence. In contrast, BERT was pre-trained using masked language modeling, which is more of a fill-in-the-blanks exercise: guessing missing (\u201cmasked\u201d) words given the words that came before and after. This bidirectional architecture enabled BERT to learn richer representations and ultimately perform better across NLP benchmarks.", "So in late 2018, it seemed that OpenAI\u2019s GPT would be forever known to history as that generically-named, quaintly-unidirectional predecessor to BERT.", "But 2019 has told a different story. It turns out that the unidirectional architecture that led to GPT\u2019s downfall in 2018 gave it powers to do something that BERT never could (or at least wasn\u2019t designed for): write stories about talking unicorns:", "You see, left-to-right language modeling is more than just a pre-training exercise; it also enables a very practical task: language generation. If you can predict the next word in a sentence, you can predict the word after that, and the next one after that, and pretty soon you have\u2026 a lot of words. And if your language modeling is good enough, these words will form meaningful sentences, and the sentences will form coherent paragraphs, and these paragraphs will form, well, just about anything you want.", "And on February 14, 2019, the OpenAI\u2019s language model did get good enough \u2014 good enough to write stories of talking unicorns, generate fake news, and write anti-recycling manifestos. It was even given a new name: GPT-2.", "So what was the secret to GPT-2's human-like writing abilities? There were no fundamental algorithmic breakthroughs; this was a feat of scaling up. GPT-2 has a whopping 1.5 billion parameters (10X more than the original GPT) and is trained on the text from 8 million websites.", "How does one make sense of a model with 1.5 billion parameters? Let\u2019s see if visualization can help.", "OpenAI did not release the full GPT-2 model due to concerns of malicious use, but they did release a smaller version equivalent in size to the original GPT (117 M parameters), trained on the new, larger dataset. Although not as powerful as the large model, the smaller version still has some language generation chops. Let\u2019s see if visualization can help us better understand this model.", "Note: Code for creating these visualizations can be found on GitHub.", "Let\u2019s see how the GPT-2 small model finishes this sentence:", "The dog on the ship ran", "The dog on the ship ran off, and the dog was found by the crew.", "Seems pretty reasonable, right? Now let\u2019s tweak the example slightly by changing dog to motor and see what the model generates:", "The motor on the ship ran", "The motor on the ship ran at a speed of about 100 miles per hour.", "By changing that one word at the beginning of the sentence, we\u2019ve got a completely different outcome. The model seems to understand that the type of running a dog does is completely different from the type that a motor does.", "How does GPT-2 know to pay such close attention to dog vs motor, especially since these words occur earlier in the sentence? Well, the GPT-2 is based on the Transformer, which is an attention model \u2014 it learns to focus attention on the previous words that are the most relevant to the task at hand: predicting the next word in the sentence.", "Let\u2019s see where GPT-2 focuses attention for \u201cThe dog on the ship ran\u201d:", "The lines, read left-to-right, show where the model pays attention when guessing the next word in the sentence (color intensity represents the attention strength). So, when guessing the next word after ran, the model pays close attention to dog in this case. This makes sense, because knowing who or what is doing the running is crucial to guessing what comes next.", "In linguistics terminology, the model is focusing on the head of the noun phrase the dog on the ship. There are many other linguistic properties that GPT-2 captures as well, because the above attention pattern is just one of the 144 attention patterns in the model. GPT-2 has 12 layers, each with 12 independent attention mechanisms, called \u201cheads\u201d; the result is 12 x 12 = 144 distinct attention patterns. Here we visualize all of them, highlighting the one we just looked at:", "We can see that these patterns take many different forms. Here\u2019s another interesting one:", "This layer/head focuses all attention on the previous word in the sentence. This makes sense, because adjacent words are often the most relevant for predicting the next word. Traditional n-gram language models are based on this same intuition.", "But why do so many attention patterns look like this?", "In this pattern, virtually all attention is focused on the first word in the sentence, and other words are ignored. This appears to be the null pattern, indicating that the attention head hasn\u2019t found whatever linguistic phenomenon it is looking for. The model seems to have repurposed the first word as the place to look when it has nothing better to focus on.", "Well, if we\u2019re going to let NLP taint our memories of Sesame Street, then I guess Dr. Seuss is fair game as well. Let\u2019s see how GPT-2 finishes these lines from the timeless classic, Cat in the Hat:", "On the string of one kite we saw Mother\u2019s new gown! Her gown with the dots that are pink, white and\u2026", "Here\u2019s how GPT-2 completed the last sentence:", "Her gown with the dots that are pink, white and blue.", "Not too bad! The original text has red, so at least we know it\u2019s not just memorizing.", "So how did GPT-2 know to choose a color? Perhaps due to the following attention pattern that seems to recognize comma-separated lists:", "To decide the word after and, the model focuses attention on the preceding items in the list \u2014 pink and white. It knew to pick a word that matched the type of the previous items, in this case a color.", "GPT-2 seems particularly adept at writing short biographies based solely on a name.", "As an experiment, try having GPT-2 generate text from the prompt \u201cWho is <your name>?\u201d This particular prompt generally triggers the model to write a short biography, likely because it is a common preface for author bio\u2019s in articles on the Web.", "Here are a couple of bios generated for the prompt \u201cWho is Jesse Vig?\u201d:", "\u201cJesse Vig is a social media marketing expert and former social media marketing manager. He is the co-founder and CEO of VigMedia.com and recently the founder and CEO of VigMedia.com.\u201d", "Not bad! A little bit repetitive, but does a nice job of personalizing the story in an unexpected way. Here\u2019s another:", "\u201cJesse Vig is the son of an evangelical preacher named James Vig. He moved to the United States in 1964 and became a preacher at the University of Michigan, where he taught for 18 years until his death in October 2007.\u201d", "In this last example, GPT-2 was smart enough to know that the father of my alter ego had the same last name. Let\u2019s see where GPT-2 focused its attention when picking this last name:", "When deciding the word to predict after James, this pattern focuses attention on previous mentions of my last name. (Note that, internal to the model, Vig has been broken into word pieces \u201cV\u201d and \u201cig\u201d because it is an uncommon word.) It seems that this attention pattern specializes in identifying relationships between familial names. To test this, let\u2019s change the text slightly:", "\u201cJesse Vig is the colleague of an evangelical preacher named James\u2026\u201d", "Now that James is just a colleague, this attention pattern ignores my last name almost entirely.", "Note: GPT-2 seems to generate biographies based on the perceived ethnicity and sex associated with a name. Further study is needed to see what biases the model may encode; you can read more about this topic here.", "In just the last year, the ability to generate content of all kinds\u2014 images, videos, audio and text \u2014 has improved to the point where we can no longer trust our own senses and judgment about what is real or fake.", "And this is just the beginning; these technologies will continue to advance and become more integrated with one another. Soon, when we look at the generated faces on thispersondoesnotexist.com, they will meet our gaze, and chat with us about their generated lives, revealing the quirks of their generated personalities.", "The most immediate danger is perhaps the mixing of the real and the generated. We\u2019ve seen the videos of Obama as AI puppet and the Steve Buscemi-Jennifer Lawrence chimera. Soon, these deepfakes will become personal. So when your mom calls and says she needs $500 wired to the Cayman Islands, ask yourself: Is this really my mom, or is it a language-generating AI that acquired a voice skin of my mother from that Facebook video she posted 5 years ago?", "But for now, let\u2019s just enjoy the stories about talking unicorns.", "For updates on this and related projects, feel free to follow me on Twitter.", "[UPDATE]: ERNIE is now officially taken (twice!) \u2014 see Enhanced Representation through kNowledge IntEgration and Enhanced Language Representation through INformative Entities \u2014 as is GROVER.", "GitHub repo for visualization tool with Jupyter and Colab notebooks, built using these awesome tools/frameworks:", "Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters", "Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention", "A.I. Plays Mad Libs and the Results are Terrifying", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8252f683b2f8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JesseVig?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Jesse Vig"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7b984a1f8d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&user=Jesse+Vig&userId=c7b984a1f8d1&source=post_page-c7b984a1f8d1----8252f683b2f8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP"}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "pre-trained"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT"}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "GLUE leaderboard"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "Illustrated BERT"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "another paper"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "GPT"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer"}, {"url": "https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/", "anchor_text": "language modeling"}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "fill-in-the-blanks"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "write stories about talking unicorns"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "https://blog.openai.com/better-language-models/"}, {"url": "https://www.nytimes.com/interactive/2019/06/07/technology/ai-text-disinformation.html", "anchor_text": "generate fake news"}, {"url": "https://blog.openai.com/better-language-models/#sample8", "anchor_text": "anti-recycling manifestos"}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "GitHub"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "attention"}, {"url": "https://en.wikipedia.org/wiki/N-gram", "anchor_text": "n-gram"}, {"url": "https://gpt2.apps.allenai.org/?text=Who%20is", "anchor_text": "generate text"}, {"url": "http://www.fatml.org/", "anchor_text": "here"}, {"url": "https://thispersondoesnotexist.com/", "anchor_text": "images"}, {"url": "https://youtu.be/ayPqjPekn7g", "anchor_text": "videos"}, {"url": "https://deepmind.com/blog/wavenet-generative-model-raw-audio/", "anchor_text": "audio"}, {"url": "http://thispersondoesnotexist.com", "anchor_text": "thispersondoesnotexist.com"}, {"url": "https://www.theverge.com/tldr/2018/4/17/17247334/ai-fake-news-video-barack-obama-jordan-peele-buzzfeed", "anchor_text": "Obama as AI puppet"}, {"url": "https://www.youtube.com/watch?v=r1jng79a5xc", "anchor_text": "Steve Buscemi-Jennifer Lawrence chimera"}, {"url": "https://en.wikipedia.org/wiki/Deepfake", "anchor_text": "deepfakes"}, {"url": "https://www.americaninno.com/boston/funding-boston/cambridge-voice-skins-startup-modulate-raises-2m-in-seed-funding/", "anchor_text": "voice skin"}, {"url": "https://twitter.com/jesse_vig", "anchor_text": "Twitter"}, {"url": "https://arxiv.org/abs/1904.09223", "anchor_text": "Enhanced Representation through kNowledge IntEgration"}, {"url": "https://arxiv.org/abs/1905.07129", "anchor_text": "Enhanced Language Representation through INformative Entities"}, {"url": "https://grover.allenai.org/", "anchor_text": "GROVER"}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "GitHub repo"}, {"url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization", "anchor_text": "Tensor2Tensor visualization tool"}, {"url": "https://medium.com/u/a3ad3e75263b?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Llion Jones"}, {"url": "https://medium.com/huggingface", "anchor_text": "HuggingFace"}, {"url": "https://medium.com/syncedreview/hugging-face-releases-pytorch-bert-pretrained-models-and-more-b8a7839e7730", "anchor_text": "Pytorch implementation"}, {"url": "https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77", "anchor_text": "Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters"}, {"url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "anchor_text": "Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention"}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "A.I. Plays Mad Libs and the Results are Terrifying"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Illustrated Transformer"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8252f683b2f8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8252f683b2f8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8252f683b2f8---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8252f683b2f8---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----8252f683b2f8---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&user=Jesse+Vig&userId=c7b984a1f8d1&source=-----8252f683b2f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&user=Jesse+Vig&userId=c7b984a1f8d1&source=-----8252f683b2f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8252f683b2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8252f683b2f8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8252f683b2f8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8252f683b2f8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8252f683b2f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jesse Vig"}, {"url": "https://medium.com/@JesseVig/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "987 Followers"}, {"url": "http://jessevig.com", "anchor_text": "jessevig.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7b984a1f8d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&user=Jesse+Vig&userId=c7b984a1f8d1&source=post_page-c7b984a1f8d1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdd746ee1255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8&newsletterV3=c7b984a1f8d1&newsletterV3Id=cdd746ee1255&user=Jesse+Vig&userId=c7b984a1f8d1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}