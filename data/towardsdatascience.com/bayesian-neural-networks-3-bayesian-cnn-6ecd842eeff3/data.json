{"url": "https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3", "time": 1683014763.561839, "path": "towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3/", "webpage": {"metadata": {"title": "Bayesian Neural Networks: 3 Bayesian CNN | by Adam Woolf | Towards Data Science", "h1": "Bayesian Neural Networks: 3 Bayesian CNN", "description": "This is the third chapter in the series on Bayesian Deep Learning. The previous article is available here. We already know that neural networks are arrogant. But another failing of standard neural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://github.com/DoctorLoop/BayesianDeepLearning", "anchor_text": "https://github.com/DoctorLoop/BayesianDeepLearning", "paragraph_index": 6}], "all_paragraphs": ["This is the third chapter in the series on Bayesian Deep Learning. The previous article is available here.", "We already know that neural networks are arrogant. But another failing of standard neural nets is a susceptibility to being tricked. Imagine a CNN tasked with a morally questionable task like face recognition. It gets its input from a security camera. But what happens when the security camera sends a picture of a dog, a mannequin or a child\u2019s doll? The CNN will still output classifications having been tricked by something with a resemblance to human face. CNNs cry out for the Bayesian treatment, because we don\u2019t want our work undermined by silly mistakes and because where the consequences of misclassification are high we want to know how sure the network is. How likely is a doll, or any real person, being a wanted criminal? We need to know.", "As we\u2019ve discovered in earlier articles, Bayesian analysis deals in distributions and not single values. We\u2019ve seen it with normal distributions where we\u2019re getting a continuous floating point back with the most likely return value of the mean. The distribution for a categorical becomes the discreet (piano key rather than violin string). For probabilities we\u2019ll get a specific result such as a class, an index (or a note if we squeeze the musical analogy). The distribution is informed by the logits from our model.", "Being able to perform very well with considerably less data, while throwing in an ability to generalise better makes Bayesian neural networks desirable. And that\u2019s without considering the other advantages. There is however one disadvantage to a Bayesian implementations that become important in this chapter. Bayesian implementations need more parameters. Considering that a whole distribution replaces every weight value parameter it\u2019s surprising that only twice as many parameters are required. It\u2019s simple to see where the quite specific \u2018twice as many\u2019 figure comes from as the weight distributions are mostly normal distributions and those normal distributions each have two parameters of their own.", "In the deviant manor that comes naturally to the author, a strange new dataset has been created to highlight the problem. We find ourselves helping parents solve an important problem. Parents are always interested in the measuring the height of their children. Well, no longer do they need fret about carrying a ruler around with them all day. We\u2019ll create a model that estimates height from a picture. A dataset has been created consisting of 836 silhouettes of babies and toddlers in addition to their heights. Rather than classification problem we\u2019re therefore solving a regression problem. We aim to return a single floating-point value corresponding to the height of the child silhouetted in the photograph. It\u2019s a slightly harder problem than the classification exercise from the last chapter and made even more so by the occasional presence of a spider. While the training set consists only of valid human silhouettes, after training we\u2019ll throw in spiders just for fun. And this is where it gets fun, because we want to avoid returning height measurements for insects but we aren\u2019t allowed any pictures of insects in training.", "Of course spiders are usually smaller things than children. So to prevent simple discrimination based on a ridiculousl difference in size, spiders were upscaled to occupy a space comparable to the children. Furthermore, in the spirit of making the task quite arbitrarily difficult, the children have been randomly rescaled so silhouettes are anywhere between 1/2 and 2x their original. Of course, rescaling doesn\u2019t make much sense when we\u2019re interested in the height of the children! But the rather elaborately contrived senseless problem perfectly demonstrates the power of Bayesian Deep Learning. You\u2019ll see how well the models generalise to real world situations with few training examples and without any examples of the corrupted data they likely to receive!", "Lets get stuck into the problem with TensorFlow probability. The full code as well as the data is available in a Jupyter Notebook online at: https://github.com/DoctorLoop/BayesianDeepLearning. First we\u2019ll define the architecture.", "In the first line we clear any session that might already be in memory, emptying any parameter stores and variables so there\u2019s no interference with a fresh run. Next we define a lambda function that helps us update the loss via the Kullback-Leibler (KL) divergence that we discussed in the previous chapter. We then pass this lambda to each convolutional layer so the loss can be updated with reference to the divergence between an approximate distribution and our prior. Strictly speaking this isn\u2019t absolutely necessary to specify as the default parameter for the layer is almost same. The difference however is that while the default parameter just gets the KL divergence, we go one step further and divide it by the total number of examples (836). The default implementation applies the epoch\u2019s total KL to every example. But what we\u2019d prefer is to apply only a proportion of the total epoch\u2019s KL to each example rather than the total each time. While both will train we see better results through scaling the loss. Experiment and see for yourself.", "The actual model is defined just as it is for any other keras sequential. Of course we\u2019re using a Convolutional2DFlipout layer (we\u2019ll discuss that later) rather than the usual Conv2D. You might be surprised we\u2019re only using two convolutional layers in a time when its near enough a fashion to use hundreds. We\u2019re using two simply because the results are impressive and for this problem we really don\u2019t really need more. We\u2019ve also thrown in two maxpool layers between neurone layers and both have quite large strides/pool sizes. If you\u2019ve a problem that requires particularly sensitive pixel perfect measurement you might want to try removing these. Of course, the cost of doing so will be in terms of escalating hardware demands so it\u2019s recommended to compare both.", "The very last layer is a single dense (Bayesian) neurone because we\u2019re interested in just one output. This output will be our measurement. It\u2019s as simple as that.", "Finally we compile the model with mean squared error loss (MSE). This is deceptive as although we only specify MSE we\u2019re also adding the KL on each layer. However we defined the KL ourselves, because we\u2019re independent Bayesianists who wanted to give Keras a well-deserved rest. We\u2019ll see proof that KL is involved when we print the loss during training. It\u2019s noticeably different (greater) than the MSE alone. The difference between the two is our KL.", "Lets start the training and take a look at that loss:", "There\u2019s are few things to note here. The loss is relatively high while the batch size is relatively low!", "To address the loss first we\u2019ll repeatedly find that with Bayesian model the loss value is an even worse indicator of model performance than it is for conventional models. The first reason is because we\u2019re combining at least two losses. Of course we\u2019re interested in the change in loss rather than the explicit value, but even then change isn\u2019t always clear as we often change the relative influence of the two losses progressively over the training epochs. We\u2019ll discuss these considerations in later chapters. Just remember for now that it isn\u2019t unusual to see a classification model with a loss of several thousand(!) while having perfect validation metrics.", "While some people may scoff at my puny batch size and assume resources are scarce \u2014 they couldn\u2019t be more wrong. With Bayesian model the batch size has a much greater influence on training than we\u2019d expect. It\u2019s an example of a number of areas of neural network theory we often think we understand but that\u2019ll demand a review of our beliefs. We usually think of batch size as of predominant importance to training speed. Some people also appreciate the reduced variance a larger batch brings. However with Bayesian models batch size directly influences training performance. Have a look and see by running the same model repeatedly with a batch size of 5 and with 50. You\u2019ll notice that when the batch size is 50 epochs are of course much quicker but we never get loss or performance metrics as good as we do with a batch size of 5. It\u2019s not a small difference \u2014 it\u2019s enormous! This is important because it\u2019ll quickly become clear that batch size is a very influential hyperparameter to Bayesian deep learning success.", "It\u2019ll quickly become clear that batch size is a very influential hyperparameter to Bayesian deep learning success", "While at first it seems frustrating that we\u2019ve another hyperparameter to optimize, we\u2019ll find ourselves being able to rocket the performance with a very simple change of architectures that are simpler than we\u2019ve relied upon in the past (in the appendix at the bottom of this article we\u2019ll discuss the layers like Flipout that drive the changes).", "Finally we get to inference. We\u2019re interested in making multiple predictions from our Bayesian master model. Each output be slightly different because each prediction will be made with a fresh model that\u2019s been filled with weights sampled from the weight distributions of the Bayesian master we trained.", "In the above code we use a list comprehension style for loop to make each prediction. Wouldn\u2019t it be quicker if we just provided a single input array (1000 x 126 x 126 x 1) and make all the predictions at once? Indeed it would be much quicker. But at the same time it would defeat the purpose because it\u2019s the separate model.predict calls that sample fresh weights from the distributions of our Bayesian training. Each predict call therefore is responsible for creating a unique new model that\u2019s constrained by the distributions we created in training. If we made just 1 predict call with an input of 1000 images all the predictions would be identical because we\u2019d be working with a single sample of weights, and thereby emulating a standard model. We\u2019re more interested in the ability to exploit the infinite bag of models our Bayesian training creates. We call the bag a model ensemble, and we take advantage of the ensemble of multiple different models to get many different perspectives on the same data. The agreement of the many perspectives is most important, it tells us the quality of the data we input.", "In the above code and figure we produce a density plot of 1000 height predictions of a single baby image (green) and a single spider image (red). We can see that predictions for the baby\u2019s height are very tightly packed together around 51 pixels (its mean and expected value). While around 30% of predictions are at exactly this measurement (the true value coincidently) and nearly all predictions are within a single pixel of the truth! On the bother hand, while predictions for the spider also centre on a value (90 pixels) fewer than 4% of predictions are at the expected value and the predictions are far more widely dispersed (spread out) over a range going from 51pixels all the way to 134pixels. Clearly the predictions on the spider don\u2019t agree with each other. We can intuit therefore that our Bayesian model is uncertain about predictions on invalid objects while our Bayesian model is confident about predictions related to objects from training. This is exactly how we want it to be.", "In the next article we\u2019ll explore how we can make simple Bayesian models better than complex standard models. We\u2019ll also find out how other types of uncertainty can be exploited to guide training and how to optimise and compare models to find the very best.", "If you\u2019ve read the documentation or any papers recently you may have found different ways to tackle Bayesian deep learning. TensorFlow Probability implements two approaches for convolutional layers (more are available for dense layers). The first is the reparameterization layer:", "tfp.layers.Convolution2DReparameterization. Reparameterization lets us calculate gradients via a distribution\u2019s most likely value. We therefore manipulate the parameters that describe the distributions instead of the weight values in the neural network. Dealing with distribution parameters means the actual distribution can be ignored and is effectively abstracted away. The parameters describe the distribution can be thought of as stand-ins for the distribution object in the same as paper money stands in for real assets like gold. In both cases a stand in is preferred because it\u2019s more convenient. In training we conveniently avoid the embarrassment of attempting backpropagation through random variables (embarrassing because it doesn\u2019t work\u00b9).", "Reparameterization is fast but sadly it suffers from a practical need to set all the weights of examples in a batch to the same value. If weights were individually recorded instead of shared the memory requirements would skyrocket. Sharing weights is efficient but increases variance to make training require more epochs.", "The flipout layer: tfp.layers.Convolution2DFlipout takes a different approach. While it\u2019s similar it benefits from a special estimator for loss gradients. This flipout estimator shakes up the weights in a mini-batch to make them more independent of each other. In turn the shake-up reduces variance and requires fewer training epochs than the reparameterization method. But there\u2019s a catch. While flipout needs fewer epochs it actually requires twice as many calculations! Luckily these calculations can be parallelised but we\u2019ll still tend to find a model taking 25\u201350% longer per epoch (dependant on hardware) even though training requires fewer epochs in total.", "\u00b9 Without a reparametrized distribution we break the assumption that taking a large sample gives us a better estimate. While many of us don\u2019t think of training in these terms we\u2019re depending on the assumption all the time. So with reparameterization we describe the change in the most likely value instead of the most likely change in a sample which we can\u2019t predict as the variable isn\u2019t random if we can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Specialist, Research Associate and PhD Candidate University of Cambridge"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6ecd842eeff3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adam.woolf?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adam.woolf?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Adam Woolf"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9e2de06d9f71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&user=Adam+Woolf&userId=9e2de06d9f71&source=post_page-9e2de06d9f71----6ecd842eeff3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/adam-bayesian-nn", "anchor_text": "Bayesian Neural Net"}, {"url": "https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4", "anchor_text": "here"}, {"url": "https://github.com/DoctorLoop/BayesianDeepLearning", "anchor_text": "https://github.com/DoctorLoop/BayesianDeepLearning"}, {"url": "https://gist.github.com/DoctorLoop/293ae5cc3bda2ccc333d9b216eacc301", "anchor_text": "https://gist.github.com/DoctorLoop/293ae5cc3bda2ccc333d9b216eacc301"}, {"url": "https://gist.github.com/DoctorLoop/4b10c410a709e0dfd71ace8b004255bc", "anchor_text": "https://gist.github.com/DoctorLoop/4b10c410a709e0dfd71ace8b004255bc"}, {"url": "https://gist.github.com/DoctorLoop/09552736976a7e0a32e3f27d28a4ee1c", "anchor_text": "https://gist.github.com/DoctorLoop/09552736976a7e0a32e3f27d28a4ee1c"}, {"url": "https://gist.github.com/DoctorLoop/41abe385934fb0f7728ba048564e26d4", "anchor_text": "https://gist.github.com/DoctorLoop/41abe385934fb0f7728ba048564e26d4"}, {"url": "https://medium.com/tag/bayesian-neural-network?source=post_page-----6ecd842eeff3---------------bayesian_neural_network-----------------", "anchor_text": "Bayesian Neural Network"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6ecd842eeff3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6ecd842eeff3---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----6ecd842eeff3---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/adam-bayesian-nn?source=post_page-----6ecd842eeff3---------------adam_bayesian_nn-----------------", "anchor_text": "Adam Bayesian Nn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&user=Adam+Woolf&userId=9e2de06d9f71&source=-----6ecd842eeff3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&user=Adam+Woolf&userId=9e2de06d9f71&source=-----6ecd842eeff3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6ecd842eeff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6ecd842eeff3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6ecd842eeff3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adam.woolf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adam.woolf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adam Woolf"}, {"url": "https://medium.com/@adam.woolf/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "162 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9e2de06d9f71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&user=Adam+Woolf&userId=9e2de06d9f71&source=post_page-9e2de06d9f71--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba88274d2fd4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3&newsletterV3=9e2de06d9f71&newsletterV3Id=ba88274d2fd4&user=Adam+Woolf&userId=9e2de06d9f71&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}