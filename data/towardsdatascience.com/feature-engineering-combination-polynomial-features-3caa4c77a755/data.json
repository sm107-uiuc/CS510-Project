{"url": "https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755", "time": 1683006609.148146, "path": "towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755/", "webpage": {"metadata": {"title": "Feature Engineering. Improving a Linear Regression through\u2026 | by Andrew Cole | Towards Data Science", "h1": "Feature Engineering", "description": "Last week, I published a blog which walked through all steps of the linear regression modeling process. In this post, we will manipulate the data slightly in order to decrease our model result\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/walking-through-a-linear-regression-dca9942111e4", "anchor_text": "blog", "paragraph_index": 0}, {"url": "https://github.com/andrewcole33/carseat_sales_linreg", "anchor_text": "notebook", "paragraph_index": 0}], "all_paragraphs": ["Last week, I published a blog which walked through all steps of the linear regression modeling process. In this post, we will manipulate the data slightly in order to decrease our model result metrics. We will then walk through the most critical step in any linear regression: Feature Engineering. All code can be found in this notebook. This blog begins halfway through the notebook at the \u201cFeature Engineering\u201d heading.", "Feature Engineering is the process of taking certain variables (features) from our dataset and transforming them in a predictive model. Essentially, we will be trying to manipulate single variables and combinations of variables in order to engineer new features. By creating these new features, we are increasing the likelihood that one of the new variables has more predictive power over our outcome variable than the original, un-transformed variables.", "To recap, we set out with the goal of trying to predict carseat sales given generated data with several features. We pre-processed our data to account for nominal and categorical data, scaled and normalized our data, removed outliers, built our model, and verified our linear regression assumptions. Our features looked as such:", "The approach that we took allowed for us to piece together a well-performing model that returned an r-squared metric of 0.88 with a 0.96 root mean squared error. Both of these metrics indicate a very strong model, but we must keep in mind that this data was generated solely for the purpose of the exercise, and is not real world data. It would be nice, and save a ton of headaches, but very rarely will real-world data result in such a strong result after the first iteration through the model.", "Before we get into our feature engineering, let\u2019s first take a look at our previous regression data. Because we had such a strong performing regression, for the sake of this blog\u2019s purpose, we want to remove the strongest impacting variable on our target variable. Covariance measures how two variables will vary in respect to each other (scale = [0, 1]). If the covariance between variable X and variable Y is high, that means that the two variables have a strong impact on each other. The code block below will show us how to see the covariance among all variables in our model training set:", "This simple method will output the above DataFrame, which is extremely informative but also a lot to digest. We read the variables on the y-axis and find it\u2019s corresponding covariance scores underneath each of the x-axis variables. What we are trying to identify is which variable has the highest impact on the other variables, so that we might remove them from this model. To reiterate, we are only identifying and removing significant variables to reduce our model performance for the sake of feature engineering. In a real-world model, this would be absolutely ridiculous to do without justification from domain knowledge.", "We can make the covariance output a bit more digestible by visualizing it in a seaborn heatmap.", "The heatmap just shows our covariance degrees via color intensity. The lighter the color, the higher the covariance of the two variables (per the scale on the right hand side of the map). We see that \u2018CompPrice\u2019 has a strong impact on itself and on \u2018Price\u2019. This inherently makes sense that the change in a competitor\u2019s price would influence the change in the original business changing its price to reflect. Again, this is where domain knowledge is so important. Because we have two very similar predicting variables in \u2018CompPrice\u2019 and \u2018Price\u2019, and \u2018CompPrice\u2019 is a variable not native to the business selling the car seats, we will remove \u2018CompPrice\u2019 to worsen our model.", "We see that simply removing one strong predictor decreases our R-Squared value to a significantly worse value of 0.73. In the real-world, this would actually be a somewhat promising score after an initial iteration of the model. However, we would not be satisfied with it, and would then move to feature engineering to try and improve our model.", "Now that we have dropped \u2018CompPrice\u2019 from our DataFrame, it looks like this (data is already scaled and normalized):", "During feature engineering, we want to try to create a wide variety of interactions between multiple variables in order to create new variables. For example, multiplying price by income, or advertising by urban-specific populations. By manipulating them together, we create opportunities to have new and impactful features which could potentially impact our target variable, thus engineering our features. For this argument, we will create as many bivariate combinations of our predicting variables using the \u2018combinations\u2019 method from itertools library.", "We only need to manipulate the column titles to generate all the possible combinations, for now. Our result is a list of 45 possible bivariate combinations of our features.", "While we have our combinations, it would be incredibly tedious and time consuming to test individually every single combination in a regression. Instead, we will add each combination to a dictionary, and then index the respective dictionary items as arguments in an iterative linear regression:", "In the above code block we set the X_train to a new copy variable, establish a new feature, \u2018int\u2019, by multiplying the two features together, and then use that new DataFrame containing the new combination feature as the only arguments for our new LinearRegression (so excluding the original features which are already in our DataFrame).", "We now actually have all of our new regressions completed, thanks to the iteration, but we can\u2019t see them because they are all stored in the \u2018interaction_dict\u2019. Let\u2019s sort the dictionary to return the five best performing (r-squared score) combinations.", "At this point, domain knowledge kicks it into an even higher gear. The variables returned here should start to move some cranks and gears in your brain as to why certain variables are more impactful. Get creative with it, we are engineering after all. For the sake of this blog, let\u2019s say that our domain knowledge already assumes any categorical variable to be irrelevant because location doesn\u2019t matter. We will only be looking at \u2018Advertising x Education\u2019 and \u2018Price x Age\u2019. Let\u2019s run a new regression including both these combinations. First, we add the features to our original DataFrame that contained the univariate data.", "Now we run another regression with our new features in the DataFrame.", "We can see that our linear regression score increased from .73 to .74. This is obviously a minimal increase, and one that is not super significant, but nonetheless we can see how creating new bivariate feature terms can play a significant role in improving our model.", "Again, there are no clear-cut guidelines to test all possible variable manipulations. We have just seen how to make two variables interact together, but what is another way we can engineer new features? Polynomials! A very strong (usually) option for new features is increasing the power of a single variable. For our purposes, we will try and see if all the existing variables, including our bivariate combinations, can improve our regression by being increased in power.", "To do so, we will use the \u2018PolynomialFeatures\u2019 object from the sklearn.preprocessing library. We create a new, empty dictionary for storage of our new feature possibilities (same as bivariates). We will then iterate through our X_train_int feature set and create a new feature for each respective feature to be squared thru fifth-power. We will then fit a linear regression with each of the new individual features and choose the best performing feature for use in our final regression.", "Once again, we see a tiny yet still positive increase in our R-squared metric. There are numerous methods for engineering new possibilities and improving your regression, but these are a great starting point for any new data scientist!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Using Data Science to drive curiosity."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3caa4c77a755&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andrewcole.817?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrewcole.817?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Andrew Cole"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb2c8eb9bc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&user=Andrew+Cole&userId=cb2c8eb9bc8&source=post_page-cb2c8eb9bc8----3caa4c77a755---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/walking-through-a-linear-regression-dca9942111e4", "anchor_text": "blog"}, {"url": "https://github.com/andrewcole33/carseat_sales_linreg", "anchor_text": "notebook"}, {"url": "https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Winkler"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3caa4c77a755---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3caa4c77a755---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----3caa4c77a755---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/business-analytics?source=post_page-----3caa4c77a755---------------business_analytics-----------------", "anchor_text": "Business Analytics"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----3caa4c77a755---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&user=Andrew+Cole&userId=cb2c8eb9bc8&source=-----3caa4c77a755---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&user=Andrew+Cole&userId=cb2c8eb9bc8&source=-----3caa4c77a755---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3caa4c77a755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3caa4c77a755---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3caa4c77a755--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3caa4c77a755--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3caa4c77a755--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrewcole.817?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrewcole.817?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrew Cole"}, {"url": "https://medium.com/@andrewcole.817/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "233 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb2c8eb9bc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&user=Andrew+Cole&userId=cb2c8eb9bc8&source=post_page-cb2c8eb9bc8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F39e7de80b84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-engineering-combination-polynomial-features-3caa4c77a755&newsletterV3=cb2c8eb9bc8&newsletterV3Id=39e7de80b84b&user=Andrew+Cole&userId=cb2c8eb9bc8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}