{"url": "https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850", "time": 1683004346.0600169, "path": "towardsdatascience.com/my-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850/", "webpage": {"metadata": {"title": "Temporal-Difference Learning. Optimizing value functions by\u2026 | by Reuben Kavalov | Towards Data Science", "h1": "Temporal-Difference Learning", "description": "Welcome to the next exciting chapter of my reinforcement learning studies, in which we\u2019ll cover temporal-difference learning. As always I\u2019ll link to the resources that taught and guided me at the\u2026"}, "outgoing_paragraph_urls": [{"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto", "paragraph_index": 14}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver", "paragraph_index": 15}], "all_paragraphs": ["Welcome to the next exciting chapter of my reinforcement learning studies, in which we\u2019ll cover temporal-difference learning. As always I\u2019ll link to the resources that taught and guided me at the bottom of the post.", "In my last two posts, we talked about dynamic programming (DP) and Monte Carlo (MC) methods. Temporal-difference (TD) learning is a kind of combination of the two ideas in several ways. Like MC, TD learns directly from experiencing episodes without needing a model of the environment. Like DP, TD learning can happen from incomplete episodes, utilizing a method called bootstrapping to estimate the remaining return for the episode. Basically put, it is making a guess of the value function, taking some steps, and then making another guess, updating our original guess towards this new one.", "where G\u209c is the return and \u03b1 is a constant step-size parameter. We look at the error term between our estimated value and the return, and update our value function in the direction of that error. The simplest version of TD learning is TD(0) or one-step TD, which updates the value function towards the estimated return after one step. That estimate, like in the Bellman equation, consists of two parts: the immediate reward plus the discounted value of the next step. Now we can substitute this estimated return for our real return G\u209c, to come up with an estimated, biased algorithm.", "Let\u2019s see how TD methods might prove advantageous over MC methods with an example. The idea of the example is making predictions on how long it will take to get home from work. Here is the sequence of states, times, and predictions.", "How can we update our value function based on this trajectory of experience? The following visualizations make the difference between MC and TD methods clear.", "On the left, we see the changes recommended by MC methods. Since we update each prediction based on the actual outcome, we have to wait until we get to the end and see that the total time took 43 minutes, and then go back to update each step towards that time. With TD learning, as seen on the right in the above diagrams, at every step, after taking a step, we can immediately update the previous step.", "A major advantage of TD learning is that it learns even without knowing the final outcome. After all, it is entirely possible for an agent to receive incomplete sequences, or even work in continuing environments that don\u2019t actually have a final outcome.", "By substituting TD in for MC in our control loop, we get one of the best known algorithms in reinforcement learning. The idea is called Sarsa. We start with our Q-values, and move our Q-value slightly towards our TD target, which is the reward plus our discounted Q-value of the next state minus the Q-value of where we started.", "Sarsa uses every element of the quintuple of events that describes the transition from one state-action pair to the next. Starting to get an inkling of where it got its name from?", "Let\u2019s take a look at the pseudocode of the Sarsa Algorithm for On-Policy TD Control:", "To bring this to life, we\u2019ll take a look at an example that may seem familiar, Windy Gridworld. Windy Gridworld is the same as the gridworld we came to know and love in previous posts, but with a twist: there is a crosswind running up the middle of the grid!", "The strength of the wind varies column by column, represented by the numbers underneath each column. The strength indicates the numbers of cells shifted upwards after taking an action. For example, if in the cell directly to the right of the goal, taking the action of moving left will put you in the space directly above the goal.", "An important thing to note is the fact that Monte Carlo methods may not work here, as a policy may be found that causes the agent to stay in the same space, effectively making the next episode never-ending. Using Sarsa allows us to learn within the episode that such a policy is poor, and will try something else.", "Next time, I will be diving into Q-learning for off-policy TD control, where we will be approximating the optimal state-value function independently of the policy followed, similarly to how is done in off-policy MC methods. Thanks very much for reading, I look forward to learning more in this fascinating sub-realm of machine learning.", "Reinforcement Learning: An Introduction by Sutton and Barto", "RL Course by David Silver on YouTube", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist and machine learning engineer with a passion for connecting people through technology and information."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd0cae79e850&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0cae79e850--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----d0cae79e850--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451----d0cae79e850---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/a-journey-into-r-l", "anchor_text": "A Journey Into Reinforcement Learning"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto"}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "Reinforcement Learning Github"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d0cae79e850---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d0cae79e850---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d0cae79e850---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/temporal-difference?source=post_page-----d0cae79e850---------------temporal_difference-----------------", "anchor_text": "Temporal Difference"}, {"url": "https://medium.com/tag/a-journey-into-r-l?source=post_page-----d0cae79e850---------------a_journey_into_r_l-----------------", "anchor_text": "A Journey Into R L"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&user=Reuben+Kavalov&userId=11db47030451&source=-----d0cae79e850---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&user=Reuben+Kavalov&userId=11db47030451&source=-----d0cae79e850---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd0cae79e850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d0cae79e850---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d0cae79e850--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d0cae79e850--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d0cae79e850--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d0cae79e850--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d0cae79e850--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/@reubena.kavalov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "134 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce29c814d50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-5-temporal-difference-learning-d0cae79e850&newsletterV3=11db47030451&newsletterV3Id=ce29c814d50b&user=Reuben+Kavalov&userId=11db47030451&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}