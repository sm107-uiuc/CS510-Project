{"url": "https://towardsdatascience.com/marios-gym-routine-6f095889b207", "time": 1683018157.340916, "path": "towardsdatascience.com/marios-gym-routine-6f095889b207/", "webpage": {"metadata": {"title": "Mario\u2019s Gym Routine. A practical guide to writing a RL\u2026 | by Robert Clark | Towards Data Science", "h1": "Mario\u2019s Gym Routine", "description": "A bit over a year ago, I saw a video of a Python program which was able to train a machine to play and win a game of Atari\u2019s Pong through a process known as Reinforcement Learning (RL). Since viewing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "rllib", "paragraph_index": 1}, {"url": "https://virtualenv.pypa.io/en/stable/", "anchor_text": "virtualenv", "paragraph_index": 2}, {"url": "https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html", "anchor_text": "conda", "paragraph_index": 2}, {"url": "https://github.com/roclark/super-mario-bros-rllib", "anchor_text": "repository I created", "paragraph_index": 4}, {"url": "https://github.com/uvipen/Super-mario-bros-A3C-pytorch/blob/master/src/env.py", "anchor_text": "Super-mario-bros-A3C-pytorch", "paragraph_index": 12}, {"url": "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf", "anchor_text": "DQN paper by Minh et. al", "paragraph_index": 14}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala.yaml", "anchor_text": "official Ray repository", "paragraph_index": 20}, {"url": "https://github.com/roclark/super-mario-bros-rllib/tree/main", "anchor_text": "repository", "paragraph_index": 29}], "all_paragraphs": ["A bit over a year ago, I saw a video of a Python program which was able to train a machine to play and win a game of Atari\u2019s Pong through a process known as Reinforcement Learning (RL). Since viewing that video and my mind being subsequently blown by the potential of this application of AI, I have gone on a journey to learn how to leverage RL to beat various retro video games, and even documented my early stages in a previous post.", "Throughout the past year, I\u2019ve written various projects from scratch which aim to beat levels in the original Super Mario Bros. on the NES, but wanted to find a better way to switch between models and parameters without writing several hundred additional lines in each scenario. After experimenting with many RL libraries available in Python, I found rllib from the Ray Project to be the most effective and flexible, while also being able to scale up and out on machines \u2014 a requirement for my desires to take on larger and more complex environments. This article is intended to provide an example of how to build your own application which can be used to train an agent to beat Super Mario Bros. levels.", "Before I jump into the code, a development environment needs to be created to install dependencies and get your machine ready to learn. For installing dependencies in an isolated location for my project, I use either a virtualenv or conda environment to ensure I won\u2019t be installing anything that could effect other apps on my computer. While I won\u2019t go over setup for either in this post, you can read the official docs for either virtualenv or conda.", "Inside an active environment of your choice, several dependencies need to be installed for the program to function. These can be installed with Python\u2019s PIP:", "A more concrete list of requirements can be found on the repository I created for this project, but note that while the repository and dependencies may change with time, the dependencies will always stay compatible with the latest upstream code.", "While this code might work on other Python versions (such as Python 3.6), I have only tested it on 3.7 and newer versions, which is highly recommended.", "Whenever I look at examples for a new application, framework, method, or library, I always prefer to look at the entire chunk of sample code before diving into specifics. This allows me to see where the application begins, what needs to be initialized, any arguments that are parsed, which libraries are imported, and the overall structure of the code. For me personally, it\u2019s much more valuable to have a high-level understanding of the code before diving into specific lines without any context, and that\u2019s exactly what I\u2019m going to do here.", "The following example is a completely self-contained Python program which can train Mario to beat a specified level of the game. For those that like to get their hands dirty and just run with this code and don\u2019t need explanations, enjoy! I will dive into more detail on each block of code for those sticking around. But, without further ado, here\u2019s a sample application to give Mario a new brain:", "With the full application out of the way, let\u2019s look at each section in greater detail to understand what it does.", "We first need to import several libraries to make our lives easier. This is one of the great aspects of Python\u200a\u2014\u200athere\u2019s a high chance that something you need has already been created and is available in a library. I won\u2019t mention all of the imports, but some of the key ones are as follows:", "One of the great features of OpenAI\u2019s gym is how easy it is to create custom wrappers around an environment to alter various aspects of the game or virtual world, such as modifying image shapes and sizes, stacking multiple frames together, changing reset conditions, and many, many more.", "In some cases, as is the case with playing games from the Atari environment with OpenAI\u2019s Gym, it is possible to get away with using the various atari_wrappers which are included with most RL libraries, without the need to add any additional wrappers. Most of these wrappers are imported above, but I had to update one of them, the EpisodicLifeEnv wrapper, as it expects the environment to have an Atari Learning Environment (ALE) object, which our Super Mario Bros. environment does not contain. I simply copied the code for this class as-is from the ray.rllib.env.atari_wrappers module, but modified lines 18 and 37 above to point to the unwrapped property instead of the ale. By including this code locally in my application and changing those two lines, I still get the full functionality of the wrapper (which I will explain further below) but it won\u2019t complain that my environment doesn\u2019t have an ale component.", "I also modified a snippet of code from Uvipen\u2019s awesome Super-mario-bros-A3C-pytorch repository which updates the reward each learner receives after every step. By default, gym_super_mario_bros determines the reward at each step by calculating Mario\u2019s velocity (positive points while moving right, negative points while moving left, zero while standing still), plus a penalty for every frame that passes to encourage movement, and a penalty if Mario dies for any reason. While this is a fairly robust reward system, the levels could be played out more \u201cnormally\u201d (as in, the way most humans would play them) by rewarding Mario for increasing his in-game score by defeating enemies, grabbing coins, and collecting power-ups. This information is saved in the info dictionary, and the previous score can be compared with the current game score to find a difference in the latest step and add that to the reward.", "In addition to improving the reward when Mario increases his in-game score, a sizable reward is added if he collects the flag (or defeats Bowser) at the end of the level to encourage him to successfully beat the stage. He also receives a relatively large penalty if he doesn\u2019t make it to the end of the level before he dies.", "This new reward is then scaled down to be in a smaller range, which is similar to a reward clipping technique applied in the original DQN paper by Minh et. al.", "Up next we parse arguments passed by the CLI during runtime. The user can modify a few components of the application by specifying various flags. These flags have the following impacts:", "Before feeding it into a neural network, an environment needs to be built and wrapped with our modifications to make it easier for the agent to learn. I will go down these modifications line-by-line.", "This function is not strictly necessary, but I find the standard output which can be printed by RLLib to be fairly verbose, as I prefer to run a training loop in the background and only periodically look at certain values, such as the max, min, and mean rewards. The tabulate library makes it very easy to print clean tables of information which is what I use here, similar to what RLLib does with their output. If desired, this function can be omitted, and the result object below can be pretty_print\u2018ed instead.", "The main function in this case is where a lot of the magic happens and there\u2019s a lot going on (which slightly pains me given how I normally write Python applications with a slim main function, but I wanted to be relatively concise in this case).", "Breaking it down, the function starts with a env_creator_lambda definition which allows us to build a custom environment. This is necessary as RLLib takes a gym environment\u2019s name by default and builds that environment at the beginning of a training run. As we added several custom wrappers to our environment, they need to be included in a new function which is handled specially by RLLib to let our Python application build the environment instead of RLLib doing it automatically.", "I borrowed the IMPALA config tuned specifically for Atari Pong from the official Ray repository as a baseline for the Super Mario Bros. config. I chose IMPALA for my agent as I have a fairly beefy workstation which allows me to leverage a few dozen workers and two GPUs to accelerate the training process. IMPALA tends to be on the faster side amongst some of the common RL-models when given enough resources, hence my usage here. One of the reasons I moved to a RL-library, however, was to be able to seamlessly transition between various models without writing a bunch of new code to support new algorithms. Moving forward, I plan on adding new models in this application.", "Eagle-eyed readers will also notice I use PyTorch as my framework of choice. While I haven\u2019t done any comparisons for this specific application between PyTorch and TensorFlow (both are supported by RLLib), I tend to pick PyTorch when given the option as I prefer the framework\u2019s more Pythonic and intuitive style, generally better performance, and seamless support for GPUs. Assuming both packages are installed, switching between the two frameworks is as easy as changing the value of the framework setting.", "After initializing the config settings, several Ray-specific functions are called to startup a cluster on the current node, register an environment using our custom environment creation function with wrappers above using register_env, and building an IMPALA trainer based on the config that we created.", "If the user specified a checkpoint file during runtime, the trainer will read the requested checkpoint file, and update the trainer\u2019s weights based on that checkpoint prior to starting the run.", "Lastly, the training sequence is created by looping through the requested number of iterations and calling the train() method against the trainer to begin that phase of the pipeline. This will automatically update the model weights as progress is made and new policies are learned. The results from each iteration are then printed using the custom function created earlier to display the episode rewards over time. On every 50th iteration, a new checkpoint file is created and saved alongside model parameters, typically in the ~/ray_results/ directory.", "At this point, you should now be able to replicate the training environment on your own machine by copying the first code snippet above, installing dependencies, and running the code with any necessary parameter changes.", "If the code was saved as train.py, the training can be kicked off by simply running the following:", "Note that if you have a different number of available CPU cores than the default four or have NVIDIA GPUs, you can specify the result with the --workers and--gpus flags, respectively, similar to the following:", "Congratulations! If you followed this guide, you should now be able to train Mario to beat various levels of the original game that started it all. It may take several hours to complete a level depending on resources, and some are harder than others, but this offers a base foundation to achieve greater heights in the exciting realm of reinforcement learning.", "Have any suggestions for improvement? Feel free to drop a new issue or pull request on my repository or comment below. And as always, enjoy the new adventures with everyone\u2019s favorite Italian plumber. Wahoo!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software engineer passionate about sports and artificial intelligence and, apparently, a blogger by night."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6f095889b207&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f095889b207--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@roclark?source=post_page-----6f095889b207--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@roclark?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Robert Clark"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a72525d0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&user=Robert+Clark&userId=d2a72525d0b3&source=post_page-d2a72525d0b3----6f095889b207---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@claudiolcastro?utm_source=medium&utm_medium=referral", "anchor_text": "Cl\u00e1udio Luiz Castro"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://link.medium.com/RSJ21dAvncb", "anchor_text": "From Zero to Flagpole HeroFrom Zero to Flagpole Hero. How I went from a reinforcement learning novice to writing an AI from scratch that beat\u2026link.medium.com"}, {"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "rllib"}, {"url": "https://virtualenv.pypa.io/en/stable/", "anchor_text": "virtualenv"}, {"url": "https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html", "anchor_text": "conda"}, {"url": "https://github.com/roclark/super-mario-bros-rllib", "anchor_text": "repository I created"}, {"url": "https://github.com/uvipen/Super-mario-bros-A3C-pytorch/blob/master/src/env.py", "anchor_text": "Super-mario-bros-A3C-pytorch"}, {"url": "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf", "anchor_text": "DQN paper by Minh et. al"}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala.yaml", "anchor_text": "official Ray repository"}, {"url": "https://unsplash.com/@victorfreitas?utm_source=medium&utm_medium=referral", "anchor_text": "Victor Freitas"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/roclark/super-mario-bros-rllib/tree/main", "anchor_text": "repository"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----6f095889b207---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/mario?source=post_page-----6f095889b207---------------mario-----------------", "anchor_text": "Mario"}, {"url": "https://medium.com/tag/python?source=post_page-----6f095889b207---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nvidia?source=post_page-----6f095889b207---------------nvidia-----------------", "anchor_text": "Nvidia"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----6f095889b207---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&user=Robert+Clark&userId=d2a72525d0b3&source=-----6f095889b207---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&user=Robert+Clark&userId=d2a72525d0b3&source=-----6f095889b207---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f095889b207--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6f095889b207&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6f095889b207---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6f095889b207--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6f095889b207--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6f095889b207--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6f095889b207--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6f095889b207--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@roclark?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@roclark?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robert Clark"}, {"url": "https://medium.com/@roclark/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "275 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a72525d0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&user=Robert+Clark&userId=d2a72525d0b3&source=post_page-d2a72525d0b3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F18c672322a1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarios-gym-routine-6f095889b207&newsletterV3=d2a72525d0b3&newsletterV3Id=18c672322a1d&user=Robert+Clark&userId=d2a72525d0b3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}