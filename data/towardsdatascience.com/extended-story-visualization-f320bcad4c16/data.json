{"url": "https://towardsdatascience.com/extended-story-visualization-f320bcad4c16", "time": 1683007958.393427, "path": "towardsdatascience.com/extended-story-visualization-f320bcad4c16/", "webpage": {"metadata": {"title": "Extended Story Visualization. A key-frame sequence approach towards\u2026 | by Jorge Vizcayno | Towards Data Science", "h1": "Extended Story Visualization", "description": "Context-aware image generation using conditional GANs"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1609.02612", "anchor_text": "[21]", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1812.02784", "anchor_text": "[1]", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1906.04165", "anchor_text": "[3]", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1606.01323", "anchor_text": "[23]", "paragraph_index": 9}, {"url": "https://jorgeviz.me/extended-story-visualization-with-cond-gans.html#appendix", "anchor_text": "appendix", "paragraph_index": 22}], "all_paragraphs": ["Until today, many methods in video generation [21] focus mainly on motion consistency rather than the long-term context of the generated video. In other words, the model is trained to improve the smoothness of each frame with respect to the prior.", "Although this has yielded good results, if we foresee machines creating more relatable visual content that takes into account context, this becomes a non-trivial task. As an alternative, it is possible to separate the task into two steps, first to create models capable of generating specific initial target frames to relate a story in a visual manner, and then to use a motion-focused module that can illustrate each piece of the story in a keyframe-by-keyframe interpolation.", "Cause we all love new content \ud83d\ude0d!", "This key frame-like generation is proposed as a relatively new task by [1], with the name of Story Visualization. Where the task is treated as a sequence conditional generation problem, using story embeddings as the prior for the hidden state of the generator, and creates a new image for each individual sentence given in the story. This approach showed significantly good results where it achieved context-aware generation taking into account character and background distribution given a written story, nonetheless, it showcased a limitation in which each generated image was paired to a given sentence.", "Thus, in the aim of long-term video generation, we considered relevant to propose an extension to the method in which the model would receive an unstructured text entry and then be able to select the relevant features to generate the best number of images that illustrate the story.", "As shown in the image above and similar to the StoryGAN, the model comprehends a sequential Generative Adversarial Network (GAN) with two discriminators (the image discriminator for local consistency and story discriminator for global consistency). For the generation, it receives the encoded story coming from a DistilBERT pre-trained encoder as the first hidden vector (h_0 ), and a set of stacked vectors compounded by a random vector sampled from a standard normal distribution and the embeddings from the sentences selected by the extractive summarizer. At training time the generator is updated in two independent loops, first taking into account the image discriminator output and the second one with respect to the story discriminator.", "In the case of the proposed architecture, the key contribution relies on the implementation of a selective module that is able to choose the most relevant features of the story in order to avoid repetition in the image generation and maintains the context of the whole story, as well as a network capable of dealing with variable length stories coming from the generator and fixed-length ground truths.", "For the story encoder module, we considered using BERT by Google. BERT is a stack of transformers pre-trained on the Wikipedia corpus. It comes in two sizes, BERT base and BERT large. The base model has 12 transformer layers and the large model has 24. The BERT model, however, is too large and requires too many resources for training as well as inference. After much deliberation, we chose to use DistilBERT in our story encoder. DistilBERT is a smaller version of BERT. By using DistilBERT we reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. This is achieved by leveraging knowledge distillation in the pre-training phase. Knowledge distillation is a compression technique in which a compact model, the student, is trained to reproduce the behavior of a larger model, the teacher, or an ensemble of models.", "We started by taking a pre-trained DistilBERT model and trained (fine-tuned) it on text from the Pororo-SV dataset. We did this to improve the encodings generated for sentences in the dataset (i.e. it learns that Pororo is a name). Next, we used this model to encode the stories in our dataset. The text encodings are then passed to the recurrent generator. We also added a linear layer that maps the 768 dimension vector embeddings from DistilBERT into a 228 dimension vector corresponding to two 124-dimension vectors for the mean and variance respectively. Furthermore, the embeddings were stacked to a one-hot encoded vector that represents the characters present in the story to help the model identify them in the generation process.", "This summarization approach was inspired by [3], where it takes advantage of the similarity of the sentences and removes sentences where the information is already represented by other parts in the text. The process starts with a neural coreference resolution of the personal pronouns, in which every time the reference is found the model replaces the mention with its corresponding nominal pronoun. For the sake of this resolution problem, a pre-trained neural model based on [23] is used and then input the resolved sentences to the before-mentioned DistilBERT encoder to return the respective embeddings. The resolved embeddings are then clustered and the summarizer outputs the embeddings that are closer to each of the cluster centroid, the proportion of clusters generated is a hyperparameter tuned over the experimentation.", "In order to train the generator and both discriminators in an end-to-end pipeline, we use two independent loops, the first one to allow local consistency between the generated image and text input, and the second one for global consistency in which it\u2019s taking into account the set of generated images conforming a story and the written story itself when computing the losses. The overall optimization objective is described below:", "Where \u03b8, \u03d5_I, and \u03d5_S are the parameters of the generator, image discriminator, and story discriminator networks respectively.", "Essentially, for the updates corresponding to the image loss, we used a binary cross-entropy loss that summed up the outputs of the discriminator with respect to the generated and ground truth images over the corresponding story. To allow the simultaneous evaluation of the image distribution and its correlation with the input story sentence in the discriminator, it concatenates the encoded raw image output of a convolutional layer with the sentence and story embeddings to determine whether it came from the generator or not.", "Similar to the image discriminator, the computed story loss utilizes also a binary cross-entropy loss from the story discriminator outputs, but instead of taking into account every image-sentence embeddings pair separately, it stacks and averages every story related set of vectors to then be fed into the discriminator.", "This process was particularly challenging to implement due to the fact that the ground truths and the generated length of the image stories would not necessarily be of the same length. The key idea of this discriminator is to evaluate the story embedded mean information remains the same between ground truths and generated samples even when they had different lengths.", "For the final term of the optimization objective, we made use of a Kulback-Liebler Divergence penalty between the story embeddings distribution and a normal standard distribution to avoid the mode-collapse issue in the image generation process.", "In our first experiment, we implemented a DistilBERT story encoder to verify if it will provide enough context to the generator so the output images make sense. We noticed that as the training progressed, the images generated became clearer and made more sense. In most of the generated sequences, we saw that the DistilBERT embeddings passed enough context information to accurately identify the characters present in the input story as well as their exact positions with respect to each other and other objects in the scene. One such sequence is described below, where the model generates all the characters in their correct positions but it has not been able to detect the bed should be in the generated distribution.", "Additionally, the color distribution of the scene, as well as the background environment (outdoors/indoors), are generally well illustrated by the model as seen in the below examples.", "Once we achieved considerable good results in the fixed-length image generation experiments in contrast to the StoryGAN baseline, we continued to set the experimentation pipeline in the variable-length approach proposed with the Extractive Text Summarizer module. This added a higher level of complexity to the optimization problem, the GAN training became more unstable and made the job easier for the discriminator since it was harder for the generator to create an interpolated image distribution that covered the full story information with less amount of images.", "As shown in the figure above, the model was capable of identifying the underlying distribution in terms of background (i.e. the framed sequence shows a snowy background in the first image even when none of the selected sentences mentioned it) and in terms of story characters selection; since the summarizer chose the features illustrating the most relevant ones in the story, but the image resolution was harmed in attaining this objective.", "The StoryGAN\u2019s approach documented in [1], has a reliable framework to solve the story visualization problem capable to identify key features from a given written story. And while an extractive summarizer fetches the relevant sentences to tell the story in a visual way, it is still unable to reach a reliable image quality. This approach is capable of learning the underlying context distribution of the Pororo dataset but needs extra information to derive the image space that lies in between each frame of the full image sequence. Throughout experimentation, the generator showed to be a slower learner with respect to the story discriminator since during the summarization process it needed to infer the image-to-image distribution while interpolating or trying to create images that showed the same amount of information in less amount of images.", "Another complicated parameter to tune in the training process was the ratio of summarized sentences of the extractive module. Hence, we believe adding a soft-relaxation to the sentence selection process would be an interesting way to help the model identify the correct amount of sentences to generate at training time, as it would backpropagate the error when generating the incorrect amount of images for a given story.", "For a more model details and review the complete appendix.", "Several other tasks which are also related to story visualization are story image retrieval from a pre-collected training set instead of generating images [15], a \u201ccut and paste\u201d technique for cartoon generation [8]. The opposite task of story visualization is visual storytelling where the output is a paragraph describing the input sequence of images. For visual storytelling task text generation models or reinforcement learning [11, 14, 10] are used.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff320bcad4c16&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jorgeviz?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jorgeviz?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Jorge Vizcayno"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6ffd11a6bca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&user=Jorge+Vizcayno&userId=b6ffd11a6bca&source=post_page-b6ffd11a6bca----f320bcad4c16---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1609.02612", "anchor_text": "[21]"}, {"url": "https://arxiv.org/abs/1812.02784", "anchor_text": "[1]"}, {"url": "https://giphy.com", "anchor_text": "GIPHY"}, {"url": "https://arxiv.org/abs/1906.04165", "anchor_text": "[3]"}, {"url": "https://arxiv.org/abs/1606.01323", "anchor_text": "[23]"}, {"url": "https://hugginface.co/neuralcoref", "anchor_text": "HuggingFace Co."}, {"url": "https://jorgeviz.me/extended-story-visualization-with-cond-gans.html#appendix", "anchor_text": "appendix"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f320bcad4c16---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f320bcad4c16---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/gans?source=post_page-----f320bcad4c16---------------gans-----------------", "anchor_text": "Gans"}, {"url": "https://medium.com/tag/image-generation?source=post_page-----f320bcad4c16---------------image_generation-----------------", "anchor_text": "Image Generation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&user=Jorge+Vizcayno&userId=b6ffd11a6bca&source=-----f320bcad4c16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&user=Jorge+Vizcayno&userId=b6ffd11a6bca&source=-----f320bcad4c16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff320bcad4c16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f320bcad4c16---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f320bcad4c16--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f320bcad4c16--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f320bcad4c16--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jorgeviz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jorgeviz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jorge Vizcayno"}, {"url": "https://medium.com/@jorgeviz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6ffd11a6bca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&user=Jorge+Vizcayno&userId=b6ffd11a6bca&source=post_page-b6ffd11a6bca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fb6ffd11a6bca%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fextended-story-visualization-f320bcad4c16&user=Jorge+Vizcayno&userId=b6ffd11a6bca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}