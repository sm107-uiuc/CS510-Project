{"url": "https://towardsdatascience.com/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504", "time": 1683002579.004363, "path": "towardsdatascience.com/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504/", "webpage": {"metadata": {"title": "Model-Agnostic Methods for Interpreting any Machine Learning Model | by Hennie de Harder | Towards Data Science", "h1": "Model-Agnostic Methods for Interpreting any Machine Learning Model", "description": "More and more companies are using complex machine learning models, like neural networks and gradient boosting machines. The reason they use complex models is because they outperform traditional\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@hennie_de_harder/interpretable-machine-learning-models-aef0c7be3fd9", "anchor_text": "another article", "paragraph_index": 1}, {"url": "https://www.kaggle.com/ronitf/heart-disease-uci", "anchor_text": "Heart Disease UCI dataset on Kaggle", "paragraph_index": 3}, {"url": "https://github.com/henniedeharder/interpretability-heart/blob/master/Demo_Forest_SHAP_Heart.ipynb", "anchor_text": "GitHub", "paragraph_index": 4}, {"url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G", "anchor_text": "Amazon scraps secret AI recruiting tool that showed bias against women", "paragraph_index": 31}, {"url": "https://academic.oup.com/bioinformatics/article/26/10/1340/193348", "anchor_text": "Permutation importance: a corrected feature importance measure", "paragraph_index": 32}, {"url": "https://explained.ai/rf-importance/index.html", "anchor_text": "Beware Default Random Forest Importances", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1309.6392.pdf", "anchor_text": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation", "paragraph_index": 34}, {"url": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf", "anchor_text": "\u201cWhy Should I Trust You?\u201d Explaining the Predictions of Any Classi\ufb01er", "paragraph_index": 35}, {"url": "https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "A Uni\ufb01ed Approach to Interpreting Model Predictions", "paragraph_index": 36}, {"url": "https://arxiv.org/pdf/1903.10464.pdf", "anchor_text": "Explaining Individual Predictions when Features are Dependent: More Accurate Approximations to Shapley Values", "paragraph_index": 37}], "all_paragraphs": ["More and more companies are using complex machine learning models, like neural networks and gradient boosting machines. The reason they use complex models is because they outperform traditional models, like decision trees or logistic regression. The negative side effect of using complex models is that you can\u2019t interpret those models directly. You don\u2019t want a biased model or a model that makes choices based on strange or unrelated knowledge. Experiences in the past, like what happened at Amazon\u00b9 or with teachers\u00b2 show the importance of interpreting complex models. There are positive side effects in knowing why your model makes it\u2019s decision, e.g. you can understand new patterns the model finds and learn more about your data.", "A lot of research has been done on the interpretability of machine learning models. There are different ways to interpret machine learning models. The easiest split is between interpretable models and model-agnostic methods. Interpretable models are models who explain themselves, for instance from a decision tree you can easily extract decision rules. Model-agnostic methods are methods you can use for any machine learning model, from support vector machines to neural networks. In this article the focus will be on model-agnostic methods. There\u2019s another article about interpretable models.", "A field where you can use model interpretability is health care. To find out how a model decides whether or not a person has a heart disease, we use a dataset from the Cleveland database with the following features:", "We will try to predict the target with a random forest and interpret this model with model-agnostic methods. You can find the Heart Disease UCI dataset on Kaggle.", "The code for this article about model-agnostic methods (and for the article about interpretable models) can be found on GitHub.", "Unfortunately it\u2019s not possible to directly interpret the most machine learning models. For popular models like random forests, gradient boosted machines and neural networks you need model-agnostic methods. At the moment there are some interesting methods available, like permutation feature importance\u00b3, Partial Dependence Plots (PDPs), Individual Conditional Expectation (ICE) plots, global surrogate models, Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). We will dive into these methods and discuss their advantages and disadvantages.", "Do you use feature importances from scikit-learn\u2074? These feature importances are based on the mean decrease in criterion, like gini impurity (for decision trees and random forests). It\u2019s better to use permutation feature importances. With this method, the importances are based on measuring the increase of the prediction error when you permute the feature\u2019s values. So you compute the prediction error two times, before and after permutation of the feature. The higher the difference between the prediction errors, the more important the feature.", "Now we\u2019re going to compare the scikit-learn feature importances for the random forest with the permutation feature importance:", "Wow! Everything is shuffled if you compare the order of features from the scikit-learn feature importances with the permutation feature importance! According to the last picture we should try to exclude chol and exang, because when these features are permuted the model performs better! And for the features fbs, trestbps and age nothing happens (if we ignore the variance).", "These plots help visualize the average partial relationship between the predicted target and one or more features. The plots are created by forcing all the instances to have the same feature value. Then you make predictions for these instances and you average them, this gives the average prediction for this feature value. Due to visualization most of the time only one or two features are investigated.", "To have an idea what PDPs look like, below you can see examples of PDPs on the heart disease dataset. The first two images have one feature on the x-axis and the probability of having a heart disease on the y-axis. In the third image you can see two features (one on the x-axis and one on the y-axis).", "In PDPs, you force all the instances to have the same feature value. The plots can be misleading if you only have a small amount of instances who have a certain feature value. It\u2019s better to include data distributions in your plot, so you can see if the data is equally distributed.", "Watch out with PDPs! It is assumed that features for which you compute the partial dependence are independent. So they shouldn\u2019t be correlated with other features. You can also easily miss complexity of the model, because the predictions are averaged.", "A way to deal with the problem of missing complexity in the model with PDPs is to show them in combination with ICE plots\u2075. ICE plots are more detailed and show how the prediction of each instance changes when a feature value is changed. In the following images, every blue line represents one instance.", "The ICE plot for the sex (female = 0, male =1) variable is shown above. The average is the thick line in the middle (same line as in the PDPs). You see that for some instances the prediction changes a lot when sex is changed to male, but for some instances the prediction almost stays the same, although it always has a negative effect to be female. On the bottom of the image you see the data distribution.", "This is interesting! The cholesterol variable shows that the pattern is more complicated than you would expect from the PDP, because the instances are spread out all over the place and often they don\u2019t follow the pattern of the thick line. Sometimes a higher cholesterol has a (small) positive effect and sometimes the effect is negative. There are not that many instances that have a cholesterol value above 400 (check out the distribution on the bottom) so we should be careful here!", "With ICE plots we solved the problem of PDPs by showing more complexity, but what about the independent features problem? That problem isn\u2019t solved with ICE plots, because the feature you plot still needs to be uncorrelated with the other features.", "Global surrogates are really easy to understand, that\u2019s an advantage of this method. First you build a black box model on the training data with the real labels. Then you let the model predict the labels for the same data and you build an interpretable model on the data with the predicted labels. Because the surrogate model is interpretable and build on the predictions of the black box model you learn how the black box model makes its prediction.", "This method is nice, because it\u2019s intuitive. There are some disadvantages. The interpretable model will perform worse than the black model (otherwise you should replace the black box model). You need to decide what an acceptable metric is for the performance of the interpretable model. Besides that, the interpretable model draws conclusions about the black box model, not about the data.", "When you want to explain an individual prediction, you can use LIME. With LIME, a local surrogate model is trained. This interpretable surrogate model can be used to explain the individual prediction. You can use LIME not only on tabular data, but also on images or text\u2076.", "The following image shows the way LIME works in an intuitive way. The red pluses and the blue dots are samples from the different classes. The border between the pink and the blue area is the decision boundary of the black box model. If you want to explain the big red plus on the image below, you can create other instances that are close to make a local decision boundary (dotted line) with an interpretable model. This local decision boundary is a lot easier to explain than the boundary between the pink and blue area (the decision boundary of the black box model).", "Let\u2019s take a new record from the test set:", "And now, let\u2019s use LIME to explain the prediction of this record:", "Close call! We see that the prediction probability is slightly higher for true (heart disease = 1) than for false (heart disease = 0). On the right we can see which features contributed most to the prediction.", "Just like with other interpretation methods, you need to be careful with LIME. If you explain the same record twice, the explanations can be different! Another disadvantage is that you can only explain one instance, so it\u2019s not possible to interpret the whole black box model.", "If you want a really fancy and good way to display how a feature value contributes to the prediction, you should use SHAP. For this method, shapley values (from game theory) and LIME are combined\u2077. In short, shapley values use coalitions to see what the contribution of a feature value is to the final prediction.", "The record that\u2019s been investigated is the same record we used for LIME. The predicted probability is equal to 0.53. That value is slightly below the base value of 0.5486. The red features, like cp and oldpeak increase the probability having a heart disease, while ca and thal decrease the probability.", "If we turn all the samples from the test set 90 degrees, stack them and order them by similarity, we get the image above.", "The summary plot above shows the different SHAP values for high or low feature values. If you look at the ca feature, you see that when this feature has a low value the SHAP value is high, this means a higher probability having a heart disease. This plot also shows us the most important features to the least important ones (top to bottom).", "SHAP values have some advantages, because we can use them for local and global explanations. And they have a strong theoretical foundation. In the beginning they couldn\u2019t handle dependent features, but research shows that it\u2019s possible to use them with dependent features\u2078. A problem for SHAP values is the computation speed, when you have many features the time for the computations will increase significantly.", "Hopefully you can use these methods to investigate your data and models!", "[1] J. Dastin, Amazon scraps secret AI recruiting tool that showed bias against women (2018), Reuters", "[3] A. Altmann and L. Tolo\u015fi, Permutation importance: a corrected feature importance measure (2010), Bioinformatics", "[4] T. Parr, K. Turgutlu, C. Csiszar and J. Howard, Beware Default Random Forest Importances (2018), explained.ai", "[5] A. Goldstein, A. Kapelner, J. Bleich and E. Pitkin, Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation (2014), Journal of Computational and Graphical Statistics", "[6] M. T. Ribeiro, S. Singh and C. Guestrin, \u201cWhy Should I Trust You?\u201d Explaining the Predictions of Any Classi\ufb01er (2016), ResearchGate", "[7] S. Lundberg and S. Lee, A Uni\ufb01ed Approach to Interpreting Model Predictions (2017), NIPS", "[8] K. Aas and M. Jullum, Explaining Individual Predictions when Features are Dependent: More Accurate Approximations to Shapley Values (2019), ArXiv", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83d\udcc8 Data Scientist with a passion for math \ud83d\udcbb Currently working at IKEA and BigData Republic \ud83d\udca1 I share tips & tricks and fun side projects"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4f10787ef504&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://hennie-de-harder.medium.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": ""}, {"url": "https://hennie-de-harder.medium.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Hennie de Harder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb96be98b7b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&user=Hennie+de+Harder&userId=fb96be98b7b9&source=post_page-fb96be98b7b9----4f10787ef504---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@szmigieldesign?utm_source=medium&utm_medium=referral", "anchor_text": "Lukasz Szmigiel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@hennie_de_harder/interpretable-machine-learning-models-aef0c7be3fd9", "anchor_text": "another article"}, {"url": "https://www.kaggle.com/ronitf/heart-disease-uci", "anchor_text": "Heart Disease UCI dataset on Kaggle"}, {"url": "https://github.com/henniedeharder/interpretability-heart/blob/master/Demo_Forest_SHAP_Heart.ipynb", "anchor_text": "GitHub"}, {"url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G", "anchor_text": "Amazon scraps secret AI recruiting tool that showed bias against women"}, {"url": "https://we.riseup.net/assets/404114/Weapons+of+Math+Destruction+Cathy+O%27Neil.pdf", "anchor_text": "Weapons Of Math Destruction"}, {"url": "https://academic.oup.com/bioinformatics/article/26/10/1340/193348", "anchor_text": "Permutation importance: a corrected feature importance measure"}, {"url": "https://explained.ai/rf-importance/index.html", "anchor_text": "Beware Default Random Forest Importances"}, {"url": "https://arxiv.org/pdf/1309.6392.pdf", "anchor_text": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation"}, {"url": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf", "anchor_text": "\u201cWhy Should I Trust You?\u201d Explaining the Predictions of Any Classi\ufb01er"}, {"url": "https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "A Uni\ufb01ed Approach to Interpreting Model Predictions"}, {"url": "https://arxiv.org/pdf/1903.10464.pdf", "anchor_text": "Explaining Individual Predictions when Features are Dependent: More Accurate Approximations to Shapley Values"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----4f10787ef504---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://medium.com/tag/partial-dependence?source=post_page-----4f10787ef504---------------partial_dependence-----------------", "anchor_text": "Partial Dependence"}, {"url": "https://medium.com/tag/shap?source=post_page-----4f10787ef504---------------shap-----------------", "anchor_text": "Shap"}, {"url": "https://medium.com/tag/lime?source=post_page-----4f10787ef504---------------lime-----------------", "anchor_text": "Lime"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4f10787ef504---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&user=Hennie+de+Harder&userId=fb96be98b7b9&source=-----4f10787ef504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&user=Hennie+de+Harder&userId=fb96be98b7b9&source=-----4f10787ef504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4f10787ef504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4f10787ef504---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4f10787ef504--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4f10787ef504--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4f10787ef504--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4f10787ef504--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4f10787ef504--------------------------------", "anchor_text": ""}, {"url": "https://hennie-de-harder.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://hennie-de-harder.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hennie de Harder"}, {"url": "https://hennie-de-harder.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb96be98b7b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&user=Hennie+de+Harder&userId=fb96be98b7b9&source=post_page-fb96be98b7b9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F262470d3fe9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504&newsletterV3=fb96be98b7b9&newsletterV3Id=262470d3fe9a&user=Hennie+de+Harder&userId=fb96be98b7b9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}