{"url": "https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b", "time": 1682994312.8595672, "path": "towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b/", "webpage": {"metadata": {"title": "Reinforcement learning (RL) 101 with Python | by Gerard Mart\u00ednez | Towards Data Science", "h1": "Reinforcement learning (RL) 101 with Python", "description": "In this post we will introduce few basic concepts of classical RL applied to a very simple task called gridworld in order to solve the so-called state-value function, a function that tells us how\u2026"}, "outgoing_paragraph_urls": [{"url": "https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view", "anchor_text": "here", "paragraph_index": 26}, {"url": "http://CryptoDatum.io", "anchor_text": "CryptoDatum.io", "paragraph_index": 28}], "all_paragraphs": ["In this post we will introduce few basic concepts of classical RL applied to a very simple task called gridworld in order to solve the so-called state-value function, a function that tells us how good is to be in a certain state t based on future rewards that can be achieved from that state. To do so we will use three different approaches: (1) dynamic programming, (2) Monte Carlo simulations and (3) Temporal-Difference (TD).", "Reinforcement learning is a discipline that tries to develop and understand algorithms to model and train agents that can interact with its environment to maximize a specific goal. The idea is quite straightforward: the agent is aware of its own State t, takes an Action At, which leads him to State t+1 and receives a reward Rt. The following scheme summarizes this iterative process of St \u2192At \u2192Rt \u2192St+1 \u2192At+1 \u2192Rt+1 \u2192St+2\u2026:", "An example of this process would be a robot with the task of collecting empty cans from the ground. For instance, the robot could be given 1 point every time the robot picks a can and 0 the rest of the time. You can imagine that the actions of the robot could be several, e.g. move front/back/left/right, extend the arm up/down, etc. If the robot was fancy enough, the representation of the environment (perceived as states) could be a simple picture of the street in front of the robot. The robot would be set free to wander around and learn to pick the cans, for which we would give a positive reward of +1 per can. We could then set a termination state, for instance picking 10 cans (reaching reward = 10). The robot would loop in the agent-environment cycle until the terminal state would be achieved, which would mean the end of the task or episode, as it is known.", "The gridworld task is similar to the aforementioned example, just that in this case the robot must move through the grid to end up in a termination state (grey squares). Each grid square is a state. The actions that can be taken are up, down, left or right and we assume that these actions are deterministic, meaning every time that the robot picks the option to go up, the robot will go up. There\u2019s an exception, which is when the robot hits the wall. In this case, the final state is the same as the initial state (cannot break the wall). Finally, for every move or attempt against the wall, a reward of -1 will be given except if the initial state is a terminal state, in which case the reward will be 0 and no further action will needed to be taken because the robot would have ended the game.", "Now, there are different ways the robot could pick an action. These rules based on which the robot picks an action is what is called the policy. In the simplest of cases, imagine the robot would move to every direction with the same probability, i.e. there is 25% probability it moves to top, 25% to left, 25% to bottom and 25% to right. Let\u2019s call this the random policy. Following this random policy, the question is: what\u2019s the value or how good it is for the robot to be in each of the gridworld states/squares?", "If the objective is to end up in a grey square, it is evident that the squares next to a grey one are better because there\u2019s higher chance to end up in a terminal state following the random policy. But how can we quantify how good are each of these squares/states? Or, what is the same, how can we calculate a function V(St) (known as state-value function) that for each state St gives us its real value?", "Let\u2019s first talk about the concept of value. Value could be calculated as the sum of all future rewards that can be achieved from a state t. The intuitive difference between value and reward is like happiness to pleasure. While immediate pleasure can be satisfying, it does not ensure a long lasting happiness because it is not taking into consideration all the future rewards, it only takes care of the immediate next one. In RL, the value of a state is the same: the total value is not only the immediate reward but the sum of all future rewards that can be achieved.", "A way to solve the aforementioned state-value function is to use policy iteration, an algorithm included in a field of mathematics called dynamic programming. The algorithm is shown in the following box:", "The key of the algorithm is the assignment to V(s), which you can find commented here:", "The idea is that we start with a value function that is an array of 4x4 dimensions (as big as the grid) with zeroes. Now we iterate for each state and we calculate its new value as the weighted sum of the reward (-1) plus the value of each neighbor states (s\u2019). Notice two things: the V(s\u2019) is the expected value of the final/neighbor state s\u2019 (at the beginning the expected value is 0 as we initialize the value function with zeroes). Finally, the V(s\u2019) is multiplied by a gamma, which is the discounting factor. In our case we use gamma=1 but the idea of the discounting factor is that immediate rewards (the r in our equation) are more important than the future rewards (reflected by the value of s\u2019) and we can adjust the gamma to reflect this fact.", "Finally, notice that we can repeat this process over and over in which we \u201csweep\u201d and update the state-value function for all the states. These values can get iteratively updated until reaching convergence. In fact in the iterative policy evaluation algorithm, you can see we calculate some delta that reflect how much the value of a state changes respect the previous value. These deltas decay over the iterations and are supposed to reach 0 at the infinity.", "Here\u2019s an example of how the value function is updated:", "Notice in the right column that as we update the values of the states we can now generate more and more efficient policies until we reach the optimal \u201crules\u201d a robot must follow to end up in the termination states as fast as possible.", "Finally, here\u2019s a Python implementation of the iterative policy evaluation and update. Observe in the end how the deltas for each state decay to 0 as we reach convergence.", "While the previous approach assumes we have a complete knowledge of the environment, many times this is not the case. Monte Carlo (MC) methods are able to learn directly from experience or episodes rather than relying on the prior knowledge of the environment dynamics.", "The term \u201cMonte Carlo\u201d is often used broadly for any estimation method whose operation involves a significant random component.", "Interestingly, in many cases is possible to generate experiences sampled according to the desired probability distributions but infeasible to obtain the distributions in explicit form.", "Here\u2019s the algorithm to estimate the value function following MC:", "The Monte Carlo approach to solve the gridworld task is somewhat naive but effective. Basically we can produce n simulations starting from random points of the grid, and let the robot move randomly to the four directions until a termination state is achieved. For each simulation we save the 4 values: (1) the initial state, (2) the action taken, (3) the reward received and (4) the final state. In the end, a simulation is just an array containing x arrays of these values, x being the number of steps the robot had to take until reaching a terminal state.", "Now, from these simulations, we iterate from the end of the \u201cexperience\u201d array, and compute G as the previous state value in the same experience (weighed by gamma, the discount factor) plus the received reward in that state. We then store G in an array of Returns(St). Finally, for each state we compute the average of the Returns(St) and we set this as the state value at a particular iteration.", "Here you can find a Python implementation of this approach applied to the same previous task: the worldgrid.", "Note that varying the gamma can decrease the convergence time as we can see in the last two plots using gamma=1 and gamma=0.6. The good side of this approach is that:", "Finally, the last method we will explore is temporal-difference (TD). This third method is said to merge the best of dynamic programming and the best of Monte Carlo approaches. Here we enumerate some of its strong points:", "Here\u2019s the algorithm to calculate the value function using temporal-difference:", "And here\u2019s the jupyter notebook with the Python implementation", "Notice that adjusting alpha and gamma parameters is critical in this case to reach convergence.", "Finally, I\u2019d like to mention that most of the work here is inspired or drawn from the latest edition of the Andrew G. and Richard S. book called Reinforcement Learning: An Introduction, amazing work that these authors have made publicly accessible here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trading strategy developer at Primer Quant Firm \u2014 Founder of CryptoDatum.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe1aa0d37d43b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gerardmartnez?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Gerard Mart\u00ednez"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa21c8bce638f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=post_page-a21c8bce638f----e1aa0d37d43b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e1aa0d37d43b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e1aa0d37d43b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----e1aa0d37d43b---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/tag/python?source=post_page-----e1aa0d37d43b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/ai?source=post_page-----e1aa0d37d43b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=-----e1aa0d37d43b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=-----e1aa0d37d43b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe1aa0d37d43b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e1aa0d37d43b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e1aa0d37d43b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gerard Mart\u00ednez"}, {"url": "https://medium.com/@gerardmartnez/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "http://CryptoDatum.io", "anchor_text": "CryptoDatum.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa21c8bce638f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=post_page-a21c8bce638f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4ae84817a9e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-rl-101-with-python-e1aa0d37d43b&newsletterV3=a21c8bce638f&newsletterV3Id=4ae84817a9e0&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}