{"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "time": 1683001873.035074, "path": "towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9/", "webpage": {"metadata": {"title": "Linear Regression with one or more variables | by Luca Zammataro | Towards Data Science", "h1": "Linear Regression with one or more variables", "description": "Linear regression is a statistical model used in Machine Learning. We will describe how to apply it to a dataset of patients checked for Systolic Blood Pressure, in order to predict new outcomes."}, "outgoing_paragraph_urls": [{"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook", "paragraph_index": 9}, {"url": "https://numpy.org/", "anchor_text": "NumPy", "paragraph_index": 10}, {"url": "https://scipy.org/", "anchor_text": "SciPy.org", "paragraph_index": 12}, {"url": "https://www.geeksforgeeks.org/python-pandas-dataframe/", "anchor_text": "link to the GeeksForGeeks portal", "paragraph_index": 14}], "all_paragraphs": ["Linear Regression is a statistical model used in Machine Learning that falls in the \u201cSupervised Learning\u201d class of algorithms, and it applies to the analysis of biomedical data. We use it for predicting continuous-valued outputs, differently from the Logistic Regression, that instead is used for predicting discrete-valued outputs (i.e., classification).", "Before starting, I suggest readers following the interesting course in Machine Learning at Coursera by Andrew NG [1]. The course provides an excellent explanation of all the arguments treated in this post.", "Let\u2019s start with our example: we have a dataset of patients checked for Systolic Blood Pressure (SBP) and monitored for age and weight, and we want to predict the SBP for a new patient. Implicitly we hypothesize that factors such as weight and age influence the SBP. For our dataset, we will take values of a Table from [2]", "The Variable to be explained (SBP) is called the Dependent Variable, or Response Variable, and it matches with our output variable or target vector. Instead, the variables that explain the input (age and weight) are called Independent Variables or Predictor Variables, or Features. If the dependent and independent variables are continuous, as is the case for SBP, age, and weight, then a Correlation coefficient can be calculated as a measure of the strength of the relationship between them. [3]", "We say that Linear Regression represents an evolution of the Correlation. The difference between them is: Correlation refers to the strength of the relationship between two or more variables. The Regression, instead, refers to an ensemble of statistical techniques and algorithms for describing the relationship between two or more variables [2].", "Linear Regression assumes that the relationship between one or multiple input features and the relative target vector (outputs) is approximatively linear. [4], and it enables the identification and characterization of this relationship. The consequence of this assumption is that, in the Linear Regression model, the input features have an \u201ceffect\u201d on the target vector (output), and this effect is constant.", "The \u201ceffect\u201d is often identified as \u201ccoefficient\u201d, \u201cweight\u201d or \u201cparameter\u201d, and more simply, we say that Linear Regression computes a weighted sum of the input features, plus a constant called \u201cbias term\u201d or intercept [5].", "To simplify, let\u2019s start with the application of the Linear Regression with one variable.", "Starting One Variable is a fundamental step if we want to understand thoroughly how the Linear Regression works. We will use the dataset in Table 1, only extrapolating the feature \u201cAge\u201d and using the \u201cSBP\u201d column as output.", "All the code presented in this post is written in Python 2.7, and it\u2019s self-explicative also for the porting to many other languages. For the implementation environment, I suggest the use of Jupyter Notebook.", "Linear Algebra calculation will be primarily used, because of one the intrinsic advantages in avoiding, where possible, \u2018while\u2019 and \u2018for\u2019 loops. To accomplish this goal, we will use NumPy, a robust library of math functions for scientific computing with Python.", "Before starting with the description of the Linear Regression model, it\u2019s necessary to take a glance at our data and try to understand whether Linear Regression could apply to the data. The aim is predicting, for example, systolic pressure value, based on the patient age.", "First of all, we import all the packages required for the Python code that will be discussed in this post: NumPy, Pandas and matplot. These packages belong to SciPy.org, which is a Python-based ecosystem of open-source software for mathematics, science, and engineering.", "Numpy is necessary for the Linear Algebra calculations.", "Pandas is an open-source library providing high-performance, data structures, and data analysis tools for Python. A pandas DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure. It consists of three principal components, the data, rows, and columns, with labeled axes (rows and columns). Follow this link to the GeeksForGeeks portal, for having access to detailed information about the use of the pandas DataFrame.", "Matplotlib is fundamental for creating all the plots.", "In this step, we will create a dataset with our values. An example of the comma-separated-values format, with a header, is the following:", "Copy and paste the values in a file, and save it as \u201cSBP.csv\u201d", "Once we have created the SBP.csv dataset, upload it and create a DataFrame using the Pandas pd object. The Python code for the dataset uploading is the following:", "Typing \u201cdf\u201d in a Jupyter cell, will display the DataFrame content as following:", "The SBP dataset is formed by 3 columns (Age, Weight, and SBP), but we will upload the first and the last columns (Age and SBP); our model will determine the strength of the relationship between Age and SBP. Pandas makes easy accessing to DataFrame variables, that will be copied in an X vector, containing the input, and a y vector, for the output.", "Before going ahead, the SBP dataset reported here has to be rescaled and normalized. In Machine Learning, scaling and normalization are required, especially when discrepancies in order of magnitude among the features and the output occurs. As mentioned in the Introduction, we will use Linear Algebra on matrixes to avoid, where possible, while and for loops on variables. Moreover, programming Linear Algebra will result in good readability of the code.", "For this purpose, In Python, we can take advantage of NumPy. As mentioned before, NumPy is a library of math functions for scientific calculation and for Linear Algebra. All the operations can be simplified, creating a NumPy object, and using some of the associated methods. The reason why we want to use NumPy is that, even though many operations on matrixes can be done using the regular operators (+, -, *, /), NumPy guarantees a better control over the operations, especially if the matrixes are big. For example, with NumPy, we can multiply arguments element-wise, like the features matrix X and the output vector y:", "An in-depth discussion on the use of NumPy goes beyond this post. To implement Scaling and Normalization of features, this code is what we need:", "Code 5 implements a Python function called FeatureScalingNormalization(). This function takes X that is the features vector as an argument, and return 3 arguments: 1) the same X vector but scaled and normalized (X_norm), 2) mu, that is the average values of X in training set) and 3) sigma that is the Standard Deviation. Also, we will store mu and sigma because these parameters will be fundamental later. Copy the following code and paste it in a new Jupyter Notebook cell:", "Typing \u2018X\u2019 in a Notebook cell will display the new values of X:", "The X vector containing the Age values is now normalized.", "Now we will add a column of ones to the X vector.", "This is the new structure of X:", "Plotting our data is a useful practice when we want to have an idea of how they are distributed. Plot the data using the matplotlib scatter method:", "Visualizing the data at a glance, we can notice a pattern of increasing relationship, between Age and SBP. This is what we expect since systolic pressure is physiologically connected to age increasing.", "The idea underlying the Linear Regression is represented by a function that predicts the output y based on the input feature X.", "The predicted output is the h = \u03b8 * X term that is equal to a constant called \u201cbias term\u201d or \u201cintercept term\u201d or \u03b8_0 plus a weighted sum of the input features X, where \u03b8_1 represents the weight for X. We will call this function \u201cHypothesis\u201d , and we will use it to \u201cmap\u201d from X (Age) to y (SBP).", "Since we are using Linear Algebra, for all the calculation, we can write the Hypothesis model in the vectorized form:", "The best performance in predicting y consists of finding \u03b8 values for which the distance between the predicted y value and the actual y value is closer to the minimum.", "The Hypothesis model in Figure 2, represented by the red line, should predict y (the SBP). It represents our h=\u03b8X vector in predicting y for the values of \u03b8 = [140.0, 5.0]. But this model, obviously, does not fit our data. As highlighted by the blue lines connecting dots with the red line, the Hypothesis \u201ctouches\u201d some of the y values, but the rest of the h vector is far from the minimum. So we are tempted to guess which \u03b8 could predict y when setting with different values. We could choose \u03b8 \u201cby trial and error\u201d to minimize all the distances between the Hypothesis and y. To accomplish this goal, we can calculate the Cost Function for our model.", "The Cost Function can register how much far we are from the minimum of the Hypothesis model and can help us in finding the best \u03b8. The equation describing the Cost Function is the following:", "Where m is the length of the X vector (in our case = 16), and i is the index assigned to each item in the dataset. The Equation is composed of three components:", "Since we use Linear Algebra, the vectorized implementation of Equation 3 is the following:", "In order to simplify the explanation, let\u2019s try to manually calculate the Cost Function for a smaller dataset composed only of the first 3 values of the SBP dataset, and with \u03b8 = [120.0, 10.0]. These parameters are chosen randomly, at the moment, because we don\u2019t have to set the best \u03b8 for now. We will split the X and y, producing arrays X_1 and y_1:", "Also, we have to set m=3, because we have three samples now. Let\u2019s plot the data and the Hypothesis as following:", "The vector y corresponding to the first three values (the blue dots) of the SBP is:", "Since our \u03b8 is = [120, 10.0], the product of h = \u03b8*X_1 will be represented by the following vector, (highlighted by the dots on the red line):", "The dashed blue lines highlight the distances between actual y_1 values and predicted values. Now, we have all we need, to calculate the Cost Function J. We will apply the Cost Function as described in Solution 1:", "The following Python code implements the Cost Function:", "The code implements step by step the Cost Function described in Equation 4 (vectorized). Let\u2019s repeat again:", "Now that we have understood the mechanism underlying the Cost Function calculation, let\u2019s go back to the complete SBP dataset (16 patients). If we want to calculate the Cost Function for the whole SBP dataset, using \u03b8 = [140.0; 5.0], we will type :", "The function will return J = 138.04, which is the Cost Function calculated for \u03b8 = [140.0; 5.0]. This J is not the minimum J that we could find, since that we have manually set \u03b8, without any idea about how to minimize it. The following Intuition II could help us in understanding better the limit of our manual approach.", "The following code generates randomly 10 \u03b8 vectors and passes them to the calcCostFunction, producing a table of the relative Cost Functions (J):", "The \u201ctake-home message\u201d is that trying to handly minimize J, is not the correct way to proceed. After 10 runs on randomly selected \u03b8\u2019s, the behavior of J is unpredictable. Moreover, there is no way to guess J basing on \u03b8. So the question is: How we can choose \u03b8, to find the minimum J? We need an algorithm that can minimize J for us, and this algorithm is the argument of the next Step.", "We are interested in finding the minimum of the Cost Function using Gradient Descent, which is an algorithm that can automatize this search. The Gradient Descent calculates the derivative of the Cost Function, updating the vector \u03b8 by mean of the parameter \u03b1, that is the learning rate. From this moment on, we will refer to the SBP dataset, as the training set. This clarification is essential since Gradient Descent will use the difference between the actual vector y of the dataset and the h vector prediction, to \u201clearn\u201d how to find the minimum J. The algorithm will repeat until it will converge. \u03b8 updating has to be simultaneous.", "Since we use Linear Algebra, the vectorized implementation is the following:", "Note that here we have to transpose X since X is a [16, 2] matrix and Error is a [16, 1] vector.", "The following Python code implements the Gradient Descent. We will use the vectorized form of Equation 5:", "To run the Gradient Descent, we have to initialize \u03b8, iterations, and \u03b1, that together with X and y are the arguments of the gradientDescent function:", "The results are collected in the \u201cresults\u201d list. This list is composed of the found \u03b8, plus two lists containing the \u03b8 and J histories. After 2000 iterations the Gradient Descent has found \u03b8 = [128.4, 9.9], and J = 59.7, which is the minimum J. We will use the two lists for plotting the Gradient Descent activity. The following code will plot the training set and h.", "The Hypothesis h now fits with our data!", "Now let\u2019s plot the J history:", "After circa 200 iterations the Cost Function falls down, stabilizing around 59.7 after 1500 iterations. The J curve depends on \u03b1, that we have set to 0.01.", "Now that we have found the best \u03b8, we can predict the Systolic Blood Pressure for a 75 years-old person. The query is a vector, composed of two numbers [1, 75]. The first number corresponds to the feature x_0. To run the prediction, we have to scale and normalize the query vector, using the mu and sigma parameters that we have calculated in Step 5: Feature Scaling and Normalization. The query vector will be [1, 1.29]. Then, we have to multiply the query for the \u03b8 vector (\u03b8 = [128.4, 9.95]). The following code implements the prediction.", "Let\u2019s do some experiments with the learning rate \u03b1. Changing \u03b1 will affect the dynamics of J. If \u03b1 is too little, the Gradient Descent will converge slowly, and we need to train it with more iterations for finding the minimum of J. Contrarily, if \u03b1 is too big, the Gradient Descent risks to never converge. Interestingly, for values of \u03b1 around 1.9, the Gradient Descent converges, but for the first 40 iterations, the behavior of \u03b8 is turbulent, for successively reaching stability. (Figure 8)", "We can create a contour plot, which is a graph containing many concentric tracks. For each track there are various pairs of \u03b8 associated with a constant value of J. The \u03b8 corresponding to the minimum J, lies at the center (the red dot). The other concentric lines correspond to all the different values of J. The most the distance from the center, the higher the value of the Cost Function J.", "Code 20 produces the following plot:", "Note that \u03b8 = [128.4; 9.9] corresponding to the minimum J (59.7), is the red dot in the center of the graph. The blue dots track the path of the Gradient Descent in converging to the minimum.", "We have explained the statistical mechanisms underlying Linear Regression with one variable: the feature Age in the SBP dataset. The major part of the code here proposed works also with multiple variables. The SBP dataset is composed of 2 features (Age and Weight) and one output: SBP. In this step, we will update the code in a way that it can fit with multiple variables. The only adjustments required concern:", "The code for uploading the dataset has to be modified in order to produce a new X vector containing Age and Weight of each patient:", "The numpy method used for producing the new X vector is .vstack() because we now want an X vector with two sets of distinct features.", "The code concerning Feature Scaling and Normalization is modified in vectors mu and sigma. Now the two vectors will accept two parameters each.", "Adding a column of \u201cones\u201d to the vector X", "The line for adding \u201cones\u201d to the X vector is modified as follows:", "The query for the predictions and the code for the normalization are modified as follows:", "With these changes, the Python code is ready for Linear Regression with multiple variables. You have to update the code every time you will add new features from your training set!", "I hope you find this post useful!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a data science passionate. My interests mainly encompass Machine Learning and Bioinformatics applications for Immuno-oncology and immunoinformatics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7d43461cdfa9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://luca-zammataro.medium.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Luca Zammataro"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ac72d0e7a58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&user=Luca+Zammataro&userId=5ac72d0e7a58&source=post_page-5ac72d0e7a58----7d43461cdfa9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/ml-for-bio-data", "anchor_text": "Machine Learning for Biomedical Data"}, {"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook"}, {"url": "https://numpy.org/", "anchor_text": "NumPy"}, {"url": "https://scipy.org/", "anchor_text": "SciPy.org"}, {"url": "https://www.geeksforgeeks.org/python-pandas-dataframe/", "anchor_text": "link to the GeeksForGeeks portal"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Andrew NG, Machine Learning | Coursera."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7d43461cdfa9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7d43461cdfa9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/biomedical?source=post_page-----7d43461cdfa9---------------biomedical-----------------", "anchor_text": "Biomedical"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----7d43461cdfa9---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/ml-for-bio-data?source=post_page-----7d43461cdfa9---------------ml_for_bio_data-----------------", "anchor_text": "Ml For Bio Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&user=Luca+Zammataro&userId=5ac72d0e7a58&source=-----7d43461cdfa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&user=Luca+Zammataro&userId=5ac72d0e7a58&source=-----7d43461cdfa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7d43461cdfa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7d43461cdfa9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7d43461cdfa9--------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Luca Zammataro"}, {"url": "https://luca-zammataro.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "100 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ac72d0e7a58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&user=Luca+Zammataro&userId=5ac72d0e7a58&source=post_page-5ac72d0e7a58--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6cc40fdafc55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-for-biomedical-data-linear-regression-7d43461cdfa9&newsletterV3=5ac72d0e7a58&newsletterV3Id=6cc40fdafc55&user=Luca+Zammataro&userId=5ac72d0e7a58&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}