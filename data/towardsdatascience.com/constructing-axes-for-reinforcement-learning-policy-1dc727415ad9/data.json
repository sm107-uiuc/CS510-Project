{"url": "https://towardsdatascience.com/constructing-axes-for-reinforcement-learning-policy-1dc727415ad9", "time": 1683016175.699751, "path": "towardsdatascience.com/constructing-axes-for-reinforcement-learning-policy-1dc727415ad9/", "webpage": {"metadata": {"title": "Constructing Axes for Reinforcement Learning Policy | by Nathan Lambert | Towards Data Science", "h1": "Constructing Axes for Reinforcement Learning Policy", "description": "Reinforcement learning (RL) \u2014 the framework of interacting with an environment to learn a policy to act and achieve some objective \u2014 is on the up in many domains. Some domains make sense, some will\u2026"}, "outgoing_paragraph_urls": [{"url": "https://democraticrobots.substack.com/p/recommendations-are-a-game-a-dangerous", "anchor_text": "recommender systems", "paragraph_index": 0}, {"url": "https://geesegraduates.org/blog/", "anchor_text": "GEESE", "paragraph_index": 2}, {"url": "https://cltc.berkeley.edu/", "anchor_text": "CLTC", "paragraph_index": 2}, {"url": "https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40", "anchor_text": "big technology players have applied RL", "paragraph_index": 4}, {"url": "https://livrepository.liverpool.ac.uk/3034527/1/RevisedManuscript_ReinforcementLearning.pdf", "anchor_text": "1", "paragraph_index": 4}, {"url": "https://ieeexplore.ieee.org/abstract/document/8834806", "anchor_text": "2", "paragraph_index": 4}, {"url": "https://www.sciencedirect.com/science/article/pii/S0378779620304193", "anchor_text": "3", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/2010_flash_crash", "anchor_text": "one flash crash", "paragraph_index": 5}, {"url": "https://dennybritz.com/blog/ai-trading", "anchor_text": "Here is a good blog post in the area", "paragraph_index": 5}, {"url": "https://www.nabla.com/blog/gpt-3/", "anchor_text": "this blog probed", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Maxwell%27s_equations", "anchor_text": "Maxwell\u2019s equations", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1911.10635", "anchor_text": "overview", "paragraph_index": 16}, {"url": "http://robotic.substack.com", "anchor_text": "Democratizing Automation", "paragraph_index": 21}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 23}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 23}], "all_paragraphs": ["Reinforcement learning (RL) \u2014 the framework of interacting with an environment to learn a policy to act and achieve some objective \u2014 is on the up in many domains. Some domains make sense, some will never work, and most are bound to fall somewhere in the middle. RL is so popular because of its elegance: we know that most creatures learn by interacting with their environment (but, we should note that evolutions are really a strong prior on these behaviors that we don\u2019t know how to model). The problem is RL has deleterious effects on our society at individual and group levels (see my writing on recommender systems) \u2014 this is not a piece to describe those problems, but teasing at how we may address them in a more precise manner.", "Recommender systems are, although, a great reminder of how difficult (and sometimes backward) it is to model society as a reinforcement learning problem. From the point of view of the engineer, recommender systems are an RL loop where the consumer (us) is the environment, the application is the agent, and money is the reward. It is troubling that technology companies are going to explore your inner workings as the environment for their research. The engineering problem ultimately turns into a game between the designer/user and the agent \u2014 RL traditionally is very sensitive to tweaks and sudden, unexpected changes in behavior. After a couple of examples of real-world RL, I propose three initial axes to start a discussion on RL policy:", "I\u2019ve been in a reading group with Berkeley\u2019s Graduates for Engaged and Extended Scholarship in Computing and Engineering (GEESE) and the Center for Long-Term Cybersecurity (CLTC), hoping to culminate in a white paper on the topic (thanks and credit to collaborators).", "RL is being used in the real world in positive ways, and I am an optimist that it will work in more domains than initially expected. Here are a few examples to get the thoughts going:", "Power systems have proven very tractable for study with reinforcement learning. It can be used to manage network load, plan production at different facilities, and more. Many of the papers leverage accurate simulation of the dynamics and fail-safes so that the learning process doesn\u2019t cause any weird behaviors. The history of studying power dynamics in electrical engineering provided clear bounds and fail-safes for the new learning-based technology. These themes behind the power grid is why some big technology players have applied RL to optimization of data-center operations. The system is closed in its abstractions, has accurate to-model dynamics, and reasonable to-model goals. [See 1, 2, 3]", "Automated trading may be the most logical problem space for RL \u2014 the reward function is to make money and keep making money. The actions are to buy and sell, but how do you put a bounding box on the underlying environment \u2014 it extends beyond just moving money to all the end-users\u2019 bank accounts. Another problem is that the action space is so diverse and hard to constrain (the constraints are set by mistakes humans have made in the past, and the one flash crash), and the dynamics are very unknown. The uncertainty has not stopped people from making a ton of money in the area, but I don\u2019t think we are done with flash crashes and other periods where the stock market seems separated from reality. Here is a good blog post in the area. I think people will continue to make money in the area, we won\u2019t see much regulation unless something truly breaks, and the money-market to end-user dynamics will be impossible to capture (people don\u2019t always like to talk about where their money is).", "RL for medical treatment could very well look like a computer figuring out how to make new drugs or administer previously unstudied combinations (sounds alright in an offline fashion), but it can also take many forms and is intentionally vague (highlights the challenge). Current medical policy could inform future data-driven medicine policy, but this may be too much optimism. Specifically, I bring up medical treatment because RL has a lot to learn from this area across applications beyond just medicine. Clinical trials in medicine compare any new method to the current standard of care. Implementing this standard in RL would be that the change from going to the current human-centric medical system (or other processes) to a data-driven system would not decrease performance at an individual level (even if it\u2019s cheaper to send someone a computer). This performance comparison should be made irrespective of access (human doctors are more expensive because they are scarce) and would prevent too early adoption of computer-docs. If you\u2019re interested as to if data-medicine is ready, this blog probed the intuitions of the Reddit-corpus GPT3 on medical ideas, and it didn\u2019t perform great.", "Here is my first pass at sketching RL policy. Such a policy should not prohibit studying RL in systems in extreme areas of one of these axes, but it should require additional oversight or better data-practices. An exciting area for slowing the feedback loop for RL is limiting some areas of study to offline RL \u2014 the process of distilling logged data to a policy and updating occasionally rather than consistently.", "Consider the difference from power systems to medical treatment: we know Maxwell\u2019s equations and how they work in engineering systems, but we do not know the human body (relatively). Modeling the physics behind power systems lets the machine-learning engineer be bounded by physical realities \u2014 we know when certain actions will turn the power off and they are removed from the action space (this does make the learning problem slightly harder, albeit).", "This bound makes sure we don\u2019t cause harm at the low-level of the system we are controlling.", "My biggest gripe for reinforcement learning in the real world is when the targeted end-user, e.g. the news feed in a social network, clearly has ramifications beyond just the individual users. This axis seems very similar to the above, but I separate it intentionally. For example, the potential harm for a datacenter costing more to run versus the power grid is very different (we don\u2019t necessarily know how much harm Google going down, would actually cause though). This axis could be refined more, but I think some sense of scale is important. We can apply RL to our smart fridge, but maybe not to the food distribution infrastructure.", "This box lets us consider the scale for potential damage outside the bounds of the control algorithm.", "The medical example highlights the potential for existing normative behaviors and regulation helping to define a problem space. Power systems are in the middle on this: being a utility helps constrain some actions. Other examples are further off. RL policy, and other technology litigation, should lean on this existing scaffolding. Knowing that the Food and Drug Administration (FDA) and other government agencies were founded by patching over historical errors, I think this is a hard trend to be ahead on, but I am a proponent for founding a Data and Algorithms Administration.", "This axis lets us remember that there is more potential harm in applying RL when the environment and agent are relatively free systems (cough, cough, social media).", "Google Maps algorithms could probably be improved by RL (maybe they are? Let me know). Transportation and routing have nice abstractions, rewards, and dynamics. First, consider medicine versus transportation: medicine is a much more regulated, and more formal normative structure to be navigated. People are excited to apply RL to transportation \u2014 transportation is ripe for the extraction of reward functions and easier than things like medicine that have more normative content. Normative content is correlated with how badly people want to maintain the \u201chumanness.\u201d Transportation is an example where at-scale RL could translate into complex multi-agent interactions and unintended problems (e.g. autonomous vehicles make normally non-traveled roads un-usable for pedestrians).", "Transportation I would put safe on axis 1 (essentially traversing a graph), moderate on axis 2 (not sure how autonomous routing will go because it\u2019s such a massive system), and safe on axis 3 (we have a lot of rules for cars to follow already).", "Multi-agent RL (MARL) can be defined in many ways, but it is where at-scale reinforcement learning with individual fine-tuning is going. It is really relevant to discuss some documented key challenges of MARL (see overview):", "I am interested in formulating my view of the social media world through the lens of cooperative vs competitive games (only some are zero-sum), sequential vs parallel MDP, centralized vs de-centralized control, and more. I have become increasingly interested in MARL from a research perspective because it is what most app-based algorithms are doing to us. The current consensus in MARL is that almost any complex reward function is intractable, so that\u2019s the experiment all the tech companies decided to run!", "That last sentence was intentionally a lot, flipping the subject-object ordering for some drama. But, it seriously seems like it: tech companies determine more and more of our interactions each day, and, while they are optimizing for profits, each individual agent (us) has our reward functions. The downstream effects are playing out.", "I\u2019m sure I will touch on this more in the future, as I have in the past with the recommender systems post. This election season has more weight on my need to work in community and common-good building technology, maybe I should start with a mental-health improving and long-term oriented social network.", "We all need more positive energy and a bit more discursive engagement.", "This was a peak to an issue of my free newsletter on robotics & automation, Democratizing Automation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1dc727415ad9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----1dc727415ad9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://democraticrobots.substack.com/p/recommendations-are-a-game-a-dangerous", "anchor_text": "recommender systems"}, {"url": "https://geesegraduates.org/blog/", "anchor_text": "GEESE"}, {"url": "https://cltc.berkeley.edu/", "anchor_text": "CLTC"}, {"url": "https://www.pexels.com/@ken123films?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Kendall Hoopes"}, {"url": "https://www.pexels.com/photo/architecture-building-capitol-dawn-616852/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40", "anchor_text": "big technology players have applied RL"}, {"url": "https://livrepository.liverpool.ac.uk/3034527/1/RevisedManuscript_ReinforcementLearning.pdf", "anchor_text": "1"}, {"url": "https://ieeexplore.ieee.org/abstract/document/8834806", "anchor_text": "2"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0378779620304193", "anchor_text": "3"}, {"url": "https://en.wikipedia.org/wiki/2010_flash_crash", "anchor_text": "one flash crash"}, {"url": "https://dennybritz.com/blog/ai-trading", "anchor_text": "Here is a good blog post in the area"}, {"url": "https://www.nabla.com/blog/gpt-3/", "anchor_text": "this blog probed"}, {"url": "https://en.wikipedia.org/wiki/Maxwell%27s_equations", "anchor_text": "Maxwell\u2019s equations"}, {"url": "https://arxiv.org/abs/1911.10635", "anchor_text": "overview"}, {"url": "http://robotic.substack.com", "anchor_text": "Democratizing Automation"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1dc727415ad9---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ethics?source=post_page-----1dc727415ad9---------------ethics-----------------", "anchor_text": "Ethics"}, {"url": "https://medium.com/tag/regulation?source=post_page-----1dc727415ad9---------------regulation-----------------", "anchor_text": "Regulation"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----1dc727415ad9---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----1dc727415ad9---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&user=Nathan+Lambert&userId=890b1765e6d&source=-----1dc727415ad9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&user=Nathan+Lambert&userId=890b1765e6d&source=-----1dc727415ad9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1dc727415ad9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1dc727415ad9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1dc727415ad9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1dc727415ad9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1dc727415ad9--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstructing-axes-for-reinforcement-learning-policy-1dc727415ad9&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}