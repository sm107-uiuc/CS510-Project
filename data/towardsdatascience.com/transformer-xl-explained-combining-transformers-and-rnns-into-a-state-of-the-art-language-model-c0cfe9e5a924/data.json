{"url": "https://towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924", "time": 1682994690.6353128, "path": "towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924/", "webpage": {"metadata": {"title": "Transformer-XL Explained: Combining Transformers and RNNs into a State-of-the-art Language Model | by Rani Horev | Towards Data Science", "h1": "Transformer-XL Explained: Combining Transformers and RNNs into a State-of-the-art Language Model", "description": "Language modeling has become an important NLP technique thanks to the ability to apply it to various NLP tasks, such as machine translation and topic classification. Today, there are two leading\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer-XL", "paragraph_index": 1}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradients", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers", "paragraph_index": 4}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog post", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1808.04444", "anchor_text": "Al-Rfou et al.", "paragraph_index": 7}, {"url": "https://github.com/kimiyoung/transformer-xl/", "anchor_text": "implemented", "paragraph_index": 19}, {"url": "https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/", "anchor_text": "BERT", "paragraph_index": 21}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI", "paragraph_index": 22}], "all_paragraphs": ["Language modeling has become an important NLP technique thanks to the ability to apply it to various NLP tasks, such as machine translation and topic classification. Today, there are two leading architectures for language modeling \u2014 Recurrent Neural Networks (RNNs) and Transformers. While the former handles the input tokens \u2014 words or characters \u2014 one by one to learn the relationship between them, the latter receives a segment of tokens and learns the dependencies between at once them using an attention mechanism.", "Though both architectures have reached impressive achievements, their main limitation is capturing long-term dependencies, e.g. use of important words from the beginning of the document to predict words in a subsequent part. A new paper by Google and Carnegie Mellon University, \u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context\u201d, combines these two approaches. The new model uses the Transformer\u2019s attention modules on each segment of input data and a recurrence mechanism to learn dependencies between consecutive segments.", "Transformer-XL achieves state-of-the-art (SOTA) results on multiple language modeling datasets such as enwik8 (word-level) and text8 (character-level), while being significantly faster during inference (300x-1800x) than the previous SOTA Transformer architecture.", "A popular approach for language modeling is Recurrent Neural Networks (RNNs) as they capture dependencies between words well, especially when using modules such as LSTM. However, RNNs tend to be slow and their ability to learn long-term dependencies is still limited due to vanishing gradients.", "Transformers, invented in 2017, introduced a new approach \u2014 attention modules. Instead of processing tokens one by one, attention modules receive a segment of tokens and learn the dependencies between all of them at once using three learned weight matrices \u2014 Query, Key and Value \u2014 that form an Attention Head. The Transformer network consists of multiple layers, each with several Attention Heads (and additional layers), used to learn different relationships between tokens.", "As in many NLP models, the input tokens are first embedded into vectors. Due to the concurrent processing in the attention module, the model also needs to add information about the order of the tokens, a step named Positional Encoding, that helps the network learn their position. In general, this step is done with a sinusoidal function that generates a vector according to the token\u2019s position, without any learned parameters.", "Note: An in-depth review of Transformers can be found in Jay Alammar\u2019s great blog post.", "While the original Transformers were used for machine translation (with an encoder-decoder mechanism), Al-Rfou et al. presented an architecture for language modeling. Its goal is to predict a character in a segment based on its previous characters, for example, it predicts character Xn using X1\u2026Xn-1, while the next characters to the right are masked (See image below). This 64-layer Transformer model is limited to relatively short inputs of 512 characters, therefore it splits the input to into segments and learns from each one separately. To process longer inputs in evaluation, it predicts one character at a time by shifting the input by one in each step.", "This model outperforms RNN models on popular benchmarks (enwik8 and text8), however, it still suffers from two shortcomings:", "Transformer-XL heavily relies on the vanilla Transformer (Al-Rfou et al.) but introduces two innovative techniques \u2014 Recurrence Mechanism and Relative Positional Encoding \u2014 to overcome vanilla\u2019s shortcomings. An additional advantage over the vanilla Transformer is that it can be used for both word-level and character-level language modeling.", "The goal of the recurrence mechanism is to enable long-term dependencies by using information from previous segments. Similarly to the vanilla version, Transformer-XL processes the first segment of tokens but keeps the outputs of the hidden layers. When the following segment is processed, each hidden layer receives two inputs:", "Technically, the two inputs are concatenated and then used to calculate the Key and the Value matrices of the (current Head of the current layer of the) current segment. This addition provides the network with more information in regards to the weights (importance) of each token, but it doesn\u2019t change the Value matrix.", "The concept can be expanded to incorporate longer dependencies by using information from several previous segments in the same way (under the limitations of the GPU memory), even only during evaluation.", "Another advantage of the recurrence mechanism is its speed in evaluation \u2014 In each step, it can advance by an entire segment (and not by one token as in the vanilla version) and use the previous segments\u2019 data to predict the current segment tokens.", "The recurrence mechanism also introduces a new challenge \u2014 The original positional encoding handles each segment separately and, as a result, tokens from different segments have the same positional encoding. For example, the first token of the first and the second segments will have the same encoding, although their position and importance are different (the one from the first segment is probably lower). This confusion might affect the network incorrectly.", "Instead, the paper presents a new positional encoding that is part of each attention module, as opposed to encoding position only before the first layer, and is based on the relative distance between tokens and not their absolute position. Technically, it expands the simple multiplication of the Attention Head\u2019s Score (Qi\u22c5Kj) to include four parts:", "The authors compared the model\u2019s performance on word-level and character-level datasets and compared them to other prominent models (RNNs and Transformers). Transformer-XL achieved state-of-the-art (SOTA) results on several different datasets benchmarks:", "The benefits of the recurrence mechanism and the relative positional encoding can be seen in the following chart. It compares the perplexity score without the recurrence or the new encoding for different context lengths (number of previous tokens used in the attention head). The full Transformer-XL significantly outperforms the others and is able to exploit longer-term dependencies. In addition, it\u2019s also capable of capturing longer dependencies than RNN (80% longer).", "Lastly, as mentioned before, the model is also significantly faster during inference than the vanilla Transformer, especially for longer contexts. For example, for context-length of 800 characters, it\u2019s 363 times faster and 1,874 times faster for 3,800 characters.", "The model is open-source and is implemented in both TensorFlow and PyTorch (including pre-trained models). Training duration for each dataset wasn\u2019t specified.", "Transformer-XL presents state-of-the-art results for language modeling on several different datasets (big/small, characters/words, etc). Its combination of two prominent concepts of deep learning \u2014 recurrence and attention \u2014 allows the model to learn long-term dependencies and might be effective to other fields of deep learning that require that capability, such as audio analysis (e.g speech data with 16k samples per second).", "This model hasn\u2019t been tested yet on NLP tasks like sentiment analysis or question answering, and it\u2019s still an open question what the benefit from this strong language model will be compared to other Transformer-based models such as BERT.", "To stay updated with the latest Deep Learning research, subscribe to my newsletter on LyrnAI", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learn something new every day. Currently Deep Learning :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc0cfe9e5a924&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ranihorev?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8----c0cfe9e5a924---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer-XL"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM"}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradients"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog post"}, {"url": "https://arxiv.org/abs/1808.04444", "anchor_text": "Al-Rfou et al."}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer-XL"}, {"url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/", "anchor_text": "WikiText-103"}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity"}, {"url": "https://arxiv.org/abs/1809.10853", "anchor_text": "Baevski & Auli"}, {"url": "http://mattmahoney.net/dc/textdata.html", "anchor_text": "enwik8"}, {"url": "https://arxiv.org/abs/1808.04444", "anchor_text": "Al-Rfou et al."}, {"url": "http://www.statmt.org/lm-benchmark/", "anchor_text": "One Billion Word"}, {"url": "https://catalog.ldc.upenn.edu/LDC99T42", "anchor_text": "Penn Treebank"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer-XL"}, {"url": "https://github.com/kimiyoung/transformer-xl/", "anchor_text": "implemented"}, {"url": "https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/", "anchor_text": "BERT"}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c0cfe9e5a924---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c0cfe9e5a924---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c0cfe9e5a924---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/lyrnai?source=post_page-----c0cfe9e5a924---------------lyrnai-----------------", "anchor_text": "Lyrnai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----c0cfe9e5a924---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----c0cfe9e5a924---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc0cfe9e5a924&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c0cfe9e5a924---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c0cfe9e5a924--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/@ranihorev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9bc3579798b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924&newsletterV3=53f9e9fdd8d8&newsletterV3Id=9bc3579798b7&user=Rani+Horev&userId=53f9e9fdd8d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}