{"url": "https://towardsdatascience.com/let-the-machine-write-next-16ba1a0cbdda", "time": 1683008672.383861, "path": "towardsdatascience.com/let-the-machine-write-next-16ba1a0cbdda/", "webpage": {"metadata": {"title": "Let the machine write next..!. \u2014 Text generation using Vanilla LSTM\u2026 | by Khushali Vaghani | Towards Data Science", "h1": "Let the machine write next..!", "description": "As time flies from the seventeenth century when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages to 1950 when Alan Turing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz", "anchor_text": "Leibniz", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Descartes", "anchor_text": "Descartes", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Alan_Turing", "anchor_text": "Alan Turing", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence", "anchor_text": "Computing Machinery and Intelligence", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1906.00080.pdf", "anchor_text": "paper", "paragraph_index": 2}, {"url": "https://www.universetoday.com/", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://github.com/khushi810/Text-Generation/blob/master/universal-today-scrapper.py", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy?version=nightly", "anchor_text": "sparse_categorical_cross_entropy", "paragraph_index": 23}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop?version=nightly", "anchor_text": "RMSprop", "paragraph_index": 24}, {"url": "https://www.tensorflow.org/guide/eager#eager_training", "anchor_text": "eager execution", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "this", "paragraph_index": 27}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "this", "paragraph_index": 27}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1906.00080", "anchor_text": "Gmail smart compose: Real-time assisted writing", "paragraph_index": 45}, {"url": "https://github.com/khushi810/Text-Generation", "anchor_text": "here", "paragraph_index": 47}, {"url": "https://www.linkedin.com/in/khushali-vithani/", "anchor_text": "LinkedIn", "paragraph_index": 48}], "all_paragraphs": ["Objective: Generate new sentences automatically in continuation of given input sentences.", "As time flies from the seventeenth century when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages to 1950 when Alan Turing published an article titled \u201cComputing Machinery and Intelligence\u201d \u2014 NLP(Natural language processing) was set to be explored. NLP has seen the evolution from complex sets of hand-written rules (1980) to statistical models, machine learning models to deep learning models. Today, tasks from NLP like Text classification, Text summarization, Question-Answering systems, Language modelling, Speech recognition, Machine translation, Caption Generation, Text generation are leveraging the state of the art models like Transformer, BERT, Attention. Although nuts and bolts are \u2014 \u2018Neurons\u2019, their connectivity, training, learning and behaviour depend on model to model like in RNNs, seq2seq, Encoder-Decoder, Attention, Transformers, etc. Let\u2019s explore one of the tasks \u2014 Text Generation.", "What if instead of note down the sentences manually suppose our well designed and learnt system either generate or suggest the next paragraph or sentences? It will make writing easier. It will suggest the continuation of a sentence. It will generate new headlines or stories or articles or chapters of a book. Gmail is also taking advantages of this concept in Auto-reply and Smart-compose(paper).", "Develop a model, which can learn to generate the next sentences or words in continuation and can complete the paragraph or content.", "As we all are aware of the abilities of DL to learn from data as ANNs were inspired by information processing and distributed communication nodes in biological systems.", "So, we can pose this problem as DL task, in which we will feed our model with data and train it in such a way that it can predict next sequences. We will use all tools and techniques of DL to make our model learn itself.", "I have used data on space and astronomy news from here. I have scrapped almost 3700 articles from the website. You can also find the script for scrapping here.", "As all articles are scraped from the web, so they contain some extra information regarding adds, pop-ups, video links, information about sharing on various social platforms, etc. Therefore it\u2019s better to clean data properly, as our model will learn from the gathered data.", "Here it comes the turn to explore and understand data. It is often said in the field of Data Science that \u2018The more you understand the Data, the more your model can learn from it\u201d! So let\u2019s try to explore it.", "Our primary goal is to make our text data clean and crisp so that our model can learn and understand the patterns and behaviour of the sentences in English.", "We will analyze words as well as characters from our text data. Let\u2019s analyze various plots for total words and total characters from our articles.", "After splitting the train and validation set we will now analyze text data from each set separately.", "Let\u2019s analyze words from the training part.", "Analysis of characters from train and validation set:", "Analysis of Words from train and validation set:", "We can use either a character-based or word-based approach to solving this problem. The character-based approach might end up with the sentences with incorrect spellings so let\u2019s focus on word-based method, in which we will replace each and every word with some fixed number, as models only understand numbers, not words! (We will create embedding matrix from pre-trained 300-dim glove vectors (here) for words present in our train data set).", "Below function is to convert text data to numbers.", "Let\u2019s create an embedding matrix for words present in the train data set only. It is to avoid data leakage problem. The words whose vectors are not present in the glove model, we will generate random vectors for them instead of assigning them with zero values. We will use random.seed() to reproduce same random vectors again if we execute the snippet again.", "As we have analyzed our articles are having different word_counts so let\u2019s decide to fix data_length and pad them.", "Our task is to handle sequence data for which we will use Recurrent Neural Networks (LSTM to be more precise!). The main aspect is to grab the data and pass through the model.", "We will experiment with three models: 1). LSTM 2). Attention 3). GPT-2 (Generative Pre-trained Transformer). Among them, we will build the first two models. So let\u2019s start building one by one.", "LSTMs are much more powerful in remembering sequence information along with states. Christopher Olah has written a terrific blog to understand about LSTM which you can find here. We will use stateful LSTM along with return_state to maintain sequence information during training as well as the inference stage. So without further delay let\u2019s start building model using subclass and tf.keras.Model API.", "The above class will implement LSTM functionalities and return output generated by model along with hidden states, which we will use during predictions.", "We will use sparse_categorical_cross_entropy as a loss function. The reason behind its use is to reduce some computation time as there is no need to encode labels in one-hot-encodings. We can encode our labels in 2-dimensions only so it will speed up the process. We will also set from_logits=True thus there is no need to convert our logits into probabilities which will result in an output layer without activation function.", "From various optimizers, we will use RMSprop, as it gives better performance for our dataset.", "We will use TensorFlow\u2019s eager execution for training as it evaluates operations immediately, without building graphs and help us to debug our model. We will write our loss and gradients for tensorboard and save our model for inference stage.", "We will do prediction and some experiments in the next section.", "Bahdanau has discussed Attention-based approach in this paper. It is basically giving more weightage to the words in sequence. These weights are learned through another neural network. Here, in this blog, you can find details about various attention algorithms. In our model, we will implement Bahadanau style attention. Code is inspired from here the task is language translation, but in our task, we can experiment with it.", "So without further delay let\u2019s dig into it.", "The encoder will take an input sequence of words and pass through the LSTM layer via Embedding and return its outputs and hidden states.", "The decoder is used to concatenate the context vector with its inputs and generate outputs.", "We will use the same loss function and optimizers. Training will be done the same as above, but here we will feed encoder with the first sequence of words and decoder with its next sequence of words. So our batch generator is a little bit different from the first one. In the decoder mechanism, we will use \u2018Teacher-Forcing\u2019 mechanism.", "Inference using Attention model is discussed in the upcoming section.", "The above function will fine-tune with our data and learn from it. For experiments, we have to jump into Inference section, which is just below:)", "For the inference part, we will use the best weights from the checkpoint. So let\u2019s predict and see it.", "We will take samples from random categorical distribution from logits. And convert them into words.", "we will initialize our model with batch_size=1 and restore the trainedweights from the checkpoint.", "Here, we are also passing hidden states from LSTM to maintain the state information according to the sequence of words. Let\u2019s see some results!", "From the result, it can be said that it can understand little bit arrangement of verbs, nouns, adjectives.", "But what about meaningful sentences? Well, our GPT-2 will take care of it.", "Wooo! As we can see it generates sentences like it really knows about space!! Let\u2019s see some other results.", "We can see that it is also able to connect the full form of ISRO- Indian Space Research Organization. It can also relate the month July of 2019 as it is the month of launch. We can see that it is really powerful.", "Now let\u2019s give sentence which is not related to space and see what it will do.", "It can also generate a movie title related to space and can include details about the movie without teaching. Isn\u2019t it amazing?", "we did experiment first two models with different batch_size, sequence_length, lstm_units, epochs and learning_rate. we got minimum loss in GPT-2 model and about generated sentences we have already seen GPT-2 is the best. Below is the table of comparison.", "[2] Chen MX, Lee BN, Bansal G, Cao Y, Zhang S, Lu J, Tsay J, Wang Y, Dai AM, Chen Z, Sohn T. Gmail smart compose: Real-time assisted writing. InProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining 2019 Jul 25 (pp. 2287\u20132295).", "Thank you for your interest. You can leave comments, feedback or anysuggestions if you feel any.", "You can find complete code on my Github repo (here).", "Happy to connect with you on LinkedIn :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F16ba1a0cbdda&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@khushi810?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@khushi810?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Khushali Vaghani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb02315d65487&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&user=Khushali+Vaghani&userId=b02315d65487&source=post_page-b02315d65487----16ba1a0cbdda---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@laurachouette?utm_source=medium&utm_medium=referral", "anchor_text": "Laura Chouette"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz", "anchor_text": "Leibniz"}, {"url": "https://en.wikipedia.org/wiki/Descartes", "anchor_text": "Descartes"}, {"url": "https://en.wikipedia.org/wiki/Alan_Turing", "anchor_text": "Alan Turing"}, {"url": "https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence", "anchor_text": "Computing Machinery and Intelligence"}, {"url": "https://arxiv.org/pdf/1906.00080.pdf", "anchor_text": "paper"}, {"url": "https://www.universetoday.com/", "anchor_text": "here"}, {"url": "https://github.com/khushi810/Text-Generation/blob/master/universal-today-scrapper.py", "anchor_text": "here"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/guide/data", "anchor_text": "tf.data"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy?version=nightly", "anchor_text": "sparse_categorical_cross_entropy"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop?version=nightly", "anchor_text": "RMSprop"}, {"url": "https://www.tensorflow.org/guide/eager#eager_training", "anchor_text": "eager execution"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "this"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "this"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/ImageNet", "anchor_text": "Image-Net"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "this"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformer"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "here"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "blog"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks.", "anchor_text": "transfer-learning"}, {"url": "https://github.com/minimaxir/gpt-2-simple", "anchor_text": "here"}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "blog"}, {"url": "https://arxiv.org/abs/1803.07133", "anchor_text": "Neural text generation: Past, present and beyond"}, {"url": "https://arxiv.org/abs/1906.00080", "anchor_text": "Gmail smart compose: Real-time assisted writing"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "https://www.tensorflow.org/tutorials/text/text_generation"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "https://www.tensorflow.org/tutorials/text/nmt_with_attention"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "https://openai.com/blog/better-language-models/"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "http://jalammar.github.io/illustrated-transformer/"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "http://jalammar.github.io/illustrated-gpt2/"}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "https://minimaxir.com/2019/09/howto-gpt2/"}, {"url": "https://colab.research.google.com/", "anchor_text": "https://colab.research.google.com/"}, {"url": "https://www.appliedaicourse.com/course/11/Applied-Machine-", "anchor_text": "https://www.appliedaicourse.com/course/11/Applied-Machine-"}, {"url": "https://github.com/khushi810/Text-Generation", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/khushali-vithani/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/text-generation?source=post_page-----16ba1a0cbdda---------------text_generation-----------------", "anchor_text": "Text Generation"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----16ba1a0cbdda---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/attention?source=post_page-----16ba1a0cbdda---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/gpt-2?source=post_page-----16ba1a0cbdda---------------gpt_2-----------------", "anchor_text": "Gpt 2"}, {"url": "https://medium.com/tag/lstm?source=post_page-----16ba1a0cbdda---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&user=Khushali+Vaghani&userId=b02315d65487&source=-----16ba1a0cbdda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&user=Khushali+Vaghani&userId=b02315d65487&source=-----16ba1a0cbdda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F16ba1a0cbdda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----16ba1a0cbdda---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----16ba1a0cbdda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@khushi810?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@khushi810?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Khushali Vaghani"}, {"url": "https://medium.com/@khushi810/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "14 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb02315d65487&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&user=Khushali+Vaghani&userId=b02315d65487&source=post_page-b02315d65487--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcb713bb96088&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-the-machine-write-next-16ba1a0cbdda&newsletterV3=b02315d65487&newsletterV3Id=cb713bb96088&user=Khushali+Vaghani&userId=b02315d65487&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}