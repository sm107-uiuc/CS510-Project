{"url": "https://towardsdatascience.com/the-fundamentals-of-reinforcement-learning-177dd8626042", "time": 1683012412.407637, "path": "towardsdatascience.com/the-fundamentals-of-reinforcement-learning-177dd8626042/", "webpage": {"metadata": {"title": "The Fundamentals of Reinforcement Learning | by Ruben Winastwan | Towards Data Science", "h1": "The Fundamentals of Reinforcement Learning", "description": "Reinforcement learning is probably one of the most relatable scientific approaches that resemble the way humans learn about things. Every day, we, the learner, learn by interacting with our\u2026"}, "outgoing_paragraph_urls": [{"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "Reinforcement Learning: An Introduction book by Richard Sutton and Andrew Barto", "paragraph_index": 59}, {"url": "https://www.coursera.org/learn/fundamentals-of-reinforcement-learning?", "anchor_text": "Fundamentals of Reinforcement Learning course by the University of Alberta on Coursera", "paragraph_index": 59}, {"url": "https://github.com/marcellusruben/Dynamic-Programming-Reinforcement-Learning", "anchor_text": "GitHub page", "paragraph_index": 61}], "all_paragraphs": ["Reinforcement learning is probably one of the most relatable scientific approaches that resemble the way humans learn about things. Every day, we, the learner, learn by interacting with our environment to know what to do in certain situations, to know about the consequences of our actions, and so on.", "When we were babies, we didn\u2019t know that touching a hot kettle would hurt our hand. However, as we learned about how the environment responded to our action, i.e touching the hot kettle hurts our hand, then we learned not to touch a hot kettle. This illustrates the fundamental theory of reinforcement learning.", "Reinforcement learning is learning what to do in order to maximize a numerical reward. This means that the learner should discover which actions that yield the highest reward in the long run by trying them.", "In this article, I want to discuss the fundamentals behind reinforcement learning which includes: Markov decision process, policies, value function, Bellman equation, and of course, dynamic programming.", "Markov decision process is the fundamental problem which we try to solve in reinforcement learning. But, what is the definition of Markov Decision Process?", "Markov Decision Process or MDP is a formulation of sequential interaction between agent and environment.", "Here, the learner and decision maker is called agent, where the thing it interacts with is called environment. In MDP, the agent makes certain decisions or actions, then the environment responds by giving the agent a new situation or state and immediate reward.", "In reinforcement learning, the main goal of an agent is to make a decision or action that maximizes the total amount of rewards it received from the environment in the long run.", "Let\u2019s say we want to train a robot to play the game of chess. Each time the robot win the game, the reward would be +1 and if it loses the game, the reward would be -1. In another example, if we want to train the robot to escape from a maze, the reward would decrease by -1 as the more time has passed prior to the escape.", "The reward in reinforcement learning is the way how you communicate to the agent what you want it to achieve, not how you want it achieved.", "Now the question is, how do we compute the cumulative amount of rewards that have been gathered by the agent after sequences of actions? The mathematical formulation of cumulative reward is defined as follows.", "Above, R is the reward in each sequence of action made by the agent and G is the cumulative reward or expected return. The goal of the agent in reinforcement learning is to maximize this expected return G.", "However, the equation above only applies when we have an episodic MDP problem, meaning that the sequence of agent-environment interaction is episodic or finite. What if we have a situation where the interaction between agent-environment is continuous and infinite?", "Suppose we have a problem where the agent works like an air conditioner, its task is to adjust the temperature given certain situations or states. In this problem:", "In order to avoid negative rewards, the agent needs to learn and interact with the environment continuously, meaning that there is no end of MDP sequence. To solve this continuous task from the agent, we can use the discounted expected returns.", "In the equation above, \u03b3 is the discount rate, where its value should be within the range 0 \u2264 \u03b3 \u22641.", "The intuition behind this discount rate is that the reward the agent received in earlier sequence will be worth more than the reward it received several sequences later. This assumption also makes sense in real life. 1 Euro in today\u2019s life is worth more than 1 Euro several years in the future because of the inflation.", "If \u03b3= 0, this means that the agent is short-sighted, meaning that it takes more weight for immediate rewards of an action in the next sequence.", "If \u03b3 is closer to 1, this means that the agent is far-sighted, meaning that it puts more and more weight for future rewards.", "With the discounted return, as long as the reward is non-zero and \u03b3 < 1, the output of expected return would no longer be infinite.", "Now we know that the goal of an agent in reinforcement learning is to maximize the cumulative reward. In order to maximize the reward, the agent needs to choose which action it needs to take in a given state such that it gets a high cumulative reward. The probability of an agent choosing a certain action in a given state is called policy.", "Policy is a probability of an agent selecting an action A in a given state S.", "In reinforcement learning, a policy is normally represented by \u03c0. This means that \u03c0(A|S) is the probability of the agent choosing action A given that it is in state S.", "Now if an agent is in state S and it follows policy \u03c0, then its expected return will be called state-value function for policy \u03c0. Thus, the state-value function is normally denoted as V\u03c0.", "Similar to state-value function, if an agent is in state S and it determines its next action based on policy \u03c0, then its expected return will be called action-value function for policy \u03c0. Thus, the action-value function is normally denoted as q\u03c0.", "In some sense, the value function and the reward have some similarities. However, a reward refers to what is good in an immediate sense, while a value function refers to what is good in the long run. Thus, a state might have a low immediate reward but has a high value function because it is regularly followed by other states that have high rewards.", "To compute the value function, the Bellman equation is commonly applied.", "In Reinforcement Learning, the Bellman equation works by relating the value function in the current state with the value in the future states.", "Mathematically, the Bellman equation can be written as the following.", "As you can see from the mathematical equation above, what Bellman equation expressed is that it averages over all of the possible states and future rewards in any given states, depending on the dynamics environment p.", "To make it easier for us to understand how the Bellman equation actually works intuitively, let\u2019s relate it to our everyday moment.", "Let\u2019s say that two months ago, you learned how to ride a bike for the first time. One day when you rode your bike, the bike lost its balance when you pull the brake on a surface full of sand, making you slipped and injured. This means that you got a negative reward from this experience.", "One week later, you rode the bike again. When you rode it on a surface full of sand, you slowed down the speed. This is because you know that when the bike loses its balance, bad things will happen even though this time you didn\u2019t actually experience it.", "Whenever we try to solve reinforcement learning task, we want the agent to choose an action which maximizes the cumulative reward. To achieve this, it means that the agent should follow a policy that maximizes the value function we have just discussed above. The policy that maximizes the value function in all of the states is called optimal policy and normally it is defined as \u03c0*.", "To understand the optimal policy better, let\u2019s take a look at the following illustration.", "As shown in the illustration above, we can say that \u03c0` is an optimal policy compared to \u03c0 because the value function at any given state following policy \u03c0` is as good as or better than policy \u03c0.", "If we have an optimal policy, then we can actually rewrite the Bellman equation into the following:", "The final equation above is called the Bellman optimality equation. Note that in the final form of above equation, there is no specific reference regarding certain policy \u03c0. The Bellman optimality equation basically tells us that the value function of a state under an optimal policy should be equal to the expected return of the best action from that state.", "From the equation above, it is very straightforward to find the optimum state-value function once we know the optimal policy. However, in real life, we often don\u2019t know what the optimal policy is.", "To find the optimal policy, the dynamic programming algorithm is normally applied. With dynamic programming, the state-value function of each state will be evaluated iteratively until we find the optimal policy.", "Now let\u2019s dive into the theory behind dynamic programming to find the optimal policy. At its core, dynamic programming algorithm uses Bellman equation iteratively to do two things:", "Policy evaluation is a step to evaluate how good a given policy is. In this step, the state-value function V\u03c0 for an arbitrary policy \u03c0 is computed. We have seen that the Bellman equation actually helps us to compute the state-value function with a system of linear equation as follows:", "With dynamic programming, the state-value function will be approximated iteratively based on Bellman equation until the value function converged in each of the state. The converged approximation of a value function can be called as the value function of a given policy V\u03c0.", "After we found the value function of a given policy V\u03c0, we need to improve the policy. Recall that we can define an optimal policy if and only if the value function of one policy is equal or bigger than other policies in any given state. With policy improvement, the new, strictly better policy in any given state can be generated.", "Notice that in the above equation, we use the V\u03c0 that we have computed in policy evaluation step to improve the policy. If the policy doesn\u2019t improve after we apply the above equation, it means that we have found the optimal policy.", "Overall, these two steps, policy evaluation and policy improvement are done iteratively using dynamic programming. First, under any given policy, the corresponding value function is computed. Then, the policy is improved. With the improved policy, the next value function is computed and so on. If a policy does not improve anymore compared to the previous iteration, it means that we have found the optimal policy for our problem.", "Now that we know all of the theory regarding dynamic programming and optimal policy, let\u2019s implement it in a code with a simple use case.", "Suppose that we want to control the increased demand of the use of parking space in a city. To do so, what we need to do is to control the price of parking system depending on the city\u2019s preference. In general, the city council has a perspective that the more parking space is being used, the higher the social welfare is. However, the city council also prefers that at least one spot is left unoccupied for emergency use.", "We can define the use case above as Markov Decision Process (MDP) with:", "For this example, let\u2019s assume that there are ten parking spots and four different price range. This means that we have eleven states (10 plus 1 because there can be a situation where no parking space is occupied) and four actions.", "To find the optimal policy with the given use case, we can use the dynamic programming with Bellman optimality equation. First, we evaluate the policy and then we improve the policy. We do these two steps iteratively until the result converges.", "As a first step, let\u2019s define a function to compute the Bellman optimality equation as shown below.", "The Bellman optimality equation above evaluates the value function at any given state.", "Next, let\u2019s define a function to improve the policy. We can improve the policy by greedifying the policy. This means that we transform the policy such that the policy will have the probability of 1 of choosing action which maximize the value function at a given state.", "Finally, we can wrap the policy evaluation and policy improvement into one function.", "Now if we run the function above, we will get the following result:", "From the result above, we can see that the value function increases as the number of parking space that is occupied increased, except when all of the parking space are occupied. This is fully expected as we can see from the preference of the city council in the use case example.", "The city council has a perspective that the more the parking space is used, the higher the social welfare is and they prefer to have at least one parking space left unoccupied. Thus, the more the parking spot is occupied, the higher the value function will be apart from the last state.", "Also note that when the parking occupancy is high (state nine and ten), the action change from 0 (the lowest price value) to 4 (the highest price value) to avoid the full occupancy.", "The material in this article was inspired by Reinforcement Learning: An Introduction book by Richard Sutton and Andrew Barto as well as Fundamentals of Reinforcement Learning course by the University of Alberta on Coursera.", "Make sure you read the book or take the course to dive deeper into the detail of Reinforcement Learning.", "If you want to play around with the code in the above example, you can find it on my GitHub page.", "Data Science || Machine Learning || Computer Vision || NLP"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F177dd8626042&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@marcellusruben?source=post_page-----177dd8626042--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Ruben Winastwan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----177dd8626042---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F177dd8626042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----177dd8626042---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F177dd8626042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=-----177dd8626042---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@lenin33?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lenin Estrada"}, {"url": "https://unsplash.com/s/photos/robots?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "Reinforcement Learning: An Introduction book by Richard Sutton and Andrew Barto"}, {"url": "https://www.coursera.org/learn/fundamentals-of-reinforcement-learning?", "anchor_text": "Fundamentals of Reinforcement Learning course by the University of Alberta on Coursera"}, {"url": "https://github.com/marcellusruben/Dynamic-Programming-Reinforcement-Learning", "anchor_text": "GitHub page"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----177dd8626042---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----177dd8626042---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----177dd8626042---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/markov-decision-process?source=post_page-----177dd8626042---------------markov_decision_process-----------------", "anchor_text": "Markov Decision Process"}, {"url": "https://medium.com/tag/dynamic-programming?source=post_page-----177dd8626042---------------dynamic_programming-----------------", "anchor_text": "Dynamic Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F177dd8626042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----177dd8626042---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F177dd8626042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----177dd8626042---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F177dd8626042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=post_page-----177dd8626042--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----177dd8626042---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F46c6747bd93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&newsletterV3=5dae9da73c9b&newsletterV3Id=46c6747bd93b&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----177dd8626042---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Written by Ruben Winastwan"}, {"url": "https://medium.com/@marcellusruben/followers?source=post_page-----177dd8626042--------------------------------", "anchor_text": "925 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----177dd8626042---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F46c6747bd93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fundamentals-of-reinforcement-learning-177dd8626042&newsletterV3=5dae9da73c9b&newsletterV3Id=46c6747bd93b&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----177dd8626042---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Ruben Winastwan"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Text Classification with BERT in PyTorchHow to leverage a pre-trained BERT model from Hugging Face to classify text of news articles"}, {"url": "https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "\u00b79 min read\u00b7Nov 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F887965e5820f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-bert-in-pytorch-887965e5820f&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----887965e5820f----0-----------------clap_footer----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=author_recirc-----177dd8626042----0---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F887965e5820f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-bert-in-pytorch-887965e5820f&source=-----177dd8626042----0-----------------bookmark_preview----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----177dd8626042----1---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----177dd8626042----1-----------------bookmark_preview----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----177dd8626042----2---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----177dd8626042----2-----------------bookmark_preview----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Ruben Winastwan"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "Named Entity Recognition with BERT in PyTorchHow to leverage a pre-trained BERT model for custom data to predict the entity of each word in a text"}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": "\u00b711 min read\u00b7May 3, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa454405e0b6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-with-bert-in-pytorch-a454405e0b6a&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----a454405e0b6a----3-----------------clap_footer----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a?source=author_recirc-----177dd8626042----3---------------------bd5bee57_2832_4739_a53a_e8e3b91624ee-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa454405e0b6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-with-bert-in-pytorch-a454405e0b6a&source=-----177dd8626042----3-----------------bookmark_preview----bd5bee57_2832_4739_a53a_e8e3b91624ee-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=post_page-----177dd8626042--------------------------------", "anchor_text": "See all from Ruben Winastwan"}, {"url": "https://towardsdatascience.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----177dd8626042----0-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----177dd8626042----1-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://venali.medium.com/an-introduction-to-volatility-targeting-985605abb797?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://venali.medium.com/?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://venali.medium.com/?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Venali Sonone"}, {"url": "https://venali.medium.com/an-introduction-to-volatility-targeting-985605abb797?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "An Introduction to Volatility TargetingThe Theory of Portfolio Management blog series presents my favorite selected list of research articles related to essential portfolio\u2026"}, {"url": "https://venali.medium.com/an-introduction-to-volatility-targeting-985605abb797?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b713 min read\u00b7Dec 10, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F985605abb797&operation=register&redirect=https%3A%2F%2Fvenali.medium.com%2Fan-introduction-to-volatility-targeting-985605abb797&user=Venali+Sonone&userId=7aecdc84c720&source=-----985605abb797----0-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://venali.medium.com/an-introduction-to-volatility-targeting-985605abb797?source=read_next_recirc-----177dd8626042----0---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F985605abb797&operation=register&redirect=https%3A%2F%2Fvenali.medium.com%2Fan-introduction-to-volatility-targeting-985605abb797&source=-----177dd8626042----0-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----1-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----177dd8626042----1---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----177dd8626042----1-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----177dd8626042----2---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----177dd8626042----2-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----3-----------------clap_footer----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----177dd8626042----3---------------------9567dac1_b70f_4734_95b5_182cff258c5f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----177dd8626042----3-----------------bookmark_preview----9567dac1_b70f_4734_95b5_182cff258c5f-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----177dd8626042--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----177dd8626042--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}