{"url": "https://towardsdatascience.com/beyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b", "time": 1683004295.429868, "path": "towardsdatascience.com/beyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b/", "webpage": {"metadata": {"title": "Beyond A/B Testing: Primer on Causal Inference | by Wicaksono Wijono | Towards Data Science", "h1": "Beyond A/B Testing: Primer on Causal Inference", "description": "Applied statistics is seeing rapid adoption of Pearlian causality and Bayesian statistics, which closely mimic how humans learn about the world. Certainly, increased computational power and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/how-this-frequentist-turned-bayesian-7066e210a301?source=friends_link&sk=578af7782e8824afe55043bc5d6c2838", "anchor_text": "Bayesian statistics", "paragraph_index": 0}, {"url": "https://en.m.wikipedia.org/wiki/Welch's_t-test", "anchor_text": "Welch\u2019s t-test", "paragraph_index": 9}, {"url": "https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be", "anchor_text": "really know what you\u2019re doing", "paragraph_index": 10}, {"url": "https://clincalc.com/stats/samplesize.aspx", "anchor_text": "sample size calculator", "paragraph_index": 12}, {"url": "https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d", "anchor_text": "post by booking.com", "paragraph_index": 17}, {"url": "https://bookingcom.github.io/powercalculator/", "anchor_text": "Booking.com\u2019s power calculator", "paragraph_index": 18}, {"url": "https://github.com/bookingcom/powercalculator", "anchor_text": "open-source code here", "paragraph_index": 18}, {"url": "https://en.m.wikipedia.org/wiki/Random_effects_model", "anchor_text": "random effect", "paragraph_index": 21}, {"url": "https://en.m.wikipedia.org/wiki/Multilevel_model", "anchor_text": "Bayesian hierarchical model", "paragraph_index": 21}, {"url": "https://en.m.wikipedia.org/wiki/Empirical_Bayes_method", "anchor_text": "empirical Bayes", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/stop-using-uniform-priors-47473bdd0b8a", "anchor_text": "my other article", "paragraph_index": 21}, {"url": "https://en.m.wikipedia.org/wiki/Factorial_experiment", "anchor_text": "factorial design", "paragraph_index": 24}, {"url": "https://web.ma.utexas.edu/users/mks/384E06/unbalanced.pdf", "anchor_text": "which sum of squares you use,", "paragraph_index": 25}, {"url": "http://www.utstat.utoronto.ca/reid/sta442f/2009/typeSS.pdf", "anchor_text": "four types", "paragraph_index": 25}, {"url": "https://www.methodology.psu.edu/ra/most/factorial/", "anchor_text": "reuse the samples to test multiple hypotheses", "paragraph_index": 29}, {"url": "https://help.optimizely.com/Build_Campaigns_and_Experiments/Multivariate_tests_for_Optimizely_X#Change_traffic_allocation", "anchor_text": "complete lack of understanding of how factorial designs work", "paragraph_index": 30}, {"url": "https://en.m.wikipedia.org/wiki/Fractional_factorial_design", "anchor_text": "fractional factorial design", "paragraph_index": 33}, {"url": "https://online.stat.psu.edu/stat509/node/123/", "anchor_text": "crossover experiment", "paragraph_index": 37}, {"url": "http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/SAS/SAS4-OneSampleTtest/SAS4-OneSampleTtest7.html", "anchor_text": "paired t-test", "paragraph_index": 37}, {"url": "https://www.itl.nist.gov/div898/handbook/pri/section3/pri3333.htm", "anchor_text": "Blocking", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Response_surface_methodology", "anchor_text": "response surface method", "paragraph_index": 46}, {"url": "https://www.itl.nist.gov/div898/handbook/pri/section5/pri542.htm", "anchor_text": "Simplex lattice designs", "paragraph_index": 46}, {"url": "https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d", "anchor_text": "counterfactuals", "paragraph_index": 46}, {"url": "http://docs.pyro.ai/en/stable/contrib.oed.html", "anchor_text": "choosing observation sites", "paragraph_index": 46}, {"url": "https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55", "anchor_text": "clever ways", "paragraph_index": 46}, {"url": "https://www.mailman.columbia.edu/research/population-health-methods/difference-difference-estimation", "anchor_text": "difference-in-differences", "paragraph_index": 47}, {"url": "https://www.bmj.com/content/350/bmj.h2750", "anchor_text": "interrupted time series", "paragraph_index": 47}, {"url": "https://economics.mit.edu/files/11859", "anchor_text": "synthetic controls", "paragraph_index": 47}, {"url": "https://google.github.io/CausalImpact/", "anchor_text": "Google\u2019s CausalImpact package", "paragraph_index": 47}, {"url": "https://google.github.io/CausalImpact/CausalImpact.html", "anchor_text": "CausalImpact documentation", "paragraph_index": 52}, {"url": "https://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X", "anchor_text": "Pearl\u2019s causality framework", "paragraph_index": 58}, {"url": "https://www.econlib.org/archives/2016/07/immigrant_quali.html", "anchor_text": "source", "paragraph_index": 59}, {"url": "https://medium.com/analytics-vidhya/how-this-frequentist-turned-bayesian-7066e210a301?source=friends_link&sk=578af7782e8824afe55043bc5d6c2838", "anchor_text": "previous article about Bayesian statistics", "paragraph_index": 60}, {"url": "http://bayes.cs.ucla.edu/BOOK-2K/ch6-5.pdf", "anchor_text": "collapsible", "paragraph_index": 61}, {"url": "https://gking.harvard.edu/files/gking/files/psnot.pdf", "anchor_text": "it\u2019s terrible", "paragraph_index": 62}, {"url": "http://www.dagitty.net/", "anchor_text": "algorithms", "paragraph_index": 84}, {"url": "http://bayes.cs.ucla.edu/BOOK-2K/ch3-3.pdf", "anchor_text": "front-door adjustment", "paragraph_index": 91}, {"url": "https://en.wikipedia.org/wiki/Selection_bias", "anchor_text": "Selection bias", "paragraph_index": 95}, {"url": "https://arxiv.org/pdf/1902.07409.pdf", "anchor_text": "Causal forests", "paragraph_index": 107}, {"url": "https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/", "anchor_text": "doubly robust estimation", "paragraph_index": 107}, {"url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff", "anchor_text": "biased", "paragraph_index": 114}, {"url": "https://en.wikipedia.org/wiki/Instrumental_variables_estimation", "anchor_text": "two-stage least squares", "paragraph_index": 122}], "all_paragraphs": ["Applied statistics is seeing rapid adoption of Pearlian causality and Bayesian statistics, which closely mimic how humans learn about the world. Certainly, increased computational power and accessible tools have contributed to their popularity. The real appeal, however, is much more fundamental:", "Outside of experimentation teams, data scientists have traditionally focused on prediction. Yet, as the adage goes, \u201ccorrelation does not imply causation\u201d.", "Your umbrella company wants to increase revenue. You notice that wet ground predicts an increase in sales. Should you go around spraying water on the ground to stimulate sales?", "Predictive models, on their own, cannot answer such questions. Important business questions demand causal inference.", "Prediction and inference are opposite goals. Correct inference often requires us to sacrifice predictive power. A model tuned to maximize predictive power can lead to incorrect causal inference. We will talk about this in the second half of the article.", "This article serves as an introductory guide/reference to causality. As such, it\u2019s quite lengthy. It\u2019s divided into three parts:", "As we go down the list, the study becomes less costly and more feasible to carry out. In exchange, we have to make stronger and more outlandish assumptions.", "This introductory article won\u2019t go into heterogeneity of effects, inference on graphs / networks, interference, or time-varying effects. A basic knowledge of hypothesis testing is assumed.", "We have come to the point where people colloquially refer to experiments as \u201cA/B tests\u201d. Subjects are randomized into control and treatment groups. Any difference observed is the causal effect from the intervention.", "From a frequentist standpoint, you want to do a Welch\u2019s t-test or proportion test. These frequentist tests assume that you run the test once with an appropriate sample size. I recommend setting up a monitoring dashboard to quickly find out if something is implemented incorrectly (e.g. conversion drops to zero) but the testing should be done only once at the end of an agreed-upon period.", "A dashboard of p-values is dangerous because we shouldn\u2019t act on p-values. Early termination (unless you really know what you\u2019re doing) or running an experiment until significance (ew, gross) will make false positive rate absurdly high, rendering the experiments largely useless. Here is R code to simulate an oversimplified situation where the null hypothesis is true and we\u2019re supposed to test on the sixth week with \u03b1 = 0.05:", "If you do the test on any single week, then on average we get a false positive rate at the predetermined \u03b1 = 0.05. Using this seed, early termination results in 26% false positive rate and running the experiment until significance, up to 20 weeks, results in 61% false positive rate. It is very important to pre-register the experiment and agree on the duration in advance.", "You should have some historical data to guesstimate the mean and standard deviation of the control group, which you plug into a sample size calculator to determine how long the experiment should be run. Ideally that\u2019s the case, but the mean will change depending on how long the experiment is run as the proportion of new vs loyal customers will go up with time.", "The last one relates to how A/B testing is often used for decision making. Consider the scenarios:", "If you\u2019re going to subscribe to the binary nature of the NHST framework, you might as well binarize the decisions too to reap higher statistical power.", "As a toy example, suppose the baseline conversion rate is a stable 20% and we get 1000 new visitors per week. How many weeks should we run the experiment, if our decision rule is to ship B if it\u2019s significantly better than A? We can present this plot:", "Effect size of 20% is unrealistic. If we care about 10%, then a 6-week experiment gives us pretty high power. And if we want to detect a 5% effect size, then we might have to turn to other options. The power plot is a contour plot to visually help in decision making.", "Getting an adequate sample size is often infeasible. According to a post by booking.com:", "Detecting small effects can be challenging. Imagine running an e-commerce website with a typical conversion rate of 2%. Using Booking.com\u2019s power calculator (open-source code here), you can discover that detecting a relative change of 1% to your conversion rate will require an experiment with over 12 million users.", "For most companies, A/B testing on 12 million users is downright impossible. In fact, one of the main competitive advantages of large tech companies is that they can run much more powerful experiments \u2014 smaller companies cannot replicate this. In this case, I recommend going by gut feel or going Bayesian.", "Going by gut feel sounds contrary to \u201cdata-driven decision making\u201d but running a frequentist analysis on far too few samples is more like \u201cnoise-driven decision making\u201d. It is a waste of resources for something that we know in advance will be inconclusive (due to bad methodology, not significance of results). Garbage in, garbage out.", "The Bayesian approach to A/B testing is to treat the group assignment as a random effect, i.e. we use a Bayesian hierarchical model. Otherwise, the prior can be chosen using empirical Bayes. Using a uniform / extremely weak prior is heavily frowned upon (for more details, check my other article).", "Word of caution: your metric should be aggregated by your sampling unit. If you randomize which users get A or B, then the metric to be tested needs to be X per user. Something like average order value will lead to misleading results because we are not randomizing the orders. Intuitively, something like average order value places 0 weight on customers who didn\u2019t buy, while placing more weight for customers who order frequently.", "Aggregating by user will discard some information but it is necessary to get the correct results. You can set up a Bayesian hierarchical model instead of aggregating, but those are computationally challenging for large data sets.", "The factorial design is generally superior to a simple A/B test. You either:", "Unless you are a tech giant who can get enough statistical power from 1% of the userbase, you should do equal splits. This typically yields the most powerful experiment and has an added benefit of clear methodology. Conclusions can change depending on which sum of squares you use, but the four types yield the same result if your design is balanced (equally split).", "Although dummy / one-hot encoding is more common, for experiments we prefer encoding control and treatment as -1 and 1. The effect sizes are more interpretable. Dummy encoding yields coefficients that are relative to a reference cell \u2014 rarely the quantity we\u2019re interested in.", "In a balanced factorial design with three covariates, you should equally split the subjects into eight groups:", "The correlation matrix is a diagonal matrix so the effect size estimates are independent of each other. In fact, even the interaction effects are independent of everything else. Try computing the correlation of this design matrix:", "This lets us reuse the samples to test multiple hypotheses and get higher quality insights. To test each hypothesis, we compare the white vs red cells:", "I am sorry to call some people out, but there is some terrible advice going around that shows complete lack of understanding of how factorial designs work. A factorial experiment is not the same as \u201cmultivariate testing\u201d. It is not true that you need more samples to obtain the same power as in a simple A/B test. You need the same number of samples because the samples get reused to test each hypothesis \u2014 the entire point of the factorial design. You can compute sample sizes the same way you would in a simple A/B test, though you might not detect the interaction effect unless it\u2019s glaringly large.", "Pretend you want to optimize your banner. We want to test the effect of font size and image size. Perhaps if you test them individually, users prefer large font, and users prefer large pictures. But large font and large picture together makes the banner too crowded \u2014 there is a negative interaction effect.", "The insights can be transferred to future banner design. If you test A = small font and small picture vs B = large font and large picture, we don\u2019t know what specific change caused the impact. Even worse, B might perform worse than A even though some components work because the large negative (interaction) effects mask the positive effects.", "If you want to test many changes at once, the number of banners to make grows exponentially. Testing n covariates requires 2^n combinations. Using the fractional factorial design, you can cut down the number of combinations by at least half if you are okay with being unable to estimate some higher-order interaction effects.", "Sometimes changes are deemed too risky and you are only allowed to do an experiment on, say, 20% of the user base. When it makes sense to have a low, medium, and high settings, keep the 80% \u201cuntouchable\u201d as all 0 (medium setting) while equally splitting the 20% into -1 and +1 (low and high setting) groups. This way you don\u2019t have to discard any of your user base and the \u201cuntouchable\u201d customers enhance statistical power by providing a tighter estimate of the intercept, if linearity is assumed.", "Bayesian analysis would treat the covariates as crossed random effects. (Not sure how to analyze the interaction terms.)", "Word of caution: factorial designs do not make sense when the covariates cannot possibly be independent, e.g. because of funnels. Suppose your website\u2019s funnel is X \u2192 Y \u2192 Z. Change(X) will affect who ends up in Y. The effect of change(Y) is dependent on change(X) even if we set it up as a factorial experiment. If possible, I recommend splitting the user base so some get tested only on change(X) while others get tested only on change(Y).", "In a crossover experiment, you randomize the subjects on the order: whether they receive AB or BA. When applicable, this results in tighter intervals because you\u2019re using a paired t-test (or a random effect with many levels). Key assumptions:", "Due to these restrictions, crossovers are typically used on products rather than people. Products are more reliably on the shelf for two periods. Users might arrive in the first period but not the second, and vice versa.", "An example application is to evaluate the display of a snippet on product listings. Hold the product rankings constant for the experiment period. On the product ranking / search page, let half of the products have a snippet (B) and the other without a snippet (A). Reverse it in the second period. You can reasonably estimate the impact of the snippet on customer behavior.", "Well, actually, no. Can you spot what\u2019s wrong with this experiment?", "If customers are more likely to choose B over A, then some of the purchases are shifted from A to B. To minimize interference, we can choose product categories that are unrelated to each other and do AB on half of the categories and BA on the other half. The data should be analyzed using a mixed model. Carryover effect is still possible if B is so bad that it causes some customers to leave.", "Classically, blocking is used when we use multiple measuring devices or need to conduct the test in multiple batches, e.g. you are a bakery and use multiple scales to weigh ingredients and don\u2019t have an absurdly large oven. Maybe today has higher humidity than yesterday, which affects the end product. We want to take that into account.", "Blocking has powerful applications in surveys to keep them short. It splits a factorial experiment into multiple batches, each of which is a fractional factorial.", "I like forced choice surveys (\u201cWhich do you prefer: A or B?\u201d) because we can infer things from behavior. What people say and what people do are completely different things. If we set up the options like a factorial experiment, then we can split the questionnaire into smaller surveys.", "For instance, one time I ideally wanted each person to answer 32 questions. This is a dumb idea. People won\u2019t complete the survey, and even if they do, they\u2019ll answer randomly towards the end. Instead, through blocking, we can get solid inferences from 8 questions per person.", "There are other, more advanced ways to do your experiment. The response surface method is like gradient-based optimization. Simplex lattice designs are used for optimizing proportions of ingredients and can be used for establishing ranking weights. Much of the research into experimental methodology in industry deals with increasing the power of the experiment, usually through counterfactuals, stratification, choosing observation sites to maximize information gain, or clever ways to essentially turn a t-test into a paired t-test, among others.", "Now that we have covered experiments, we start to go into the territory of uncomfortable assumptions. There are many methods that fall under quasi-experiments: difference-in-differences, interrupted time series, and synthetic controls. However, Google\u2019s CausalImpact package is so powerful and flexible that it should cover most practical needs. These are extremely useful for ad performance measurement and pricing experiments.", "The basic intuition is simple: we observe a time series X with some intervention (like sales data and we launched a marketing campaign). We want to build a counterfactual: what would the time series have been like without the intervention? We look for ingredients (possibly many time series) to put into a blender, and hopefully the end result is a good counterfactual. The difference between observed and counterfactual is our causal effect estimate.", "Most of the work in this type of analysis is in finding and validating the ingredients for the counterfactual. Bad ingredients result in completely arbitrary estimates: even for periods with no intervention, a large \u201ccausal effect\u201d might be estimated. Garbage in, garbage out. Here\u2019s an illustrative example:", "We simulated two random walks with no intervention, yet CausalImpact estimates a huge positive effect:", "Garbage in, garbage out. As we start to go beyond experiments, most of the work is on validating the reasonableness of assumptions. In many cases, we cannot even validate assumptions; significant human judgment is needed to assess the sanity of the model.", "Once we have the ingredients, the analysis is very easy to do if you follow the CausalImpact documentation.", "In practice, the ingredients relate to geographic location, e.g. using LA time series to predict NYC time series.", "As a rule of thumb, the post-intervention period shouldn\u2019t be too long (1\u20133 weeks is ideal) because forecasts break down the farther we look ahead. The pre-intervention period should be about 3\u20134 times as long as the post-intervention period. We hope there are no major structural changes spanning the entire period, and the series too far in the past might have a different relationship.", "Personally what works for me is:", "The ingredients and X should be chosen before the quasi-experiment is run. The worst case scenario is that the intervention has been done but we cannot build a decent counterfactual for X because none of the ingredients work.", "The two previous sections covered cases where we have directly intervened. Sometimes we cannot intervene due to real-life constraints such as ethics or cost, so we only have observational data to work with.", "Whether we like it or not, the overwhelming majority of data is observational. Experiments are expensive; tracking user activity is much easier. It\u2019s unfortunate, since small well-collected data can be more useful than large observational data \u2014 they can be analyzed with fewer assumptions. It\u2019s hard to draw valid conclusions from observational data alone. Pearl\u2019s causality framework has grown increasingly popular in this space. It is equivalent to Rubin\u2019s potential outcomes framework, but presents assumptions in easy-to-digest diagrams instead of obtuse mathematical equations.", "Controlling for barometric pressure, Mount Everest has the same altitude as the Dead Sea. (source)", "The most important takeaway is that adding too many predictors can lead to wrong results. Models tuned for predictive performance can lead to incorrect causal inferences. There is a big push against \u201ccontrolling for everything\u201d. In my previous article about Bayesian statistics, I explained how applied statistics is moving away from mindless procedures to critical thinking. Choosing predictors through lasso or, even worse, stepwise regression, simply does not work for observational data.", "In this field, it is common to use linear regression instead of logistic regression to estimate causal effect on a binary outcome. You might scoff at the idea, but it\u2019s done for good reason. Log odds are not collapsible. With observational data, we can get wildly different conclusions if we use non-collapsible metrics and add too many predictors or fall victim to the omitted variable bias. Unless you are absolutely certain that you have the perfect set of predictors (as is the case for randomized controlled experiments), it might be prudent to use linear regression to estimate causal effects.", "Propensity matching is omitted from this article. Much has been written about why it\u2019s terrible and how it will lead to wrong estimates. If you can compute propensity scores, you can use inverse propensity weighting instead. Just like stepwise regression, propensity matching is a terrible practice that needs to die out. Please, just don\u2019t.", "A Bayesian network is a directed acyclic graph (DAG) that implies a factorization of a joint distribution. The edges (arrows) show dependency. For instance, X \u2192 Y means that Y depends on X and the joint distribution p(x,y) can be factored as p(x) p(y|x).", "A causal DAG makes the stronger statement that arrows indicate causality. X \u2192 Y means that X causes Y. For simplicity, this article will assume that all variables are binary.", "Why go through all this trouble? Because once we go past a few variables, it gets really messy, even with four variables:", "The last term is not p(x|u,v,w) because u affects x only through v and w. Once we know v and w, knowing u provides no additional information. Thus, DAGs have the Markov property.", "The DAG is much easier to inspect than the joint probability distribution. Even non-technical folks can chime in on the reasonableness of the assumptions and we can better involve the domain experts. \u201cDoes U really cause V?\u201d", "The most uncomfortable thing about causal DAGs is that we cannot verify our assumptions, except in cases where the DAG implies some conditional independencies. Why should the edge X \u2192 Y exist? This should be discussed with domain experts. Some things we can all agree on, e.g. drunk driving \u2192 car accident. Other assumptions can be murky. DAGs are very \u201csubjective\u201d.", "Yet, not using DAGs is much worse. A single regression equation can correspond to many DAGs. Which is it? When someone does regression analysis and computes a p-value, what are they even testing? All of these DAGs correspond to Z~X+Y:", "There are many other possible DAGs with unobserved variables. When someone computes a p-value on the coefficient of Y, what are they even testing? I\u2019ll explain in a bit, but the interpretation of the coefficient is very different depending on the causal diagram. Analysis done purely through regression sidesteps the question under the guise of \u201cobjectivity\u201d.", "These causal diagrams allow non-statisticians to discuss the model and assumptions. They put everything in a digestible graphical format. Perhaps the regression model is wrong; perhaps it answers the wrong business question. Other people can help us find out. These causal diagrams should be a must for analyzing observational data.", "We can classify paths going through triplet nodes:", "Simply put, in X \u2192 Y \u2192 Z, Z is caused by X. Conditioning on Y will flip the on/off switch. Once we know Y, knowing X provides no additional information because X affects Z only through Y. Information from X to Z gets blocked.", "Why are the rules the way they are?", "Rain causes people to fall by making the ground slippery through mediation. If we know that the floor is slippery, then knowing whether it\u2019s raining or not provides no additional information in regards to people falling. Rain cannot make people fall if the ground is not slippery.", "Rain makes the ground wet. Rain causes people to buy umbrellas. Wet ground is correlated with people buying umbrellas. A fork is commonly known as confounding. We can predict umbrella sales by whether or not the ground is wet, but it\u2019s silly to claim that wet ground causes rain. Controlling for rain will render \u201cwet ground\u201d useless as a predictor. If we know it\u2019s raining, we don\u2019t need to know that the ground is wet.", "Colliders are tricky to conceptualize. Spilling water makes the ground wet. Rain makes the ground wet. But obviously spilling water does not cause rain or vice versa. If we know the ground is wet and we just spilled water on the floor, then it reduces the probability of rain. They are two competing explanations. Once the phenomenon has been explained by one thing, it becomes less likely that both causes are at play. When we condition on a collider, two independent events become correlated (\u201ccollider bias\u201d). We often condition on a collider not by choice but because of missing data, i.e. censoring or selection bias.", "Now that we understand how information flows through a DAG, we can use it to estimate causal effects.", "The do() notation indicates that we are changing something. In general, P(Y|X=x) is not the same as P(Y|do(X=x)). The former deals with observation (\u201cGiven that we observe X=x\u2026\u201d) while the latter deals with intervention (\u201cGiven that we set X to x\u2026\u201d). Or, to put it another way, the former deals with prediction while the latter deals with causal inference. The two goals are usually at odds with each other.", "The do(X=x) operator wants to block all paths with an edge coming into X (\u201cbackdoor\u201d) and adjust for confounders, while keeping all \u201cfront-foor\u201d paths open (paths starting with an edge coming out of X). Or, if you prefer thinking in terms of trees: we want to construct a new tree with X as the root node. Let\u2019s use this DAG for illustration:", "What is the causal effect of V on X? We want to compute E[X|do(V=1)] - E[X|do(V=0)]. Take a look at all the possible paths:", "The only path that has an edge coming into V is the last one. Break it down into triplets:", "This path is already blocked by the collider without us doing anything! Had we conditioned on W, we would\u2019ve opened the backdoor and get wrong estimates. The correct regression equation is X~V. We don\u2019t need to adjust for anything. While adding U would\u2019ve had no impact, adding W would\u2019ve led to disastrously wrong results (\u201ccollider bias\u201d) \u2014 it improves predictive power but the causal estimate is garbage, reiterating that prediction and inference are opposite goals. Here, I emphasize again: think about what predictors you use. Throwing everything into the regression will lead to wrong results.", "This example is very simple. In practice, you will need to supply the DAG and let some algorithms determine which variables you should throw into the regression. (Though there are some heuristics to learn the \u201csimplest DAG\u201d that is consistent with the data.) This concept will be used throughout the rest of this article: variable selection for causal inference should be determined by DAGs.", "Sometimes, you run a DAG through the algorithm and discover that it is impossible to estimate the causal effect given your observations! But using the DAG we can discuss what variables we need and which ones are feasible to collect.", "Mediation analysis is concerned with a DAG that looks like this. There is a direct path X \u2192 Z and an indirect path X \u2192 Y \u2192 Z. Remember when I said interpretation of coefficients depends on the DAG?", "This DAG claims that a change in X will cause a change in Y, which through the chain will cause a change in Z.", "Suppose we fit the regression Z~X+Y. What is the interpretation for \u03b2, the coefficient of X? Holding Y constant, an increase of 1 unit of X is associated with an increase of \u03b2 in Z. See the problem? When we intervene and change X, then Y will change as well.", "The total effect is given by (X \u2192 Z) + (X \u2192 Y \u2192 Z). If we assume the latter effect is multiplicative, then we can claim (X \u2192 Y \u2192 Z) = (X \u2192 Y) \u00d7 (Y \u2192 Z). This assumption is reasonable if we work with linear models. A unit increase in X causes an increase of (X \u2192 Y) in Y, and that (X \u2192 Y) increase in Y translates to a (X \u2192 Y) \u00d7 (Y \u2192 Z) increase in Z. We can estimate the causal effect of X on Z using two different ways:", "Why bother with the second method? Comprehending it is key to understanding front-door adjustment and instrumental variables, which will be covered at the end of the article.", "Sometimes, you cannot block all the backdoors but can still estimate the causal effect using the front-door adjustment, which works for DAGs that look like:", "Where U is unobserved and we want to the causal impact of X on Z. To be precise, we require:", "This method is actually very clever. Personally it\u2019s an \u201caha!\u201d moment once I saw it. Recall the relationship (X \u2192 Y \u2192 Z) = (X \u2192 Y) \u00d7 (Y \u2192 Z). Then:", "So we can compute the causal effect by breaking it down into (X \u2192 Y \u2192 Z) = (X \u2192 Y) \u00d7 (Y \u2192 Z) since we can estimate the components in the right hand side.", "Selection bias is one of the biggest problems of observational data, making inverse propensity weighting (IPW) the bread and butter of this field. For instance, a business might ask \u201cHow does subscription affect user engagement?\u201d A naive approach is to compare the means of subscribers vs non-subscribers. However, why would a non-active user even buy a subscription? Active users are more likely to subscribe \u2014 some reverse causation. It is completely ridiculous to claim that the act of subscribing causes user engagement to jump that much.", "IPW creates a pseudo-population that better represents the true population. It corrects for selection bias. We take weighted averages using 1/propensity as the weights, where propensity is defined as the probability of being selected into that group. In this case, if the user is subscribed, then their propensity is the probability that they are subscribed given the predictors.", "Suppose this is the true DAG. We simulate some data and analyze it using R code:", "In this simulation, we make a subscription increase user engagement by 20. The simulated data:", "If we naively compare the averages aggregated by subscribed, we get 114 - 63 = 51. But we know the true coefficient is 20! This estimate is completely wrong.", "If we use IPW, we get the estimate of 103 - 83 = 20, the true causal effect.", "But wait a second. While the previous example is colloquially called selection bias, it\u2019s a confounder in causal literature. We could\u2019ve simply run regression:", "In causality, selection bias refers to conditioning on a collider, so something like:", "I struggle coming up with an example so I\u2019ll use Hernan\u2019s example. X is folic acid supplement given to pregnant mothers, which reduces the chance of cardiac malformation. Y is cardiac malformation, which increases the chance of mortality in the womb. While X does reduce mortality by reducing Y, it also leads to healthier fetuses and reduces mortality in other ways. We can only observe cardiac malformation conditional on the baby being born. What is the total effect of X on Y?", "Regression on its own cannot answer the question because we are missing the data when the child was never born. We need to use IPW using predicted probability of being missing.", "This kind of problem often appears in surveys where non-response rate can depend on multiple factors.", "IPW is a very powerful technique but can be difficult to use in practice. Propensities can be very close to 0, leading to unstable estimates (and a near-violation of the positivity assumption). Also, the propensity estimates should be unbiased, so we are restricted to logistic regression without regularization. The propensity scores might be biased anyway if we misspecify the model, e.g. nonlinearities or omitted variables. Use it with care.", "Causal forests are hot topic but can be confusing; and, from my experience, it doesn\u2019t scale well. I prefer the simpler doubly robust estimation described by Hernan. I\u2019ll walk us through this DAG:", "Using part (4) we can vary X=0 and X=1 to get Yhat estimates for both cases. We can then plug the predictions into the doubly robust estimator formula:", "And we can estimate the causal effect as:", "It\u2019s called doubly robust because as long as at least one of the models (propensity or conditional mean) makes unbiased predictions, then this causal effect estimate is unbiased. In other words, even if we get one model wrong, we\u2019re still fine.", "So why does this work well? Let\u2019s take a look at DR1 (the proof for DR0 is similar):", "If our propensity model is unbiased, then the part on the right has expectation 0, while the part on the left is IPW. IPW is unbiased if the propensity model is unbiased. Hence, this DR1 estimator is unbiased.", "If our conditional mean model is unbiased, then the part on the left has expectation 0, while the part on the right is an unbiased estimator of the potential outcome. Hence, this DR1 estimator is unbiased.", "Most machine learning algorithms are biased due to regularization, so it\u2019s likely that both the propensity and conditional mean models are biased. We hope that the bias is small enough such that when we multiply the two biased predictions, the total bias shrinks quickly as n grows. Asymptotically, the estimate follows a normal distribution centered at the truth.", "This section has been left for last because it takes a completely different approach", "Much of the literature on instrumental variables (IV) are super confusing, but a DAG makes the intuition super clear. In the above example, we want to estimate the causal effect of Y on Z. However, U is an unobserved confounder: we know it\u2019s there, but we can\u2019t measure it so we can\u2019t control for it.", "X is an IV because it satisfies the following criteria:", "So now we think in terms of the causal effect of X on Z. The causal effect X \u2192 Z can be decomposed into two parts: X \u2192 Y and Y \u2192 Z. Based on the assumptions, we:", "Do you see it? Through some algebra, we can estimate Y \u2192 Z !", "This is often done by regressing Y~X and then using the fitted values of Y to predict Z. Conceptually, we change Y so that only the edge X \u2192 Y is accounted for, breaking the edge U \u2192 Y.", "The whole concept can seem odd, but think about it: how do we interpret IV if X is a perfect predictor of Y? That\u2019s an A/B test! Instrumental variables are \u201cimperfect randomizers\u201d.", "In reality, causal effect estimates using IV can have extremely large intervals. If X is weakly predictive of Y and X \u2192 Y is small, then we are dividing by a very small number that has a lot of noise and it can lead to bias. Conceptually, a weak IV is like if you ran an A/B test but a lot of people randomized into A choose to go into B, and vice versa. If this noncompliance gets bad enough, you can\u2019t estimate anything. Thus, if you are using two-stage least squares, then X should be a strong predictor of Y. Otherwise, be extremely wary of IV estimates; consider other methods such as Bayesian modeling.", "This article covers the major simple approaches to causality as I know it. The focus is on practical application. These approaches should be enough to handle the vast majority of problems.", "If you think anything is missing or incorrect, please let me know in the comments so I can amend the article. I want this article to be the go-to reference for newer practitioners in this field. It should provide enough guidance to point them to the right methodology for their specific problem and let them search the keywords and concepts on their own.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Bayesian data scientist. Alternates between light reading and more in-depth articles about applied statistics and machine learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd8e462d90a0b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wwijono?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wwijono?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Wicaksono Wijono"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5e69e4814607&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&user=Wicaksono+Wijono&userId=5e69e4814607&source=post_page-5e69e4814607----d8e462d90a0b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/analytics-vidhya/how-this-frequentist-turned-bayesian-7066e210a301?source=friends_link&sk=578af7782e8824afe55043bc5d6c2838", "anchor_text": "Bayesian statistics"}, {"url": "https://www.seobility.net/en/wiki/AB_Testing", "anchor_text": "Source"}, {"url": "https://en.m.wikipedia.org/wiki/Welch's_t-test", "anchor_text": "Welch\u2019s t-test"}, {"url": "https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be", "anchor_text": "really know what you\u2019re doing"}, {"url": "https://clincalc.com/stats/samplesize.aspx", "anchor_text": "sample size calculator"}, {"url": "https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d", "anchor_text": "post by booking.com"}, {"url": "https://bookingcom.github.io/powercalculator/", "anchor_text": "Booking.com\u2019s power calculator"}, {"url": "https://github.com/bookingcom/powercalculator", "anchor_text": "open-source code here"}, {"url": "https://en.m.wikipedia.org/wiki/Random_effects_model", "anchor_text": "random effect"}, {"url": "https://en.m.wikipedia.org/wiki/Multilevel_model", "anchor_text": "Bayesian hierarchical model"}, {"url": "https://en.m.wikipedia.org/wiki/Empirical_Bayes_method", "anchor_text": "empirical Bayes"}, {"url": "https://towardsdatascience.com/stop-using-uniform-priors-47473bdd0b8a", "anchor_text": "my other article"}, {"url": "https://en.m.wikipedia.org/wiki/Factorial_experiment", "anchor_text": "factorial design"}, {"url": "https://web.ma.utexas.edu/users/mks/384E06/unbalanced.pdf", "anchor_text": "which sum of squares you use,"}, {"url": "http://www.utstat.utoronto.ca/reid/sta442f/2009/typeSS.pdf", "anchor_text": "four types"}, {"url": "https://www.methodology.psu.edu/ra/most/factorial/", "anchor_text": "reuse the samples to test multiple hypotheses"}, {"url": "https://help.optimizely.com/Build_Campaigns_and_Experiments/Multivariate_tests_for_Optimizely_X#Change_traffic_allocation", "anchor_text": "complete lack of understanding of how factorial designs work"}, {"url": "https://en.m.wikipedia.org/wiki/Fractional_factorial_design", "anchor_text": "fractional factorial design"}, {"url": "https://online.stat.psu.edu/stat509/node/123/", "anchor_text": "crossover experiment"}, {"url": "http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/SAS/SAS4-OneSampleTtest/SAS4-OneSampleTtest7.html", "anchor_text": "paired t-test"}, {"url": "https://commons.wikimedia.org/wiki/File:Faced_products_on_a_supermarket_shelf.JPG", "anchor_text": "source"}, {"url": "https://www.itl.nist.gov/div898/handbook/pri/section3/pri3333.htm", "anchor_text": "Blocking"}, {"url": "https://www.pxfuel.com/en/free-photo-otosn", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Response_surface_methodology", "anchor_text": "response surface method"}, {"url": "https://www.itl.nist.gov/div898/handbook/pri/section5/pri542.htm", "anchor_text": "Simplex lattice designs"}, {"url": "https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d", "anchor_text": "counterfactuals"}, {"url": "http://docs.pyro.ai/en/stable/contrib.oed.html", "anchor_text": "choosing observation sites"}, {"url": "https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55", "anchor_text": "clever ways"}, {"url": "https://www.mailman.columbia.edu/research/population-health-methods/difference-difference-estimation", "anchor_text": "difference-in-differences"}, {"url": "https://www.bmj.com/content/350/bmj.h2750", "anchor_text": "interrupted time series"}, {"url": "https://economics.mit.edu/files/11859", "anchor_text": "synthetic controls"}, {"url": "https://google.github.io/CausalImpact/", "anchor_text": "Google\u2019s CausalImpact package"}, {"url": "https://google.github.io/CausalImpact/CausalImpact.html", "anchor_text": "CausalImpact documentation"}, {"url": "https://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X", "anchor_text": "Pearl\u2019s causality framework"}, {"url": "https://www.econlib.org/archives/2016/07/immigrant_quali.html", "anchor_text": "source"}, {"url": "https://medium.com/analytics-vidhya/how-this-frequentist-turned-bayesian-7066e210a301?source=friends_link&sk=578af7782e8824afe55043bc5d6c2838", "anchor_text": "previous article about Bayesian statistics"}, {"url": "http://bayes.cs.ucla.edu/BOOK-2K/ch6-5.pdf", "anchor_text": "collapsible"}, {"url": "https://gking.harvard.edu/files/gking/files/psnot.pdf", "anchor_text": "it\u2019s terrible"}, {"url": "http://www.dagitty.net/", "anchor_text": "algorithms"}, {"url": "http://bayes.cs.ucla.edu/BOOK-2K/ch3-3.pdf", "anchor_text": "front-door adjustment"}, {"url": "https://en.wikipedia.org/wiki/Selection_bias", "anchor_text": "Selection bias"}, {"url": "https://arxiv.org/pdf/1902.07409.pdf", "anchor_text": "Causal forests"}, {"url": "https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/", "anchor_text": "doubly robust estimation"}, {"url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff", "anchor_text": "biased"}, {"url": "https://en.wikipedia.org/wiki/Instrumental_variables_estimation", "anchor_text": "two-stage least squares"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d8e462d90a0b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----d8e462d90a0b---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/analytics?source=post_page-----d8e462d90a0b---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/tag/experiment?source=post_page-----d8e462d90a0b---------------experiment-----------------", "anchor_text": "Experiment"}, {"url": "https://medium.com/tag/causality?source=post_page-----d8e462d90a0b---------------causality-----------------", "anchor_text": "Causality"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&user=Wicaksono+Wijono&userId=5e69e4814607&source=-----d8e462d90a0b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&user=Wicaksono+Wijono&userId=5e69e4814607&source=-----d8e462d90a0b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd8e462d90a0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d8e462d90a0b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d8e462d90a0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wwijono?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wwijono?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wicaksono Wijono"}, {"url": "https://medium.com/@wwijono/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "785 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5e69e4814607&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&user=Wicaksono+Wijono&userId=5e69e4814607&source=post_page-5e69e4814607--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd2f99368ce0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b&newsletterV3=5e69e4814607&newsletterV3Id=d2f99368ce0a&user=Wicaksono+Wijono&userId=5e69e4814607&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}