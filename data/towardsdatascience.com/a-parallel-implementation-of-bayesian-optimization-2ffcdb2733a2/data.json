{"url": "https://towardsdatascience.com/a-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2", "time": 1683014143.082367, "path": "towardsdatascience.com/a-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2/", "webpage": {"metadata": {"title": "A Parallel Implementation of Bayesian Optimization | by Sam Von Wilson | Towards Data Science", "h1": "A Parallel Implementation of Bayesian Optimization", "description": "The concept of \u2018optimization\u2019 is central to data science. We minimize loss by optimizing weights in a neural network. We optimize hyper-parameters in our gradient boosted trees to find the best\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=92-98SYOdlY", "anchor_text": "this video", "paragraph_index": 4}, {"url": "https://github.com/AnotherSamWilson/ParBayesianOptimization#how-long-should-it-run-for", "anchor_text": "here", "paragraph_index": 19}], "all_paragraphs": ["The concept of \u2018optimization\u2019 is central to data science. We minimize loss by optimizing weights in a neural network. We optimize hyper-parameters in our gradient boosted trees to find the best bias-variance trade-off. We use A-B testing to optimize behavior on our websites. Whether our function is a neural network, consumer behavior, or something more sinister, we all have something we want to optimize.", "Sometimes the functions we are trying to optimize are expensive, and we wish to get to our destination in as few steps as possible. Sometimes we want to be confident that we find the best possible solution, and sometimes our functions don\u2019t have a tractable gradient, so there is no nice arrow to point us in the right direction. Often, our functions have random elements to them, so we are really trying to optimize f(x) = y + e, where e is some random error element. Bayesian optimization is a function optimizer (maximizer) which thrives in these conditions.", "Let\u2019s say we have a function f, and we want to find the x which maximizes (or minimizes) f(x). We have many, many options. However, if our function fits the description right above the table of contents, we will definitely want to consider Bayesian optimization.", "There are several different methods for performing Bayesian optimization. All of them involve creating an assumption about how certain things are distributed, making a decision based on that assumption, and then updating the assumption.", "The method in this article uses Gaussian processes to create an assumption about how f(x) is distributed. These processes can be thought of as a distribution of functions \u2014 where drawing a random sample from a Gaussian distribution results in a number, drawing a random sample from a Gaussian process results in a function. If you are not familiar with Gaussian processes, this is a little hard to picture. I recommend this video, which is what made the concept click for me.", "The algorithm itself can be summarized as such:", "Here we walk through a single iteration of Bayesian optimization without using a package. The process is pretty straightforward. First, we define a toy function func we want to maximize, and then we sample it 4 times:", "We are pretending we don\u2019t know the true function, so all we see in practice are the 4 points we sampled. For the sake of keeping this walk-through interesting, we did a pretty miserable job of selection our initial points. Let\u2019s fit a Gaussian process to the 4 points to define our assumption about how output is distributed for each input.", "Let\u2019s take a look at our Gaussian process next to the points we have sampled and the true function value:", "The Gaussian process allows us to define a normal distribution of the output for each input. In the picture above, the purple lines show the Gaussian process. The middle line is the mean, and the upper/lower lines are the 95th percentiles of the normal distribution at that input. So, for example, if we wanted to know how we assume the output is distributed at input = 30, we could do:", "This tells us that we are assuming, at input = 30, our output follows a normal distribution with mean = 0.0558, and sd = 0.0078.", "Now that we have defined our assumption about the distribution of the output, we need to determine where to sample the function next. To do this, we need to define how \u2018promising\u2019 an input is. We do this by defining an acquisition function. There are several to choose from:", "Of these, the upper confidence bound is the easiest to implement, so let\u2019s define the function and plot it on our chart:", "We can see that our upper confidence bound is maximized (green diamond) somewhere between 10 and 15, so let\u2019s find the specific spot, sample it, and update our GP:", "We have just completed one iteration of Bayesian optimization! If we continued to run more, we would see our chart evolve:", "We won\u2019t implement this part from scratch. Instead, we will use the ParBayesianOptimization R package to do the heavy lifting. This package allows us to sample multiple promising points at once. If there is only 1 promising point, it samples the surrounding area multiple times. So, in our first example, we would sample all 5 of the local maximums of the acquisition function:", "Let\u2019s get it up and running and see what comes out. We initialize the process with the 4 same points as above, and then run 1 optimization step with 5 points:", "Our score summary shows us that bayesOpt ran our 4 initial points (Epoch = 0) and then ran 1 optimization step (Epoch = 1) in which it sampled all 5 of the local optimums of the acquisition function. If we ran for more iterations, we would continue to sample 5 points at a time. If our function to maximize was actually expensive, this would allow us to find the global optimum much faster.", "The acqThresh parameter in bayesOpt is crucial to the sampling process. This parameter represents the minimum percentage of the global optimum of the acquisition function that a local optimum must reach for it to be sampled. For example, if acqThresh=0.5, then each local optimum (upper confidence bound, in our case) must be at least 50% of the global optimum, or it will be ignored. We set acqThresh=0, so all local optimums would be sampled.", "Draw your attention to the gpUtility field above. This is the scaled value of the acquisition function (upper confidence bound in our case) at each of the points sampled. If you notice this value converging to 0 over Epochs, then the Gaussian process\u2019 opinion is that there is not many promising points left to explore. A more thorough, package specific explanation can be found here.", "Bayesian optimization is an amazing tool for niche scenarios. In modern data science, it is commonly used to optimize hyper-parameters for black box models. However, being a general function optimizer, it has found uses in many different places. I personally tend to use this method to tune my hyper-parameters in both R and Python.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist @ Farrell Day, Open Source Contributor"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ffcdb2733a2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@anothersamwilson?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anothersamwilson?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Sam Von Wilson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F905a42098909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&user=Sam+Von+Wilson&userId=905a42098909&source=post_page-905a42098909----2ffcdb2733a2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=92-98SYOdlY", "anchor_text": "this video"}, {"url": "https://github.com/AnotherSamWilson/ParBayesianOptimization#how-long-should-it-run-for", "anchor_text": "here"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&user=Sam+Von+Wilson&userId=905a42098909&source=-----2ffcdb2733a2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&user=Sam+Von+Wilson&userId=905a42098909&source=-----2ffcdb2733a2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2ffcdb2733a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2ffcdb2733a2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2ffcdb2733a2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anothersamwilson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anothersamwilson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sam Von Wilson"}, {"url": "https://medium.com/@anothersamwilson/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "35 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F905a42098909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&user=Sam+Von+Wilson&userId=905a42098909&source=post_page-905a42098909--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F590015a25093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-parallel-implementation-of-bayesian-optimization-2ffcdb2733a2&newsletterV3=905a42098909&newsletterV3Id=590015a25093&user=Sam+Von+Wilson&userId=905a42098909&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}