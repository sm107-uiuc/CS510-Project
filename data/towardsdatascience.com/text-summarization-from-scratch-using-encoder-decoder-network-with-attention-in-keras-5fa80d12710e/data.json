{"url": "https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e", "time": 1683009186.253144, "path": "towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e/", "webpage": {"metadata": {"title": "Text Summarization from scratch using Encoder-Decoder network with Attention in Keras | by Varun Saravanan | Towards Data Science", "h1": "Text Summarization from scratch using Encoder-Decoder network with Attention in Keras", "description": "During our school days, most of us would have encountered the reading comprehension section of our English paper. We would be given a paragraph or Essay based on which we need to answer several\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/sunnysai12345/News_Summary/blob/master/news_summary_more.csv", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://github.com/thushv89/attention_keras/blob/master/layers/attention.py", "anchor_text": "here", "paragraph_index": 38}, {"url": "http://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 54}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "here", "paragraph_index": 55}], "all_paragraphs": ["During our school days, most of us would have encountered the reading comprehension section of our English paper. We would be given a paragraph or Essay based on which we need to answer several questions.", "How do we as humans approach this task at hand? We go through the entire text, make sense of the context in which the question is asked and then we write answers. Is there a way we can use AI and deep learning techniques to mimic this behavior of us?", "Automatic text summarization is a common problem in machine learning and natural language processing (NLP). There are two approaches to this problem.", "2. Abstractive Summarization-Abstractive text summarization, on the other hand, is a technique in which the summary is generated by generating novel sentences by either rephrasing or using the new words, instead of simply extracting the important sentences. For example, some questions in the reading comprehension might not be straightforward in such cases we do rephrasing or use new words to answer such questions.", "We humans can easily do both kinds of text summarization. In this blog let us see how to implement abstractive text summarization using deep learning techniques.", "Given a news article text, we are going to summarize it and generate appropriate headlines.", "Whenever any media account shares a news story on Twitter or in any social networking site, they provide a crisp headlines /clickbait to make users click the link and read the article.", "Often media houses provide sensational headlines that serve as a clickbait. This is a technique often employed to increase clicks to their site.", "Our problem statement is to generate headlines given article text. For this we are using the news_summary dataset. You can download the dataset here", "Before we go through the code, let us learn some concepts needed for building an abstractive text summarizer.", "Techniques like multi-layer perceptron(MLP) work well your input data is vector and convolutional neural networks(CNN) works very well if your input data is an image.", "What if my input x is a sequence? What if x is a sequence of words. In most languages sequence of words matters a lot. We need to somehow preserve the sequence of words.", "The core idea here is if output depends on a sequence of inputs then we need to build a new type of neural network which gives importance to sequence information, which somehow retains and leverages the sequence information.", "We can build a Seq2Seq model on any problem which involves sequential information. In our case, our objective is to build a text summarizer where the input is a long sequence of words(in a text body), and the output is a summary (which is a sequence as well). So, we can model this as a Many-to-Many Seq2Seq problem.", "A many to many seq2seq model has two building blocks- Encoder and Decoder. The Encoder-Decoder architecture is mainly used to solve the sequence-to-sequence (Seq2Seq) problems where the input and output sequences are of different lengths.", "Generally, variants of Recurrent Neural Networks (RNNs), i.e. Gated Recurrent Neural Network (GRU) or Long Short Term Memory (LSTM), are preferred as the encoder and decoder components. This is because they are capable of capturing long term dependencies by overcoming the problem of vanishing gradient.", "Let us see a high-level overview of Encoder-Decoder architecture and then see its detailed working in the training and inference phase.", "Intuitively this is what happens in our encoder-decoder network:", "1. We feed in our input (in our case text from news articles) to the Encoder unit. Encoder reads the input sequence and summarizes the information in something called the internal state vectors (in case of LSTM these are called the hidden state and cell state vectors).", "2. The encoder generates something called the context vector, which gets passed to the decoder unit as input. The outputs generated by the encoder are discarded and only the context vector is passed over to the decoder.", "3. The decoder unit generates an output sequence based on the context vector.", "We can set up the Encoder-Decoder in 2 phases:", "In the training phase at every time step, we feed in words from a sentence one by one in sequence to the encoder. For example, if there is a sentence \u201cI am a good boy\u201d, then at time step t=1, the word I is fed, then at time step t=2, the word am is fed, and so on.", "Say for example we have a sequence x comprising of words x1,x2,x3,x4 then the encoder in training phase looks like below:", "The initial state of the LSTM unit is zero vector or it is randomly initiated. Now h1,c1 is the state of LSTM unit at time step t=1 when the word x1 of the sequence x is fed as input.", "Similarly h2,c2 is the state of the LSTM unit at time step t=2 when the word x2 of the sequence x is fed as input and so on.", "The hidden state (hi) and cell state (ci) of the last time step are used to initialize the decoder.", "Now the initial states of the decoder are initialized to the final states of the encoder. This intuitively means that the decoder is trained to start generating the output sequence depending on the information encoded by the encoder.", "<start> and <end> are the special tokens that are added to the target sequence(in our case the headlines we want to predict)before feeding it into the decoder.", "The target sequence is unknown while decoding the test sequence. So, we start predicting the target sequence by sending the first word into the decoder which would be always the <start> token. And the <end> token signals the end of the sentence.", "Now at the inference phase, we want our decoder to predict our output sequence(in our case headlines). After training, the model is tested on new source sequences for which the target sequence is unknown. So, we need to set up the inference architecture to decode a test sequence", "At every time step, the LSTM unit in my decoder gives me outputs y\u00b9,y\u00b2,y\u00b3\u2026y^k. where k is the length of the output sequence. At time step t=1 output y\u00b9 is generated, at time t=2 output y ^2 is generated and so on.", "But in the testing stage as mentioned earlier we do not know what the length of our target sequence would be. How do we tackle this problem? Or in other words, how do we decode the test sequence. We follow the below steps for the same :", "Researchers observed that the BLEU score deteriorates as the sentence length for the source and reference text increases. It does a reasonable job up-till a sentence length of 20, after that the score falls.", "For our task both the source and target sentence length are higher than 20, hence we need to overcome this shortcoming of the encoder-decoder network.", "There are 2 different classes of attention mechanism depending on the way the attended context vector is derived:", "We will be using global attention for our task at hand.", "Now that we have learned all the concepts lets dive deep into code. First, let us import all the necessary libraries", "Keras does not officially support the attention layer. So, we can either implement our attention layer or use a third-party implementation. We will go with the latter option for this blog. You can download the attention layer from here and copy it in a different file called attention.py and then we can import the same.", "Now let us read our dataset. Due to computational constraints we shall just load 20000 rows from our dataset.", "Now we need to clean our text, we perform the following steps for the text and headlines pair:", "We use the following function to expand contractions", "We preprocess the text and headline pairs using the below function:", "The above code snippet does preprocessing for article text. The same code can be used for the headlines' column also.", "Here the text is our Source and the headlines are our target. We need to add start and end tokens to our target sequences ie our headlines as we saw earlier.", "Now we will add a new feature word count. We will add this feature for text as well as headlines. Then let us see the percentile values of word count for text and headlines. This will help us to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence", "Let us get the percentile values of the word count of text.", "Let us see the percentile values from 90 to 100 for the word count of text", "We take the 95th percentile value ie 62 to be our maximum length of text. Similarly we plot the percentile values of headlines and we take 15 to be the maximum length of headlines.", "Before moving on to build our model, we need to do a test train split for our data. We use the sklearn to do the same. We will use 70 % of the data as training data and evaluate the performance on the remaining 30 %.", "We earlier saw that to the encoder we send the text sequences at every time step. The words from the input sequence at every time step is passed to the encoder network. Before it is passed as input to the encoder unit we add an embedding layer.", "An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.", "For instance, \u201ccoconut\u201d and \u201cpolar bear\u201d are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \u201ckitchen\u201d and \u201cdinner\u201d are related words, so they should be embedded close to each other.", "Word embeddings are computed by applying dimensionality reduction techniques to datasets of co-occurrence statistics between words in a corpus of text. This can be done via neural networks (the \u201cword2vec\u201d technique), or matrix factorization.", "One of the most popularly used embedding technique is GloVe embedding. GloVe stands for \u201cGlobal Vectors for Word Representation\u201d.We will be using GloVe embeddings for our input text sequence(you can read about GloVe here).", "Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them here", "Now we need to check how many words from our input text sequence corpus are present in the Glove Corpus. This is very important as our model will be only learning the words present in the Glove corpus. We can check this by using the code below :", "We observe that the percentage of words present in both glove vectors and our corpus is nearly 73% which is very good.", "Any Machine learning or deep learning model cannot understand text directly. We need to convert it into numbers. We can do this through tokenization.", "Tokenization is the process of dividing the text into a set of meaningful pieces. These pieces are called tokens. For example, we can divide a chunk of text into words, or we can divide it into sentences. A tokenizer builds the vocabulary and converts a word sequence to an integer sequence.", "We tokenize the text as well as the headline, then we convert it to integers and then pad it to maximum length ( we arrived at this maximum length in the previous section)", "Next, let\u2019s build the dictionary to convert the index to word for target and source vocabulary", "Now in the earlier section, we had loaded the glove model. Let us now get the weight matrix for the text sequences from the glove model. Words not found in the glove embedding would be initialized to zero vectors. We can use the following piece of code to get the weight matrix :", "We build our Encoder by stacking 3 LSTMs vertically with the horizontal axis being the time axis. Stacking helps in increasing the model complexity and also helps in better representation of our input text sequence.", "A stacked LSTM model would visually look like this :", "(we are using stacking only for our Encoder architecture and not for decoder as shown in the image below.)", "We can use the below code to build our model:", "Let us have a look at our model summary :", "Now we see that there are non-trainable parameters. These parameters are from weight vectors of our embedding matrix. We have set trainable as False and hence these weights will not be learned by our model.", "We shall now go ahead and compile our model", "Now let us checkpoint our model and save the best weights. Also, let us define early stopping for our model with patience of 3 (which means the training stops if the validation loss does not reduce after 3 epochs)", "Now let us train our model with a batch size of 128 and", "We plot the Epoch vs Val loss plot", "We observe that the val loss does not reduce after the 18th Epoch so we stop the training.", "Now we build the inference for our model as discussed in the previous sections", "Now we are defining a function below which is the implementation of the inference process (decoding test sequence)", "Now let us define the functions to convert an integer sequence to a word sequence for text as well as headlines :", "Finally we have the prediction function, lets see how our model performs!", "The following are some predictions made by our model. The predictions may not be very good as we have trained our model on fewer data points (around 15 K).", "Even though the predicted and actual headlines do not match exactly in terms of words but somewhat the same meaning is conveyed. This is quite a decent result we have got having trained our model on a very small dataset. Google /Facebook/Amazon train models on Giga/Tera Bytes of data :).", "We can still improve our model to a great extent by", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning and Deep Learning Enthusiast :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5fa80d12710e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@varunsaravanan?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@varunsaravanan?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Varun Saravanan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31db5a4f0059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&user=Varun+Saravanan&userId=31db5a4f0059&source=post_page-31db5a4f0059----5fa80d12710e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@craftedbygc?utm_source=medium&utm_medium=referral", "anchor_text": "Green Chameleon"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/sunnysai12345/News_Summary/blob/master/news_summary_more.csv", "anchor_text": "here"}, {"url": "https://github.com/thushv89/attention_keras/blob/master/layers/attention.py", "anchor_text": "here"}, {"url": "http://analyticsvidhya.com", "anchor_text": "analyticsvidhya.com"}, {"url": "http://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918", "anchor_text": "https://towardsdatascience.com/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918"}, {"url": "http://www.analyticsvidhya.com", "anchor_text": "www.analyticsvidhya.com"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "https://arxiv.org/abs/1706.03762"}, {"url": "https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c", "anchor_text": "https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c"}, {"url": "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/", "anchor_text": "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5fa80d12710e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/text-summarization?source=post_page-----5fa80d12710e---------------text_summarization-----------------", "anchor_text": "Text Summarization"}, {"url": "https://medium.com/tag/encoder-decoder?source=post_page-----5fa80d12710e---------------encoder_decoder-----------------", "anchor_text": "Encoder Decoder"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----5fa80d12710e---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&user=Varun+Saravanan&userId=31db5a4f0059&source=-----5fa80d12710e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&user=Varun+Saravanan&userId=31db5a4f0059&source=-----5fa80d12710e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5fa80d12710e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5fa80d12710e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5fa80d12710e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5fa80d12710e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5fa80d12710e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@varunsaravanan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@varunsaravanan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Varun Saravanan"}, {"url": "https://medium.com/@varunsaravanan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "19 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31db5a4f0059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&user=Varun+Saravanan&userId=31db5a4f0059&source=post_page-31db5a4f0059--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba9371bd185c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e&newsletterV3=31db5a4f0059&newsletterV3Id=ba9371bd185c&user=Varun+Saravanan&userId=31db5a4f0059&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}