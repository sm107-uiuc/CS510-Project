{"url": "https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1", "time": 1683009203.822767, "path": "towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1/", "webpage": {"metadata": {"title": "On Optimization of Deep Neural Networks | by Parmeet Bhatia | Towards Data Science", "h1": "On Optimization of Deep Neural Networks", "description": "The success of AlexNet on a popular Image classification benchmark brought Deep Learning to spotlight in 2011. Since then, it has achieved extraordinary results in numerous domains. Most notably\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "AlexNet", "paragraph_index": 0}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf", "anchor_text": "Efficient Backpropagation", "paragraph_index": 5}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527", "anchor_text": "fast learning for Deep Belief Networks", "paragraph_index": 11}, {"url": "http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf", "anchor_text": "Greedy Layer-Wise Training of Deep Networks", "paragraph_index": 11}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks", "paragraph_index": 23}, {"url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html", "anchor_text": "Delving Deep into Rectifiers", "paragraph_index": 25}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "dropout", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition", "paragraph_index": 39}, {"url": "https://arxiv.org/abs/1505.00387", "anchor_text": "Highway networks", "paragraph_index": 40}, {"url": "https://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Long Short Term Memory (LSTM) networks", "paragraph_index": 40}, {"url": "https://twitter.com/alecrad", "anchor_text": "Alec Radford", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Adam: A method for stochastic optimization", "paragraph_index": 53}, {"url": "https://cloud.google.com/blog/products/gcp/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow", "anchor_text": "cucumber farmer scenario", "paragraph_index": 57}], "all_paragraphs": ["The success of AlexNet on a popular Image classification benchmark brought Deep Learning to spotlight in 2011. Since then, it has achieved extraordinary results in numerous domains.", "Most notably, deep learning has had an exceptional bearing on computer vision, speech recognition, and natural language processing (NLP), single-handedly bringing AI back to life again.", "On the surface, one could assign credit to large datasets and high availability of computing resources. But from a fundamental standpoint, they only provide fuel to neural networks to advance the extra mile.", "What helped neural networks make sense of huge datasets?", "What exactly are the few but extraordinary developments in the last decade that made neural networks the ultimate learning machines?", "Back in 1998, Yann LeCun et al. published a chapter on Efficient Backpropagation. To efficiently train the neural networks, the authors proposed a few practical guidelines.", "These guidelines emphasize on stochastic learning, examples shuffling, normalizing input, activation functions, network initialization, and adaptive learning rate.", "Unfortunately, there were no standardized tools to apply these ideas out-of-the-box. It was extremely difficult to converge a deep network, let alone generalize it well on unseen data.", "Fortunately, these guidelines set the path to develop more formal tools in the coming years.", "What is it that neural networks were struggling with?", "Many of the difficulties in training deep neural networks occur due to poor network initialization and insufficient gradients at the lower layers.", "Hinton et al. and Yoshio et al. explored the idea of greedy layer-wise training. Two remarkable papers that stand-out in this paradigm are \u201cfast learning for Deep Belief Networks\u201d and \u201cGreedy Layer-Wise Training of Deep Networks\u201d. Both these papers managed to train much deeper networks for the first time.", "Unfortunately, the aforementioned still require additional efforts to train the network layer by layer followed by fine-tuning of the entire network.", "To attain the magnitude of success we see today with Deep Learning, there was a need to develop formal techniques and not just hacks.", "We needed tools that work across-domain and in a wide range of settings. We needed an eco-system to train neural networks end-2-end without bells and whistles.", "Fortunately, with all the guiding principles and extra-ordinary efforts of the elite few, we have come passed the tipping point where deep learning can show its true potential.", "The following are a few but vital developments in the Deep Learning ecosystem that made it the household name of every engineer\u2019s toolkit.", "Non-linearity is the powerhouse of neural networks. Without it, neural networks lack expressiveness and simply boil down to a linear transformation of input data.", "Unfortunately, the activations involving exponentials (sigmoid, hyperbolic tangents, etc.) used for so long to realize non-linearity have had their issues. They become exhaustively reluctant to allow gradient flow once they reach their saturating zones, vastly inhibiting the network\u2019s learning potential.", "Fortunately, AlexNet came to rescue and demonstrated for the first time the untouched potential of the rectified linear units (ReLU) as activation functions.", "ReLU helps to train the network much faster (due to its non-saturating nature) and made it possible to train a much deeper network in an end-to-end fashion.", "ReLU became the default choice of activation for the Deep Learning toolkit. From an optimization viewpoint, it solves the long-standing issue of the so-called vanishing gradient.", "Optimizing neural networks is a highly non-convex problem. There are very few things that are more harmful than having a bad initialization of network weights.", "The challenge of identifying a good initialization strategy was first explored and addressed in the influential work, \u201cUnderstanding the difficulty of training deep feedforward neural networks\u201d.", "Here, the authors derived an appropriate range of uniform distribution for each layer. This distribution is used to sample the weights for the network initialization. It commonly goes by the name Xavier\u2019s initialization.", "The assumptions made by authors to derive the sampling distribution range, sadly, lead to a bad marriage of Xavier\u2019s initialization with ReLUs (remember the default choice of activations!). Fortunately, the paper \u201cDelving Deep into Rectifiers\u201d, provided a fix by adapting the range of uniform distribution appropriate for ReLUs. The fix made it work so well that this paper became winning entry for ImageNet classification. This initialization is commonly referred to as Kaiming\u2019s initialization.", "Both Xavier\u2019s and Kaiming\u2019s initialization provide out-of-the-box initialization tool-kits. Now researchers have one less thing to blame when things don\u2019t go as planned!", "Neural networks can easily (over-)adapt to the training examples, given a huge number of parameters. This naturally calls for network regularization.", "Weight decay (also known as L2 regularization) is one of the common regularization techniques employed in machine learning. Unfortunately, it may restrict the network to utilize its full potential.", "Hinton et al. introduce the idea of dropout that is perhaps the most elegant solution to date for network regularization. The key idea proposed is to randomly drop units (hence the name dropout) during training. This helps to avoid co-adaptation of features and hence greatly reduce overfitting.", "From an optimization viewpoint, only a part of the network weights is updated for any given training example.", "This is similar in the spirit of ensemble methods where each learner is exposed to only part of the training data. Eventually, the final trained network is an ensemble of (exponential large number of) networks with shared weights.", "Dropout provided an out-of-the-box solution to regularize deep neural networks.", "The concept of data normalization or standardization is not new in machine learning. Data normalization enables efficient optimization of learning algorithms.", "Deep learning is simply a stack of layers that transform its input and pass it on to the next layer. Hence there is no good reason why one should restrict data normalization to the very first layer.", "The idea of normalizing inputs for every layer was first formally presented in the paper called Batch Normalization. This work achieved the best performance on the ImageNet classification benchmark by beating all the previous records at an order of magnitude faster training time.", "Feature normalization is perhaps the most effective technique in accelerating the optimization of deep networks. Batch Normalization is a clear winner in this paradigm!", "Batch Normalization also works at a wide selection of learning rates and alleviates the need for explicit network regularization. It provides an out-of-the-box solution to expedite training.", "With depth comes the power of abstraction. This leads to better generalization and improvements in the test error. Unfortunately, insufficient gradient flow became the bottleneck for training deeper networks.", "This time ResNets came to rescue. Researchers at Microsoft-Research proposed the idea of residual connections in their milestone paper called Deep Residual Learning for Image Recognition.", "The idea of shortcut connections and residual learning is not new. One may draw parallels to other classical works including Highway networks and Long Short Term Memory (LSTM) networks. Outside deep learning, Gradient Boosted Decision Trees (GBDT) also learns residual function in each of the consecutive trees.", "That being said, the authors of Deep Residual learning paper are the first to show how to enable Residual learning effectively by the means of identity mappings!", "The key postulate in the paper is that it is easier to learn a residual function. This can be realized by simply providing a direct connection from the layer input to the layer\u2019s output. This leads to better information flow during the forward pass and an effective gradient flow in the backward pass.", "From an optimization standpoint, it simply eliminates the problem of vanishing gradients and makes it theoretically possible to train extremely deep networks.", "Residual connections provided an out-of-the-box solution to go deeper.", "The aforementioned tools provide the necessary elements to obtain proper gradients for the network parameter updates. Ultimately we needed to devise an effective strategy to utilize these gradients.", "This time, the inspiration came from physics in the form of momentum.", "One of the most commonly used optimizers is Stochastic gradient descent (SGD). Unfortunately, SGD is inherently limiting as it employs first-order information only. It cannot compete with optimizers that use second-order information, the so-called momentum.", "By utilizing second-order dynamics, optimizers can effectively get an adaptive learning rate for each parameter. This, in turn, lends itself to extremely fast convergence as presented in the figure below by Alec Radford. Here, SGD is compared against other optimizers that use momentum.", "How to intuitively think about Momentum?", "Imagine two balls, one rolling on a slope and another rolling inside a bowl. What will happen if both the balls are left to their destiny?", "For the first bowl, it keeps increasing its velocity (in other words momentum), and for the second it will simply come to stand-still at the bottom of the bowl after a few swings.", "The same thing happens to parameters. If the weight tends to move in a particular direction, momentum builds up (just like a rolling ball on a downward slope). By taking this momentum into account, one could potentially update the weights faster and converge in lesser steps. On the contrary, if the weight is mostly fluctuating around a certain point (just like a rolling ball inside a bowl), this destroys the momentum. This will effectively set the learning rate close to zero, eventually settling down the parameter to its local optima.", "The most notable work in this paradigm is Adam: A method for stochastic optimization. With more than 40k citations as of this writing (roughly of the same order as ResNet), it is a clear winner and a common choice of optimizer among deep learning practitioners.", "A chapter on Efficient Backpropagation was published back in 1998, which inspired the development of many of the aforementioned contemporary tools.", "It took almost two decades to develop tools that gracefully solve several training issues in a wide range of settings. Fortunately, with all these advancements, training a neural network is no longer an art.", "A diligent engineer could simply bring various pieces together in a few lines of Python code and is almost guaranteed to achieve good results.", "This truly brings the cucumber farmer scenario to life that led to the widespread adaption of deep learning and the field is no longer confined to the elite few.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F21de9e83e1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----21de9e83e1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bhatia.parmeet?source=post_page-----21de9e83e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhatia.parmeet?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Parmeet Bhatia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68eb2b2d729b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&user=Parmeet+Bhatia&userId=68eb2b2d729b&source=post_page-68eb2b2d729b----21de9e83e1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1805.04829.pdf", "anchor_text": "Spatial Uncertainty Sampling for End-to-End Control"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "AlexNet"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf", "anchor_text": "Efficient Backpropagation"}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527", "anchor_text": "fast learning for Deep Belief Networks"}, {"url": "http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf", "anchor_text": "Greedy Layer-Wise Training of Deep Networks"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks"}, {"url": "https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html", "anchor_text": "Delving Deep into Rectifiers"}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "dropout"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition"}, {"url": "https://arxiv.org/abs/1505.00387", "anchor_text": "Highway networks"}, {"url": "https://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Long Short Term Memory (LSTM) networks"}, {"url": "https://twitter.com/alecrad", "anchor_text": "Alec Radford"}, {"url": "https://twitter.com/alecrad", "anchor_text": "Alec Radford"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Adam: A method for stochastic optimization"}, {"url": "https://cloud.google.com/blog/products/gcp/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow", "anchor_text": "cucumber farmer scenario"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----21de9e83e1---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----21de9e83e1---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----21de9e83e1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----21de9e83e1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&user=Parmeet+Bhatia&userId=68eb2b2d729b&source=-----21de9e83e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&user=Parmeet+Bhatia&userId=68eb2b2d729b&source=-----21de9e83e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F21de9e83e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----21de9e83e1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----21de9e83e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----21de9e83e1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----21de9e83e1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----21de9e83e1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----21de9e83e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhatia.parmeet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhatia.parmeet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Parmeet Bhatia"}, {"url": "https://medium.com/@bhatia.parmeet/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "72 Followers"}, {"url": "http://www.linkedin.com/in/parmeet-bhatia-616b0923", "anchor_text": "www.linkedin.com/in/parmeet-bhatia-616b0923"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68eb2b2d729b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&user=Parmeet+Bhatia&userId=68eb2b2d729b&source=post_page-68eb2b2d729b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F68eb2b2d729b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-optimization-of-deep-neural-networks-21de9e83e1&user=Parmeet+Bhatia&userId=68eb2b2d729b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}