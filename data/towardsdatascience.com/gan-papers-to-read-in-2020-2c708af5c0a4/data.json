{"url": "https://towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4", "time": 1683008898.788121, "path": "towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4/", "webpage": {"metadata": {"title": "GAN Papers to Read in 2020. Reading suggestions on Generative\u2026 | by Ygor Serpa | Towards Data Science", "h1": "GAN Papers to Read in 2020", "description": "Generative Adversarial Networks (GANs) are one of the most innovative ideas proposed in this decade. At its core, GANs are an unsupervised model for generating new elements from a set of similar\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets", "anchor_text": "\u201cGenerative adversarial nets.\u201d", "paragraph_index": 4}, {"url": "https://www.wired.com/2017/04/googles-dueling-neural-networks-spar-get-smarter-no-humans-required/", "anchor_text": "was in a bar with his friends", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1906.02691", "anchor_text": "Variational Auto-Encoders (VAEs)", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1601.06759", "anchor_text": "Autoregressive models", "paragraph_index": 10}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html", "anchor_text": "\u201cA style-based generator architecture for generative adversarial networks.\u201d", "paragraph_index": 11}, {"url": "https://github.com/NVlabs/ffhq-dataset", "anchor_text": "Flickr-Faces-HQ Dataset (FFHQ)", "paragraph_index": 12}, {"url": "https://github.com/NVlabs/stylegan", "anchor_text": "eight Tesla V100 GPUs for a whole week", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1912.04958", "anchor_text": "a follow-up paper was released", "paragraph_index": 17}, {"url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html", "anchor_text": "Image2StyleGAN", "paragraph_index": 17}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "Image-to-image translation with conditional adversarial networks.\u201d", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1703.10593", "anchor_text": "\u201cUnpaired image-to-image translation using cycle-consistent adversarial networks.\u201d", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?time_continue=57&v=xkLtgwWxrec", "anchor_text": "turning Fortnite into PUBG", "paragraph_index": 21}, {"url": "https://www.tensorflow.org/tutorials/generative/dcgan", "anchor_text": "original GAN", "paragraph_index": 25}, {"url": "https://www.tensorflow.org/tutorials/generative/pix2pix", "anchor_text": "the Pix2Pix model", "paragraph_index": 25}, {"url": "https://www.tensorflow.org/tutorials/generative/cyclegan", "anchor_text": "the CycleGAN", "paragraph_index": 25}, {"url": "https://papers.nips.cc/paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html", "anchor_text": "\u201cImproved techniques for training gans.\u201d", "paragraph_index": 26}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "Turing award winner Yann LeCun", "paragraph_index": 29}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "AAAI 20 keynote speech", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1708.05509", "anchor_text": "\u201cTowards the automatic anime characters creation with generative adversarial networks.\u201d", "paragraph_index": 34}, {"url": "https://waifulabs.com", "anchor_text": "waifu generation", "paragraph_index": 35}, {"url": "https://www.sbgames.org/sbgames2019/files/papers/ComputacaoFull/197880.pdf", "anchor_text": "sprite completion", "paragraph_index": 35}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf", "anchor_text": "cartoon stylization", "paragraph_index": 35}, {"url": "https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them", "anchor_text": "deep fakes", "paragraph_index": 35}, {"url": "https://affinelayer.com/pixsrv/", "anchor_text": "Pix2Pix", "paragraph_index": 38}, {"url": "https://make.girls.moe/#/", "anchor_text": "AnimeGAN", "paragraph_index": 38}, {"url": "https://reiinakano.com/arbitrary-image-stylization-tfjs/", "anchor_text": "StyleTransfer", "paragraph_index": 38}, {"url": "https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html", "anchor_text": "SketchRNN", "paragraph_index": 38}, {"url": "https://www.autodraw.com/", "anchor_text": "AutoDraw", "paragraph_index": 38}, {"url": "https://www.artbreeder.com/", "anchor_text": "ArtBreeder", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1603.08155", "anchor_text": "Perceptual", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1605.01368", "anchor_text": "Total Variance", "paragraph_index": 47}, {"url": "https://en.wikipedia.org/wiki/Lipschitz_continuity", "anchor_text": "1-Lipschitz", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "SELU activation function", "paragraph_index": 51}, {"url": "https://en.wikipedia.org/wiki/Dropout_(neural_networks)", "anchor_text": "dropout", "paragraph_index": 52}, {"url": "https://arxiv.org/abs/1803.08494", "anchor_text": "Group Normalization", "paragraph_index": 53}, {"url": "http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html", "anchor_text": "Adaptive Instance Normalization", "paragraph_index": 53}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "Attention is All You Need", "paragraph_index": 54}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "original paper on the attention mechanism", "paragraph_index": 58}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer, the efficient Transformer", "paragraph_index": 58}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "AI Papers to Read in 2020", "paragraph_index": 58}, {"url": "https://arxiv.org/abs/1809.11096", "anchor_text": "BigGAN paper", "paragraph_index": 63}, {"url": "https://arxiv.org/abs/1710.10196", "anchor_text": "ProGAN paper", "paragraph_index": 63}, {"url": "https://arxiv.org/abs/1711.10337", "anchor_text": "\u201cAre gans created equal? a large-scale study.\u201d", "paragraph_index": 64}, {"url": "https://arxiv.org/abs/1606.03498", "anchor_text": "Inception Score", "paragraph_index": 69}, {"url": "https://arxiv.org/abs/1706.08500", "anchor_text": "Fr\u00e9chet Inception Distance", "paragraph_index": 69}, {"url": "https://arxiv.org/abs/1806.00035", "anchor_text": "Sajjadi et al.", "paragraph_index": 69}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "AI Papers to Read in 2020", "paragraph_index": 71}, {"url": "https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b", "anchor_text": "Ten More AI Papers to Read in 2020", "paragraph_index": 71}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me", "paragraph_index": 72}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing", "paragraph_index": 72}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow", "paragraph_index": 72}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up.", "paragraph_index": 72}, {"url": "https://www.buymeacoffee.com/ygorreboucas", "anchor_text": "buying me a cup of coffee", "paragraph_index": 72}], "all_paragraphs": ["Generative Adversarial Networks (GANs) are one of the most innovative ideas proposed in this decade. At its core, GANs are an unsupervised model for generating new elements from a set of similar elements. For instance, to produce original face pictures given a collection of face images or create new tunes out of preexisting melodies.", "GANs have found applications for image, text, and sound generation, being at the core of technologies such as AI music, deep fakes, and content-aware image editing. Besides pure generation, GANs have also been applied to transforming images from one domain to another and as a means for style transfer. To add one more application, they fit as an intelligent data augmentation technique for semi-supervised learning.", "In this article, I selected ten of the most eye-opening readings on the GANs by 2020. In particular, I selected five papers that expanded the horizons of where and how GANs can be used and five papers that tackle specific technical challenges of training GANs.", "As for my previous articles on reading suggestions, I provide a brief overview of each paper and a set of reasons for reading it plus a couple of ideas on further readings on the same or related topics.", "Goodfellow, Ian, et al. \u201cGenerative adversarial nets.\u201d Advances in neural information processing systems. 2014.", "In 2014, Ian Goodfellow was in a bar with his friends discussing how to synthesize images using artificial intelligence. A typical talk over a beer. While his friends were discussing statistical approaches, he argued for using two neural networks to jointly learn \u201chow to draw\u201d and to \u201cjudge the drawing\u201d. The former trains on the feedback of the latter, which trains to recognize real images from fake images, just to bash the former.", "Initially, the idea was met with criticism: training one network is hard, training two is madness. Nonetheless, slightly drunk, Goodfellow went back home and coded it overnight. To his surprise, it worked.", "Reason #1: Good ideas can pop-up anywhere, at any time. Carry a notes app.", "Reason #2: As with any \u201cclassical paper\u201d, reading it takes us back in time to a moment in which not everything we use today was present. Which deep learning techniques were current in 2014? Which weren\u2019t?", "Reason #3: Goodfellow demonstrated GANs using the MNIST and CIFAR-10 datasets. It took some time for more massive datasets (and resolutions) to be tamed. If he had started with ImageNet, he would have thrown GANs away as a bad idea. This paper is a reminder that we should never downplay innovations proposed on toy datasets as being toys as well. Any idea can be two or three papers away from being a breakthrough.", "Further Reading: Although GANs are the most successful and accessible approach to the generative task, they are not the only ones. Two relevant alternatives are the Variational Auto-Encoders (VAEs) and the Autoregressive models. Both are worth checking out and have their pros and cons over GANs.", "Karras, Tero, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative adversarial networks.\u201d Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.", "Five years later, GANs are now able to generate high-resolution customizable portraits trained on massive corpora. While many novel ideas were proposed from 2014 to 2019 and they certainly inspired many of the authors\u2019 decisions, it all boils down to the architecture, AdaIN, Wasserstein loss, and novel dataset: Flickr-Faces-HQ Dataset (FFHQ).", "The paper\u2019s core idea is to input different noise vectors at each level of upsampling. This goes in stark contrast to most previous works, which employ noise only as a first step. By tweaking the noise vectors of each resolution, the authors can control \u201chigh-level details\u201d by tampering with the lower levels noise and \u201clow-level details\u201d tampering with the later levels.", "Reason #1: While the amount of contributions made each year in deep learning is overwhelming, some papers show that only a small fraction of it is needed to achieve state-of-the-art results. This paper is a good example.", "Reason #2: In essence, the main contribution of this work is a novel way to employ noise. There is no significant new algorithm or mathematical formulation involved. In fact, page three onwards is dedicated to evaluation.", "Reason #3: On the other hand, they had to scrape an entire dataset and run on eight Tesla V100 GPUs for a whole week. This usage of p3.16xlarge instances is about four thousand dollars on AWS. The technical contribution might be ingenious, but the computational cost to reach it is pretty high.", "Further Reading: By the end of 2019, a follow-up paper was released, trimming several of the model\u2019s edges. However, more impressive is the work developed independently by other authors using StyleGAN as a tool. My favorite is Image2StyleGAN. The authors\u2019 idea was to train a network to find the corresponding noise for a face that would generate it back using StyleGAN. This way, the network can be used to style pre-existing images as if they were any other generated picture.", "Isola, Phillip, et al. \u201cImage-to-image translation with conditional adversarial networks.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.", "Zhu, Jun-Yan, et al. \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks.\u201d Proceedings of the IEEE international conference on computer vision. 2017.", "The original GAN formulation is unsupervised: images are generated out of nowhere. However, many authors have devised supervised formulations, in which some prior knowledge given to the network. These are named \u201cconditional\u201d approaches. Among these, the Pix2Pix and CycleGAN architectures are among the most well known.", "The Pix2Pix model deals with problems such as converting line drawings to finished paintings, allowing users to have some degree of artistic control by improving/altering their sketches. In turn, the CycleGAN model relaxes the need for paired training examples. It\u2019s most famous uses are to replace horses for zebras or apples for oranges. However, turning Fortnite into PUBG is undoubtedly more appealing to the younger generation.", "Reason #1: As GANs improve in quality, artistic control becomes a much more exciting concern. Conditional models provide an avenue for GANs to become useful in practice.", "Reason #2: The main component behind Pix2Pix is the U-Net architecture, which was initially proposed for biomedical image segmentation. This highlights how the many applications of deep learning contribute back to each other. You might not be a GAN researcher, but some GAN innovation might be just what you need right now.", "Reason #3: The CycleGAN paper, in particular, demonstrates how an effective loss function can work wonders at solving difficult problems. As for #2, much can be improved by just connecting things differently.", "Further Reading: I highly recommend coding a GAN if you never have. They are an excellent exercise for deeply learning deep learning :). The TensorFlow 2 documentation has three excellent articles on building the original GAN, the Pix2Pix model, and the CycleGAN.", "Salimans, Tim, et al. \u201cImproved techniques for training gans.\u201d Advances in neural information processing systems. 2016.", "While generating stuff is usually the spotlight, GANs can also be used as an auxiliary tool to improve other tasks. One of these uses is semi-supervised learning: when plenty of unlabeled data is available, but only a small set of labeled examples is given. In other words, how to leverage unlabeled data.", "This paper gives the basic formulation for training a classifier with the help of a GAN. In essence, the classifier is jointly trained to classify images and detect fakes while the generator strives to generate classifiable photos, which are not seen as fake. This way, the unlabeled data can be used to train the generator, which in turn provides more data for the classifier to learn.", "Reason #1: Semi-supervised learning is one of the most promising ideas to lower the cost of data. Many domains, such as medical imaging, are incredibly expensive to annotate. In these contexts, this formulation might save money. This idea is championed by the Turing award winner Yann LeCun.", "Reason #2: Most of us are familiar with the concept of data augmentation, such as adding random crops, zooms, rotations, and mirrors. In that regard, GANs can be employed as a \u201cgourmet data augmentation.\u201d", "Reason #3: While this work is not directly focused on the philosophy of it, it is said that \u201cif you can generate something, you know all about it\u201d. This is one of the main reasons generative learning is deemed essential for higher-level intelligence. This paper is an example of how GANs can help other tasks.", "Reason #4: Besides the focus on semi-supervised learning, this paper shares a set of tricks to improve and evaluate GANs. Among them, they proposed the widely used Inception Score, one of the primary metrics to evaluate GANs.", "Further Reading: I highly recommend watching the AAAI 20 keynote speech given by the three Turning award winners. The topics are the future of CNNs, semi-supervised learning, and reasoning with neural networks. I have been recommending this talk on most of my articles, and I can\u2019t stress enough how important it is to watch top-notch keynotes. Reading papers is important, but listening to what the authors have to say is the other half of the picture.", "Jin, Yanghua, et al. \u201cTowards the automatic anime characters creation with generative adversarial networks.\u201d arXiv preprint arXiv:1708.05509 (2017).", "The fifth entry in this list is, informally, the AnimeGAN. All previously presented approaches handle natural images. However, GANs can be quite useful in other domains too, such as waifu generation, sprite completion, cartoon stylization, deep fakes, and so on. While this paper is not particularly renowned or is as well known as others in this list, it beautifully showcases that there are many avenues for exploration besides faces and cars.", "Reason #1: GANs are not restricted to natural images and good looking people. Generative methods can be employed for a wide variety of tasks (and well beyond the image domain).", "Reason #2: This paper showcases the use of a tagged dataset, which can be leveraged to guide the generator into picking a specific hairstyle or eye color. The formulation is almost the same as in semi-supervised learning. However, in this case, we don\u2019t care about the trained classifier.", "Further Reading: Although GANs evolved quickly, many results are stuck in academia. It is nice when models are made public as interactive tools. Many things can only be learned live, such as forcing algorithms to the extreme or playing with unusual choices. Here are some interesting links: Pix2Pix, AnimeGAN, StyleTransfer, SketchRNN, AutoDraw, and ArtBreeder.", "So far, the presented works have addressed what GANs are and how they can be applied to solve several tasks within computer vision. The following works, on the other hand, focus on the technical aspects of training and implementing GANs, such as novel losses and regularization techniques", "Since their debut, GANs are known as hard to train: most of the time, the generator is incapable of generating anything useful or produces the same thing over and over (mode collapse). While many \u201ctricks\u201d have been proposed over the years, the Wasserstein loss is one of the first principled approaches to become mainstream and pass the test of time.", "The general idea is to replace de discriminator, which judges images as real or fake, by a critic, which merely states a score for each image. Unlike the judge, the critic is unbounded; it can give any score: one, minus three, a hundred, etc. During training, the critic trains to score reals as much apart from fakes as it can while the generator trains to minimize this gap.", "Informally, the main advantage of this approach is that it poses a non-stationary problem. The critic can change its views and score real images lower, or fake images higher. It is a cat and mouse game. The generator is always chasing the score of reals, which is always running away.", "Reason #1: Most papers start with a gentle introduction; this paper goes from 0 to 100 real quick. It deserves the read just for its boldness (and if you love math, this is just right for you)", "Reason #2: Nothing is perfect. While many papers try to hide their shortcomings, others are keen on highlighting and acknowledging them. This should always be praised, as it invites others to contribute. Quoting the paper:", "Weight clipping is a clearly terrible way to enforce a Lipschitz constraint", "Reason #3: Meany famous GANs published after 2017 use the Wasserstein loss and some that didn\u2019t now do in a second version. This is undoubtedly a milestone paper on the field and should be read by anyone on it.", "Further Reading: Although this paper has become more or less the go-to loss for training GANs, it is hardly the only loss used. Two that deserve special attention are the Perceptual and Total Variance losses. The former uses a frozen VGG network as a measure of perceptual similarity while the latter penalizes the image gradients, encouraging smoothness.", "The Wasserstein loss significantly improves convergence at the cost of requiring gradients to be 1-Lipschitz. To enforce this, the authors proposed to clip weights to [-0.01, 0.01]. A more elegant approach was later introduced by Miyato et al.: Spectral Normalization.", "The main idea of this paper is to constrain the largest eigenvalue of a weight matrix to be one, which, in turn, guarantees the Lipschitz requirement. To keep computational costs low, the authors also propose to approximate the eigenvalue computation by using power iteration, which is pretty efficient.", "Reason #1: As mentioned in the previous entry, highlighting your weaknesses invites other authors to contribute. Although this paper is not worded as such, it acts as a direct reply to weight clipping.", "Reason #2: Normalization is a much bigger topic than most people are aware of. Many select properties can be enforced through specialized norms and careful activation function design, such as the SELU activation function.", "Reason #3: Besides being a norm, this is also a regularization, which is an often overlooked topic in neural networks. It is refreshing to read a successful paper on the matter besides dropout.", "Further Reading: Other recent advances in normalization techniques are the Group Normalization and Adaptive Instance Normalization techniques. The former solves some of batch norm shortcomings with small batch sizes while the latter is one of the critical breakthroughs in arbitrary style transfer. The latter is also used on #2, to condition the data flow to the noise vectors.", "While the overall innovations on text and image processing are considerably different, once in a while, one inspires the other. The Attention is All You Need paper, from the natural language processing community, proposed the attention mechanism, a way to add global-level reasoning to CNNs.", "In this paper, this concept has been brought to the GAN domain as a means to improve image quality. In broad terms, the self-attention mechanism computes \u201cwhat each pixel thinks of each other pixel.\u201d Thus, it allows the network to reason on global level relationships. This is accomplished by taking the outer product of the flattened image with itself.", "Reason #1: Attention is taking the deep learning world by storm. Every field has an attention-based solution now in 2020. GANs are no exception.", "Reason #2: Taking the outer product of the flattened image is very time and memory consuming, limiting the applicability of this technique to a few low-resolution layers. This would not be possible two GPU generations ago. Which power-hungry methods will we see two generations from now?", "Further Reading: I highly recommend reading the original paper on the attention mechanism, as it is almost everywhere by now. In a sense, it has shown the world what global-level reasoning can bring to the table. Nevertheless, it is damn expensive. For this reason, many teams are working on more efficient ways to tackle attention. One such method is the Reformer, the efficient Transformer. If you are curious about my views on both papers, I have covered them in my AI Papers to Read in 2020 article.", "Most GAN issues are attributed to a lack of balance between generator and discriminator networks. In this paper, Google researchers propose a method to enforce an equilibrium among the two players and a convergence metric to evaluate the generator evolution over the epochs. Additionally, by tuning the balance, the authors are able to trade-off image quality for variety.", "Reason #1: The proposal is quite radical: the discriminator is an autoencoder, and we compare the losses of encoding real and fake images.", "Reason #2: It is pretty hard to tell if a GAN is improving, as manual inspection is not reliable after a certain quality level is reached. Having a simple to compute convergence metric is pretty handy.", "Reason #3: Besides the equilibrium idea, everything else is pretty standard. This shows how accurately modeling a problem can do wonders. No fancy layers, norms, or expensive operations are needed.", "Further Reading: Likewise, the BigGAN paper shows that merely using larger models and four to eight times the batch size can vastly improve image quality. Similarly, the ProGAN paper shows that training with a small resolution first and then doubling it progressively can help to reach higher resolutions without changes to the model.", "Lucic, Mario, et al. \u201cAre gans created equal? a large-scale study.\u201d Advances in neural information processing systems. 2018.", "I saved this one for last for a reason: it is one of the most extensive comparative studies of GANs to date. All previous nine entries garnered much praise for their high-quality results and technical breakthroughs. However, many other formulations exist. In this paper, you can take a look at not so known ideas, such as least-squares GAN or DRAGAN.", "Reason #1: If you came so far, you are likely into GANs. What best than a survey paper to keep on reading?", "Reason #2: This paper has a handy table with the generator and discriminator losses for seven different formulations. If implementing GANs is not your plan, this table still counts a lot as a math reading exercise.", "Reason #3: Being a comparative study, this paper has some dedicated paragraphs on how GANs can be compared quantitatively and the challenges involved in making a fair comparison.", "Further Reading: On the topic of comparing GANs, the Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) are widely used measures of similarity between image sets. A less known approach was proposed by Sajjadi et al., which adapts the precision and recall scores measures of quality and diversity, respectively. Understanding how to compare GANs is as much important as the GANs themselves.", "I hope this has been an exciting read for you as it was for me. Please let me know of other papers you believe would fit this list. I will be glad to read and consider them for future lists. \ud83d\ude03", "If you liked this article and would like to keep going. I have also written AI Papers to Read in 2020 and Ten More AI Papers to Read in 2020. Together, they cover more than fifty other articles (counting links and further reading)", "Feel free to comment or connect with me. If you are new to Medium, I highly recommend subscribing. Medium articles are the perfect pair to StackOverflow for Data and IT professionals, and even more so for new comers. Please consider using my affiliate link when signing up. You can also support me directly by buying me a cup of coffee :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Former game developer turned data scientist after falling in love with AI and all its branches."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2c708af5c0a4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ygorserpa.medium.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----2c708af5c0a4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets", "anchor_text": "\u201cGenerative adversarial nets.\u201d"}, {"url": "https://www.wired.com/2017/04/googles-dueling-neural-networks-spar-get-smarter-no-humans-required/", "anchor_text": "was in a bar with his friends"}, {"url": "https://arxiv.org/abs/1906.02691", "anchor_text": "Variational Auto-Encoders (VAEs)"}, {"url": "https://arxiv.org/abs/1601.06759", "anchor_text": "Autoregressive models"}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html", "anchor_text": "\u201cA style-based generator architecture for generative adversarial networks.\u201d"}, {"url": "https://github.com/NVlabs/ffhq-dataset", "anchor_text": "Flickr-Faces-HQ Dataset (FFHQ)"}, {"url": "https://unsplash.com/@chrisjoelcampbell?utm_source=medium&utm_medium=referral", "anchor_text": "Christopher Campbell"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/NVlabs/stylegan", "anchor_text": "eight Tesla V100 GPUs for a whole week"}, {"url": "https://arxiv.org/abs/1912.04958", "anchor_text": "a follow-up paper was released"}, {"url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html", "anchor_text": "Image2StyleGAN"}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "Image-to-image translation with conditional adversarial networks.\u201d"}, {"url": "https://arxiv.org/abs/1703.10593", "anchor_text": "\u201cUnpaired image-to-image translation using cycle-consistent adversarial networks.\u201d"}, {"url": "https://www.youtube.com/watch?time_continue=57&v=xkLtgwWxrec", "anchor_text": "turning Fortnite into PUBG"}, {"url": "https://www.tensorflow.org/tutorials/generative/dcgan", "anchor_text": "original GAN"}, {"url": "https://www.tensorflow.org/tutorials/generative/pix2pix", "anchor_text": "the Pix2Pix model"}, {"url": "https://www.tensorflow.org/tutorials/generative/cyclegan", "anchor_text": "the CycleGAN"}, {"url": "https://papers.nips.cc/paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html", "anchor_text": "\u201cImproved techniques for training gans.\u201d"}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "Turing award winner Yann LeCun"}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "AAAI 20 keynote speech"}, {"url": "https://arxiv.org/abs/1708.05509", "anchor_text": "\u201cTowards the automatic anime characters creation with generative adversarial networks.\u201d"}, {"url": "https://waifulabs.com", "anchor_text": "waifu generation"}, {"url": "https://www.sbgames.org/sbgames2019/files/papers/ComputacaoFull/197880.pdf", "anchor_text": "sprite completion"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf", "anchor_text": "cartoon stylization"}, {"url": "https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them", "anchor_text": "deep fakes"}, {"url": "https://make.girls.moe/#/", "anchor_text": "make.girls.moe"}, {"url": "https://affinelayer.com/pixsrv/", "anchor_text": "Pix2Pix"}, {"url": "https://make.girls.moe/#/", "anchor_text": "AnimeGAN"}, {"url": "https://reiinakano.com/arbitrary-image-stylization-tfjs/", "anchor_text": "StyleTransfer"}, {"url": "https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html", "anchor_text": "SketchRNN"}, {"url": "https://www.autodraw.com/", "anchor_text": "AutoDraw"}, {"url": "https://www.artbreeder.com/", "anchor_text": "ArtBreeder"}, {"url": "https://arxiv.org/abs/1701.07875", "anchor_text": "\u201cWasserstein gan.\u201d"}, {"url": "https://arxiv.org/abs/1603.08155", "anchor_text": "Perceptual"}, {"url": "https://arxiv.org/abs/1605.01368", "anchor_text": "Total Variance"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "\u201cSpectral normalization for generative adversarial networks.\u201d"}, {"url": "https://en.wikipedia.org/wiki/Lipschitz_continuity", "anchor_text": "1-Lipschitz"}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "SELU activation function"}, {"url": "https://en.wikipedia.org/wiki/Dropout_(neural_networks)", "anchor_text": "dropout"}, {"url": "https://arxiv.org/abs/1803.08494", "anchor_text": "Group Normalization"}, {"url": "http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html", "anchor_text": "Adaptive Instance Normalization"}, {"url": "https://arxiv.org/abs/1805.08318", "anchor_text": "\u201cSelf-attention generative adversarial networks.\u201d"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "Attention is All You Need"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "original paper on the attention mechanism"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer, the efficient Transformer"}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "AI Papers to Read in 2020"}, {"url": "https://arxiv.org/abs/1703.10717", "anchor_text": "\u201cBegan: Boundary equilibrium generative adversarial networks.\u201d"}, {"url": "https://arxiv.org/abs/1809.11096", "anchor_text": "BigGAN paper"}, {"url": "https://arxiv.org/abs/1710.10196", "anchor_text": "ProGAN paper"}, {"url": "https://arxiv.org/abs/1711.10337", "anchor_text": "\u201cAre gans created equal? a large-scale study.\u201d"}, {"url": "https://arxiv.org/abs/1606.03498", "anchor_text": "Inception Score"}, {"url": "https://arxiv.org/abs/1706.08500", "anchor_text": "Fr\u00e9chet Inception Distance"}, {"url": "https://arxiv.org/abs/1806.00035", "anchor_text": "Sajjadi et al."}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "AI Papers to Read in 2020"}, {"url": "https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b", "anchor_text": "Ten More AI Papers to Read in 2020"}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing"}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up."}, {"url": "https://www.buymeacoffee.com/ygorreboucas", "anchor_text": "buying me a cup of coffee"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2c708af5c0a4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2c708af5c0a4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2c708af5c0a4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2c708af5c0a4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2c708af5c0a4---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&user=Ygor+Serpa&userId=857d8734c7da&source=-----2c708af5c0a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&user=Ygor+Serpa&userId=857d8734c7da&source=-----2c708af5c0a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2c708af5c0a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2c708af5c0a4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2c708af5c0a4--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://ygorserpa.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3644829c90d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-papers-to-read-in-2020-2c708af5c0a4&newsletterV3=857d8734c7da&newsletterV3Id=3644829c90d9&user=Ygor+Serpa&userId=857d8734c7da&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}