{"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "time": 1683006308.021201, "path": "towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883/", "webpage": {"metadata": {"title": "Recommendation System Series Part 5: The 5 Variants of Multi-Layer Perceptron for Collaborative Filtering | by James Le | Towards Data Science", "h1": "Recommendation System Series Part 5: The 5 Variants of Multi-Layer Perceptron for Collaborative Filtering", "description": "Update: This article is part of a series where I explore recommendation systems in academia and industry. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, and Part 6. Collaborative\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "anchor_text": "Part 5", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Part 6", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "matrix factorization", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1606.07792.pdf", "anchor_text": "Wide and Deep Learning for Recommender Systems", "paragraph_index": 7}, {"url": "https://play.google.com/store?hl=en_US", "anchor_text": "Google Play", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1703.04247.pdf", "anchor_text": "DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1803.05170.pdf", "anchor_text": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1708.05027.pdf", "anchor_text": "Neural Factorization Machines for Sparse Predictive Analytics", "paragraph_index": 33}, {"url": "https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf", "anchor_text": "Neural Collaborative Filtering", "paragraph_index": 46}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Recommendation System Series Part 6", "paragraph_index": 56}, {"url": "https://medium.com/@james_aka_yale", "anchor_text": "Medium", "paragraph_index": 57}, {"url": "https://github.com/khanhnamle1994", "anchor_text": "GitHub", "paragraph_index": 57}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/", "paragraph_index": 57}, {"url": "https://twitter.com/le_james94", "anchor_text": "Twitter", "paragraph_index": 57}, {"url": "http://www.linkedin.com/in/khanhnamle94", "anchor_text": "find me on LinkedIn", "paragraph_index": 57}, {"url": "http://eepurl.com/deWjzb", "anchor_text": "Sign up for my newsletter", "paragraph_index": 57}], "all_paragraphs": ["Update: This article is part of a series where I explore recommendation systems in academia and industry. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, and Part 6.", "Collaborative Filtering algorithms are most commonly used in the applications of Recommendation Systems. Due to the use of the Internet and the enormous amount of information that is generated, it becomes a very tedious task for users to find their preferences. Users\u2019 preferences for items are represented in the form of a rating matrix, which is used to build the relation between users and items to find users\u2019 relevant items. Thus, collaborative filtering algorithms nowadays face the problem with large datasets and sparseness in the rating matrix.", "Among the various collaborative filtering techniques, matrix factorization is the most popular one, which projects users and items into a shared latent space, using a vector of latent features to represent a user or an item. After that, a user\u2019s interaction on an item is modeled as the inner product of their latent vectors.", "Despite the effectiveness of matrix factorization for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function: the inner product. For example, for the task of rating prediction on explicit feedback, it is well known that the performance of the matrix factorization model can be improved by incorporating user and item bias terms into the interaction function. While it seems to be just a trivial tweak for the inner product operator, it points to the positive effect of designing a better, dedicated interaction function for modeling the latent feature interactions between users and items. The inner product, which simply combines the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction data.", "In this post and those to follow, I will be walking through the creation and training of recommendation systems, as I am currently working on this topic for my Master Thesis.", "In Part 5, I explore the use of Multilayer Perceptron for collaborative filtering. A multi-layer perceptron is a feed-forward neural network with multiple hidden layers between the input layer and the output layer. It can be interpreted as a stacked layer of non-linear transformations to learn hierarchical feature representations. It is a concise but practical network that can approximate any measurable function to any desired degree of accuracy (a phenomenon known as Universal Approximation Theorem). As such, it is the basis of numerous advanced approaches and is widely used in many areas.", "More specifically, I will walk through 5 papers that incorporate Multi-layer Perceptron into their recommendation framework.", "Memorization and generalization are both critical for recommender systems. The paper \u201cWide and Deep Learning for Recommender Systems\u201d (2016) by Google proposes a framework to combine the strengths of wide linear models and deep neural networks to address both issues. This framework has been production-ized and evaluated on the recommender system of Google Play, a massive-scale commercial app store.", "As shown in the figure below, the wide learning component is a single-layer perceptron which can effectively memorize sparse feature interactions using cross-product feature transformations. The deep learning component is a multi-layer perceptron that can generalize to previously unseen feature interactions through low-dimensional embeddings.", "Mathematically speaking, wide learning is defined as:", "where y is the prediction, x is a vector of features, W is a vector of model parameters, and b is the bias. The feature set includes both raw inputs and transformed inputs (via cross-product transformation to capture the correlation between features).", "In the deep learning component, each hidden layer performs the following computation:", "where l is the layer number, f is the activation function, a_l is the vector of activations, b_l is the vector of biases, and W_l is the vector of model weights at the l-th layer.", "The wide and deep learning model is attained by fusing these models:", "where Y is the binary class label, W_{wide} is the vector of all wide model weights, W_{deep} is the vector of weights applied on the final activation a_{last}, and b is the bias term.", "Let\u2019s see how this looks like in code:", "As an extension of the Wide and Deep Learning approach, \u201cDeepFM: A Factorization-Machine Based Neural Network for CTR Prediction\u201d (2017) by Huifeng Guo et al. is an end-to-end model that seamlessly integrates Factorization Machine (the wide component) and Multi-Layer Perceptron (the deep component). Compared to the Wide and Deep Model, DeepFM does not require tedious feature engineering.", "As shown in the figure below, the Factorization Machine utilizes addition and inner product operations to capture the linear and pairwise interactions between features. The Multi-Layer Perceptron leverages the non-linear activations and deep structure to model the high-order interactions.", "Mathematically speaking, the input of DeepFM is an m-fields data consisting of pairs (u, i) \u2014 which are the identity and features of user and item, as well as a binary label y that indicates user click behaviors (y = 1 means the user clicked the item, and y = 0 otherwise). The task here is to build a prediction model to estimate the probability of a user clicking a specific app in a given context.", "For any particular feature i, a scalar w_i is used to weigh its 1st-order importance, and a latent vector V_i is used to measure its impact of interactions with other features. V_i is fed in the wide component to model 2nd-order feature interactions, and fed in the deep component to model high-order feature interactions. All parameters, including w_i, V_i, and the network parameters are trained jointly for the combined prediction model:", "where y_hat is the predicted CTR (between 0 and 1), y_{FM} is the output of the wide Factorization Machine component, and y_{DNN} is the output of the Multi-Layer Perceptron component.", "In the wide component, besides a linear (1st-order) interactions among features, the Factorization Machine models pairwise (2nd-order) feature interactions as the inner product of respective feature latent vectors. This helps capture 2nd-order feature interactions very effectively when the dataset is sparse. The output of Factorization Machine is the summation of an Addition unit and several Inner Product units:", "with given features i and j. The Addition unit (first term) reflects the importance of 1st-order features, and the Inner Product units (second term) represent the impact of 2nd-order feature interactions.", "In the deep component, the output of the Multi-Layer Perceptron looks like this:", "where |H| is the number of hidden layers, a is the vector output of the embedding layer, W is the vector of model weights, and b is the vector of bias units.", "Let\u2019s see how this looks like in code:", "As an extension of the Deep Factorization Machine, \u201cxDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems\u201d (2018) from Jianxun Lian et al. can jointly model the explicit and implicit feature interactions. The explicit high-order feature interactions are learned via a Compressed Interaction Network, while the implicit high-order feature infractions are learned via a Multi-Layer Perceptron. This model also requires no manual feature engineering and releases data scientists from tedious feature searching work.", "The Compressed Interaction Network is designed with the following considerations:", "The structure of the Compressed Interaction Network is very similar to the Recurrent Neural Network, where the outputs of the next hidden layer are dependent on the last hidden layer and additional input. The structure of embedding vectors at all layers is kept in the status quo; thus, the interactions are applied at the vector-wise level.", "xDeepFM combines the Compressed Interaction Network above with plain Multi-Layer Perceptrons via the wide and deep learning framework. On the one hand, this model includes both low-order and high-order feature interactions; on the other hand, it also contains both implicit and explicit feature interactions. The architecture is shown here.", "Mathematically speaking, the resulting output unit is:", "where a is the vector of raw features, x_{mlp} is the vector of outputs from the plain Multi-Layer Perceptron, p+ is the vector of outputs from the Cross Interaction Network. W and b are the learnable parameters \u2014 weights and biases, respectively.", "Let\u2019s see how this looks like in code:", "Another parallel work that seamlessly integrates Factorization Machines and Multi-Layer Perceptron is Xiangnan He and Tat-Seng Chua\u2019s \u201cNeural Factorization Machines for Sparse Predictive Analytics\u201d (2017). This model brings together the effectiveness of linear factorization machines with the strong representation ability of non-linear neural networks for sparse predictive analytics.", "As seen below, the key to its architecture is an operation called Bilinear-Interaction pooling that allows a neural network model to learn more informative feature interactions at the lower level. Through stacking non-linear layers above the Bilinear-Interaction layer, the authors were able to deepen the shallow linear Factorization Machine, modeling higher-order, and non-linear feature interactions effectively to improve Factorization Machine\u2019s expressiveness. In contrast to traditional deep learning methods that simply concatenate or average embedding vectors in the low level, this use of Bilinear-Interaction pooling encodes more informative feature interactions, greatly facilitating the following \u201cdeep\u201d layers to learn meaningful information.", "Let\u2019s dive into the math of the Neural Factorization Machine model. Given a sparse vector x as input, the model estimates the target as:", "where the first term models global bias of data of features, the second term models global bias of weight of features, and the third term f(x) is a Multi-Layer Perceptron (as shown in figure 2) that models feature interactions. The design of f(x) consists of these layer components:", "This is a fully-connected layer that projects each feature to a dense vector representation. Let v_i be the embedding vector for the i-th feature. Then after the embedding step, the authors obtain a set of embedding vectors to represent the input feature vector x.", "Due to the possible sparse representation of x, the authors only include the embedding vectors for non-zero features, where x_i does not equal to 0.", "Then the embedding set V_x is fed into a Bilinear-Interaction layer, which is a pooling operation that converts a set of embedding vectors to one vector:", "where v_i.x_j denotes the element-wise product of two vectors v_i and x_j. The output of this pooling is a k-dimension vector that encodes the second-order interactions between features in the embedding space.", "Above the Bilinear-Interaction pooling layer is a stack of fully-connected layers, which are capable of learning higher-order interactions between features. The definition of these hidden layers is:", "where L is the number of hidden layers; W_L, b_L, and activation_L correspond to the weight matrix, bias vector, and activation function for the l-th layer, respectively. The choice of activation functions can be sigmoid, tanh, or ReLU to learn higher-order feature interactions non-linearly.", "Lastly, the output vector of the last hidden layer z_L is transformed into the final prediction score:", "where h^T denotes the neuron weights of the prediction layer.", "Let\u2019s see how this looks like in code:", "The paper \u201cNeural Collaborative Filtering\u201c (2018) by Xiangnan He et al. pushed the use of multi-layer perceptrons for learning the interaction function from data one step further. Note here that they are also the same authors of the Neural Factorization Machine paper mentioned above. They formalized a modeling approach for collaborative filtering that focuses on the implicit feedback, which indirectly reflects users\u2019 preference through behaviors like watching videos, purchasing products, and clicking items. Compared to explicit feedback such as ratings and reviews, implicit feedback can be tracked automatically and is thus much more natural to collect for content providers. However, it is more challenging to utilize since user satisfaction is not observed, and there is an inherent scarcity of negative feedback.", "The user-item interaction value y_ui to model users\u2019 implicit feedback can be either 1 or 0. A value of 1 indicates that there is an interaction between user u and item i, but it does not mean u likes i. This poses challenges in learning from implicit data since it provides only noisy signals about users\u2019 preferences. While observed entries at least reflect users\u2019 interest in items, the unobserved entries can be just missing data, and there is a natural scarcity of negative feedback.", "The authors adopt a multi-layer representation to model a user-item interaction y_ui, as shown below, where the output of one layer serves as the input of the next one.", "The above framework can be summed up with the scoring function below:", "where y-hat_ui is the predicted score of interaction y_ui, and theta denotes the model parameters. f is the multi-layer perceptron that maps model parameters to the predicted score. More specifically, P is the latent-factor matrix for users, Q is the latent-factor matrix for items, v_u^U is the side information associated with user features, and v_i^I is the side information associated with item features.", "The paper argues that traditional matrix factorization can be viewed as a special case of Neural Collaborative Filtering. Therefore, it is convenient to fuse the neural interpretation of matrix factorization with Multi-Layer Perceptron to formulate a more general model which makes use of both linearity of Matrix Factorization and non-linearity of Multi-Layer Perceptron to enhance recommendation quality.", "Let\u2019s see how this looks like in code:", "The result table is at the bottom of the README, and as you can see from the Weights and Biases dashboard visualization:", "In this post, I have discussed the intuitive meaning of Multi-Layer Perceptron and its use in collaborative filtering. I also walked through 5 different papers that use MLP for the recommendation framework: (1) Wide and Deep Learning, (2) Deep Factorization Machine, (3) Extreme Deep Factorization Machine, (4) Neural Factorization Machine, and (5) Neural Collaborative Filtering. These models complement the mainstream shallow models for collaborative filtering, thus opening up a new avenue of research possibilities for recommendations based on deep learning.", "Stay tuned for future blog posts of this series that go beyond discriminative models and enter the realm of generative models for collaborative filtering.", "Now goes read Recommendation System Series Part 6!", "If you would like to follow my work on Recommendation Systems, Deep Learning, and Data Science Journalism, you can check out my Medium and GitHub, as well as other projects at https://jameskle.com/. You can also tweet at me on Twitter, email me directly, or find me on LinkedIn. Sign up for my newsletter to receive my latest thoughts on machine learning in research and in the industry right at your inbox!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa0aebfe15883&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://le-james94.medium.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "James Le"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52aa38cb8e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&user=James+Le&userId=52aa38cb8e25&source=post_page-52aa38cb8e25----a0aebfe15883---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4"}, {"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "anchor_text": "Part 5"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Part 6"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "matrix factorization"}, {"url": "https://blog.prototypr.io/how-spotify-discovery-algorithm-works-fae8f63466ab", "anchor_text": "https://blog.prototypr.io/how-spotify-discovery-algorithm-works-fae8f63466ab"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4"}, {"url": "https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f", "anchor_text": "https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f"}, {"url": "https://arxiv.org/pdf/1606.07792.pdf", "anchor_text": "Wide and Deep Learning for Recommender Systems"}, {"url": "https://play.google.com/store?hl=en_US", "anchor_text": "Google Play"}, {"url": "https://arxiv.org/pdf/1606.07792.pdf", "anchor_text": "https://arxiv.org/pdf/1606.07792.pdf"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch"}, {"url": "https://arxiv.org/pdf/1703.04247.pdf", "anchor_text": "DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction"}, {"url": "https://arxiv.org/pdf/1703.04247.pdf", "anchor_text": "https://arxiv.org/pdf/1703.04247.pdf"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/DeepFM-PyTorch", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/DeepFM-PyTorch"}, {"url": "https://arxiv.org/pdf/1803.05170.pdf", "anchor_text": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems"}, {"url": "https://arxiv.org/pdf/1803.05170.pdf", "anchor_text": "https://arxiv.org/pdf/1803.05170.pdf"}, {"url": "https://arxiv.org/pdf/1803.05170.pdf", "anchor_text": "https://arxiv.org/pdf/1803.05170.pdf"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/xDeepFM-PyTorch", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/xDeepFM-PyTorch"}, {"url": "https://arxiv.org/pdf/1708.05027.pdf", "anchor_text": "Neural Factorization Machines for Sparse Predictive Analytics"}, {"url": "https://arxiv.org/pdf/1708.05027.pdf", "anchor_text": "https://arxiv.org/pdf/1708.05027.pdf"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Neural-FM-PyTorch", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Neural-FM-PyTorch"}, {"url": "https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf", "anchor_text": "Neural Collaborative Filtering"}, {"url": "https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf", "anchor_text": "https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Neural-CF-PyTorch-Version2", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments/Neural-CF-PyTorch-Version2"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/ml-1m", "anchor_text": "MovieLens 1M"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "my last article"}, {"url": "https://app.wandb.ai/khanhnamle1994/multi_layer_perceptron_collaborative_filtering", "anchor_text": "Weights and Biases"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Recommendation System Series Part 6"}, {"url": "https://arxiv.org/pdf/1606.07792.pdf", "anchor_text": "Wide and Deep Learning for Recommender Systems"}, {"url": "https://arxiv.org/pdf/1703.04247.pdf", "anchor_text": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"}, {"url": "https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf", "anchor_text": "Neural Collaborative Filtering"}, {"url": "https://arxiv.org/pdf/1708.05027.pdf", "anchor_text": "Neural Factorization Machines for Sparse Predictive Analytics"}, {"url": "https://arxiv.org/pdf/1803.05170.pdf", "anchor_text": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems"}, {"url": "https://medium.com/@james_aka_yale", "anchor_text": "Medium"}, {"url": "https://github.com/khanhnamle1994", "anchor_text": "GitHub"}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/"}, {"url": "https://twitter.com/le_james94", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/in/khanhnamle94", "anchor_text": "find me on LinkedIn"}, {"url": "http://eepurl.com/deWjzb", "anchor_text": "Sign up for my newsletter"}, {"url": "https://medium.com/tag/recommendations?source=post_page-----a0aebfe15883---------------recommendations-----------------", "anchor_text": "Recommendations"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----a0aebfe15883---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/collaborative-filtering?source=post_page-----a0aebfe15883---------------collaborative_filtering-----------------", "anchor_text": "Collaborative Filtering"}, {"url": "https://medium.com/tag/research?source=post_page-----a0aebfe15883---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----a0aebfe15883---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&user=James+Le&userId=52aa38cb8e25&source=-----a0aebfe15883---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&user=James+Le&userId=52aa38cb8e25&source=-----a0aebfe15883---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa0aebfe15883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a0aebfe15883---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a0aebfe15883--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a0aebfe15883--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a0aebfe15883--------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Le"}, {"url": "https://le-james94.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17.6K Followers"}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52aa38cb8e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&user=James+Le&userId=52aa38cb8e25&source=post_page-52aa38cb8e25--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F171511b90ce0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883&newsletterV3=52aa38cb8e25&newsletterV3Id=171511b90ce0&user=James+Le&userId=52aa38cb8e25&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}