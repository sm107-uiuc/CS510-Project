{"url": "https://towardsdatascience.com/deep-transfer-learning-for-image-classification-f3c7e0ec1a14", "time": 1683004293.4082692, "path": "towardsdatascience.com/deep-transfer-learning-for-image-classification-f3c7e0ec1a14/", "webpage": {"metadata": {"title": "Deep Transfer Learning for Image Classification | by Vegard Flovik | Towards Data Science", "h1": "Deep Transfer Learning for Image Classification", "description": "The following tutorial covers how to set up a state of the art deep learning model for image classification. The approach is based on the machine learning frameworks \u201cTensorflow\u201d and \u201cKeras\u201d, and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1312.4400.pdf", "anchor_text": "this paper", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "confusion matrix", "paragraph_index": 29}, {"url": "https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2", "anchor_text": "image augmentation", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2", "anchor_text": "here", "paragraph_index": 34}, {"url": "https://medium.com/@vflovik", "anchor_text": "https://medium.com/@vflovik", "paragraph_index": 50}], "all_paragraphs": ["The following tutorial covers how to set up a state of the art deep learning model for image classification. The approach is based on the machine learning frameworks \u201cTensorflow\u201d and \u201cKeras\u201d, and includes all the code needed to replicate the results in this tutorial (unfortunately, the syntax when including code blocks in medium articles does not look very nice, but it should hopefully be readable).", "The prerequisites for setting up the model is access to labelled data, and as an example case I have used images of various traffic signs (which can be downloaded here). The task of the model is thus to predict what kind of traffic sign it sees. To make the example case more realistic, I have reduced the amount of data to max 200 images per class (as limited amount of data is usually the case in practical applications of machine learning).", "These images are of course only included as an example to get you started. They can easily be replaced with your own images as long as you follow the same folder structure as the current setup, as explained below.", "Place your images in subfolders under the main folder \u201cdata/\u201d with the name of the image category as subfolder name, as in the example folder structure shown below. First, you need to split your images into training, validation and test data. The images from the \u201ctraining_data\u201d folder are the actual images used to train the model, whereas images from \u201cvalidation_data\u201d are used for optimizing training and model hyper-parameters. The test data is then used as the final assesment, to evaluate the accuracy of the model on a completely independent set of images.", "Example folder structure with included dataset:", "One way of splitting the images between \u201ctrain\u201d \u201cvalidate\u201d and \u201ctest\u201d is e.g to use 80% of the images for training the model, and validate/test on 10% each. For a brief introduction to the importance of seperating \u201ctrain\u201d, \u201cvalidation\u201d and \u201ctest\u201d data, you can also have a read here", "These are some useful python libraries/packages that make our life a lot easier, as we do not have to write all the code and functionality from scratch. Building a deep learning model without these libraries/packages would actually be quite a tremendous task!", "Here, we define the location of train/val/test images and the names of all the different categories we want to classify. We then plot the number of images per category in the training set.", "Let us also plot a few example images from the various sign categories, to visualize the typical image quality:", "As you can see from the example images above, the resolution and quality are not great. However, both image quality and amount of data are often quite limited in practical applications of machine learning. As such, low quality images limited to a maximum of 200 training images per class represents a more realistic example than using thousands of \u201cperfect\u201d high quality images.", "There is no need at this stage to understand all the details of various types of deep learning models, but a summary of some common ways of building models can be found here for those interested.", "In this tutorial, we use a pre-trained deep learning model (VGG16) as the basis for our image classifier model, and then retrain the model on our own data, i.e. transfer learning", "You might notice the parameter \u201cpooling= \u2018max\u2019 \u201c above. The reason for that, is that rather than connecting the convolutional base of the VGG16 model to a couple of fully connected layers before the final output layer (which is done in the original VGG16 model), we rather use a max-pooling output(one can also use \u201caverage pooling\u201d, as it depends on the use case which approach works best). This approach is an alternative to using fully connected layers to transition from feature maps to an output prediction for the model. In my experience this approach usually works very well, and makes the model less prone to overfitting, as also described in this paper:", "Conventional convolutional neural networks perform convolution in the lower layers of the network. For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer. This structure bridges the convolutional structure with traditional neural network classifiers. It treats the convolutional layers as feature extractors, and the resulting feature is classified in a traditional way.", "However, the fully connected layers are prone to overfitting, thus hampering the generalization ability of the overall network. In this paper, we propose another strategy called global average pooling to replace the traditional fully connected layers in CNN. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the softmax layer. One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Futhermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input. We can see global average pooling as a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts (categories).", "Having loaded the pre-trained VGG16 model, we can also choose to freeze the \u201cdeeper layers\u201d of the model in the code block below, and only re-train the last few layers on our own data. This is a common transfer learning strategy, and is often a good approach when the amount of data available for training is limited.", "This option is currently commented out from the code (using the #symbol), and we are thus retraining all layers of the model. The number of layers to train represents a parameter you can experiment with yourselves. How does the number of trainable layers affect model performance?", "As a check we can also print a list of all layers of the model, and whether they are trainable or not (True/False)", "Using the VGG16 model as a basis, we now build a final classification layer on top to predict our defined classes. We then print a model summary, lisiting the number of parameters of the model. If you decide to \u201cfreeze\u201d some of the layers, you will notice that the number of \u201cTrainable parameters\u201d below will be lower.", "As you can see, the output shape of the final layer of the model corresponds to the number of classes, which in our case is 10.", "We then need to define some functions that read images from our folders and feeds them to the image classifier model. As a part of this we also add some basic image preprocessing, where the input images are scaled to have pixel values in the range [0,1], (from 0\u2013255 in the original images).", "Here, we define some of the parameters that controls the training process of the model. Important parameters are e.g. training rate, how many epochs to train the model and which optimizer to use. You do not need to understand all these terms to follow the tutorial, but those interested can have a quick read here.", "We also define a checkpoint parameter, where we keep track of the validation accuracy after each epoch during training. Using this, we always keep a copy of the model that performs best during the training process.", "We are now ready to start training the model on our own data, and for each \u201cepoch\u201d we print the training and validation loss and accuracy. The model accuracy, as measured on the training data, is given by \u201cacc\u201d, and the accuracy on the images in the validation set is given by \u201cval_acc\u201d. This is the most important quantity, as it tells us how accurate the model is on images it has not already seen during the training process.", "Ideally, the \u201cval_acc\u201d should increase for each epoch as we keep training the model, and eventually reach a steady value when our model is not able to learn any more useful information from our training data.", "From the output shown above, we see that the loss decreases while the accuracy increases during the training process. Each time the validation accuracy reaches a new maximum value, the checkpoint file is saved (output: \u201csaving model to sign_classifier.h5\u201d. After the training has completed, we then load the checkpoint file which had the best validation accuracy during training:", "We first visualize the changes in model accuracy and loss during the training process, as this gives us important information to evaluate what we can do to improve accuracy. For a nice introduction to this topic, you can also have a look at this video:", "Code for plotting and saving the learning curves:", "Starting with the left figure, showing the training/valication accuracy: The blue line represents the model accuracy as measured on the training images, and we see that this quickly reaches a value of almost 1 (which represents classifying 100% of the training images correctly). However, the validation accuracy is the accuracy measured on the validation set, which is the accuracy we really care about. In this case, the accuracy leveled off at around 97\u201398%, meaning that we succesfully classified almost all of the images in our validation set to the correct category.", "To learn more about the accuracy for the different categories, we can calculate and plot the \u201cconfusion matrix\u201d. This represents an illustrative way of evaluating model accuracy, as it compares the \u201ctrue\u201d vs. \u201cpredicted\u201d class for all images in the test set. Note: do not worry if you do not get exactly the same numbers when re-running the code! There are some inherent randomness in model initialization etc. which make the results differ slightly from time to time.", "(The code to calculate and plot the confusion matrix is included below the figure)", "The is the code from the script \u201cplot_conf.py\u201d, which contains the function for plotting the confusion matrix, \u201cplot_confusion_matrix\u201d.", "As seen from the confusion matrix above, the main category the model misclassified was\u201cIntersection\u201d, where it mistakes the category with that of \u201cYield\u201d in 10 of the images. As a final metric, we can also calculate the total accuracy evaluated on the test set", "This gives as output an accuracy of 98%, which is not bad! But, can we do better? We have a limited amount of data, so how about trying to improve that using image augmentation?", "In our case, the model already performs very well, with an accuracy of 97\u201398%. However, one strategy when dealing with limited amount of training data is that of \u201cimage augmentation\u201d. That is, we make a collection of copies of the existing images, but with some minor changes. Those changes could be transformations like e.g. slight rotations, zooming, flipping images horizontally, ++. Further examples of image augmentation are also covered here.", "In the following, we define the same model as before, but here we also incorporate image augmentation as a way of artficially increasing the amount of training data.", "Code to build a new model, using the same convolutional base and model structure as before:", "The only thing we need to change in our code, is the definition of the training data generator shown below. We can here add some data augmentation strategies, such as e.g. random rotations in the range [-10,10] degrees, a random zoom and width/height shift in the range +-10%, and changes in brightness in the range +-10%.", "As examples of augmented images, we can save them to a specified folder \u201caugm_images\u201d as defined in the function \u201ctrain_generator\u201d below. This option is currently commented out (to avoid saving thousands of images), but you can change that if you want to visualize the augmentations you incorporate. This is often a good idea, just to make sure that the augmented images still make sense for the use-case you are working on.", "We are now ready to train the same model using additional augmented data, which should hopefully increase model accuracy.", "After the training has completed, we again load the checkpoint file which had the best validation accuracy during training:", "Calculate the final accuracy, as evaluated on the test set:", "This gives an output of 99.3%, which is an improvement compared to our initial model without augmented images!", "As seen from the above results for model accuracy, data augmentation indeed increased the accuracy of our model. In the current example, we obtained a final accuracy of approximately 99%. In addition, by inspecting the confusion matrix above, we can check which of the sign categories the model classifies incorrectly. Here, we notice that the model still misclassified \u201cIntersection\u201d as \u201cYield\u201d in a few cases, but significantly better than the model without image augmentation.", "Note: Do not worry if you do not get exactly the same numbers when re-running the code! There is some inherent randomness in the model initialization etc. which could make the results differ slightly from time to time.", "As a final visualization of model accuracy, we can plot a subset of the test images along with the corresponding model prediction.", "Define a folder for \u201ctest_subset\u201d, where I have included 50 of the images from the test set:", "Make predictions for the images contained in this folder, and visualize the images along with the predicted and actual class. Do you agree with the classifications?", "If you managed to run through the entire tutorial using the included dataset, you have hopefully gotten a feeling for how deep learning and image recognition can be used to solve a real-world problem of traffic sign classification. Best of luck exploring the model further with other images, either from your company or from resources such as e.g. kaggle, or simply google image search!", "If you want some more detailed information regarding this image classification tutorial (and machine learning in general), I also cover the material in the workshop presentation below: \u201c From hype to real-world applications\u201d. (tutorial walkthrough starts approx. 35 minutes into the video).", "Did you find the article interesting? If so, you might also like some of my other articles on topics such as AI, Machine Learning, physics, etc., which you can find in the links below and on my medium author profile: https://medium.com/@vflovik", "And, if you would like to become a Medium member to access all material on the platform freely, you can also do so using my referral link below. (Note: If you sign up using this link, I will also receive a portion of the membership fee)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff3c7e0ec1a14&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vflovik?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vflovik?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Vegard Flovik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17ff8967433&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&user=Vegard+Flovik&userId=17ff8967433&source=post_page-17ff8967433----f3c7e0ec1a14---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning"}, {"url": "https://arxiv.org/pdf/1312.4400.pdf", "anchor_text": "this paper"}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "confusion matrix"}, {"url": "https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2", "anchor_text": "image augmentation"}, {"url": "https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2", "anchor_text": "here"}, {"url": "https://medium.com/@vflovik", "anchor_text": "https://medium.com/@vflovik"}, {"url": "https://medium.com/@vflovik/membership", "anchor_text": "Join Medium with my referral link \u2014 Vegard FlovikAs a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story\u2026medium.com"}, {"url": "https://towardsdatascience.com/a-gentle-introduction-to-monte-carlo-methods-98451674018d", "anchor_text": "A Gentle Introduction to Monte Carlo Methods"}, {"url": "https://towardsdatascience.com/q-a-with-a-data-scientist-1f872518315f", "anchor_text": "The transition from Physics to Data Science"}, {"url": "https://builtin.com/machine-learning/graph-theory", "anchor_text": "What is Graph theory, and why should you care?"}, {"url": "https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-the-sequel-e117e6ff55f1", "anchor_text": "How (not) to use Machine Learning for time series forecasting: The sequel"}, {"url": "https://towardsdatascience.com/building-an-ai-that-can-read-your-mind-8b22ad5a7f05", "anchor_text": "Building an AI that can read your mind"}, {"url": "https://medium.com/me/stats/post/3332d77dfa6", "anchor_text": "The hidden risk of AI and Big Data"}, {"url": "https://towardsdatascience.com/how-to-use-machine-learning-for-anomaly-detection-and-condition-monitoring-6742f82900d7", "anchor_text": "How to use machine learning for anomaly detection and condition monitoring"}, {"url": "https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424", "anchor_text": "How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls"}, {"url": "https://towardsdatascience.com/machine-learning-for-production-optimization-e460a0b82237", "anchor_text": "How to use machine learning for production optimization: Using data to improve performance"}, {"url": "https://towardsdatascience.com/how-do-you-combine-machine-learning-and-physics-based-modeling-3a3545d58ab9", "anchor_text": "How do you teach physics to AI systems?"}, {"url": "https://medium.com/predict/can-we-build-an-artificial-brain-network-using-nanoscale-magnets-1c0a925973ab", "anchor_text": "Can we build artificial brain networks using nanoscale magnets?"}, {"url": "https://towardsdatascience.com/artificial-intelligence-in-supply-chain-management-predictive-analytics-for-demand-forecasting-80d2d512f155", "anchor_text": "Artificial Intelligence in Supply Chain Management: Utilizing data to drive operational performance"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f3c7e0ec1a14---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----f3c7e0ec1a14---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f3c7e0ec1a14---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/image-classification?source=post_page-----f3c7e0ec1a14---------------image_classification-----------------", "anchor_text": "Image Classification"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----f3c7e0ec1a14---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&user=Vegard+Flovik&userId=17ff8967433&source=-----f3c7e0ec1a14---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&user=Vegard+Flovik&userId=17ff8967433&source=-----f3c7e0ec1a14---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff3c7e0ec1a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f3c7e0ec1a14---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f3c7e0ec1a14--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vflovik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vflovik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vegard Flovik"}, {"url": "https://medium.com/@vflovik/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.1K Followers"}, {"url": "https://www.linkedin.com/in/vegard-flovik/", "anchor_text": "https://www.linkedin.com/in/vegard-flovik/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17ff8967433&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&user=Vegard+Flovik&userId=17ff8967433&source=post_page-17ff8967433--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffbc5435597ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-image-classification-f3c7e0ec1a14&newsletterV3=17ff8967433&newsletterV3Id=fbc5435597ce&user=Vegard+Flovik&userId=17ff8967433&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}