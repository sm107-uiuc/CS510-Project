{"url": "https://towardsdatascience.com/gradient-descent-explanation-implementation-c74005ff7dd1", "time": 1683002585.254299, "path": "towardsdatascience.com/gradient-descent-explanation-implementation-c74005ff7dd1/", "webpage": {"metadata": {"title": "Gradient Descent Explanation & Implementation | by Jeremy Zhang | Towards Data Science", "h1": "Gradient Descent Explanation & Implementation", "description": "Gradient descent is probably the most well-known optimisation algorithm, and in the world of machine learning, you must have been either directly or indirectly used gradient descent. You likely\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/MJeremy2017/Machine-Learning-Models/blob/master/Optimisation/gradient-descent.ipynb", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-momentum-explanation-8548a1cd264e", "anchor_text": "SGD & Momentum", "paragraph_index": 22}], "all_paragraphs": ["Gradient descent is probably the most well-known optimisation algorithm, and in the world of machine learning, you must have been either directly or indirectly used gradient descent. You likely already know that it can help to minimise a loss function by moving tiny steps towards the negative direction of the gradient. But how exactly? And instead of directly using gradient methods incorporated in the package, how can we implement our own and have a closer look of gradient descent?", "In this post, let\u2019s break gradient descent piece by piece and grow a deeper understanding of it by implementing our own. Now let\u2019s start with the simplest example:", "Suppose we have the function f(x) = x^2 , where x ranges from -1 to 1, given x randomly start in the range, how to find the minimum value of f(x) ?", "Clearly in this example, the minimum locates in x = 0 , and we would like to", "So how can we do that? You might also notice that the gradient of the function", "Which is the opposite of the moving direction of x ! Gradient descent makes use of this and let x to move to the opposite direction of its gradient. In this scenario, no matter where x is, it will move to the minimum.", "The next question is how much further should x be moving, and this leads to learning rate, which comes as parameters in numerous machine learning algorithms.", "In fact, in the example above, tan\u03b1 is the gradient, and at each step, we make x move with the step size tan\u03b1 * learning_rate , where learning rate becomes an adjustable parameter which controls the speed of descent.", "With the above in mind, we can have our first implementation to find the minimum value of function x^2 .", "In each iteration, x -= lr*grad_fn(x) makes it always moving to the minimum, and we can also plot the trajectory of x :", "Starting from -1, x descent to 0 gradually. Also notice that it moves faster at the beginning and slows down when approaching the goal, this is because the absolute gradient is higher at start.", "Now let\u2019s get to an example of optimising parameters. Suppose we try to optimise parameters with function:", "The objective would be to minimise lose (y \u2014 f(x))^2 , and the corresponding gradient of parameter a and b would be:", "Note that x and y here are considered constants.", "Now let\u2019s implement the function optimisation:", "Here we update the parameter on each input x, y pair, and got result:", "You can see that the updating process is volatile and for parameter a , it first goes down(to the opposite direction) before it moves to the optimal value. This is because we updated parameters on each input, as each individual input could potentially update parameter in arbitrary direction. Are we able to generate a smoother line? The answer leads to batch gradient descent.", "In actual use cases, parameters are not updated each time on single data point, instead batch update is applied, where in each iteration(epoch), parameters are updated based on the average of a batch of data points. In this case, our updating formula would be:", "The summation of gradient of a batch is calculated and taken average as the gradient to be updated:", "Here requires a higher number of iterations, as each batch only contributes to 1 update(we took all data point into 1 batch, one could different combinations), and this time the update process will look like:", "Way smoother and stabler, but the trade-off is that more computations required.", "Hope till here you\u2019ve acquired a slightly better understanding of vanilla gradient descent. Traditional gradient descent does not guarantee optimality and in fact, it could easily fall into local minimum when there are multiple basins in the objective function, as parameters each time only move slightly based on gradient and no stochasticity is allowed. If you are interested, please check out the full implementation here.", "Next up, I will be introducing SGD & Momentum, which adds some variations to vanilla gradient descent and solve some problems of it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc74005ff7dd1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----c74005ff7dd1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/MJeremy2017/Machine-Learning-Models/blob/master/Optimisation/gradient-descent.ipynb", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-momentum-explanation-8548a1cd264e", "anchor_text": "SGD & Momentum"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"}, {"url": "https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms", "anchor_text": "https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c74005ff7dd1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----c74005ff7dd1---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----c74005ff7dd1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----c74005ff7dd1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc74005ff7dd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c74005ff7dd1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c74005ff7dd1--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explanation-implementation-c74005ff7dd1&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}