{"url": "https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e", "time": 1683006279.9561222, "path": "towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e/", "webpage": {"metadata": {"title": "Word and Text Representations in Natural Language Processing | by Arun Jagota | Towards Data Science", "h1": "Word and Text Representations in Natural Language Processing", "description": "Words are fundamental constructs in many natural languages. Characters in these languages are meaningless until arranged to form a word. When this happens, viola, meaning emerges! When words are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916", "anchor_text": "https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916", "paragraph_index": 52}], "all_paragraphs": ["Words are fundamental constructs in many natural languages. Characters in these languages are meaningless until arranged to form a word. When this happens, viola, meaning emerges! When words are arranged sequentially, such as in phrases or sentences, even more meaning emerges.", "Not surprisingly, core NLP operations involve processing words or sequences of words appropriately. (In this post a sequence of words, which we will abbreviate to text, will be a phrase, a sentence, a paragraph, a section, a chapter, or an entire document. For notational simplicity, we will treat punctuation symbols also as words.)", "Below is an inventory of key operations on text that power many use cases.", "All these involve operating on words. Some, such as text similarity, involves operating on word sequences.", "In this, every distinct word is assigned a unique identifier. As an example, say our lexicon only has 10 words: dog, cat, tiger, elephant, the, black, white, is, big, fast. Below depicts one local encoding of this lexicon.", "Now consider the text: the dog and cat are black. This would be represented as the sequence 4 0 9 1 8 5.", "This representation retains all the information in the text. This is good. On the other hand texts of differing lengths produce sequences of differing lengths, making them hard to compare.", "In this representation, each word in the lexicon forms its own dimension in a vector space. So in our example above, our space would have 10 dimensions. A word is represented by the binary vector in which the word\u2019s dimension\u2019s value is 1 and the rest are 0. We will call this vector the word\u2019s indicator vector.", "Below are two word indicator vectors from our running example.", "Okay, now comes the key point. Texts of arbitrary length are also mapped as vectors in this same space. This makes it easy to compare texts of differing lengths. Just compare their vectors using algebraic similarity measures such as cosine similarity or those based on Euclidean distance or others.", "So how is text, i.e. a sequence of words of arbitrary length, mapped to a vector in this space. There are various possibilities. The simplest one is to add up the indicator vectors of the words that appear in the text and binarize (i.e. truncate positive counts to 1). Below is the vector we would get for our example.", "This vector just captures which words from the lexicon are in the text and which not.", "This representation, also known as bag of words, ignores all word order in the text. Sometimes this is actually beneficial. The vector representations of the cat and dog are black and the dog and cat are black are the same. This is good, as the order in which cat and dog appear here is immaterial.", "Consider long texts, such as long documents. The frequencies with which various words appear may also be important. For example, consider two 10,000-word documents, one in which hospital is mentioned 200 times and one in which hospital is mentioned only once. The occurrence of hospital in the first document is more significant than in the second one.", "A text encoding that captures word frequency is simple to obtain. Just add up the indicator vectors of all the word occurrences in the text. Just don\u2019t binarize.", "In long documents common words such as the, a, of, and will appear frequently. The frequency-based representation will over-represent their importance. We don\u2019t want two documents to be deemed similar merely because they both have lots of occurrences of the in them. Yes, frequency is important, but only of informative words. In our examples, hospital is an informative word whereas the is not.", "The TF X IDF representation first involves assigning an informativeness score to each word in the lexicon. This score is estimated from a diverse corpus of text such as sentences (or even paragraphs) drawn from different types of documents. The fewer texts a word appears in, the more informative it is. This is why this measure is called \u201cinverse document frequency\u201d.", "Armed with the IDF scores of the various words, a text\u2019s TF X IDF vector is easily obtained. Run through all the word occurrences in the text. Multiply the indicator vector of each occurrence by the IDF score of its word. Add them all up. This will result in a vector in which a word\u2019s contribution will depend on its frequency in the document adjusted by the word\u2019s informativeness (i.e. IDF) score. Words uncommon in a diverse corpus but frequent in a particular text will be deemed more important in the text.", "Let\u2019s see an example. Consider the text the dog and cat are black. We saw its term frequency representation earlier: 1100110010.", "The TF X IDF representation might look more like this. (Don\u2019t take the positive numbers too literally.)", "The words the and are have a greatly reduced influence.", "Local Representations Can\u2019t Capture Word Similarity", "Local representations are definitely very useful \u2014 indeed widely used. That said, they have a limitation that is a barrier to even more widespread use. They are incapable of accounting for word similarity.", "Since lawyer and attorney are synonyms we can see that S1 and S2 are semantically identical whereas S3 is unrelated to either. The local representations are incapable of making this distinction because lawyer, attorney, and doctor will be three different dimensions.", "We can alleviate this situation in one of two ways (or a combination).", "While dictionary-based approaches are often effective in practice, they cannot deduce more nuanced relationships than those represented explicitly in the dictionaries. As a simple example consider", "We\u2019d like to be able to deem S5 and S6 as semantically more similar than S6 and S7.", "This example is just the tip of the iceberg. Among the millions of words out there, think how many word similarity relationships are buried! Now extend this to proper nouns. As an example, we\u2019d like to capture that canon and cameras are positively related.", "Correlation analysis of dimensions can alleviate these issues. From a large corpus of documents, such as Wikipedia, broken down further into smaller chunks such as paragraphs, we might be able to deduce that \u201ccomputer science\u201d and \u201cdata science\u201d are positively correlated, whereas \u201ccomputer science\u201d and \u201cart\u201d are uncorrelated \u2014 perhaps even negatively correlated.", "We won\u2019t describe the myriads of ways correlation analysis can be applied to alleviate these issues. Instead we will focus on an approach that does an alternate encoding of a word, called a distributed representation, which does a form of correlation analysis under-the-hood.", "A word\u2019s local representation just has a single 1-valued bit \u2014 the one that identifies the word. Consequently, the indicator vectors of any two words differ in exactly two bits. (See example below.) That is, all pairs of indicator vectors are equidistant.", "What if it were possible, instead, to map words to vectors in such a way that similar words get mapped to nearby vectors. It is possible, albeit more involved. First, we need a key concept.", "The context of a word is the words that frequently occur near it, such as in the same sentence.", "Why is this concept useful to us? Because two words having similar contextual representations will be positively related. Take \u201ccat\u201d and \u201cwhiskers\u201d. They will have similar contextual representations as they will both co-occur in contexts involving cats. On the other hand \u201ccat\u201d and \u201cJupiter\u201d will not appear in the same contexts. So by comparing contextual representations, we can infer that the first pair is related whereas the second pair is not.", "Okay, let\u2019s now formalize this notion. We model a word\u2019s context as a probability distribution over all words in the lexicon. We will call this the word\u2019s context vector. This is a vector in our original local representation space. Unlike an indicator vector, it is distributed. In fact, the indicator vector is a special case, one whose context is the word itself (no more, no less).", "Say our lexicon just has four words cat, whiskers, computer, and laptop. Their indicator vectors are 1000, 0100, 0010, and 0001 respectively. No pair overlaps. By contrast, plausible context vectors of these four words might be as below.", "We see that the context vectors of cat and whiskers overlap, as do those of computer and laptop. But not those of whiskers and computer. This makes sense.", "Now that we understand what a word\u2019s context vector is, how do we estimate it? Having a large corpus of documents helps. Fortunately, such a corpus, for example, Wikipedia, is easily obtained these days.", "We simply keep track of co-occurrences of other words with the word of interest in the same proximity, say in the same sentence. Say the word cat occurs in 100 sentences. In 5 of these sentences the word dog also appears. The word dog is in cat\u2019s context with probability 5/100.", "There is one issue with this approach. Especially common words such as the will appear in most sentences, so will appear in the contexts of unrelated words. For example, cat\u2019s context will include the because in most sentences in which cat appears the also appears.", "There is a simple way to alleviate this situation. We divide the word\u2019s contextual probability by its overall probability. Formally, this is as follows. Say context-score(u,v) denotes the score we give to v of being in u\u2019s context. To this point we have defined context-score(u,v) as P(v|u). Instead, we can define it as P(v|u)/P(v). Here P(v|u) is the fraction of sentences containing u that also contain v and P(v) is the fraction of all sentences that contain v.", "Here is an example. Let\u2019s say that the occurs in 75% of all sentences and 80% of the sentences that cat occurs in. So the score of the being in cat\u2019s context is 80/75. Consider whiskers. Let\u2019s say it appears in only 1% of all sentences, but in 25% of those in which cat appears. So the score of whiskers being in cat\u2019s context is 25/1.", "In short, the score of whiskers being in cat\u2019s context is much higher than that of the\u2019s. As desired.", "The vector space might have millions of dimensions. The context vectors, however, will tend to be sparse or can be made sparse without losing much information. A word\u2019s context will normally only contain a small proportion of the millions of words in the lexicon.", "This is just as before. Add the vectors of the word occurrences in the text, after adjusting for the IDF scores of the various words.", "It's important to note that adjusting with the IDF scores is even more important here than in local representations. This is because a common word such as the will have its own (especially wide) context, whose undamped contribution will pollute the text vector.", "Let\u2019s see an example. Say from a suitable corpus we picked up the following context representations.", "Note that bark\u2019s context captures its two word-senses: tree-bark and dog-bark.", "Now consider the text the bark of the dog. Assume that IDF will significantly reduce the contribution of the two occurrences of the and the one of of to the text vector. In fact to drop clutter, let\u2019s just set these influences to zero. We are left with: bark dog. Adding up the context vectors of the two words will give us something like", "The words that occur in the contexts of both the words in the text are highlighted in bold. These words will contribute more to the text\u2019s vector.", "Now imagine that the text was the bark of the labrador. Assume that our corpus is rich enough to capture the context of labrador. This text\u2019s vector will also be similar. This will let us infer that the bark of the dog and the bark of the labrador are quite similar.", "In the past decade, a different way of obtaining word context vectors, going by the name word embeddings, has emerged and gained widespread use. Instead of obtaining context vectors via simple statistical analyses, it trains a neural network to learn word embeddings. This is done in an architecture with a hidden layer. Consequently, learning forces the network to learn good latent features of words. Words are embedded in this feature space, not in the original vector space.", "We cover this approach in detail in a separate post, https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916 as its description, which involves neural network details, is fairly involved.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F33349649be2e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----33349649be2e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916", "anchor_text": "https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916"}, {"url": "https://medium.com/tag/vector-space-model?source=post_page-----33349649be2e---------------vector_space_model-----------------", "anchor_text": "Vector Space Model"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----33349649be2e---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/tf-x-idf?source=post_page-----33349649be2e---------------tf_x_idf-----------------", "anchor_text": "Tf X Idf"}, {"url": "https://medium.com/tag/bag-of-words?source=post_page-----33349649be2e---------------bag_of_words-----------------", "anchor_text": "Bag Of Words"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&user=Arun+Jagota&userId=ef9ed921edad&source=-----33349649be2e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&user=Arun+Jagota&userId=ef9ed921edad&source=-----33349649be2e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F33349649be2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----33349649be2e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----33349649be2e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----33349649be2e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----33349649be2e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----33349649be2e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----33349649be2e--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "685 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-text-representations-in-natural-language-processing-33349649be2e&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}