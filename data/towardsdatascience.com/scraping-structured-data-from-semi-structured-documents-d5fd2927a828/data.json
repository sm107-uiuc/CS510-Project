{"url": "https://towardsdatascience.com/scraping-structured-data-from-semi-structured-documents-d5fd2927a828", "time": 1683005183.1314392, "path": "towardsdatascience.com/scraping-structured-data-from-semi-structured-documents-d5fd2927a828/", "webpage": {"metadata": {"title": "Scraping Structured Data From Semi-Structured Documents | by Keith McNulty | Towards Data Science", "h1": "Scraping Structured Data From Semi-Structured Documents", "description": "One of the most powerful capabilities that data science tools bring to the table is the capacity to deal with unstructured data and to turn it into something that can be structured and analyzed. Any\u2026"}, "outgoing_paragraph_urls": [{"url": "https://keithmcnulty.github.io/friends_analysis/play_friends.html", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://github.com/keithmcnulty/friends_analysis", "anchor_text": "Github repo for my Friends project", "paragraph_index": 2}, {"url": "https://fangj.github.io/friends/", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://fangj.github.io/friends/season/0101.html", "anchor_text": "https://fangj.github.io/friends/season/[ss][ee].html", "paragraph_index": 3}, {"url": "https://github.com/keithmcnulty/friends_analysis/blob/master/edgelist_creation/scrape_friends.R", "anchor_text": "Here", "paragraph_index": 26}, {"url": "https://www.linkedin.com/in/keith-mcnulty/", "anchor_text": "LinkedIn", "paragraph_index": 29}, {"url": "https://twitter.com/dr_keithmcnulty", "anchor_text": "Twitter", "paragraph_index": 29}, {"url": "http://drkeithmcnulty.com/", "anchor_text": "drkeithmcnulty.com", "paragraph_index": 29}, {"url": "http://keithmcnulty.org", "anchor_text": "keithmcnulty.org", "paragraph_index": 31}], "all_paragraphs": ["One of the most powerful capabilities that data science tools bring to the table is the capacity to deal with unstructured data and to turn it into something that can be structured and analyzed. Any data scientist worth their salt should be able to \u2018scrape\u2019 data from documents, whether from the web, locally or any other type of text-based asset.", "In this article I am going to show how to scrape the online scripts of the TV Show Friends with the aim of creating a table of numbered scenes for each episode and the names of the characters in those scenes. This was the first step in my build of the interactive Friends network website here. It is also an excellent test case for scraping techniques, since the scripts all have a certain basic structure to them, but the structure is not entirely predictable, with the writers not always following the same formatting rules from episode to episode.", "You can find all the final code for this work on the Github repo for my Friends project.", "The scripts that I want to scrape can be found here. If you click on each one you can see that the file name has a consistent format of the form https://fangj.github.io/friends/season/[ss][ee].html where[ss] is the two digit season number and[ee] is the two digit episode number.", "To scrape effectively \u2014 you\u2019ll need a knowledge of two things:", "Let\u2019s remind ourselves of our task here. We are interested in two things:", "Let\u2019s take a look at the web code for Season 1 Episode 1. You can do this by opening the script in Google Chrome and then pressing CMD+Option+C (or Ctrl+Shift+C in Windows) to open the Elements Console where you can view the HTML of the page side by side with the page itself.", "One of the things we can see immediately is that most of the words that precede a colon are of interest to us. In fact, most of them are character names that say something in a scene. We also see that lines the contain the string \"Scene:\" are pretty reliable indicators of scene boundaries.", "The first thing we probably want to do get this HTML code in a list or vector of nodes which represent the different pieces of formatting and text in the document. Since this will contain the separated lines spoken by each character, this will be really helpful for us to work from. So we will use our nice scraping packages to download the HTML code, break it into nodes so that we have a nice tidy vector of script content.", "Now if you take a look at your nodes object, you'll see that it is a character vector that contains a lot of different split out parts, but most importantly it contains lines from the script, for example:", "To be useful for our task, we need to create a vector that contains the word \u2018New Scene\u2019 if the line represents the beginning of a scene, and the name of the character if the line represents something spoken by a character. This will be the best format for what we want to do.", "The first thing we will need to do is swap any text string containing \"Scene:\" to the string \"New Scene\". We can do this quite simply using an ifelse() on the nodes vector, where we use grepl() to identify which entries in nodes contain the string \"Scene:\".", "We can quickly check that there are entires that now contain \"New Scene\".", "That worked nicely. Now, you might also have noticed that character names precede a colon. So that might be a nice way to extract the names of characters (although it might give us a few other things that have preceded colons in the script also, but we can deal with that later).", "So what we will do is use regex to tell R that we are looking for anything preceding a colon. We will use a lookahead string as follows: \".+?(?=:)\" .", "Let\u2019s look at that string and make sure we know what it means. The .+? component means 'find any string of text of any length'. The part in brackets is known as a lookahead: it means look ahead of that string of text and find a colon as the next character. This is therefore instructing R to find any string of text that precedes a colon and return it. If we use the package stringr and its function str_extract() with this regex string, it will go through every entry of the nodes vector and transform it to just the first string of text found in front of a colon. If no such string is found it will return an NA value. This is great for us because we know that spoken characters names are always at the start of nodes, so we certainly won't miss any if we just take the first instance in each line. We should also, for safety, not mess with the scene breaks we have put into our vector:", "Let\u2019s take a look at what\u2019s in our nodes vector now", "So this is working \u2014 but we have more cleaning to do. For example, we will want to get rid of the NA values. We also notice that there are some preamble lines which usually contain the word \"by\" and we can see that strings in brackets like \"(Note\" seems to have been extracted. We can create a bunch of special cleaning commands to get rid of these if we don't want them. For example:", "This is looking good. Of course the cleaning steps I did above are not complete, and that string we have put in !grepl(\"by|\\\\(|all\", tolower(nodes)) will be expanded as we do more and more scraping to account for common failure in parsing. But you get the idea. Note that when you are using characters in regex strings which are also used as 'special characters', you'll need to escape those by putting a \\\\ in front of them. What I mean is: R needs to know when you type a ( - do you literally mean you are looking for a ( or are you starting a lookahead command or something like that. So if you want a special character to be taken literally - ie yes I really want a literal opening bracket - you should write it in regex as \\\\(. Consult the cheat sheet link above or look online for lists of specials in regex.", "Let\u2019s assume our cleaning is done and we have a nice vector that contains either the names of characters that are speaking lines in the episode or \u201cNew Scene\u201d to indicate that we are crossing a scene boundary. We now just need to convert this vector into a simple data frame with three columns: episode, scene and character.", "The episode number is obvious, and we already have our character lists, so we really just need to iterate through our nodes vector and for each entry, count the number of previous occurrences of \u201cNew Scene\u201d and add one. We can do this with:", "Then we can finalize our dataframe by putting our three vectors together and removing any repeated characters in the same scene. We can also correct for situations where the script starts with a New Scene and we can consistently format our character names to title case, to account for different case typing:", "Let\u2019s take a look at some example output from our results dataframe:", "Of course, our aim here is not to scrape just the first episode, but to scrape every episode of every season. A good data scientist will now look to generalize the work they have done and put it into a function that will accept a season number and an episode number and then scrape that episode and deliver back the data frame we just constructed.", "If we look at the format of Season 1, Episode 1, we will see that it is repeated through most \u2014 though not all \u2014 episodes. There are some exceptions:", "So it will be necessary to expand our code above with if else statements to allow for situations where we know the format will be a little different. We will also need to expand our cleanup as much as we can to account for things happening with our parsing of other episodes which we don't anticipate. It's fairly normal for there to be a little bit of human effort involved in scraping text and cleaning up for unanticipated results. It's also quite common for the end result not to be 100% error free. The data scientist has to determine when the result is close enough to what they need, and when cleaning up the remaining 1-2% of errors is not worth the effort involved in doing so.", "Here is my final episode scraping function which I\u2019m happy does the job I need. You\u2019ll see the code anchor around the work we did above, but expand it to work on all seasons and episodes.", "Now we can scrape for example Season 9 Episode 2 with this simple command:", "This function will become super useful to us as we move to try to create edgelists for the network of characters based on them appearing in the same scene together. Look out for this in an upcoming post.", "Originally I was a Pure Mathematician, then I became a Psychometrician and a Data Scientist. I am passionate about applying the rigor of all those disciplines to complex people questions. I\u2019m also a coding geek and a massive fan of Japanese RPGs. Find me on LinkedIn or on Twitter. Also check out my blog on drkeithmcnulty.com.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Expert and Author in Applied Mathematics, Data Science, Statistics. Also writes History, Science, Culture. Find me on Twitter or keithmcnulty.org"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd5fd2927a828&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://keith-mcnulty.medium.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": ""}, {"url": "https://keith-mcnulty.medium.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Keith McNulty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa859aab532a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&user=Keith+McNulty&userId=a859aab532a0&source=post_page-a859aab532a0----d5fd2927a828---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://keithmcnulty.github.io/friends_analysis/play_friends.html", "anchor_text": "here"}, {"url": "https://github.com/keithmcnulty/friends_analysis", "anchor_text": "Github repo for my Friends project"}, {"url": "https://fangj.github.io/friends/", "anchor_text": "here"}, {"url": "https://fangj.github.io/friends/season/0101.html", "anchor_text": "https://fangj.github.io/friends/season/[ss][ee].html"}, {"url": "https://drkeithmcnulty.com/2019/08/16/tidy-web-scraping-in-r%e2%80%8a-%e2%80%8atutorial-and-resources/", "anchor_text": "here"}, {"url": "https://github.com/rstudio/cheatsheets/raw/master/regex.pdf", "anchor_text": "here"}, {"url": "https://github.com/keithmcnulty/friends_analysis/blob/master/edgelist_creation/scrape_friends.R", "anchor_text": "Here"}, {"url": "https://www.linkedin.com/in/keith-mcnulty/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/dr_keithmcnulty", "anchor_text": "Twitter"}, {"url": "http://drkeithmcnulty.com/", "anchor_text": "drkeithmcnulty.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d5fd2927a828---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----d5fd2927a828---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/web-development?source=post_page-----d5fd2927a828---------------web_development-----------------", "anchor_text": "Web Development"}, {"url": "https://medium.com/tag/tv-series?source=post_page-----d5fd2927a828---------------tv_series-----------------", "anchor_text": "TV Series"}, {"url": "https://medium.com/tag/analytics?source=post_page-----d5fd2927a828---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&user=Keith+McNulty&userId=a859aab532a0&source=-----d5fd2927a828---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&user=Keith+McNulty&userId=a859aab532a0&source=-----d5fd2927a828---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd5fd2927a828&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d5fd2927a828---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d5fd2927a828--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d5fd2927a828--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d5fd2927a828--------------------------------", "anchor_text": ""}, {"url": "https://keith-mcnulty.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://keith-mcnulty.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Keith McNulty"}, {"url": "https://keith-mcnulty.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "11K Followers"}, {"url": "http://keithmcnulty.org", "anchor_text": "keithmcnulty.org"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa859aab532a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&user=Keith+McNulty&userId=a859aab532a0&source=post_page-a859aab532a0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7991b3dbcee9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-structured-data-from-semi-structured-documents-d5fd2927a828&newsletterV3=a859aab532a0&newsletterV3Id=7991b3dbcee9&user=Keith+McNulty&userId=a859aab532a0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://peopleanalytics-regression-book.org/", "anchor_text": "Handbook of Regression Modeling in People Analytics2021"}, {"url": "https://ona-book.org", "anchor_text": "Handbook of Graphs and Networks in People Analytics2022"}, {"url": "https://medium.com/@keith-mcnulty/books?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "See all (2)"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}