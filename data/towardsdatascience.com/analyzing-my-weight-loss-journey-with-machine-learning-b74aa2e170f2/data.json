{"url": "https://towardsdatascience.com/analyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2", "time": 1682995073.3265438, "path": "towardsdatascience.com/analyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2/", "webpage": {"metadata": {"title": "Analyzing my weight loss with machine learning | by Khanh Nguyen | Towards Data Science", "h1": "Analyzing my weight loss with machine learning", "description": "Analyzing my weight loss journey with machine learning"}, "outgoing_paragraph_urls": [{"url": "https://github.com/seismatica/logweight", "anchor_text": "repo", "paragraph_index": 0}, {"url": "https://www.loseit.com/", "anchor_text": "Loseit", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/C25K", "anchor_text": "Couch to 5K", "paragraph_index": 1}, {"url": "https://us.amazfit.com/shop/bip", "anchor_text": "Amazfit Bip", "paragraph_index": 2}, {"url": "https://forum.xda-developers.com/general/accessories/xiaomi-mi-band-data-extraction-t3019156", "anchor_text": "workaround", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "logistic regression", "paragraph_index": 18}, {"url": "http://linear discriminant analysis", "anchor_text": "linear discriminant analysis", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM", "anchor_text": "support vector machine", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid function", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "L2-regularized", "paragraph_index": 53}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation", "anchor_text": "2-fold cross-validation", "paragraph_index": 62}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression#Logistic_function,_odds,_odds_ratio,_and_logit", "anchor_text": "logit", "paragraph_index": 82}], "all_paragraphs": ["To see the code I wrote for this project, you can check out its Github repo", "I began my weight loss journey at the start of 2018, following the oft-cited advice of \u201cweight loss = diet + exercise\u201d. On the diet side, I started tracking my daily food consumption (using a food scale and recording calories via the Loseit app). On the exercise side, I started following the Couch to 5K program, and to date have finished four 5K\u2019s, one 10K, and from a few weeks ago, a half-marathon. Lastly, every morning, I weighed myself right after waking up and recorded my weight in the same Loseit app.", "Anyone trying to lose weight will inevitably hit a weight-loss plateau, where the initial speedy weight loss starts to slow down. Personally, I hit a major plateau right after my vacation at the start of May. After that, I abandoned tracking my progress for nearly three months. Only after I replaced my dying Pebble smartwatch with an Amazfit Bip did I gain back some motivation to hit the resume button, partly since I could start tracking my steps with the new watch. However, my weight continued to plateau, and after a further two months of frustrating weight fluctuations (see dotted region below), I stopped tracking my weight and calories altogether. That was November of last year.", "It\u2019s now 2019, and as Tet (Vietnamese New Year) recently drew to a close, I\u2019ve decided to look more closely at the data collected during that two-month period, hoping to discover interesting relationships between my weight loss and the tracked calories/steps so that I could build a more effective weight loss plan than before.", "Calories: exported from my Loseit account as CSV files. For some strange reason, Loseit only allows calories data to be exported one week at a time, but joining them together is nothing a quick Python script can\u2019t do. Each date has corresponding calories counts from all the food I logged during that day, as well as a calories \u201cbudget\u201d that the app calculated for me based on my weight that day and the weight loss goal that I initially specified (to lose 0.75 kg/week).", "Steps: the Android app of my Amazfit Bip smartwatch doesn\u2019t allow data export unless one uses some hairy workaround with third-party tools. Therefore, the quickest method to obtain my steps data was to manually scroll through the dozens of dates (within this two-month period) on my phone and enter the steps for each date into a CSV file. Not elegant at all, but hey, it works!", "Weight: thankfully the Loseit website allows me to export all my recorded weights \u2014 and the dates on which they were taken \u2014 as a single CSV file.", "After joining these 3 data sources together by date, I end up with calories+steps+weight data for only 46 dates out of this two-month period. Apparently I neglected to record at least one of these three data in quite a few dates (many of them weekends, for obvious reasons).", "From those three raw data sources, three additional data fields are calculated:", "Surplus = calories consumed - calories budget. Positive surplus means I had eaten more calories than the budget allowed for that day, and vice versa. I choose to use calories surplus instead of raw calories consumed since the calories budget from the app naturally varies as my weights rise and fall, so calories surplus (which takes into account said budget) is a more accurate measure of my eating habits than calories alone.", "Weight gain = tomorrow weight - today weight. Positive weight gain of a given date means I had gained weight over that day (duh!), and vice versa.", "Weight gain status: positive weight gain would be labeled as 1, and negative or zero weight gain would be labeled as 0. I decide to use the binary weight gain status\u2013whether I had gained weight or not\u2013 rather than the more granular weight gain amount since obsessing over gaining 0.5 kg versus 0.3 kg is quite counter-productive, not least because the amount of weight gain can be influenced by many factors beside calories and steps (such as water intake, time of eating, etc).", "When plotting calories surplus and number of steps of each day against my weight gain status over that day (see first two panels below), there seems to be no apparent patterns on how these two reportedly important factors can predict whether I would gain weight or not.", "However, when plotting both calories surplus and steps together (right panel above), very interesting patterns emerge! For example, right away I can tell that there are two distinct groups of dates in terms of my step counts: those below 5000 steps (my \"baseline\" lazy days) that lie neatly around a horizontal line, and those above 5000 steps (my active days), largely thanks to my running. In terms of calories surplus, there are three main observations:", "1. On my lazy days, if I eat above my calories budget limit, it's gonna be bad news the following day when I step onto the scale. There were some miracles, such as that one day where I ate almost 1500 calories above my limit and did not gain weight the next day, but those are few and far between.", "2. On the other hand, if I eat below my calories budget limit on my lazy days, I'm not totally in the clear either: there are quite a few days where I was a good boy and ate my salads (figuratively), yet still gained weight. This suggests that I should be even more conservative in my eating on lazy days.", "3. However, on my active days, it seems I can afford to eat more than my calories limit, as there are several active days where I ate more than what the limit allowed and still did not gain weight.", "These observations suggest that I should factor in my daily activity (in the form of step counts) instead of using the default limit from the Loseit app.", "There are several common methods to build a linear classifier from data, such as logistic regression, linear discriminant analysis, or support vector machine. However, for this project, I will use logistic regression because:", "With this choice of model, let's see how logistic regression can implemented and interpreted using my data. However, this requires us to review some math on how the logistic regression classifier works and how it can be learned from data.", "Please note that this math review is primarily to establish a common notation so that we are on the same page before the algorithm is implemented from the mathematical equations. If you want to understand how these equations are derived and the intuition behind them, I have linked to further resources at the end of this blog post that can do these jobs way better than I can.", "In logistic regression, the predicted probability that I will gain weight on a given day is the sigmoid function applied to a weighted linear combination of my features (calories surplus and step counts) plus an constant intercept term. This relationship is mathematically expressed as:", "* Note that I have added an additional feature (x_intercept), which will always be equal to 1 so that the intercept term (\u03b8_intercept) could be learned along with the coefficients of the two existing features.", "Therefore, once \u03b8\u2019s have been learned from data, logistic regression can be used to classify whether I will gain weight on any given day \u2014 given my calories surplus and step count \u2014 by checking if the probability that I will gain weight on that day is above a certain threshold (usually 50%).", "The regression coefficients (\u03b8\u2019s) are learned via maximizing the logarithm of the probability of observing my training data (also called the log-likelihood). The formula for log-likelihood is:", "From the sigmoid function, different values of \u03b8\u2019s will produce different predicted probabilities of weight gain \u2014 P(y(i)=1) \u2014 and as a result, different log-likelihood. Therefore, the goal is to find the set of \u03b8\u2019s that maximize the log-likelihood of my training data i.e. the \u03b8\u2019s that \u201cexplain\u201d my training data the best.", "One straightforward algorithm to find the \u03b8\u2019s that maximize the log-likelihood of my training data is batch gradient ascent, which is described below:", "Step 1: For each training data point i, calculate the probability of weight gain using the feature values of that data point (x_intercept*, x_surplus, x_step), and the values of \u03b8\u2019s initialized in step 0. This is done using the familiar sigmoid function:", "Step 2: For each feature j \u2014 intercept/surplus/step \u2014 find the partial derivative of the log-likelihood with respect to the \u03b8 of that feature using the below equation:", "Step 3: For each feature j, update its \u03b8 by the partial derivative of the log-likelihood with respect to that \u03b8 (from step 2), multiplied by a small constant (also called the learning rate \u03b1):", "This learning rate controls how fast the algorithm will converge to the maximum of the log-likelihood, or even whether it converges at all (see the later section on visualizing the convergence of the algorithm for more details).", "Step 4: With these updated \u03b8\u2019s, repeat step 1 to step 3 until convergence. One way to test for convergence is to see if the log-likelihood has converged to a stable value i.e. that it has reached the likely maximum.", "The summation sign within step 2 \u2014 summing (y(i) - P(y(i)=1)) * x\u2c7c(i) over all data points i to calculate partial derivative\u2014 is the reason why this gradient ascent algorithm is of the batch variety, as each partial derivative is computed using all data points in the training data. Without this summation sign i.e. the partial derivative is computed using only one data point; assuming that data point is randomly chosen, the gradient ascent algorithm is called stochastic.", "For this project, I choose to implement batch gradient ascent since my training data is very small (only 46 data points), so there\u2019s no problem using the entire training data set at once to calculate partial derivatives. Another reason is that batch gradient ascent can be implemented more easily using numpy\u2019s vectorized operations (please refer to its implementation below to see how).", "From my previous data table, I use the calories surplus and step count columns as my features, and the weight gain status column as the label (see left) to train my logistic regression classifier.", "However, before I can implement the batch gradient ascent algorithm on this data, I first need to:", "After the above two steps, my feature matrix (X) is converted to a 2\u2013D numpy array of dimension (46, 3), and my label vector (y) to a 1-D numpy array of dimension (46).", "I initialize all my \u03b8\u2019s to 0.5. The result is a 1-D numpy array theta of dimension (3)", "Step 1: For each data point, calculate the probability of weight gain using \u03b8\u2019s from step 0 and the sigmoid function", "This is where numpy\u2019s vectorized operations come in handy, as instead of calculating the probability of weight gain for each data point, numpy does it for all data points at once:", "Step 2: For each feature, calculate the partial derivative of log-likelihood to its corresponding \u03b8 using the calculated probabilities from step 1", "This is also where numpy\u2019s vectorized operations shine:", "If the first argument is 1-D, it is promoted to a matrix by prepending a 1 to its dimensions. After matrix multiplication the prepended 1 is removed.", "Step 3: For each feature, update its \u03b8 by the partial derivative in step 2 multiplied by the learning rate \u03b1", "This can\u2019t be more straightforward: instead of updating each \u03b8, we can update all 3 \u03b8\u2019s at once by multiplying the gradient vector of partial derivatives with some pre-defined learning rate alpha, and adding the product to the theta vector.", "Step 4: Repeat step 1 to 3 until convergence", "This can be easily done by nesting the previous 3 lines of code\u2014 one for each step (see bolded code block below)\u2014 into a for loop that iterates many times. Presented below is the entire algorithm for batch gradient ascent running for 100 iterations (isn\u2019t it amazing how simple the implementation can be?!):", "One way to check for convergence of the algorithm is to see if the difference in log-likelihood stays below some small tolerance level for the past few iterations of the loop, which indicates that the log likelihood has likely reached its maximum. Using the dot products of y and prob(and of their respective complements), the log-likelihood can be computed simply as:", "Another (perhaps more fun) way is to run the algorithm for some number of iterations and visualize whether the log-likelihood has reached its likely maximum. Below is a visualization of 60 iterations of the batch gradient ascent algorithm with learning rate \u03b1 = 0.01:", "Now that we have a converged logistic regression model that classifies my training data pretty well (at least from eyeballing the decision boundary), let\u2019s see how we can improve on it.", "Given a fixed number of iterations, the value of learning rate (\u03b1) can determine whether the algorithm will converge after these iterations, or even if it will converge at all:", "With \u03b1 = 0.01 as the sweet spot for our learning rate, we can of course always increase the number of iterations to make sure our model converges well and good to the maximum log-likelihood. Indeed, at 1000 iterations, the difference in average log-likelihood between iterations is effectively zero.", "Even though my logistic regression model has converged to the max log-likelihood of my training data, it might have overfit that training data i.e. it learns from the data a little too well. As a result, the model might predict well on past data that I\u2019ve collected in 2018, but might be terrible if I were to use it to predict my weight gain in 2019.", "One solution to reduce the overfitting of logistic regression is to used the L2-regularized version of the regression (also called ridge regression), which subtracts the original log-likelihood by a term consisting of squares of the \u03b8\u2019s:", "As a result, maximizing the above function is equivalent to maximizing the log-likelihood of training data as much as possible while keeping \u03b8\u2019s low (as higher \u03b8\u2019s will lower L). The lambda symbol (\u03bb) represents the degree that \u03b8\u2019s are kept low (often called the regularization hyper-parameter of the model). When \u03bb = 0, ridge regression returns exactly to the original, non-regularized logistic regression.", "In terms of implementation of ridge regression, the only difference from the original logistic regression is in the calculation of partial derivatives (equation 2) of batch gradient ascent, where a \u03bb*\u03b8\u2c7c regularization term is subtracted from the partial derivative of each feature j:", "One caveat is that this regularization is not often done to the intercept \u03b8, so the partial derivatives of \u03b8_intercept is calculated the same as the non-regularized version i.e. without subtracting this \u03bb*\u03b8 term.", "This implementation is easily integrated into our existing Python code by:", "Below is the code for ridge regression with \u03bb = 10, with modifications to the original algorithm shown in bold:", "When convergence is monitored for this ridge regression at 60 iterations, we can see that:", "As \u03bb increases, the learned regression coefficients (\u03b8) are further squeezed towards zero, except that of the intercept (see left panel below). Furthermore, ridge regression becomes less and less effective in classifying my training data, as seen from the decision boundaries: the decision boundary at \u03bb = 1 is quite close from the non-regularized boundary, while at \u03bb = 100, the boundary is virtually unusable (see right panel).", "However, the purpose of ridge regression is not to improve the fit on training data (because if so, it will always perform worse than the non-regularized version, as seen above). Rather, it is to improve the prediction on new data i.e. data that the ridge regression had not been trained from.", "To compare ridge regression with its non-regularized counterpart, I use 2-fold cross-validation as outlined below:", "1. Split the 46 data points randomly into 2 equal folds/parts: A & B (each 23 data points)", "2. Train ridge regression on part A \u2014 the training set \u2014 and record the recall on the training set (days with correctly predicted weight gain / days with true weight gain in part A)", "3. Use the \u03b8\u2019s trained on part A to predict whether I would gain weight on the days of part B \u2014 the validation set \u2014 and record the recall on the validation set (days with correctly predicted weight gain / days with true weight gain in part B)", "4. Repeat step 2 and 3 with the parts switched i.e. part B is now the training set, and part A the validation set", "5. Average the training set recalls across both trials, and similarly for the validation set recalls", "There are two fundamental metrics to measure how well a classifier works: precision and recall. In this context:", "Below are the average train and validation recalls at different levels of regularization, with \u03bb ranging from 0 (non-regularized) to 10:", "Although these results suggest I should choose the \u03bb value with the highest average validation set recall (\u03bb = 0.5), the small number of data points in the validation set (23), within which there are even smaller number of weight gain points (red), suggests that this improved performance might just be due to luck. This also explains why for some \u03bb\u2019s, the performance on the validation set is higher than that of the training set at the same \u03bb, even though the opposite usually happens; I could just have a \u201clucky\u201d validation set.", "That said, there is no harm in choosing \u03bb = 0.5, given that when trained on my entire data of 46 points, its decision boundary is virtually indistinguishable from that of \u03bb = 0, as seen from the earlier graph on decision boundaries at different \u03bb\u2019s (from 0 to 100). Therefore, I choose to stay with \u03bb = 0.5 for my final model.", "Choosing \u03bb is not the most the most influential decision in tweaking my model. Rather, choosing the threshold for my decision boundary is:", "If that\u2019s the case, should the threshold be set as low as possible, even to 0%? No, because:", "When the threshold is lowered, the decision boundary shifts up.", "To recap, we have trained a logistic regression classifier via the gradient ascent algorithm, using my daily calories surplus and step counts as features, and my weight gain status as labels. The parameters for our model are:", "After training the classifier on the entire data set, the learned regression coefficients (\u03b8\u2019s) come out to:", "A standard deviation increase in step counts (5980 steps) is 36% more effective in reducing my weight gain odds than a standard deviation decrease in calories consumed (420 kcal).", "Of course, my willingness to run 4.4 km instead of not eating that 420-kcal bowl of pho is another matter entirely!", "Although odds ratio offers some insights in how I should strategize my weight loss, a more actionable plan could be drawn from the decision boundary of my logistic regression classifier.", "Recall our trusty sigmoid function from earlier:", "It can easily be seen that for the probability of weight loss to be 50% (where the classification threshold is), the linear combination of \u03b8\u2019s and x\u2019s must be zero* (I\u2019ve also replaced x_intercept by 1):", "* For other probability thresholds, the linear combination of \u03b8\u2019s and x\u2019s can be found by taking the logit of the threshold: ln(p/(1-p))", "However, the x\u2019s used in our logistic regression were the normalized values of our original features. Therefore, we can rewrite the above equation as:", "Plugging \u03b8\u2019s and the feature means (\u03bc) and standard deviations (\u03c3) to the above equation gives the linear equation between the original calories surplus and step count features:", "Graphically, this equation represents the decision boundary in the original surplus-step plot (see left panel below). From this decision boundary, there are 3 important numbers that I should keep in mind (see right panel):", "This is how much I should be eating below the Loseit app\u2019s calories budget on my lazy days (in which I average 2480 steps). With the average budget of just above 1700 kcal, this means on those days I should be eating below 1560 kcal on average. This number sounds quite restrictive indeed.", "However, one saving grace is that according to the decision boundary, for any 1 km I run beyond my normal activity, I am allowed to increase this limit by 130 kcal. For example, if I had scheduled a 5K run on a given day, I can afford to eat -140 + 130 * 5 = 510 kcal above the calories budget from the app for that day. Hopefully this will encourage me to keep up my running schedule.", "On the flip side, when I\u2019m tempted to overeat beyond the calories limit (as dictated by the first two rules), any 100 kcal that I eat beyond the limit must be earned with at least 1070 steps. This can be done either by:", "I hope to use the above guidelines to be more successful in my weight loss journey in 2019 than in 2018. Of course I won\u2019t always succeed even with the numbers behind me, but the most important lesson I\u2019ve learned from this project is:", "I should be kind to myself.", "For example, I do lose weight even if I eat more than the budget on my active days, so I should not feel guilty for doing so. Hopefully, with this new model that incorporates both diet and exercise, I can feel less guilt and shame (like I did before) during my ongoing weight loss journey.", "Another big reason that I embarked on this project is to implement a machine project without using pre-existing libraries such as scikit-learn. Below are some lessons I learned from that process:", "I\u2019m really interested on how classes could be used to model data science problems, and I think with more relevant examples I might be able to appreciate more the powers of object-oriented programming (for now I\u2019m still not quite sure when or how I should use them).", "\u2014 Me when first learning Python", "I hope that my project can inspire others to use machine learning and data science to help them understand more about themselves and accomplish personal goals such as weight loss. Please don\u2019t hesitate to contact me on Medium if you have any question or feedback!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb74aa2e170f2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://seismatica.medium.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": ""}, {"url": "https://seismatica.medium.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Khanh Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbb9e5af5001b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&user=Khanh+Nguyen&userId=bb9e5af5001b&source=post_page-bb9e5af5001b----b74aa2e170f2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/seismatica/logweight", "anchor_text": "repo"}, {"url": "https://www.loseit.com/", "anchor_text": "Loseit"}, {"url": "https://en.wikipedia.org/wiki/C25K", "anchor_text": "Couch to 5K"}, {"url": "https://us.amazfit.com/shop/bip", "anchor_text": "Amazfit Bip"}, {"url": "https://forum.xda-developers.com/general/accessories/xiaomi-mi-band-data-extraction-t3019156", "anchor_text": "workaround"}, {"url": "https://en.wikipedia.org/wiki/Linear_classifier", "anchor_text": "linear classifier"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "logistic regression"}, {"url": "http://linear discriminant analysis", "anchor_text": "linear discriminant analysis"}, {"url": "https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM", "anchor_text": "support vector machine"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid function"}, {"url": "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html", "anchor_text": "numpy.matmul"}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "L2-regularized"}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation", "anchor_text": "2-fold cross-validation"}, {"url": "https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Definitions", "anchor_text": "true positive rate"}, {"url": "https://en.wikipedia.org/wiki/Odds_ratio#Role_in_logistic_regression", "anchor_text": "odds ratio"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression#Logistic_function,_odds,_odds_ratio,_and_logit", "anchor_text": "logit"}, {"url": "https://en.wikipedia.org/wiki/B%C3%A1nh_bao", "anchor_text": "banh bao"}, {"url": "https://www.youtube.com/watch?v=CfsAi95ghuU", "anchor_text": "Slap Chef"}, {"url": "https://medium.com/@seismatica/mit-6-00-1x-review-ef4f3561e114", "anchor_text": "learned Python"}, {"url": "https://wiki.python.org/moin/Generators", "anchor_text": "generators"}, {"url": "https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf", "anchor_text": "lecture note"}, {"url": "https://see.stanford.edu/Course/CS229/42", "anchor_text": "video"}, {"url": "https://see.stanford.edu/Course/CS229/49", "anchor_text": "lectures"}, {"url": "https://see.stanford.edu/Course/CS229/", "anchor_text": "CS229"}, {"url": "https://www.coursera.org/learn/machine-learning/", "anchor_text": "Coursera"}, {"url": "https://www.coursera.org/learn/ml-classification/", "anchor_text": "Coursera"}, {"url": "https://towardsdatascience.com/data-science-a-practical-application-7056ec22d004", "anchor_text": "Will Koehrsen"}, {"url": "https://github.com/arielf/weight-loss", "anchor_text": "Ariel Faigon"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b74aa2e170f2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/weight-loss?source=post_page-----b74aa2e170f2---------------weight_loss-----------------", "anchor_text": "Weight Loss"}, {"url": "https://medium.com/tag/quantified-self?source=post_page-----b74aa2e170f2---------------quantified_self-----------------", "anchor_text": "Quantified Self"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----b74aa2e170f2---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b74aa2e170f2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&user=Khanh+Nguyen&userId=bb9e5af5001b&source=-----b74aa2e170f2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&user=Khanh+Nguyen&userId=bb9e5af5001b&source=-----b74aa2e170f2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb74aa2e170f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b74aa2e170f2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b74aa2e170f2--------------------------------", "anchor_text": ""}, {"url": "https://seismatica.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://seismatica.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Khanh Nguyen"}, {"url": "https://seismatica.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "231 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbb9e5af5001b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&user=Khanh+Nguyen&userId=bb9e5af5001b&source=post_page-bb9e5af5001b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F23c332b6074c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2&newsletterV3=bb9e5af5001b&newsletterV3Id=23c332b6074c&user=Khanh+Nguyen&userId=bb9e5af5001b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}