{"url": "https://towardsdatascience.com/polynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87", "time": 1683015037.472044, "path": "towardsdatascience.com/polynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87/", "webpage": {"metadata": {"title": "Polynomial Regression with a Machine Learning Pipeline | by Rukshan Pramoditha | Towards Data Science", "h1": "Polynomial Regression with a Machine Learning Pipeline", "description": "Welcome back! It\u2019s very exciting to apply the knowledge that we already have to build machine learning models with some real data. Polynomial Regression, the topic that we discuss today, is such a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://drive.google.com/file/d/1U-Wj1nYje_1EZ2HA0hZ86tEEyWA_lKGH/view?usp=sharing", "anchor_text": "download here", "paragraph_index": 6}, {"url": "https://drive.google.com/file/d/1C3_GAgJtrBiJWNxkBMI30SpUtnAP538r/view?usp=sharing", "anchor_text": "download here", "paragraph_index": 67}, {"url": "https://www.linkedin.com/in/rukshan-manorathna-700a3916b/", "anchor_text": "Rukshan Pramoditha", "paragraph_index": 69}, {"url": "https://medium.com/data-science-365", "anchor_text": "Data Science 365 Blog", "paragraph_index": 69}], "all_paragraphs": ["Welcome back! It\u2019s very exciting to apply the knowledge that we already have to build machine learning models with some real data. Polynomial Regression, the topic that we discuss today, is such a model which may require some complicated workflow depending on the problem statement and the dataset.", "Today, we discuss how to build a Polynomial Regression Model, and how to preprocess the data before making the model. Actually, we apply a series of steps in a particular order to build the complete model. All the necessary tools are available in Python Scikit-learn Machine Learning library.", "If you\u2019re not familiar with Python, numpy, pandas, machine learning and Scikit-learn, please read my previous articles that are prerequisites for this article.", "Without further delay, let\u2019s go into the problem definition.", "We have a dataset which contains information about Age, Height, and Weight of 71 people. We want to build a regression model which captures the Weight of a person depending on Age and Height and evaluate its performance. Then we use that model to make predictions for new cases.", "Actually, we don\u2019t know the nature or complexity of our data until we plot them. Sometimes, we can easily fit a straight line and describe the model. But most of the time, this is not the case for real-world data. The data may be complicated and you need to consider a different approach to tackle the problem.", "We have a dataset called person_data.csv (download here) which contains information about Age, Height, and Weight of 71 people. Let\u2019s load it using the pandas read_csv() function and store it in the df variable.", "Let\u2019s check whether the data has missing values.", "Great! All the values of the 3 columns have 71 non-null values meaning that there are no missing values.", "Let\u2019s define our feature matrix and the target vector. According to the problem definition, the feature matrix \u2014 X contains the values of Age and Height. The target vector \u2014 y contains the values Weight. Building a Regression Model is a supervised learning task so that we map the input X to the output y=f(X).", "The dimension of our data is 2 because X is 2-dimensional. So, how can we plot 2-dimensional data of X with y? Obviously, we need to create a 3D plot. But there is another way. We can combine Age and Height into one variable called Z and then plot Z with y in a 2D plot. Combining Age and Height into one variable by reducing the number of features in the X is called the Dimensionality Reduction and the technique that we use to perform dimensionality reduction is the Principal Component Analysis (PCA) which uses the correlation of these two variables.", "We have two advantages of performing Dimensionality Reduction before making the model:", "Let\u2019s check whether Age and Height are correlated or not.", "It seems that the two features are highly correlated. The condition when there is a significant dependency or association between the independent variables (the variables in the feature matrix X) is called multicollinearity. Having multicollinearity will lead to misleading predictions.", "Before making the model, our next task is to remove these correlated variables. As I said earlier, we use Principal Component Analysis (PCA) to do this. But before running PCA, it is essential to perform feature scaling if there is a significant difference in the scale between the features of the dataset. This is because PCA is very sensitive to the relative ranges of the original features. So, We apply z-score standardization to get all features into the same scale by using Scikit-learn StandardScaler() class which is in the preprocessing submodule in Scikit-learn.", "Note: We apply feature scaling only for the feature matrix X. We don\u2019t need to apply it for our target vector y.", "So, the workflow for building our model is as follows. It contains a series of steps that should be applied in the given order. Building a machine learning model is not a one time task and you may come back to an earlier step and make some modifications and then you go again through the next steps.", "We apply z-score standardization to get all features into the same scale by using Scikit-learn StandardScaler(). All the features are scaled according to the following formula.", "The scaled values of X is stored in the X_scaled variable which is a 2-dimensional numpy array.", "Let\u2019s check the mean and standard deviation of the scaled X values.", "Now, you can see that mean is zero and the standard deviation is 1 for each variable in the X_scaled matrix.", "Now, our data is ready to run PCA.", "Now, we are ready to apply PCA for our dataset. We need to reduce two-dimensional data in the X_scaled matrix into one-dimensional data.", "In Scikit-learn, PCA is applied using the PCA() class. The most important hyperparameter in that class is n_components. Since we are interested in getting one-dimensional data, the value of n_components is 1.", "We have applied PCA for X_scaled values and the output values are stored in X_pca variable which is now a one-dimensional array.", "We have reduced the dimensionality of our data, and also with some information loss compared to our original dataset. In PCA, the algorithm finds a low-dimensional representation of the data while retaining as much of the variation as possible.", "Let\u2019s check how much of variation retained by the algorithm while running it.", "Wow! Awesome value! You can see that the first principal component keep about 97.36% of the variability in the dataset while reducing 1 (2\u20131) feature in the dataset. So the values of X_pca more accurately represent the values of the feature matrix X.", "Let\u2019s plot our data in a scatterplot.", "Now we have one-dimensional data (X_pca) which represent X. Now we can plot X_pca with our y (Weight) values in a 2D plot.", "Clearly, a straight line will never fit this data properly. Let\u2019s see what will happen if we would try to fit a straight line to our data.", "Where \u03b81 and \ud835\udefc are model parameters which learn during the training process.", "The fit is so bad that the straight-line approach will never give us the best model for our data.", "Only about 61% of the variability observed in the Weight is explained or captured by our model. RMSE value is 10.99. It means that on average, the predictions of the model are 10.99 units away from the actual values.", "Let\u2019s try out a different approach called Polynomial Regression to get the best fit for our data.", "You can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.", "So, polynomial regression that uses polynomials is still linear in the parameters. This is because you build the equation by only adding the terms together. So, the performance metrics like R-squared (R\u00b2-coefficient of determination) are still valid for polynomial regression. Do not get confused polynomial regression with non-linear regression where R\u00b2 is not valid!", "In our model, the only variable is X_pca. After adding powers for that variable, the model becomes:", "Where \u03b81, \u03b82, \u03b83 and \ud835\udefc are model parameters which learn during the training process.", "Let\u2019s add polynomial features to our data using Scikit-learn PolynomialFeatures() class.", "The most important hyperparameter in the PolynomialFeatures() class is degree. We set degree=4 so that it creates 3 additional features called X_pca\u00b2, X_pca\u00b3, X_pca\u2074 when the input (X_pca) is one-dimensional. The X_poly variable holds all the values of the features.", "Now, we have transformed our data into polynomial features. So, we can use the LinearRegression() class again to build the model.", "Wow! It seems that the polynomial approach gives us a better model.", "About 90% of the variability observed in the Weight is explained or captured by our model. RMSE value is 5.526. It means that on average, the predictions of the model are 5.526 units away from the actual values.", "So, we have significantly increased the performance of the model with Polynomial Regression.", "Let\u2019s check the distribution of the residuals (errors).", "By looking at the histogram, we can verify that the residuals are approximately normally distributed with mean 0.", "Model parameters are the parameters which learn during the training process. We do not manually set values for the parameters and they learn from the data we provide. For example, \u03b81, \u03b82, \u03b83 and \ud835\udefc are parameters in our polynomial regression model.", "In contrast, the model hyperparameters are the parameters that do not learn from data. So, we have to set values for them manually. We always set values for the model hyperparameters at the creation of a particular model and before we start the training process. For example, we have set n_components=1 manually at the creation of the pca object from the PCA() class. We have also set degree=4 manually at the creation of poly_features object from the PolynomialFeatures() class.", "In most cases, setting up the correct values for the model hyperparameters is one of the most challenging tasks. There is no magic formula behind this. You may try different values and get visual representations to verify your choice. Sometimes, you may try different values and evaluate the model performance and choose the best one. Sometimes, the domain knowledge of the problem will help you to deduce the correct values for the hyperparameters.", "Let\u2019s try out some different values (2\u20139) for the hyperparameter degree (Initially, I have set this as degree=4) and get some visual representations and model evaluation metrics.", "High degrees can cause overfitting. The problem of overfitting is a condition where a statistical model begins to describe the random error in the data rather than the relationships between variables. In overfitting, the model fits training data very well but fails to generalize for new input data which are not in our dataset.", "Lower degrees can cause underfitting. In underfitting, the model does not fit training data very well and also the new data.", "When we set the value for the degree hyperparameter, we should always try to avoid both overfitting and underfitting conditions", "By looking at the visual representations and the values of the performance metrics, we can decide that degree=4 or degree=5 is the ideal value for the degree hyperparameter.", "Imagine that we have 5 new observations and we want to predict Weights using our model. The new input data is stored on X_new array which is two dimensional. The first column is the Age and the second column is the Height.", "When we build our model, we have scaled and transformed our original data 3 times. So, when we use our model to make predictions on new data, it is necessary to scale and transform new data using the same methods. So, we have to call fit_transform() method 3 times and then call the predict() method 1 time. So, this is annoying for us. To overcome this problem, we can build a machine learning pipeline for our polynomial regression model.", "Scikit-learn refers to machine learning algorithms as estimators. There are three different types of estimators: classifiers, regressors, and transformers. The classifiers and regressors are called predictors.", "As our analysis and workflow become more complicated, you may need to apply multiple transformations to your data before it is ready for a supervised machine learning model. Pipelines sequentially apply a list of transformers and a final predictor (classifier or regressor). Intermediate steps of the pipeline must be \u2018transformers\u2019, that is, they must implement fit() and transform() methods. The final predictor only needs to implement the fit() method.", "So, we can build a pipeline for our model using Scikit-learn Pipeline() class. It sequentially applies the above list of transformers and the final predictor. Here is the code.", "By using pipelines, we can easily build complex models with less code!", "Let\u2019s call the fit() method of our pipeline.", "When calling poly_reg_model.fit(X, y), the following process occurs:", "Let\u2019s call the predict() method of our pipeline.", "When calling poly_reg_model.predict(X_new), the following process occurs:", "So, poly_reg_model.predict(X_new) returns predictions for our new data.", "Let\u2019s save our model with the Python dill package.", "After running this code, the poly_reg_model.dill (download here) file will appear in your current working directory.", "That\u2019s it. You can share the poly_reg_model.dill file with others and they can use the model without building it again. It is just 743 bytes in memory!", "This tutorial was designed and created by Rukshan Pramoditha, the Author of Data Science 365 Blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7e27d2dedc87&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rukshanpramoditha.medium.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff90a3bb1d400&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=post_page-f90a3bb1d400----7e27d2dedc87---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sortino?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Joshua Sortino"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/data-science-365/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0?source=friends_link&sk=1405bca22ed8b76f0100b5093b56b36e", "anchor_text": "Principal Component Analysis (PCA) with Scikit-learn"}, {"url": "https://medium.com/data-science-365/linear-regression-with-gradient-descent-895bb7d18d52?source=friends_link&sk=085716d97c63a0419a8323520a889c4a", "anchor_text": "Linear Regression with Gradient Descent"}, {"url": "https://medium.com/data-science-365/numpy-for-data-science-part-1-21e2c5ddbbd3?source=friends_link&sk=bbaaf3a0894ef18e15ebfc5c53fb5c3b", "anchor_text": "NumPy for Data Science: Part 1"}, {"url": "https://medium.com/data-science-365/pandas-for-data-science-part-1-89bc231b3478?source=friends_link&sk=9a2fd6ff88dda68a561dce240a8cd9d5", "anchor_text": "pandas for Data Science: Part 1"}, {"url": "https://drive.google.com/file/d/1U-Wj1nYje_1EZ2HA0hZ86tEEyWA_lKGH/view?usp=sharing", "anchor_text": "download here"}, {"url": "https://drive.google.com/file/d/1C3_GAgJtrBiJWNxkBMI30SpUtnAP538r/view?usp=sharing", "anchor_text": "download here"}, {"url": "https://www.linkedin.com/in/rukshan-manorathna-700a3916b/", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365", "anchor_text": "Data Science 365 Blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7e27d2dedc87---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7e27d2dedc87---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----7e27d2dedc87---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/polynomial-regression?source=post_page-----7e27d2dedc87---------------polynomial_regression-----------------", "anchor_text": "Polynomial Regression"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----7e27d2dedc87---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7e27d2dedc87---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7e27d2dedc87---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7e27d2dedc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7e27d2dedc87---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7e27d2dedc87--------------------------------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://rukshanpramoditha.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.1K Followers"}, {"url": "https://rukshanpramoditha.medium.com/membership", "anchor_text": "https://rukshanpramoditha.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff90a3bb1d400&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=post_page-f90a3bb1d400--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe85dd9473cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87&newsletterV3=f90a3bb1d400&newsletterV3Id=e85dd9473cd&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}