{"url": "https://towardsdatascience.com/making-computers-understand-the-sentiment-of-tweets-1271ab270bc7", "time": 1682994610.3883488, "path": "towardsdatascience.com/making-computers-understand-the-sentiment-of-tweets-1271ab270bc7/", "webpage": {"metadata": {"title": "Teaching computers to understand the sentiment of tweets | by Kristoffer Stensbo-Smidt | Towards Data Science", "h1": "Teaching computers to understand the sentiment of tweets", "description": "Understanding whether a tweet is meant as positive or negative is something humans rarely have problems with. For computers, however, it is an entirely different story \u2014 complicated sentence\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Natural-language_understanding", "anchor_text": "natural language understanding", "paragraph_index": 1}, {"url": "http://sobigdata.eu/index", "anchor_text": "SoBigData", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embedding models", "paragraph_index": 4}, {"url": "http://alt.qcri.org/semeval2017/task4/", "anchor_text": "SemEval-2017 Task 4", "paragraph_index": 11}, {"url": "https://tfhub.dev/", "anchor_text": "TensorFlow Hub", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "confusion matrix", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)", "anchor_text": "cross-validation", "paragraph_index": 39}, {"url": "http://sobigdata.eu/content/open-call-sobigdata-funded-transnational-access", "anchor_text": "SoBigData 2017 Short Term Scientific Mission (STSM)", "paragraph_index": 50}, {"url": "https://twitter.com/dianamaynard", "anchor_text": "Diana Maynard", "paragraph_index": 50}, {"url": "https://twitter.com/IAugenstein", "anchor_text": "Isabelle Augenstein", "paragraph_index": 50}], "all_paragraphs": ["Understanding whether a tweet is meant as positive or negative is something humans rarely have problems with. For computers, however, it is an entirely different story \u2014 complicated sentence structure, sarcasm, figurative language etc. make it difficult for computers to judge the meaning and sentiment of a sentence. However, automatically assessing the sentiment of a tweet would allow for large-scale opinion-mining of the population on all sorts of issues and could help understanding why certain groups of the population hold certain opinions.", "On a more fundamental level, understanding the sentiment of text is a key part of natural language understanding and thus an essential task to solve if we want computers to be able to communicate efficiently with us.", "In this blog post, I will present the results of a small research project carried out as part of the SoBigData project at the University of Sheffield. We tested different approaches to processing text and analysed how much of the sentiment they are able to pick up. Read on for a full tour of the project and the results!", "The aim of the project was to test how well computers can understand the sentiment of text using machine learning. To do this, we fed the computer with lots of tweets that had each been labelled as having either positive, neutral, or negative sentiment by humans. Each tweet also had an associated topic, which is important to make use of since a sentence can have very different sentiment depending on the topic discussed. For instance, the word \u201chigh\u201d is positive if we are talking about quality, but negative if we are talking about prices. \u201cGreen\u201d is positive when discussing environmental issues, but may be neutral when discussing art. The task for the computer is now to predict the sentiment given a tweet and an associated topic.", "If you do not have experience with machine learning, this might seem like an odd question. But machine learning is based on statistics, so anything a machine learning system is to work with has to be represented as numbers. Turning text into numbers happens with so-called embedding models, and it is a major research field in itself to develop these. An embedding model turns a word or a sentence into a vector, which is continuously adjusted during training such that words and sentences with similar meanings end up with similar vectors. Ideally then, the vector should capture the meaning, context, sentiment etc. of a sentence, but this is not an easy task at all, which is why many different embedding models have been developed. Generally, newer models perform better, but they may also be tuned to specific tasks.", "Full-blown machine learning systems capable of achieving state-of-the-art on, say, sentiment analysis are beasts. They consist of multiple components, of which the text embeddings are only one, and it is generally very difficult to assess which parts of the systems are the performance bottlenecks. Since any text needs to be represented as a vector for a machine learning system to be able to work with it, any analysis, including predicting the sentiment of a tweet, relies heavily on the chosen embedding model. But that is not to say that other parts of the system might be equally important.", "To make the role and contributions of text embeddings more transparent, we set out to test their performance for predicting sentiments with a system designed to be minimally obscuring.", "Our approach for predicting the sentiment is fairly simple and inspired by collaborative filtering. Each tweet has an associated topic and it is essential that the sentiment is evaluated with respect to the topic (since a statement can easily be positive towards one aspect and negative towards another). As both the tweet and the corresponding topic are represented by vectors with the same dimensionality, we can take the inner product of the two, giving us a single number representing the sentiment. There is no reason that this should work with \u201craw\u201d embeddings, so before taking the inner product, we learn and apply a transformation (further details later) to the topic vector space. In this way, we can get the sentiment even when the topic has not been seen before.", "We want to be able to predict three different kinds of sentiment (positive, neutral, negative), so we actually learn three different transformations of the topic space: one to predict positive sentiment, one to predict neutral sentiment, and one to predict negative sentiment. When taking the inner product of the tweet with each of the three transformed topic vectors, we get three numbers which can be understood as the model\u2019s bet on each of the sentiments \u2014 the higher the number, the more the model believes that this is the sentiment of the tweet.", "We want to test how much information different word embeddings carry for the sentiment of a tweet. To predict the sentiment, we train a model that learns three transformations of the topic vector such that the inner product of the tweet and each of the three topic vectors will be the model\u2019s vote for each of the three sentiments.", "We have a few different choices to make. Firstly, we have to choose which embedding models to test. Secondly, we need to decide on how to transform the topic vectors. Thirdly, we need a dataset of tweets that have been labelled with sentiment by humans, such that we have something to train and test the model on.", "We used the English dataset provided for SemEval-2017 Task 4. This consists of about 26k tweets with various topics, all manually labelled with sentiment. We keep the split defined by the task organisers, which is about 20k tweets for training and 6k tweets for testing on.", "We chose to test the following four embedding models:", "All four embedding models are conveniently available from TensorFlow Hub.", "Choosing a model for transforming the topic vector space is tricky. On one hand, we would like to keep the original vector space as unchanged as possible. On the other hand, we would like the transformation to be flexible enough that the information in the word embeddings can actually be used to predict the sentiment. We therefore decided to test two different transformation models:", "The final model will learn three transformations of each of the above types, corresponding to the three sentiments we want to predict.", "It is always challenging to work with real data. In particular, if a single sentiment or topic is hugely overrepresented, the model might focus on this entirely during training, which will make predictions of other sentiments or using other topics way off. Instead, we want to make sure that the model gives equal weight to all topics and sentiments, regardless of how frequent they are. The effect of making these corrections is rather dramatical and a good lesson to keep in mind, so let\u2019s spend a few minutes on this.", "Plotting the number of tweets per sentiment the datasets shows large class imbalances.", "Especially positive sentiment is heavily overrepresented in the training data\u2014 in fact, almost 73% of the training tweets have positive sentiment. This means that the model will benefit much more from learning to predict positive sentiment than any other. Neutral sentiment, on the other hand, is associated with less than 10% of the tweets, and the model may simply learn to ignore this sentiment if it helps with predicting positive sentiment.", "The distributions in the test set are strikingly different. Negative sentiment is more abundant than positive, and no tweets have neutral sentiment. This makes it even more important to make the model treat all sentiments equally.", "Indeed, a test with the affine transformation model on NNLM shows that the trained model clearly favours positive sentiment due to its prevalence in the training data. In this test, the topics in the training data were split into a training and an evaluation set of 90% and 10% of the topics, respectively.", "The figure shows a confusion matrix for the sentiment predictions with each column corresponding to a predicted sentiment. Each row shows the actual sentiment, and for each of these rows, the number and colour of each matrix element shows the percentage of tweets with this actual sentiment that were predicted to have the sentiment shown in the columns.", "Ideally, the diagonal should be close to 100%, meaning that the predicted sentiment was correct for almost all tweets, but even for the training set there are large off-diagonal elements. It means that even when the model knows the correct sentiment, it prefers to default to predict positive sentiment most of the time. 43% of tweets with negative sentiment and more than 55% of tweets with neutral sentiment are predicted to have positive sentiment. This is even worse for the evaluation set with 39% and 78%, respectively.", "However, also the number of tweets per topic varies a lot in both the training and the test sets.", "Especially for the training set we see a stark difference in the number of tweets per topic \u2014 some topics have more than 100 tweets, while roughly half the topics have about 20 tweets or less.", "Going back to the test with the affine model and looking at the average accuracy of sentiment prediction for tweets with a given topic shows that topics with more tweets generally have higher accuracy.", "This tendency makes sense: the model benefits more from learning a transformation that works well for topics with more tweets. But this is actually not what we want, because it means that the model may not generalise well. We want the model to perform well on even unseen topics, and overfitting to a few topics will probably not help in this regard.", "One way to deal with class imbalances like these is to weigh the penalty the model gets for a wrong prediction by the inverse of the frequency of the class. This means that the model receives a larger error for less frequent data, thus paying more attention to these. Let\u2019s see how this affects the training of the model.", "Retraining the model and penalising mistakes with the inverse of the sentiment frequency only, we already obtain a much better model.", "For the training set, the diagonal is close to 100% for all sentiments. The predictions on the evaluation set also improved, though there is plenty of room for improvement.", "We also see an improvement on the accuracy per topic for the training set, even though this was not explicitly encouraged.", "Interestingly, the performance on the evaluation set appears to have decreased. One explanation could be that most of the tweets in the evaluation set have positive sentiment, for which the model has now sacrificed some accuracy to perform better for the negative and neutral sentiments.", "Next, let\u2019s see what happens when penalising mistakes with the inverse of topic frequencies only. This, too, results in much better sentiment predictions on the training set, which might be because weighing topics equally regardless of the number of tweets associated with them exposes the model to a larger variety of sentiments.", "But the real effect is seen when looking at the accuracy per topic. For the training set, the accuracy is now pretty much independent of the number of tweets in a topic, with most topics being close to 1.", "The final model will weigh the penalty for a wrong prediction based on frequencies of both the sentiment and the topic. This is done by simply multiplying the inverses of the topic frequency and the sentiment frequency and use the resulting quantity as the weight. This should encourage the model to treat all sentiments and all topics equally during training.", "The resulting model does indeed seem to be a good trade-off between accounting for sentiment and topic imbalance. The sentiments are predicted fairly accurately, and the performance on the evaluation set hasn\u2019t suffered.", "The average accuracy per topic is again independent on the number of tweets associated with the topic.", "While correcting for class imbalances clearly helped on the training set, the performance on the evaluation set still did not change noticeably. The model does not seem to be able to generalise well to new topics, which could mean that the affine transformation is too restrictive or that the training set is not very representative of the evaluation set. We will return to that when taking a look at the final experiments.", "Now, having accounted for class imbalances in the dataset as well as having decided on embedding and transformation models, we are ready to test the models and see how much sentiment information the word embeddings have been able to pick up.", "The set-up follows the standard machine learning approach: we trained the model using a 10 fold cross-validation (CV) and evaluated the best model from each fold on the test set. This gives us a measure of how much we can expect the performance of the model to vary when trained on (slightly) different datasets.", "It is always a good idea to include some baseline experiments. These should be the simplest approaches you can imagine and if your advanced model cannot beat these, you know something is wrong. We chose two simple baselines: 1) use the most frequent sentiment from the training set (which will be \u201cpositive\u201d) as the prediction for any tweet, and 2) use random sentiments from the training set as predictions.", "The results from training all eight models and the two baselines and evaluating on the unseen test set are illustrated in the below figure. The vertical lines through the data points indicate one standard deviation across the 10 CV folds.", "There are a number of interesting observations to be done here. Firstly, there is a large improvement for any embedding model over baselines. The word embeddings therefore have, as expected, captured information that can be used to derive the sentiment of a tweet. Secondly, turning to the NNLM embeddings, there doesn\u2019t appear to be any improvement when using the nonlinear model compared to the affine model. This is interesting because it suggests that embedding space is simple enough that the affine model is able to use all the sentiment information available in the embeddings. This is in contrast to the newer embeddings, ELMo and USE, where we do observe an improvement when using the nonlinear model, suggesting that the embedding spaces learnt by these models are more complex. For NNLM, the normalised vectors do have a tendency to perform better than the unnormalised ones, but the effect is nowhere near significant in our experiments. Lastly, while ELMo and USE both contain much more information than the NNLM embeddings, they perform quite similarly in these experiments. USE seems to generally contain slightly more information than ELMo, but not significantly more. This is, however, still interesting since the USE embedding space is of much lower dimensionality than the ELMo space and, consequently, the models are much faster to train.", "No, absolutely not. There are plenty of interesting questions to answer regarding the information content of word embeddings.", "For instance, we worked with sentiment on a three-point scale (negative, neutral, positive). Extending to a more fine-grained sentiment on, say, a five-point scale would require more from the embeddings. Do the embeddings contain this much information?", "It is also reasonable to ask whether the huge embedding spaces are necessary. ELMo embeddings are 1024 dimensions, but the information is probably embedded in a much lower dimensional space. How does a dimensionality reduction of the embedding space affect the prediction of sentiments?", "When testing the two different transformations of the topic space, we found that only the newer embeddings required a nonlinear transformation. It would be interesting to extend the types of transformations, including creating some much more complex neural networks, and also test which embeddings benefit from which transformations. This might give us insight into the complexity of the different embedding spaces.", "In this project, we wanted to test how much information different word embeddings carry about the sentiment of tweets. We did this by constructing two models for predicting the sentiment that would be as nonintrusive as possible, enabling us to see how much sentiment information the raw word embeddings contain.", "The results show that both old and new word embeddings certainly do carry information about the sentiment and that the newer embeddings, unsurprisingly, contain more than the older. The results also show that a nonlinear transformation of the topic vectors perform significantly better than an affine transformation for the newer embeddings, suggesting that these spaces are more complex than for the older embeddings.", "In conclusion, word embeddings generally do contain a lot of information about the sentiment of tweets, with newer embeddings containing significantly more information. While not overly surprising, it emphasizes the importance of advanced embedding models for predicting the sentiment of tweets.", "The project was done as part of a SoBigData 2017 Short Term Scientific Mission (STSM) at the Department of Computer Science, University of Sheffield, in collaboration with Dr Diana Maynard. A big shout-out to Dr Isabelle Augenstein for numerous discussions and advice during the entirety of this project.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Postdoc in machine learning at the University of Cambridge."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1271ab270bc7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kstensbo?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kstensbo?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Kristoffer Stensbo-Smidt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe28c21a0e431&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&user=Kristoffer+Stensbo-Smidt&userId=e28c21a0e431&source=post_page-e28c21a0e431----1271ab270bc7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Natural-language_understanding", "anchor_text": "natural language understanding"}, {"url": "http://sobigdata.eu/index", "anchor_text": "SoBigData"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embedding models"}, {"url": "http://alt.qcri.org/semeval2017/task4/", "anchor_text": "SemEval-2017 Task 4"}, {"url": "http://www.jmlr.org/papers/v3/bengio03a.html", "anchor_text": "Neural-Net Language Models"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "Embeddings from Language Models"}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "Universal Sentence Encoder"}, {"url": "https://tfhub.dev/", "anchor_text": "TensorFlow Hub"}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "confusion matrix"}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "confusion matrix"}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)", "anchor_text": "cross-validation"}, {"url": "http://sobigdata.eu/content/open-call-sobigdata-funded-transnational-access", "anchor_text": "SoBigData 2017 Short Term Scientific Mission (STSM)"}, {"url": "https://twitter.com/dianamaynard", "anchor_text": "Diana Maynard"}, {"url": "https://twitter.com/IAugenstein", "anchor_text": "Isabelle Augenstein"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1271ab270bc7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1271ab270bc7---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/research-reports?source=post_page-----1271ab270bc7---------------research_reports-----------------", "anchor_text": "Research Reports"}, {"url": "https://medium.com/tag/sentiment-analysis?source=post_page-----1271ab270bc7---------------sentiment_analysis-----------------", "anchor_text": "Sentiment Analysis"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1271ab270bc7---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&user=Kristoffer+Stensbo-Smidt&userId=e28c21a0e431&source=-----1271ab270bc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&user=Kristoffer+Stensbo-Smidt&userId=e28c21a0e431&source=-----1271ab270bc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1271ab270bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1271ab270bc7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1271ab270bc7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1271ab270bc7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1271ab270bc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kstensbo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kstensbo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kristoffer Stensbo-Smidt"}, {"url": "https://medium.com/@kstensbo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe28c21a0e431&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&user=Kristoffer+Stensbo-Smidt&userId=e28c21a0e431&source=post_page-e28c21a0e431--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdc9db5e18628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-computers-understand-the-sentiment-of-tweets-1271ab270bc7&newsletterV3=e28c21a0e431&newsletterV3Id=dc9db5e18628&user=Kristoffer+Stensbo-Smidt&userId=e28c21a0e431&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}