{"url": "https://towardsdatascience.com/who-learns-faster-you-or-my-ai-681e442416b0", "time": 1683008674.084678, "path": "towardsdatascience.com/who-learns-faster-you-or-my-ai-681e442416b0/", "webpage": {"metadata": {"title": "Who Learns Faster \u2014 You or my AI? | by Jakob Schmitt | Towards Data Science", "h1": "Who Learns Faster \u2014 You or my AI?", "description": "Can you beat my AI at playing a simple game of \u201cNim\u201d? Come on, I challenge you to click here and find out. The best part: You can decide how much practice the AI gets before playing against you. So\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.jokuspokus.com/showcase/nimAI/", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Nim", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Behaviorism", "anchor_text": "behavioral psychology", "paragraph_index": 19}, {"url": "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html", "anchor_text": "Deep Q-learning", "paragraph_index": 31}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "neural networks", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56", "anchor_text": "this article", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a", "anchor_text": "a fundamental dilemma", "paragraph_index": 33}, {"url": "http://B.Sc", "anchor_text": "B.Sc", "paragraph_index": 35}, {"url": "http://M.Sc", "anchor_text": "M.Sc", "paragraph_index": 35}], "all_paragraphs": ["Can you beat my AI at playing a simple game of \u201cNim\u201d? Come on, I challenge you to click here and find out. The best part: You can decide how much practice the AI gets before playing against you.", "Afterwards, return to this article to learn how the AI became so smart\u2026", "So, how did you do? I\u2019m sure that once you got the hang of it, the \u201ceasy peasy\u201d mode was not a real challenge for you. However, to beat the more advanced levels, one has to dress warmly (as we say in German).", "But what\u2019s the difference between those levels? Did I program the more advanced agents to be smarter? Did I maybe tell them the mathematical formula to play a perfect game of Nim? (Btw, such a strategy does exist. You can read about it here.)", "The answer is: No! What I did instead is empower the agent to find out by herself how a game of Nim can be won. And that\u2019s the magic of Q-learning: You formalize a problem, throw it at your agent, sit back and relax while the agent does most of the work. Let me explain to you on a conceptual level \u2014 and without any mathematical formulae \u2014 what is happening behind the scenes. (Note: I talk about Python implementations here and there, but you don\u2019t need to understand those in order to get the idea of Q-learning.)", "In order to become a real Nim master, the agent needs to be told the basic setting and rules of the game:", "This is the initial board from my nimAI web app. In theory, you could start with any other amount of rows and coins, and our learning algorithm should be able to deal with all of those alternatives. In Python, the board above could be formalized as a list:", "Consider this board state towards the end of a game:", "Here, one has three different options: Take one coin from the first row OR take one coin from the second row OR take two coins from the second row. All we need to do is write a function that takes a board state (e.g., [1, 2, 0, 0]) and returns some representation of the possible actions. In Python, this representation could be a set of tuples, where each tuple consists of the row and the number of coins to be removed:", "If I take away one coin from the second row (see above), how will the board look like?", "Duh, that\u2019s obvious, you might say. Yes, but we need to formalize such a state transition function. Remember that the agent isn\u2019t such a smarty-pants like you (yet)!", "We need to provide a function that tells the agent who is the winner (\u201cAI\u201d, \u201cHuman\u201d, \u201cNoone yet\u201d) for a given state of the game. More precisely, we need to consider whether the board is already empty ([0, 0, 0, 0]) and, if so, who made the losing move.", "Great. Now the AI knows how the game begins (initial state), which actions it can choose in each possible board state, how such actions change the board (state transition function), and whether there is already a winner.", "Now, the interesting part begins \u2014 the actual learning!", "The idea of Q-learning is actually quite simple. We\u2019re too lazy to tell the agent explicitly how to master the game (and maybe we don\u2019t even bother to find out). Instead, we let the AI play the game against itself a number of times. Our hope is that it will encounter many different situations and discover, by trial and error, which is the optimal action in a particular state of the board.", "After the agent is trained, a human can challenge it to a game of Nim.", "\u201cTake this\u201d, the human player will shout.", "\u201cMuahaha\u201d, the AI will answer. \u201cI\u2019ve seen this situation many times before!\u201d Using its experience, it will pick a boss move to destroy the poor human. Then, it will go on and enslave humanity and\u2026 Oh, sorry, I didn\u2019t want to scare you.", "Now that you\u2019ve got an intuition about Q-learning, let\u2019s go into some more detail.", "Q-learning is a technique from the field of Reinforcement Learning (which is, in turn, a subfield of Machine Learning). Reinforcement learning is inspired by behavioral psychology \u2014 the idea that humans and many other animals learn through rewards and punishment. (That\u2019s a terrible oversimplification, but well\u2026 Ain\u2019t nobody got time for that!)", "A child who touches a hot stove will receive a natural punishment (pain) and probably be more careful next time. Lesson learned.", "The same child might later clean the kitchen and receive a reward from her mother (chocolate chip cookies). Chances are she will gladly clean the kitchen again next week. Lesson learned.", "Q-learning (and reinforcement learning in general) is based on that very principle. We let the agent try out actions and if an action results in a \u201cgood\u201d state, we reward her. If the action results in a \u201cbad\u201d state, we punish her. Please note that we obviously don\u2019t hit the agent with a whip or give her candies. Instead, we represent rewards and punishment with numbers (the higher, the better). Computer programs love numbers!", "In the case of Nim, we punish a losing move (i.e., removing the last coin) with a \u201c-1\u201d (I know, it\u2019s pretty brutal), and reward moves that immediately precede an opponent's losing move with a \u201c1\u201d.", "What about all the other moves, which don\u2019t terminate the game? Well, we don\u2019t punish or reward them directly. Instead, we use the core idea of Q-learning: Expected rewards.", "Piece by piece, the agent will learn that certain actions \u2014 although not immediately winning (or losing) the game \u2014 will put her in a favorable (or critical) situation. This knowledge will slowly \u201cpropagate\u201d from the end phase of the game to the earlier positions. Probably, you also experienced that at the beginning of a new game, it\u2019s really hard to come up with a strategy, while towards the end it becomes easier to see which moves are good or bad. There are simply fewer possibilities to consider. It\u2019s the same for an AI: First, most of her moves are totally random. She doesn\u2019t have any strategy. But then, she learns which end-game moves immediately win the game, next she learns which moves lead to moves that immediately win the game, then she learns which moves lead to moves that lead to moves that immediately win the game, and \u2014 so \u2014 on.", "If you allow the agent to practice the game many, many times, she will learn which reward to eventually expect for any given action in any given board state. However, if you only let her play a few times, she doesn\u2019t have the chance to experience many alternative courses of the game. That\u2019s why some AI moves in the easy-peasy mode (especially at the beginning of the game) seem rather random, whereas the more advanced agents immediately seem to have a plan\u2026", "A plan? A strategy? Well, I guess we should be careful with such notions. The AI doesn\u2019t move according to some brilliant master plan, nor does she reason which move might be the best. She merely remembers what worked best in the past, given a certain board state. And that\u2019s what she does way better than any human ever could.", "I hope this article gave you a good intuition about Q-learning and sparked your interest to dive deeper into the topic!", "I would like to mention some interesting details and point you to more rigorous sources in the form of a Q&A section.", "A: In classical Q-learning, you can imagine a simple table that represents all possible combinations of states and actions \u2014 and contains the respective Q-values. In Python, such a Q-table can be represented by a dictionary with state-action pairs as keys. For example, imagine that the agent has two legal actions a1 and a2 in state s. She can then look up Q_table[(s, a1)] and Q_table[(s, a2)] and simply pick the action with the higher Q-value.", "Note that there are more advanced and efficient versions of Q-learning, such as Deep Q-learning. Here, you can use neural networks to learn a function that estimates the Q-value of any state-action pair, rather than explicitly storing all the Q-values in a table.", "A: At first, all the possible moves have a Q-value of 0 \u2014 unless they immediately terminate the game (see above). The agent simply doesn\u2019t know how to tell apart good and bad moves. Then, during the training, the Q-values are updated based on previous beliefs and new experiences. These two factors need to be weighed, which is modeled by a learning rate. Check out this article for more details.", "A: In many AI problems, you will encounter a fundamental dilemma: Given that you have limited resources (time, computing power, etc.), should you explore as many alternatives as possible or exploit the ones that have already worked well? For successful Q-learning, it is crucial to find a healthy balance between exploration and exploitation. This can be achieved by sometimes exploring actions that do not have the highest available Q-value (instead of always greedily picking the action that seems best at the moment).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineering & AI student | previously B.Sc. in Psychology and M.Sc. in Neuroeconomics"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F681e442416b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----681e442416b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jakob.schmitt?source=post_page-----681e442416b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakob.schmitt?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Jakob Schmitt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7327f5cf59c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&user=Jakob+Schmitt&userId=f7327f5cf59c&source=post_page-f7327f5cf59c----681e442416b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.jokuspokus.com/showcase/nimAI/", "anchor_text": "my AI"}, {"url": "https://nimai.herokuapp.com/", "anchor_text": "nimAI web application"}, {"url": "http://www.jokuspokus.com/showcase/nimAI/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Nim", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Behaviorism", "anchor_text": "behavioral psychology"}, {"url": "https://unsplash.com/@sjcbrn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "SJ Baren"}, {"url": "https://unsplash.com/s/photos/cookie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html", "anchor_text": "Deep Q-learning"}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "neural networks"}, {"url": "https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a", "anchor_text": "a fundamental dilemma"}, {"url": "https://medium.com/tag/ai?source=post_page-----681e442416b0---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----681e442416b0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----681e442416b0---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/agents?source=post_page-----681e442416b0---------------agents-----------------", "anchor_text": "Agents"}, {"url": "https://medium.com/tag/games?source=post_page-----681e442416b0---------------games-----------------", "anchor_text": "Games"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&user=Jakob+Schmitt&userId=f7327f5cf59c&source=-----681e442416b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&user=Jakob+Schmitt&userId=f7327f5cf59c&source=-----681e442416b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----681e442416b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F681e442416b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----681e442416b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----681e442416b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----681e442416b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----681e442416b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----681e442416b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----681e442416b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakob.schmitt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakob.schmitt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jakob Schmitt"}, {"url": "https://medium.com/@jakob.schmitt/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "11 Followers"}, {"url": "http://B.Sc", "anchor_text": "B.Sc"}, {"url": "http://M.Sc", "anchor_text": "M.Sc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7327f5cf59c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&user=Jakob+Schmitt&userId=f7327f5cf59c&source=post_page-f7327f5cf59c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff7327f5cf59c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwho-learns-faster-you-or-my-ai-681e442416b0&user=Jakob+Schmitt&userId=f7327f5cf59c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}