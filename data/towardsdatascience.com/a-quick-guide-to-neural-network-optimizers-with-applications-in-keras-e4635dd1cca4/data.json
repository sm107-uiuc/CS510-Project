{"url": "https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4", "time": 1683004451.234306, "path": "towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4/", "webpage": {"metadata": {"title": "A (Quick) Guide to Neural Network Optimizers with Applications in Keras | by Andre Ye | Towards Data Science", "h1": "A (Quick) Guide to Neural Network Optimizers with Applications in Keras", "description": "With the rapid development of deep learning has come a plethora of optimizers one can choose to compile their neural networks. With so many optimizers, it\u2019s difficult to choose one to use. This\u2026"}, "outgoing_paragraph_urls": [{"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 16}], "all_paragraphs": ["With the rapid development of deep learning has come a plethora of optimizers one can choose to compile their neural networks. With so many optimizers, it\u2019s difficult to choose one to use. This article will briefly explain how various neural network optimizers differ from each other.", "Stochastic Gradient Descent, in contrast to batch gradient descent or vanilla gradient descent, updates the parameters for each training example x and y. SGD performs frequent updates with a high variance, causing the objective function to fluctuate heavily.", "SGD\u2019s fluctuation enables it to jump from a local minima to a potentially better local minima, but complicates convergence to an exact minimum.", "Momentum is a parameter of SGD that can be added to assist SGD in ravines \u2014 areas where the surface curves more steeply in one dimension than in another, common around optima. In these scenarios, SGD oscillates around the slopes of the ravine, making hesitant progress along the bottom of the local optimum.", "Momentum helps accelerate SGD in the correct direction, therefore dampening the redundant oscillations as seen in image 2.", "Nesterov momentum is an improvement over standard momentum \u2014 a ball that blindly follows the slope is unsatisfactory. Ideally, the ball would know where it is going so it can slow down before the hill slopes up again. Nesterov accelerated gradient (NAG) can give momentum a prescience by slowing SGD down before it reaches an upsloping area, helping reduce unnecessary redundancy in convergence.", "Adagrad adapts the learning rate to the parameters, performing smaller updates (low learning rates) for parameters associated with frequently occurring features and large updates (high learning rates) for parameters associated with infrequent features. Therefore, Adagrad is helpful in dealing with sparse data.", "Adagrad eliminates the need to manually tune the learning rate \u2014 most implementations leave the default value at 0.01. However, Adagrad\u2019s algorithm causes the learning rate to shrink with every iteration and eventually become infinitesimally small, at which the algorithm cannot acquire any new knowledge.", "Adadelta is an extension of Adagrad that seeks to solve the model\u2019s convergence of learning rate to 0. RMSprop is another version of Adadelta and seeks to solve the same problems Adadelta tried to.", "Adam is another method that computers adaptive learning rates for each parameter. In addition to storing previous gradients like Adadelta and RMSprop, Adam also implements a version of momentum. Adam behaves like a heavy ball with friction, preferring flat minima in the error surface, and can be viewed as a combination of RMSprop and SGD with momentum.", "Nadam is another optimizer that is a combination of Adam and NAG.", "Note that Adagrad, Adadelta, and RMSprop almost immediately head off in the right direction and converge very quickly, while SGD with momentum and NAG are led off-track, evoking the image of a ball rolling down the hill. However, NAG is quickly able to correct its course by looking ahead.", "Notice that SGD (with and without momentum) and NAG find it difficult to break towards the minima and are stuck in the middle. However, SGD with momentum and NAG eventually escape the saddle point. Adagrad, RMSprop, and Adadelta quickly head down the negative slope.", "If the input data is sparse, the best results will come from an adaptive-learning rate method. Overall Adam may be the best overall choice for deep neural networks.", "If you enjoyed reading, check out my other work on neural networks:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe4635dd1cca4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----e4635dd1cca4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png", "anchor_text": "Source"}, {"url": "https://stats.stackexchange.com/questions/366728/why-doesnt-feature-standardization-make-sgd-with-momentum-redundant", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/a-guide-to-neural-network-layers-with-applications-in-keras-40ccb7ebb57a?source=post_stats_page---------------------------", "anchor_text": "A Guide to Neural Network Layers with Applications in Keras"}, {"url": "https://medium.com/@andre_ye/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4?source=your_stories_page---------------------------", "anchor_text": "A Guide to Neural Network Loss Functions with Applications in Keras"}, {"url": "https://medium.com/tag/optimizer?source=post_page-----e4635dd1cca4---------------optimizer-----------------", "anchor_text": "Optimizer"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e4635dd1cca4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/ai?source=post_page-----e4635dd1cca4---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----e4635dd1cca4---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&user=Andre+Ye&userId=be743a65b006&source=-----e4635dd1cca4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&user=Andre+Ye&userId=be743a65b006&source=-----e4635dd1cca4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe4635dd1cca4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e4635dd1cca4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e4635dd1cca4--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}