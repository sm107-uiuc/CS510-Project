{"url": "https://towardsdatascience.com/deep-learning-optimizers-hard-not-3f5c4f7b4e96", "time": 1683006076.9043882, "path": "towardsdatascience.com/deep-learning-optimizers-hard-not-3f5c4f7b4e96/", "webpage": {"metadata": {"title": "Deep Learning Optimizers \u2014 Hard? Not.[1] | by Hmrishav Bandyopadhyay | Towards Data Science", "h1": "Deep Learning Optimizers \u2014 Hard? Not.[1]", "description": "Did you say optimization? \u2014 Whoa dude that\u2019s some super complex mathematics; right?right? Wrong!"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Loss_function", "anchor_text": "https://en.wikipedia.org/wiki/Loss_function", "paragraph_index": 3}], "all_paragraphs": ["Did you say optimization? \u2014 Whoa, dude, that\u2019s some super complex mathematics; right? right? Wrong! Most people shy away from optimization algorithms as they are mostly achievable by writing a line of code in PyTorch or TensorFlow. However, coming to neural network training, optimization algorithms are most often the factors that pack the most punch in the model trained. They have the complete say in how the weights are updated, what the model learns and what it discards.", "Deep Learning is the one of the most advancing technologies today. It is really not hard to see why so many people are interested in Artificial Intelligence or Neural Networks. The overwhelming amount of course-material available on the internet makes it extremely difficult for the novice to choose the one that teaches things the right way. As a result, the large number of people interested in artificial intelligence seldom know the basics.", "Diving into the topic, Deep Learning involves optimization in many ways and contexts. One of the hardest optimization problems in Deep Learning is neural network training. The optimization associated with neural network training revolves around finding a parameter \u03b8 that reduces the loss function J(\u03b8).", "In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \u201ccost\u201d associated with the event \u2014 https://en.wikipedia.org/wiki/Loss_function", "One of the most common mistake people make is that they can\u2019t differentiate optimization from back-propagation. It is indeed confusing, so let me break it down for you. In back-propagation, we compute the gradients and do not update weights based on those gradients. We just compute the gradients. During Optimization, we update the weights according to the gradients computed and the algorithm we have chosen for optimization. Thus, the optimization algorithm varies from one network to another, but the back-propagation remains the same.", "Optimization in neural networks is different from optimization that has been observed in various machine learning models. In ML models, we have been able to carefully design the objective function in such a way that the loss function has been convex and thus the optimization problem has been fairly easy. However, while training neural networks, things get complex and out of hand due to the following commonly faced problems:", "2. The problem of optimization doesn\u2019t end with saddle points. Cliffs in the objective function derivatives is another optimization problem associated with training. As a result of several weights being multiplied, we have steep regions, called cliffs. When the algorithm is before such a cliff, it might take too large a step, and can jump off the cliff, thereby disrupting the normal flow of the algorithm.", "3. Optimization based on local gradient computations fails if the local surface does not point towards the global minima. This may happen when the cost function derivative contains a certain \u201ccliff\u201d and cannot traverse it. It might traverse in higher dimensional space but the excessive time taken during training would make the network too slow.", "4. The problem of \u2018vanishing\u2019 and \u2018exploding\u2019 gradients is also observed in neural networks. Vanishing and Exploding gradients occur when the neural network involves repeated multiplication of the same matrix(like in RNNs) resulting in a matrix with a high exponent. When eigen-decomposition of the matrix is performed to obtain eigenvalues, any eigenvalue that is not in close proximity to 1 will either explode (if greater than 1) or vanish (if less than 1) creating the problem of vanishing and exploding gradients.", "Thus we can see that there are a lot of problems we might face if we try to optimize and train a neural network. Fortunately, we have some optimization algorithms ready up our sleeve left to us by researchers after months and years of work. I will be discussing the extremely popular optimization algorithm of SGD. (Stochastic Gradient Descent)", "Stochastic Gradient Descent is probably one of the most used optimization algorithms for Deep Learning. To understand stochastic gradient descent, we must understand what is gradient descent. Gradient descent means moving along the slope of the gradient. How do you \u2018move\u2019?", "The gradient descent algorithm first decides which way the slope points. Then it takes a step towards lowering the slope. The intuition behind this is that we are ultimately looking to minimize the cost function and we can get a minima where the slope is zero. But hey \u2014 A maxima also has zero slope! This is why we make use of a Hessian matrix. A hessian matrix stores the second derivative of the cost function and helps us differentiate a maxima from a minima.", "Now, there are primarily 3 types of Gradient Descent \u2014", "Now, we know that weights are updated based on the learning rate we have defined. However, one of the most important factors we must keep in mind while writing an SGD optimization algorithm (you generally don\u2019t) is that the learning rate must be reduced over time.", "The reduction of learning rate is needed because the SGD uses mini-batches. Mini-batches are sampled from the training data and such sampling of data results in the presence of noise. The noise does not go away even when we arrive at a minimum. Thus we need to monitor the learning rate closely for the model to converge.", "Why use SGD and not gradient descent(batch)?", "Neural networks require a large amount of data in order to function properly. The training data required even in a simple classifier is much more as compared to machine learning models. Using gradient descent with the huge set of training examples would be quite computationally expensive and would increase learning time exponentially. One of the major features of SGD and using mini-batch based models is that learning time per update does not increase with the increase in training data. This is extremely useful as we can finally break through the limitation set by gradient descent on the size of training data we can use.", "I hope this article was helpful in breaking down the concept of optimization algorithms and their necessity in the domain of Deep Learning. Let me know of any questions in the comment section :)", "Reference: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (2017)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, SketchX, CVSSP, University of Surrey"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3f5c4f7b4e96&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hmrishavbandyopadhyay?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hmrishavbandyopadhyay?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Hmrishav Bandyopadhyay"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3309993693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&user=Hmrishav+Bandyopadhyay&userId=c3309993693c&source=post_page-c3309993693c----3f5c4f7b4e96---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral", "anchor_text": "Franck V."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Loss_function", "anchor_text": "https://en.wikipedia.org/wiki/Loss_function"}, {"url": "https://commons.wikimedia.org/wiki/File:Minima_and_Saddle_Point.png", "anchor_text": "https://commons.wikimedia.org/wiki/File:Minima_and_Saddle_Point.png"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3f5c4f7b4e96---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3f5c4f7b4e96---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/optimization?source=post_page-----3f5c4f7b4e96---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3f5c4f7b4e96---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization-algorithms?source=post_page-----3f5c4f7b4e96---------------optimization_algorithms-----------------", "anchor_text": "Optimization Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&user=Hmrishav+Bandyopadhyay&userId=c3309993693c&source=-----3f5c4f7b4e96---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&user=Hmrishav+Bandyopadhyay&userId=c3309993693c&source=-----3f5c4f7b4e96---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3f5c4f7b4e96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3f5c4f7b4e96---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3f5c4f7b4e96--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hmrishavbandyopadhyay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hmrishavbandyopadhyay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hmrishav Bandyopadhyay"}, {"url": "https://medium.com/@hmrishavbandyopadhyay/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "71 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3309993693c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&user=Hmrishav+Bandyopadhyay&userId=c3309993693c&source=post_page-c3309993693c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F35cad6264837&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-hard-not-3f5c4f7b4e96&newsletterV3=c3309993693c&newsletterV3Id=35cad6264837&user=Hmrishav+Bandyopadhyay&userId=c3309993693c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}