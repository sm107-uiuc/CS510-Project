{"url": "https://towardsdatascience.com/the-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e", "time": 1683013494.952082, "path": "towardsdatascience.com/the-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e/", "webpage": {"metadata": {"title": "Bayesian Time Series - Linear Regression | Towards Data Science", "h1": "The First Step in Bayesian Time Series\u2014 Linear Regression", "description": "State-Space Models in Bayesian Time Series Analysis with PyMC3. This article is the first step for you to join state-space models with Bayesian statistics."}, "outgoing_paragraph_urls": [{"url": "http://www.ssfpack.com/DKbook.html", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Bayes'_theorem", "anchor_text": "[2]", "paragraph_index": 14}, {"url": "https://www.real-statistics.com/time-series-analysis/stochastic-processes/autocorrelation-function/", "anchor_text": "[3]", "paragraph_index": 46}], "all_paragraphs": ["Today time series forecasting is ubiquitous, and decision-making processes in companies depend heavily on their ability to predict the future. Through a short series of articles I will present you with a possible approach to this kind of problems, combining state-space models with Bayesian statistics.", "In the initial articles, I will take some of the examples from the book An Introduction to State Space Time Series Analysis from Jacques J.F. Commandeur and Siem Jan Koopman [1]. It comprises a well-known introduction to the subject of state-space modeling applied to the time series domain.", "In classical regression analysis, it is assumed a linear relationship between a dependent variable y and a predictor variable x. The standard regression model for n observations of y (denoted by y_i for i= 1, \u2026,n) and x (denoted by x_i for i= 1,\u2026,n) can be written as", "where the \u03f5_i \u223c NID(0, \u03c3_\u03f5\u00b2) states the assumption that the residuals (or errors) \u03f5 are normally and independently distributed with mean equal to zero and variance equal to \u03c3\u00b2_\u03f5.", "This dataset comprises the monthly number of drivers killed or seriously injured (KSI) in the UK for the period January 1969 to December 1984, and you can find it here.", "We will be using the log number of deaths. The log transformation can be used to turn highly skewed distributions into less skewed ones. This can be valuable both to make patterns in the data more easily interpretable and to help meeting the assumptions of inferential statistics. We will see what this means later on.", "In our present case, the independent variable is just time.", "The regression model has two unknown parameters that can be estimated with the least-squares method. It returns the values of \u03b1 and \u03b2 that yield the lowest average quadratic error between the observed y and the predicted y\u0302.", "The goal is to find the values of \u03b1 (hat) and \u03b2 (hat) that minimize the error. For that, we take the partial derivatives for each parameter and make it equal to zero as follows,", "After computing the equations above, we get the following", "We can use them to plot our line of best fit.", "The error variance using the least-squares estimate can be calculated using", "Bayes theorem without context could work as a mousetrap. Despite the relatively simple and widely known equation, there is a lot of intuition behind it. I once read that it could be seen as a lens to perceive the world. I would say that it shows a different perspective. There are useful resources to get that intuition; therefore, I will not focus too much on it. Our scope of work is on its practical aspects, making it work for our advantage. First, let\u2019s briefly define its components.", "We are starting with the basics: the prior is the probability of something happening before we include the probability of the data (the likelihood), and the posterior is the probability after incorporating the data.", "Bayes theorem [2] is written as", "In the equation above, P(B) is the evidence, P(A) is the prior, P(B | A) is the likelihood, and P(A | B) is the posterior. P(A | B) is the probability of A happening if B has happened. We can plug into the equation both probabilities and probability distributions (more important to our present work).", "Now, it is time to define our simple linear regression as a probabilistic model. We will assume that the data follow a Gaussian distribution with a mean of \u03b1 + \u03b2 x and a standard deviation of \u03f5, as follows", "We are essentially choosing our likelihood, i.e., we assigned a distribution function to our observed variable (data). As a next step, we need to define our priors. For every parameter (our unobserved variables), we need to define a prior distribution. Think of it as a prior belief or, in the case that you have previous estimates of the parameter, those previous estimates become the prior. Since we don\u2019t know much about the parameters, let\u2019s define some generic distributions", "As it is our first model, we are going to be vague about our choices. In Bayesian terms, this means that we will be using flat priors. Working with the basics of our understanding of the model, we know that \u03f5 can\u2019t be a negative number, and our slope is relatively small. We will be refining our way of choosing priors later. When applying these ideas, we will only use Gaussian and Half-Gaussian distributions", "Let\u2019s plot these distributions for a better understanding of what we mean by a flat distribution.", "We can see above that we are not informing our Bayesian model about what the values of our parameters could be. It means that the data will have a substantial impact on our posterior distributions. We will learn in the future how to reflect our prior choices in our observable variable without seeing the data. We call this model step the prior predictive check, and it helps in diagnosing poor modeling choices.", "We always like to start by generating our own data and ensuring that the model is well specified. It brings significant value to more complex models, but we will be using this approach as a good practice in all examples.", "We can see above the data that we generated and the fitted line that we are expecting to recover from it, i.e., we want to get our true parameters back from the data.", "PyMC3 lets us translate the model that we defined above in a very clean matter. First, we define the prior distributions of our parameters, followed by the likelihood. And we are ready to sample!", "We will understand these plots in much greater detail later, but I just wanted to show you that we are recovering our true values well. For now, focus on the distributions of the parameters (plots on the left). These are our posterior distributions of the parameters that we are estimating, and the vertical lines represent the true values. We can see that we are pretty close to those true values. Remember that we used flat priors, and we generated a relatively small dataset (200 points).", "Now, it is time to apply our verified model to our data.", "Another way to visualize our model and to ensure that we have correctly done the translation from the mathematical enunciation is to use Kruschke diagrams.", "It is easier for us to grasp our understanding of the model with the diagram above. Inside every node, we find both the variables and the distributions, e.g., check the circle that contains \u03f5 and you can find the Half-Normal distribution defined earlier. The observed variable is represented by the shaded node. The rounded-corner box indicates repetition, i.e., we have 192 data points in our dataset, and we will be computing the likelihood for all of them. The arrows show the dependencies; in our case, our likelihood depends on the parameters \u03b1, \u03b2, and \u03f5.", "Now that we are confident that we have setup correctly our model, it is time to analyze our results. For that, let\u2019s plot the results in a similar fashion as we did earlier with the classical approach.", "The first thing to notice is that the black line is very similar to the one that we got from the classical linear regression. As we already mentioned, we don\u2019t get just point estimates but a distribution \u2014 our posterior distribution. The gray lines are there to represent our uncertainty about the estimation. I added them to make it more interesting and to give you a first glimpse of what we will be analyzing in the next articles.", "For now, let\u2019s compare the point estimates for both models.", "Let\u2019s also check the standard deviation of our residuals.", "We are really close! The reason why they yield similar results is that the point estimate obtained by the least-squares method is, in reality, the same thing as the maximum a posteriori (MAP) (the mode of the posterior) from a Bayesian linear regression using flat priors (as we did here). Later on we will deep dive into all of this.", "One problem with our approach here is that we are violating a fundamental assumption of classical regression analysis. It is assumed that the observations y are independent of each other. It is not the case with our example because they are interrelated through time. To show that this is the case, let\u2019s consider:", "On the one hand, we can see a clear pattern on our data and also that our residuals are far from being randomly distributed.", "Let\u2019s look at other handy tools to diagnose the randomness of a set of observations. I\u2019m talking about the correlogram, which contains the correlation between an observed series, and the same series shifted k time points into the future. For the least-squares case, remember that it is computed by", "Notice that we are using t to simplify our understanding that the independent variable is nothing more than time.", "By plotting the residuals against the residuals with a lag (time difference), we are plotting the same variable against itself \u2014 therefore, the name autocorrelations. We need to grasp this idea well, so it makes sense to lose a bit more time with this subject.", "First, we are going to introduce the concept of covariance. It describes how much two variables vary together. We could say that variance is a measure for how a population varies amongst themselves, and covariance is a measure for how much two variables change with each other.", "The equation for the covariance between X and Y is", "I\u2019m using X and Y to simplify the subscripts, but you know that for us, X is Y_t and Y is Y_{t-1}. The notation \ud835\udd3c[Y] is nothing more than the expected value of Y. Assuming that each data point is equally likely, the probability of each is 1/n, giving", "for the discrete case, we will be considering. Certainly, you already spotted that this is simply the mean value of our time series (also denoted by \u0233).", "Now that we understand the covariance, we can extend this idea to correlation (also known as Pearson correlation coefficient), which is simply the covariance normalized by the square root of the variances of our random variables.", "For our problem (at least at this moment), we are not particularly interested in the correlation of two random variables but instead in one random variable with itself. Let\u2019s create our series to be able to visualize it better. We have to ensure that it is not independent across time (creating the correlations that we want to check).", "We can see that we made our Y explicitly dependent on its previous value. We used a coefficient to define how much the current value is correlated with the previous one \u2014 feel free to test with other values. We will understand much better the usefulness of these coefficients later. Let\u2019s plot Y_t against Y_{t-1} and see what we get.", "From the plot above, we can immediately see that both variables are positively correlated. What does this tell us about our model? Let\u2019s find out.", "Now we are going to generalize the autocorrelation function or ACF (see more here [3]). The ACF at lag k of a stationary stochastic process is defined as", "for any i divided by the variance of the stochastic process.", "Using the equation above, we can say that the autocorrelation function at lag k, for k \u2265 0, is defined by", "We can see the autocorrelations, and they seem high for some lags, but how high? We need a lower and an upper bound to be able to answer that question. If we assume that the time-series is Gaussian and if the autocorrelation is higher or lower than these upper or lower bounds, respectively, the null hypothesis is rejected. It states that there is no autocorrelation at and beyond a given lag at a significance level of \u03b1 (here we are doing hypothesis tests and throwing accepted but somewhat random values of significance \u2014 not so Bayesian). Our upper and lower bounds can be computed using", "where we will use \u03b1 = 5%. SE is the standard error, and r_k is the estimated autocorrelation at lag k. SE can be calculated using Barlett\u2019s formula", "Now that we know how to calculate our ACF and upper and lower bounds, we can use a library to do it for us.", "We are finally ready to do the correlogram for the residuals of our UK drivers data and, most importantly, to analyze it.", "The main idea is that if residuals are randomly distributed (what we want them to be), then they are independent of one another. It is shown in the correlogram in all the bars that do not exceed our confidence limits. We are going to use what we have learned so far.", "It doesn\u2019t look that nice; there are quite a few bars that exceed the blue shadowed area. We can see that k=1, k=2, k=11, k=12, and k=13 are outside of those limits (k=0 is always one as we showed above when calculating the ACF manually because it is the correlation of each point with itself). We say that these points are significantly different from zero, and this shows that we violated the assumption that errors are randomly distributed when we used a classical linear regression.", "We have set up two different models that fundamentally do the same thing: they use time as an explanatory variable, and they linearly model its relationship with the log number of UK drivers KSI. The first approach was a classical linear regression model fitted using the standard least-squares method. The second one was our first Bayesian model, expanding on the idea of point estimates to posterior (and prior) distributions. Nevertheless, the results are not satisfactory. Our residuals are far from randomly distributed, which is a consequence of our observations not being independent of each other. We need new tools to solve this problem. Essentially, there is an underlying dynamic evolution that cannot be observed and we are unable to model it. This is where state-space models come in. The state describes precisely the unobserved dynamic process over time. In the next article, we will be setting up our first state-space model!", "[1] J. Commandeur, S. Koopman, An Introduction to State Space Time Series Analysis (2007), Oxford University Press", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Data @ Marley Spoon | Ph.D. Researcher AI @ LIACC | Coordinator DS Masters @ NDS | CoFounder & ex-CEO @ HUUB"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F89a64b826a7e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@luisroque?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86----89a64b826a7e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.ssfpack.com/DKbook.html", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Bayes'_theorem", "anchor_text": "[2]"}, {"url": "https://www.real-statistics.com/time-series-analysis/stochastic-processes/autocorrelation-function/", "anchor_text": "[3]"}, {"url": "https://www.linkedin.com/in/luisbrasroque/", "anchor_text": "LinkedIn"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "https://en.wikipedia.org/wiki/Bayes%27_theorem"}, {"url": "https://www.real-statistics.com/time-series-analysis/stochastic-processes/autocorrelation-function/", "anchor_text": "https://www.real-statistics.com/time-series-analysis/stochastic-processes/autocorrelation-function/"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----89a64b826a7e---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----89a64b826a7e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/time-series-analysis?source=post_page-----89a64b826a7e---------------time_series_analysis-----------------", "anchor_text": "Time Series Analysis"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----89a64b826a7e---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/python?source=post_page-----89a64b826a7e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----89a64b826a7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----89a64b826a7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F89a64b826a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----89a64b826a7e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----89a64b826a7e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----89a64b826a7e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----89a64b826a7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://medium.com/@luisroque/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "902 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd8e0bc4b610d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e&newsletterV3=2195f049db86&newsletterV3Id=d8e0bc4b610d&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}