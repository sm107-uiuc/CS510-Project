{"url": "https://towardsdatascience.com/implementing-transformer-for-language-modeling-ba5dd60389a2", "time": 1683014175.1882062, "path": "towardsdatascience.com/implementing-transformer-for-language-modeling-ba5dd60389a2/", "webpage": {"metadata": {"title": "Transformer for Language Modeling | Towards Data Science", "h1": "Implementing Transformer for Language Modeling", "description": "We build the Transformer for language modeling using Fairseq and Pytorch. We train on the CMU Book Summary Dataset, evaluate the model, and generate text."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer", "paragraph_index": 0}, {"url": "https://research.google/teams/brain/", "anchor_text": "Google Brain", "paragraph_index": 0}, {"url": "https://research.google/", "anchor_text": "Google Research", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://github.com/pytorch/fairseq/", "anchor_text": "Fairseq", "paragraph_index": 1}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html#:~:text=This%20dataset%20contains%20plot%20summaries,author%2C%20title%2C%20and%20genre.", "anchor_text": "CMU Book Summary Dataset", "paragraph_index": 2}, {"url": "https://github.com/pytorch/fairseq", "anchor_text": "Fairseq", "paragraph_index": 3}, {"url": "https://ai.facebook.com/", "anchor_text": "Facebook AI", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/top-nlp-libraries-to-use-2020-4f700cdb841f", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://fairseq.readthedocs.io/en/latest/command_line_tools.html#", "anchor_text": "fairseq command-line tool", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1701.03185", "anchor_text": "beam search", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1805.04833", "anchor_text": "top-k sampling", "paragraph_index": 18}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1701.03185", "anchor_text": "Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1805.04833", "anchor_text": "Hierarchical Neural Story Generation", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1904.09751", "anchor_text": "The Curious Case of Neural Text Degeneration", "paragraph_index": 24}, {"url": "https://github.com/pytorch/fairseq/", "anchor_text": "Fairseq Github", "paragraph_index": 25}, {"url": "https://fairseq.readthedocs.io/", "anchor_text": "Fairseq Documentation", "paragraph_index": 26}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/", "paragraph_index": 28}], "all_paragraphs": ["Recent trends in Natural Language Processing have been building upon one of the biggest breakthroughs in the history of the field: the Transformer. The Transformer is a model architecture researched mainly by Google Brain and Google Research. It was initially shown to achieve state-of-the-art in the translation task but was later shown to be effective in just about any NLP task when it became massively adopted. The transformer architecture consists of a stack of encoders and decoders with self-attention layers that help the model pay attention to respective inputs. You can learn more about transformers in the original paper here.", "In this post, we will be showing you how to implement the transformer for the language modeling task. Language modeling is the task of assigning probability to sentences in a language. The goal for language modeling is for the model to assign high probability to real sentences in our dataset so that it will be able to generate fluent sentences that are close to human-level through a decoder scheme. We will be using the Fairseq library for implementing the transformer.", "In this article, we will be again using the CMU Book Summary Dataset to train the Transformer model. You can refer to Step 1 of the blog post to acquire and prepare the dataset. After preparing the dataset, you should have the train.txt, valid.txt, and test.txt files ready that correspond to the three partitions of the dataset.", "If you haven\u2019t heard of Fairseq, it is a popular NLP library developed by Facebook AI for implementing custom models for translation, summarization, language modeling, and other generation tasks. You can check out my comments on Fairseq here.", "Now, in order to download and install Fairseq, run the following commands:", "You can also choose to install NVIDIA\u2019s apex library to enable faster training if your GPU allows:", "Now, you have successfully installed Fairseq and finally we are all good to go!", "To preprocess the dataset, we can use the fairseq command-line tool, which makes it easy for developers and researchers to directly run operations from the terminal. To preprocess our data, we can use fairseq-preprocess to build our vocabulary and also binarize the training data.", "After executing the above commands, the preprocessed data will be saved in the directory specified by the --destdir .", "Finally, we can start training the transformer! To train a model, we can use the fairseq-train command:", "In our case, we specify the GPU to use as the 0th (CUDA_VISIBLE_DEVICES), task as language modeling (--task), the data in data-bin/summary , the architecture as a transformer language model (--arch ), the number of epochs to train as 12 (--max-epoch ) , and other hyperparameters. After training, the best checkpoint of the model will be saved in the directory specified by --save-dir .", "12 epochs will take a while, so sit back while your model trains! Of course, you can also reduce the number of epochs to train according to your needs. The following output is shown when the training is complete:", "Note that in each epoch, the relevant numbers are shown, such as loss and perplexity. These could be helpful for evaluating the model during the training process.", "After your model finishes training, you can evaluate the resulting language model using fairseq-eval-lm :", "Here the test data will be evaluated to score the language model (the train and validation data are used in the training phase to find the optimized hyperparameters for the model). The following shows the command output after evaluation:", "After training the model, we can try to generate some samples using our language model. To generate, we can use the fairseq-interactive command to create an interactive session for generation:", "During the interactive session, the program will prompt you an input text to enter. After the input text is entered, the model will generate tokens after the input. A generation sample given The book takes place as input is this:", "The book takes place in the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the characters\u2026", "The generation is repetitive which means the model needs to be trained with better parameters. The above command uses beam search with beam size of 5. We can also use sampling techniques like top-k sampling:", "Note that when using top-k or top-sampling, we have to add the beam=1 to suppress the error that arises when --beam does not equal to--nbest . This seems to be a bug.", "In this blog post, we have trained a classic transformer model on book summaries using the popular Fairseq library! Although the generation sample is repetitive, this article serves as a guide to walk you through running a transformer on language modeling. Take a look at my other posts if interested :D", "[1] A. Vaswani, N. Shazeer, N. Parmar, etc., Attention Is All You Need (2017), 31st Conference on Neural Information Processing Systems", "[2] L. Shao, S. Gouws, D. Britz, etc., Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models (2017), Empirical Methods in Natural Language Processing", "[3] A. Fan, M. Lewis, Y. Dauphin, Hierarchical Neural Story Generation (2018), Association of Computational Linguistics", "[4] A. Holtzman, J. Buys, L. Du, etc., The Curious Case of Neural Text Degeneration (2019), International Conference on Learning Representations", "[5] Fairseq Github, Facebook AI Research", "[6] Fairseq Documentation, Facebook AI Research", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master\u2019s Student at Carnegie Mellon, Top Writer in AI, Top 1000 Writer, Blogging on ML | Data Science | NLP. Linkedin: https://www.linkedin.com/in/itsuncheng/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fba5dd60389a2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://itsuncheng.medium.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840----ba5dd60389a2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/pytorch/fairseq/", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer"}, {"url": "https://research.google/teams/brain/", "anchor_text": "Google Brain"}, {"url": "https://research.google/", "anchor_text": "Google Research"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "here"}, {"url": "https://github.com/pytorch/fairseq/", "anchor_text": "Fairseq"}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "Fine-tuning GPT2 for Text Generation Using PytorchFine-tune GPT2 for text generation using Pytorch and Huggingface. We train on the CMU Book Summary Dataset to generate\u2026towardsdatascience.com"}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html#:~:text=This%20dataset%20contains%20plot%20summaries,author%2C%20title%2C%20and%20genre.", "anchor_text": "CMU Book Summary Dataset"}, {"url": "https://github.com/pytorch/fairseq", "anchor_text": "Fairseq"}, {"url": "https://ai.facebook.com/", "anchor_text": "Facebook AI"}, {"url": "https://towardsdatascience.com/top-nlp-libraries-to-use-2020-4f700cdb841f", "anchor_text": "here"}, {"url": "https://fairseq.readthedocs.io/en/latest/command_line_tools.html#", "anchor_text": "fairseq command-line tool"}, {"url": "https://arxiv.org/abs/1701.03185", "anchor_text": "beam search"}, {"url": "https://arxiv.org/abs/1805.04833", "anchor_text": "top-k sampling"}, {"url": "https://arxiv.org/abs/1904.09751", "anchor_text": "top-p sampling"}, {"url": "https://towardsdatascience.com/top-nlp-libraries-to-use-2020-4f700cdb841f", "anchor_text": "Top NLP Libraries to Use 2020AllenNLP, Fast.ai, Spacy, NLTK, TorchText, Huggingface, Gensim, OpenNMT, ParlAI, DeepPavlovtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "Fine-tuning GPT2 for Text Generation Using PytorchFine-tune GPT2 for text generation using Pytorch and Huggingface. We train on the CMU Book Summary Dataset to generate\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf", "anchor_text": "Controlling Text Generation for Language ModelsHands-on approach to control style and content of machine-generated texttowardsdatascience.com"}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "BERT Text Classification Using PytorchText classification is a common task in NLP. We apply BERT, a popular Transformer model, on fake news detection using\u2026towardsdatascience.com"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need"}, {"url": "https://arxiv.org/abs/1701.03185", "anchor_text": "Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models"}, {"url": "https://arxiv.org/abs/1805.04833", "anchor_text": "Hierarchical Neural Story Generation"}, {"url": "https://arxiv.org/abs/1904.09751", "anchor_text": "The Curious Case of Neural Text Degeneration"}, {"url": "https://github.com/pytorch/fairseq/", "anchor_text": "Fairseq Github"}, {"url": "https://fairseq.readthedocs.io/", "anchor_text": "Fairseq Documentation"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ba5dd60389a2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ba5dd60389a2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ba5dd60389a2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----ba5dd60389a2---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ba5dd60389a2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&user=Raymond+Cheng&userId=4c697cd55840&source=-----ba5dd60389a2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&user=Raymond+Cheng&userId=4c697cd55840&source=-----ba5dd60389a2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fba5dd60389a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ba5dd60389a2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ba5dd60389a2--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://itsuncheng.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "731 Followers"}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa637a6d3749b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-transformer-for-language-modeling-ba5dd60389a2&newsletterV3=4c697cd55840&newsletterV3Id=a637a6d3749b&user=Raymond+Cheng&userId=4c697cd55840&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}