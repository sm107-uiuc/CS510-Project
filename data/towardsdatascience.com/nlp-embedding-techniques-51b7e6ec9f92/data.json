{"url": "https://towardsdatascience.com/nlp-embedding-techniques-51b7e6ec9f92", "time": 1683004364.888993, "path": "towardsdatascience.com/nlp-embedding-techniques-51b7e6ec9f92/", "webpage": {"metadata": {"title": "NLP: Word Embedding Techniques Demystified | by Rabeh Ayari, PhD | Towards Data Science", "h1": "NLP: Word Embedding Techniques Demystified", "description": "Word Embedding is a technique of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "scikit-learn", "paragraph_index": 10}, {"url": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text", "anchor_text": "sklearn.feature_extraction.text", "paragraph_index": 10}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer", "anchor_text": "CountVectorizer", "paragraph_index": 10}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer", "anchor_text": "CountVectorizer", "paragraph_index": 10}, {"url": "http://rabeh.ayari@polymtl.ca", "anchor_text": "here", "paragraph_index": 25}], "all_paragraphs": ["Word Embedding is a technique of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix.", "Word embedding can be learned using a variety of language models. For example, a \u2018dog\u2019 will be represented by the vector [0.75, 0.22, 0.66, 0.97]. If all the words in a dictionary are encoded in such a way, it becomes possible to compare the vectors of the words with each other, for example by measuring the cosine distance or the euclidean distance between the vectors. A good representation of words will then make it possible to find that the word \u2018pet\u2019 is closer to the word \u2018dog\u2019 than it is to the word \u2018soccer\u2019 or \u2018engine\u2019. Therefore, these representations allow us to hope that, in the vector space where the embedding is made, we will have the equation king-man+woman = queen or even the equation London-England+Italy = Rome.", "Word embeddings are also very useful in mitigating the curse of dimensionality, a very recurring problem in artificial intelligence. Without word embedding, the unique identifiers representing the words generate scattered data, isolated points in a vast sparse representation. With word embedding, on the other hand, the space becomes much more limited in terms of dimensionality with a widely richer amount of semantic information. With such numerical features, it is easier for a computer to perform different mathematical operations like matrix factorization, dot product, etc. which are mandatory to use shallow and deep learning techniques. There are many techniques available at our disposal to achieve this transformation. In this article, we will be covering: Bag-Of-Words, TF-IDF, Word2Vec, Doc2vec and Doc2vecC.", "Bag-Of-Words (a.k.a. BOW) is a popular basic approach to generate document representation. A text is represented as a bag containing plenty of words. The grammar and word order are neglected while the frequency is kept the same. A feature generated by bag-of-words is a vector where n is the number of words in the input documents vocabulary.", "For instance, there are two documents:", "The vocabulary of these two documents is:", "This vocabulary will produce feature vectors of length 8 (i.e. Vocabulary Cardinality). Given such vocabulary, the bag-of-word feature representation of these two documents are:", "Using this technique we create the bag-of-words representation of each document in our dataset. If our dataset contains a big number of unique words some of that are not used very frequently, which is usually the case. So, among those, we chose N most frequent words and create a feature vector of dimension Nx1. These feature vectors are then used for any machine learning task.", "The Term Frequency-Inverse Document Frequency (a.k.a. TF-IDF) is another way to represent a document based on its words. With TF-IDF, words are given weights by TF-IDF importance instead of only frequency. TF-IDF provides a statistical measure used to evaluate the importance of the words with respect to the document in a collection or corpus. There are many commonly used words for each dataset that appear many times in the document but do not provide any important information. The weight increases in proportion to the number of occurrences of the word in the document. It also varies according to the frequency of the word in the corpus. Variants of the original formula are often used in search engines to assess the relevance of a document based on the user\u2019s search criteria.", "By definition, TF-IDF embedding is composed by two terms: the first computes the normalized Term Frequency (TF), a.k.a. the occurrence a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF) which computes the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.", "As shown in the figure below, scikit-learn library includes in the submodule sklearn.feature_extraction.text gathering utilities to build feature vectors from text documents in which we will find CountVectorizer. CountVectorizer easily convert a collection of raw documents to a matrix of TF-IDF numerical features.", "One of the most efficient techniques to represent a word is Word2Vec. Word2vec is a computationally efficient predictive model for learning word embeddings from raw text. It plots the words in a multi-dimensional vector space, where similar words tend to be close to each other. The surrounding words of a word provide the context to that word.", "Let\u2019s start with a high-level insight about where we\u2019re going. Word2Vec uses a well-known trick in deep learning. In this trick, we train a simple neural network with a single hidden layer to perform a fake task, without a real need of the results of the task we trained it on. Instead, the main objective consists of learning a representation of the input data which are actually the \u2018word vectors\u2019 gathered from the learned weights of the hidden layer. Such an approach may remind us of the way we train auto-encoders to perform dimensionality reduction.", "Word2Vec can rely on either one of two model architectures in order to produce a distributed representation of input words: Continuous Bag-of-Words (CBoW) or Continuous Skip-Gram as shown in the figure below. Vector representation extracts semantic relationships based on the co-occurrence of words in the dataset.", "The CBoW and skip-gram models are trained using a binary classification to discriminate between the real target word and the other words in the same context. The accuracy at which the model predicts the words depends on how many times the model sees these words within the same context throughout the dataset. The hidden representation is changed by more words and context co-occurrences during the training process, which allows the model to have more future successful predictions, leading to a better representation of word and context in the vector space. Skip gram is much slower than CBOW, but performs more accurately with infrequent words.", "Let\u2019s go deeper into details to understand the difference between CBOW architecture and Continuous skip-gram.", "In both models, when we say \u2018surrounding\u2019, there is actually a \u2018window size\u2019 parameter to the algorithm. The size of the context window limits how many words before and after a given word would be included as context words of the given word. For example, window size of 3 will include the 3 words to the left and the 3 words to the right for each observed word in the sentence as context. Increasing the window size increases the training time as more word-context pairs need to be trained. Also it may capture context words that are not relevant to the current word. Decreasing the context words can capture relations between words and stop words which is not preferred.", "Doc2Vec is another widely used technique that creates an embedding of a document irrespective to its length. While Word2Vec computes a feature vector for every word in the corpus, Doc2Vec computes a feature vector for every document in the corpus. Doc2vec model is based on Word2Vec, with only adding another vector (paragraph ID) to the input. The Doc2Vec model, by analogy with Word2Vec, can rely on one of two model architectures which are: Distributed Memory version of Paragraph Vector (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW).", "In the figure below, we show the model architecture of PV-DM:", "The above diagram is based on the CBOW model, but instead of using just nearby words to predict the word, we also added another feature vector, which is document-unique. So when training the word vectors W, the document vector D is trained as well, and at the end of training, it holds a numeric representation of the document.", "The inputs consist of word vectors and document Id vectors. The word vector is a one-hot vector with a dimension 1xV. The document Id vector has a dimension of 1xC, where C is the number of total documents. The dimension of the weight matrix W of the hidden layer is NxV. The dimension of the weight matrix D of the hidden layer is CxN.", "Doc2vecC tackles the problem of Doc2vec by including a global context through capturing the semantic meanings of the document. The architecture is very similar to Word2vec. In Doc2VecC, the average of the word embeddings in a document is used to represent the global context. But, unlike Word2vec, here at every iteration, words are randomly sampled from the document (document corruption). Vectors from those words are then averaged to obtain a document vector. This document vector is then used to predict the current word using the local context words as well as the global context. The global context is generated through an unbiased drop-out corruption. This corrupted document is represented as:", "Then, the probability of observing a word w_t given the local context c_t and global context x_i is given as:", "Once we have learned U using neural networks, each document can be simply represented as an average of the word embedding in that document. Hence, the document vector i is given as :", "The output of the model shares both global and local semantics of the dataset. Averaging helps us obtain vectors for unseen documents as well, hence tackling the problem presented by Doc2vec. Since only a fraction of the words are used for training, the training time is much lower in comparison to Doc2vec. In any document, words derive context from their neighbouring words. Hence to provide more context, we append our data set with publicly available data sets that are subsets of textual data for businesses, reviews, and user data for personal, educational, and academic purposes.", "Rabeh Ayari is a Principal Data Scientist working on applied AI problems for credit risk modelling & fraud analytics and conducting original research in machine learning. My areas of expertise include data analysis using deep neural networks, machine learning, data visualization, feature engineering, linear/non-linear regression, classification, discrete optimization, operations research, evolutionary algorithm and linear programming. Feel free to drop me a message here!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director, Advanced Analytics with PhD from University of Montreal. \u2764 AI, ML, DL, DS, Stats, Soccer & travelling."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F51b7e6ec9f92&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rabah.ayari?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rabah.ayari?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Rabeh Ayari, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2a92ebdc4ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&user=Rabeh+Ayari%2C+PhD&userId=f2a92ebdc4ef&source=post_page-f2a92ebdc4ef----51b7e6ec9f92---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jonasjacobsson?utm_source=medium&utm_medium=referral", "anchor_text": "Jonas Jacobsson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "scikit-learn"}, {"url": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text", "anchor_text": "sklearn.feature_extraction.text"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer", "anchor_text": "CountVectorizer"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer", "anchor_text": "CountVectorizer"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer", "anchor_text": "TfidfVectorizer using Sklearn"}, {"url": "http://@misc{mikolov2013efficient,     title={Efficient Estimation of Word Representations in Vector Space},     author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},     year={2013},     eprint={1301.3781},     archivePrefix={arXiv},     primaryClass={cs.CL} }", "anchor_text": "Word2Vec Training Model Architecture"}, {"url": "https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/", "anchor_text": "Doc2Vec Model Architecture"}, {"url": "http://rabeh.ayari@polymtl.ca", "anchor_text": "here"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----51b7e6ec9f92---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/nlp?source=post_page-----51b7e6ec9f92---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----51b7e6ec9f92---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----51b7e6ec9f92---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----51b7e6ec9f92---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&user=Rabeh+Ayari%2C+PhD&userId=f2a92ebdc4ef&source=-----51b7e6ec9f92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&user=Rabeh+Ayari%2C+PhD&userId=f2a92ebdc4ef&source=-----51b7e6ec9f92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F51b7e6ec9f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----51b7e6ec9f92---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----51b7e6ec9f92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rabah.ayari?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rabah.ayari?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rabeh Ayari, PhD"}, {"url": "https://medium.com/@rabah.ayari/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "76 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2a92ebdc4ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&user=Rabeh+Ayari%2C+PhD&userId=f2a92ebdc4ef&source=post_page-f2a92ebdc4ef--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38798d776148&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-embedding-techniques-51b7e6ec9f92&newsletterV3=f2a92ebdc4ef&newsletterV3Id=38798d776148&user=Rabeh+Ayari%2C+PhD&userId=f2a92ebdc4ef&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}