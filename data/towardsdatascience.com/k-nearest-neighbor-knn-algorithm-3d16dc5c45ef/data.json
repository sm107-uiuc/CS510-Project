{"url": "https://towardsdatascience.com/k-nearest-neighbor-knn-algorithm-3d16dc5c45ef", "time": 1683013875.169353, "path": "towardsdatascience.com/k-nearest-neighbor-knn-algorithm-3d16dc5c45ef/", "webpage": {"metadata": {"title": "k-nearest-neighbor (KNN) algorithm | by Dennis Bakhuis | Towards Data Science", "h1": "k-nearest-neighbor (KNN) algorithm", "description": "Data science is hot and it seems that everybody is working on some sort of project involving the latest state-of-the-art (SOTA) algorithm. Of course with good reason as in many cases we can use data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/master-python-in-10-minutes-a-day-ac32996b5ded", "anchor_text": "Start here", "paragraph_index": 1}, {"url": "https://www.kaggle.com/c/titanic/discussion/182810", "anchor_text": "Titanic dataset", "paragraph_index": 7}, {"url": "https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever", "anchor_text": "Shunjiang Xu / Blood is thicker than water", "paragraph_index": 13}, {"url": "https://www.kaggle.com/c/titanic/discussion/182810", "anchor_text": "Kaggle", "paragraph_index": 16}, {"url": "https://www.linkedin.com/in/dennisbakhuis/", "anchor_text": "LinkedIn", "paragraph_index": 19}, {"url": "https://github.com/dennisbakhuis/algorithms_from_scratch/tree/master/KNN", "anchor_text": "Github", "paragraph_index": 20}, {"url": "https://www.kaggle.com/dennisbakhuis/titanic-k-nearest-neighbor-knn-frmscratch-0-813", "anchor_text": "Kaggle", "paragraph_index": 20}], "all_paragraphs": ["Data science is hot and it seems that everybody is working on some sort of project involving the latest state-of-the-art (SOTA) algorithm. Of course with good reason as in many cases we can use data to give very reasonable prediction, almost in any field. While there is a lot of focus lately on SOTA algorithms, the simpler methods are sometimes forgotten.", "Want to get started with Python? Start here!", "Recently, I played around with a k-nearest-neighbor (KNN) algorithm and I was amazed how powerful it can be. The technique itself is used in many other fields. For example, I used it to identify the same particles in consecutive frames of a high-speed recording for one of my research projects during my Ph.D. The coordinates of a particle are known and we look in the next frame at the closest particles around that position. Of course, when there are multiple particles very close by, you can get into trouble. For that, you can make use of higher order information from multiple frames such as the velocity or acceleration vector. For KNN in machine learning, we generally do not have temporal data therefore, we only use its first order, which is the simplest form.", "When we want to use KNN to classify new data, i.e. make a prediction, we use the already known data (or labeled data) as a kind of look-up table. We select data that is similar to the new data we want to predict and select the most prominent class from that selection. So we compare an unknown example to an already known dataset. There is no training, no layers, no weights. The only parameter is k and specifies the amount of neighbors to take into consideration when predicting the class. For example, to classify a kind of fruit, we can select the five most similar examples from the dataset. Now we say that the most prominent class of those five selected examples is probably also the class we want to predict. If we have found three apples and two pears, we would predict an apple as well.", "Now we come to another problem: how do we select the most similar examples from a list of features. When we have a single feature, e.g. height this would be very easy. We simply calculate the difference and select the k closest matches. But what to do when we also have weight and width? We have to quantify the difference for each feature and aggregate the result to a single value. Fortunately, there are many ways to do this. One of the most common is the Euclidean distance, which can be seen as the shortest straight line between two points.", "The Euclidean distance works for many features (or dimensions), however there is one short-coming that counts for KNN in general. The features have to be numeric and to calculate a distance, the numbers have to represent meaning. The height feature has meaning attached to the number. The larger the number, the larger the object. The smaller the number, the smaller the object. This makes the distance, i.e. the difference between two heights meaningful as it is the difference in height. Now let's take color as a feature. Color is not numeric (at least we use names, not wave-lengths) and therefore, we need to convert it to a categorical unit to create numbers. Now, a value of green is represented by 0, red by 1, etc. While the numbers indicate which color is attached to the object, the value itself has no real meaning. If we have an object that has color value 100 and another object with color value 50, these numbers do not mean anything, other than that the color is different. Therefore, a difference (or distance) between these numbers is meaningless and useless for KNN.", "While real categorical variables are not usable, not everything is lost. We can for example use binary indicators such as is_green to indicate if an object is green. Still, I would only use these if I am sure that the feature adds to the prediction.", "We will create an KNN algorithm and use it on the famous Titanic dataset and predict who has survived the tragic disaster. Therefore, all the function will have some kind of link with that dataset. To create an KNN prediction algorithm we have to do the following steps:", "1. calculate the distance between the unknown point and the known dataset.2. select the k nearest neighbors for from that dataset.3. make a prediction", "As seen in the previous figure, we only have to apply Pythagoras to calculate the Euclidean distance:", "Next we need to make a loop to calculate and compare all the distance of the dataset with the point we want to predict:", "Now we have a function that gives us a list of k nearest neighbors. The final step is to take the most prominent class from that list and use that as a prediction. This is easily solved by taking the mean and rounding it. Feel free to check if this is true.", "Before we can test the algorithm we need to prepare the data. This is reading in the data and filling in the missing values in a smart way. We will not be using the Embarked and Cabin feature, so we do not have to bother about those. There is a single Fare missing, which we will fill in using the median of that Pclass. There is quite some Age data missing. We will be using the title to make a good guess for each missing age.", "Next we create a couple of additional features (features). One that I recently saw on Kaggle is family_survivability. It assumes that families help each other and that if others in your family survive, you are more likely to also have survived. The idea comes from Shunjiang Xu / Blood is thicker than water and I think it is pretty amazing. Furthermore, we will add the family size, change Sex to a binary indicator, and scale all variables and split it back to train/test.", "The algorithm is ready, the data is prepared, we are ready to make some predictions!", "It takes pretty long, because our algorithm is quite slow. Put if all is correct, you should get an accuracy of 83.5%. The only parameter we can tweak is the the number of neighbors to take in consideration. I played around a bit and 10 seems to give the best results. Now lets make a Kaggle submission dataset!", "Submitting this to Kaggle will result in a score of 81.3%. Slightly lower than the training set but still quite high. And we managed this by only using a simple KNN algorithm.", "The algorithm we created was powerful and very effective. The only downside is that our implementation is very slow. For each vector, we need to calculate and compare the distance with the complete dataset. Sci-kit learn comes with smarter implementations that use Tree-based approaches to minimize the amount of calculation required and making it tremendously faster.", "Here we used KNN as a classifier, however, it works very similar to regression as well. With regression, the result is continuous, therefore it makes sense to, for example, average nearest neighbors when making a prediction.", "I hope you had as much fun as I had. If you have any questions, feel free to contact me through LinkedIn.", "All code is available on my Github and on Kaggle.", "I would very much appreciate any upvote of my Kaggle notebooks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a passion for natural language processing and deep learning. Python and open source enthusiast. Background in fluid dynamics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3d16dc5c45ef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb----3d16dc5c45ef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ninastrehl?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Nina Strehl"}, {"url": "https://unsplash.com/s/photos/neighbor?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/master-python-in-10-minutes-a-day-ac32996b5ded", "anchor_text": "Start here"}, {"url": "https://www.kaggle.com/c/titanic/discussion/182810", "anchor_text": "Titanic dataset"}, {"url": "https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever", "anchor_text": "Shunjiang Xu / Blood is thicker than water"}, {"url": "https://www.kaggle.com/c/titanic/discussion/182810", "anchor_text": "Kaggle"}, {"url": "https://www.linkedin.com/in/dennisbakhuis/", "anchor_text": "LinkedIn"}, {"url": "https://github.com/dennisbakhuis/algorithms_from_scratch/tree/master/KNN", "anchor_text": "Github"}, {"url": "https://www.kaggle.com/dennisbakhuis/titanic-k-nearest-neighbor-knn-frmscratch-0-813", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3d16dc5c45ef---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3d16dc5c45ef---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----3d16dc5c45ef---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/python?source=post_page-----3d16dc5c45ef---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3d16dc5c45ef---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----3d16dc5c45ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----3d16dc5c45ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3d16dc5c45ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3d16dc5c45ef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3d16dc5c45ef--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://dennisbakhuis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc167ef22c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbor-knn-algorithm-3d16dc5c45ef&newsletterV3=5b8617eb89bb&newsletterV3Id=c167ef22c4d5&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}