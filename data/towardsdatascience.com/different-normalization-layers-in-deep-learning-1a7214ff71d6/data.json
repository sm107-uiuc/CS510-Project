{"url": "https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6", "time": 1683017552.228698, "path": "towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6/", "webpage": {"metadata": {"title": "Different Normalization Layers in Deep Learning | by Nilesh Vijayrania | Towards Data Science", "h1": "Different Normalization Layers in Deep Learning", "description": "Batch normalization could be replaced with weight standardization when used in combination with group normalization. Weight standardization with group normalization performs specially well with dense prediction tasks such as semantic segmentation where generally smaller batch sizes are used for training."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1602.07868.pdf", "anchor_text": "Weight Normalization paper", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1903.10520.pdf", "anchor_text": "\u201cMicro-Batch Training with Batch-Channel Normalization and Weight Standardization\u201d", "paragraph_index": 19}], "all_paragraphs": ["Presently Deep Learning has been revolutionizing many subfields such as natural language processing, computer vision, robotics, etc. Deep learning certainly involves training carefully designed deep neural networks and various design decisions impact the training regime of these deep networks. Some of these design decisions include", "Majorly these design decisions depend upon the underlying task we are trying to solve and require a deeper understanding of the different options we have at hand. In this post, I will focus on the second point \u201cdifferent Normalization Layers in Deep Learning\u201d. Broadly I would cover the following methods.", "Batch Normalization focuses on standardizing the inputs to any particular layer(i.e. activations from previous layers). Standardizing the inputs mean that inputs to any layer in the network should have approximately zero mean and unit variance. Mathematically, BN layer transforms each input in the current mini-batch by subtracting the input mean in the current mini-batch and dividing it by the standard deviation.", "But each layer doesn\u2019t need to expect inputs with zero mean and unit variance, but instead, probably the model might perform better with some other mean and variance. Hence the BN layer also introduces two learnable parameters \u03b3 and \u03b2.", "The whole layer operation is as follows. It takes an input x_i and transforms it into y_i as described in the below table.", "The question is how BN helps NN training? Intuitively, In gradient descent, the network calculates the gradient based on the current inputs to any layer and reduce the weights in the direction indicated by the gradient. But since the layers are stacked one after the other, the data distribution of input to any particular layer changes too much due to slight update in weights of earlier layer, and hence the current gradient might produce suboptimal signals for the network. But BN restricts the distribution of the input data to any particular layer(i.e. the activations from the previous layer) in the network, which helps the network to produce better gradients for weights update. Hence BN often provides a much stable and accelerated training regime.", "However below are the few cons of Batch Normalization.", "Due to the disadvantages of Batch Normalization, T. Saliman and P. Kingma proposed Weight Normalization. Their idea is to decouple the length from the direction of the weight vector and hence reparameterize the network to speed up the training.", "What does reparameterization mean for Weight Normalization?", "The authors of the Weight Normalization paper suggested using two parameters g(for length of the weight vector) and v(the direction of the weight vector) instead of w for gradient descent in the following manner.", "Weight Normalization speeds up the training similar to batch normalization and unlike BN, it is applicable to RNNs as well. But the training of deep networks with Weight Normalization is significantly less stable compared to Batch Normalization and hence it is not widely used in practice.", "Inspired by the results of Batch Normalization, Geoffrey Hinton et al. proposed Layer Normalization which normalizes the activations along the feature direction instead of mini-batch direction. This overcomes the cons of BN by removing the dependency on batches and makes it easier to apply for RNNs as well.", "In essence, Layer Normalization normalizes each feature of the activations to zero mean and unit variance.", "Similar to layer Normalization, Group Normalization is also applied along the feature direction but unlike LN, it divides the features into certain groups and normalizes each group separately. In practice, Group normalization performs better than layer normalization, and its parameter num_groups is tuned as a hyperparameter.", "If you find BN, LN, GN confusing, the below image summarizes them very precisely. Given the activation of shape (N, C, H, W), BN normalizes the N direction, LN and GN normalize the C direction but GN additionally divides the C channels into groups and normalizes the groups individually.", "Lets next understand what weight Standardization is.", "Weight Standardization is transforming the weights of any layer to have zero mean and unit variance. This layer could be a convolution layer, RNN layer or linear layer, etc. For any given layer with shape(N, *) where * represents 1 or more dimensions, weight standardization, transforms the weights along the * dimension(s).", "Below is the sample code for implementing weight standardization for the 2D conv layer in pytorch.", "The basic idea is to only transform the weights during the forward pass and calculate activations accordingly. Pytorch will handle the backward pass out of the box. Similarly, it could be implemented for the linear layer as well.", "Recently, Siyun Qiao et al. introduced Weight Standardization in their paper \u201cMicro-Batch Training with Batch-Channel Normalization and Weight Standardization\u201d and found that group normalization when mixed with weight standardization, could outperform or perform equally well as BN even with batch size as small as 1. Shown below in the graph, the authors trained GN, BN, combination of GN+WS with Resnet50 and Resnet101 on Imagenet classification and MS COCO object detection task and found that GN+WS consistently outperforms the BN version even with much smaller batches than BN uses. This has attracted attention in dense prediction tasks such as semantic segmentation, instance segmentation which are usually not trainable with larger batch sizes due to memory constraints.", "In conclusion, Normalization layers in the model often helps to speed up and stabilize the learning process. If training with large batches isn\u2019t an issue and if the network doesn\u2019t have any recurrent connections, Batch Normalization could be used. For training with smaller batches or complex layer such as LSTM, GRU, Group Normalization with Weight Standardization could be tried instead of Batch Normalization.", "One important thing to note is, in practice the normalization layers are used in between the Linear/Conv/RNN layer and the ReLU non-linearity(or hyperbolic tangent etc) so that when the activations reach the Non-linear activation function, the activations are equally centered around zero. This would potentially avoid the dead neurons which never get activated due to wrong random initialization and hence can improve training.", "Below is the list of references used for this post and should be considered for further experiment details.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Intrigued about Deep learning and all things ML."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a7214ff71d6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nilesh0109.medium.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": ""}, {"url": "https://nilesh0109.medium.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Nilesh Vijayrania"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae2c3b0d24ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&user=Nilesh+Vijayrania&userId=ae2c3b0d24ec&source=post_page-ae2c3b0d24ec----1a7214ff71d6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"url": "https://arxiv.org/pdf/1602.07868.pdf", "anchor_text": "Weight Normalization paper"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization"}, {"url": "https://arxiv.org/abs/1903.10520", "anchor_text": "Siyuan Qiao et al.: Weight Standardization"}, {"url": "https://arxiv.org/pdf/1903.10520.pdf", "anchor_text": "\u201cMicro-Batch Training with Batch-Channel Normalization and Weight Standardization\u201d"}, {"url": "https://medium.com/tag/weight-standardization?source=post_page-----1a7214ff71d6---------------weight_standardization-----------------", "anchor_text": "Weight Standardization"}, {"url": "https://medium.com/tag/layer-normalization?source=post_page-----1a7214ff71d6---------------layer_normalization-----------------", "anchor_text": "Layer Normalization"}, {"url": "https://medium.com/tag/batch-normalization?source=post_page-----1a7214ff71d6---------------batch_normalization-----------------", "anchor_text": "Batch Normalization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a7214ff71d6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/weight-normalization?source=post_page-----1a7214ff71d6---------------weight_normalization-----------------", "anchor_text": "Weight Normalization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&user=Nilesh+Vijayrania&userId=ae2c3b0d24ec&source=-----1a7214ff71d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&user=Nilesh+Vijayrania&userId=ae2c3b0d24ec&source=-----1a7214ff71d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a7214ff71d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a7214ff71d6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a7214ff71d6--------------------------------", "anchor_text": ""}, {"url": "https://nilesh0109.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nilesh0109.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nilesh Vijayrania"}, {"url": "https://nilesh0109.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "90 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae2c3b0d24ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&user=Nilesh+Vijayrania&userId=ae2c3b0d24ec&source=post_page-ae2c3b0d24ec--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcead73d39ca8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferent-normalization-layers-in-deep-learning-1a7214ff71d6&newsletterV3=ae2c3b0d24ec&newsletterV3Id=cead73d39ca8&user=Nilesh+Vijayrania&userId=ae2c3b0d24ec&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}