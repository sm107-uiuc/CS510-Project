{"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5", "time": 1683015583.8332841, "path": "towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5/", "webpage": {"metadata": {"title": "Monte Carlo Tree Search: Implementing Reinforcement Learning in Real-Time Game Player - Part 3 | by Masoud Masoumi Moghadam | Oct, 2020 | Towards Data Science", "h1": "Monte Carlo Tree Search: Implementing Reinforcement Learning in Real-Time Game Player | Part 3", "description": "In this article, we go through the ups and downs of implementing Monte Carlo Tree Search in the real-time board game called \"HEX\"."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/monte-carlo-tree-search-a-case-study-along-with-implementation-part-1-ebc7753a5a3b", "anchor_text": "reinforcement learning basics", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43", "anchor_text": "Monte Carlo Tree Search", "paragraph_index": 0}, {"url": "https://www.researchgate.net/profile/Kenny_Young", "anchor_text": "Kenny young", "paragraph_index": 3}, {"url": "https://github.com/kenjyoung/mopyhex", "anchor_text": "mopyhex", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Disjoint-set_data_structure", "anchor_text": "Wikipedia", "paragraph_index": 13}, {"url": "https://medium.com/better-programming/using-disjoint-set-union-find-to-build-a-maze-generator-7462ea3b8632", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "here", "paragraph_index": 52}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "LinkedIn", "paragraph_index": 53}], "all_paragraphs": ["In the previous articles, we learned about reinforcement learning basics and Monte Carlo Tree Search basics. We covered how MCTS can search all the state-action space and come up with a good action based on statistics that are gathered after sampling search space.", "For the ones in hurry, this is the complete code of the project:", "This gif shows a demo of the final product. As you can see by clicking the generate button in the GUI, the MCTS agent chooses the best possible move. The competitive desire between black and white players is really interesting because the selection of moves in the game board sounds strategic to us humans.", "I have to thank Kenny young for his awesome implementation of mopyhex which helped me a lot in my project. This post mostly includes explaining his code. In the next articles, we will further add some contributions to the project for a more robust search in state-action space and also optimize the code for C level speed using cython tools.", "Before getting our hands dirty with the code, to give an overview of the project, let me describe each module\u2019s functionality. There are essentially 6 classes to implement for the initial framework:", "I have implemented some algorithms which use the concept of Rapid Action Evaluation and are stronger than the UCT algorithm in short-period searches. and make it even stronger. I am going to cover the concept of those algorithms later.", "In each game state tree search agent needs three things to know to be able to transit from one state to another:", "The latter one is pretty tricky to implement. Because to detect connections, we have to keep track of the disjoint groups of cells. So we have to make use of a data structure namely as Disjoint sets (also called Union find). But the fact that HEX game board cells are connected to its neighbors from 6 sides, makes it even trickier to implement. Check out the below image:", "So how do we deal with this puzzle?", "First I used NumPy arrays to keep the values of the game board. Unoccupied cell values are 0, white cell and black cell values are 1 and 2 accordingly. This image shows a representation of an 8X8 game board as arrays:", "Each cell has 8 neighboring cells. The values you see here are the coordinations over each cell defined as the neighborhood in GameMetato limit the neighbor cells to 6:", "The neighborhood for the cell in Fig 2 can now be represented like this:", "Which eventually in our game GUI, it becomes like Fig1 in hexagonal shapes.", "Now it\u2019s time to adjust the Disjoint-set data structure to this game. According to Wikipedia:", "In computer science, a disjoint-set data structure (also called a union-find) is a data structure that tracks a set of elements partitioned into several disjoint (non-overlapping) subsets.", "These disjoint sets are illustrated in the above figure, are examples of the game environment. If you want to know more about the concept of the disjoint set data structure, you might want to check this link here. This is the code for unionfind.py which I came up with:", "Now let\u2019s define the game state module. This is the code to GameState class:", "This is what the attributes are considered for:", "This is the GameMeta in meta.py:", "Now that we have implemented the game state, it\u2019s time to define the Monte Carlo tree search agent. First, we have to define a good structure for nodes of a tree. This is the code to define the Node module:", "Let\u2019s code the MCTSAgent step by step. First, let\u2019s initiate the class:", "If you are familiar with the concept of tree data structure, you know that all the nodes in a tree are reachable if you have the tree node and traverse wherever you want. So we define root as the tree root node and rootstate as the root state of the tree. Then when we meet each move, we play that in the rootstate and that converts the state to new states (alternate the player turn, check the black_groups and white_groups if there is any connection, \u2026.). the attributes run_time, node_countand num_rollouts are only for keeping track of some stats.", "Now we define the selection and expansion phase:", "As the code implies and what I mentioned earlier about tree traverse, we first copy the root and root_state of the game (lines 6 and 7). Then until we haven\u2019t meet any leaves (which means the game has not ended or the agent still has not explored the tree), we keep selecting nodes with maximum UCT values (computed in lines 13 and 14) and then covert the state by playing the actual move (line 17). If we reach some node with N equal to 0, we choose it.Before we return the node and state , we have to make sure the node is expandable or not. So in expand method, first we make sure the game has no winner, and if not, we add children to the expanded node. So the node and its state are then returned.", "For the simulation part we got this code:", "The agent simply gets the state of the game and it keeps playing random moves which is the default strategy (In next articles, I include more awesome strategies) and converting state until we reach a terminal state in which the game-winner is detected.", "This is the code for this step:", "in this code, we got three inputs:", "what we are going to do in the backpropagation part (as discussed here, take a look at figure 6) is to start from node and traverse the selected nodes backward (line23) and update the statistics (line 21, line 22) until we reach the root node (line 20, because the tree node has no parent). We alternate between nodes which loser player has chosen and winner player has chosen, so we have to give +1 reward to winner nodes and 0 to loser nodes (line 24). Also, note that line 18 indicates that we have to give +1 reward to the player who has chosen the move and got to node state if it is the winner. if not so the player has to be penalized with 0 reward.", "Now we have to define the loop in which we are supposed to do the whole process of selection, expansion, simulation, and backpropagation which is provided here:", "At the time I was implementing this code python 3.4 was the newest version. Since in line 7, I have used clock function and in python 3.8 clock function is deprecated in time library, I have modified the from time import clock to from time import time as clock (cool trick, right!?).", "In this code, time_budget (as the name implies) is the time we can do as many MCTS simulations as possible (line 11). The selection and expansion are done in line 12, the variable turn keeps the current player turn, then the rollout and backpropagation are done in lines 14 and 15 accordingly.", "In line 18, we count the nodes in the tree and store them on variable node_count using the method tree_size which I will add in just in a second.", "This is the code for the whole MCTSAgent class:", "As you see in the code, there are some utility functions. best_move method (In line 140) returns the best move based on N value (The node which we have the most number of simulations. The more simulations, the better its explored path).", "The move method is used to play the move in the actual game environment. In this way, we have to update the tree root and rootstate. The new root would be the child of the current root with move key of the root.children dictionary. The new rootstate would be the converted state which move has been played in the game.", "So when we transit to a new state in the game environment, we have to remove the connection to its parent (line 165), we also change the rootstate (line 167).", "Note that the reason I inserted lines 172 and 173 is that in the game GUI, selecting moves without considering turns is available (like black player choose 2 cells in a row and not letting the white player choose the move in its turn). So to enable this flexibility in code, we have to reset the whole tree (Throw out the current tree) and start the MCTS process all over again.", "The method set_gamestate gives us the ability to reset the game state. So we can change the board size and start over again. The method tree_size counts the tree nodes with the help of the Queue library.we also need to define this one in meta.py:", "Lastly, we go for designing the graphical user interface for the framework. The Gui class is developed with the help of tkinter module. This module is easy to understand, so I only explain some important parts. Here go the 500 lines of code!", "Now we have to define the loop in which we are supposed to do the whole process of selection, expansion, simulation, and backpropagation which is provided here:", "At the time I was implementing this code python 3.4 was the newest version. Since in line 7, I have used clock function and in python 3.8 clock function is deprecated in time library, I have modified the from time import clock to from time import time as clock (cool trick, right!?).", "In this code, time_budget (as the name implies) is the time we can do as much MCTS simulations as possible (line 11). The selection and expansion are done in line 12, the turn variable keeps the current player turn, then the rollout and backpropagation are done in lines 14 and 15 accordingly.", "In line 18, we count the nodes in the tree and keep it in node_count variable using tree_size method which I will add it just in a second.", "This is the code for the whole MCTSAgent class:", "As you see in the code, there are some utility functions. best_move method (In line 140) returns the best move based on N value (The node which we have the most number of simulations. The more simulations, the better its explored path).", "The move method is used to play the move in the actual game environment. In this way, we have to update the tree root and rootstate. The new root would be the child of the current root with move key of the root.children dictionary. The new rootstate would be the converted state which move has been played in the game.", "So when we transit to a new state in the game environment, we have to remove the connection to its parent (line 165), we also change the rootstate (line 167).", "Note that the reason I inserted lines 172 and 173 is that in the game GUI, selecting moves without considering turns is available (like black player choose 2 cells in a row and not letting the white player choose the move in its turn). So to enable this flexibility in code, we have to reset the whole tree (Throw out the current tree) and start the MCTS process all over again.", "The method set_gamestate gives us the ability to reset the game state. So we can change the board size and start over again. The method tree_size counts the tree nodes with the help of the Queue library.we also need to define this one in meta.py:", "Lastly, we go for designing the graphical user interface for the framework. The Gui class is developed with the help of tkinter module. This module is easy to understand, so I only explain some important parts. Here go the 500 lines of code!", "Now it\u2019s time to run the code:Add main.py and copy-paste this code in there:", "In this article, we implemented a Monte Carlo Tree Search algorithm in pure python with the least possible external libraries included. The complete code of the project can be found here. This project is a simple UCT algorithm implementation. Although the UCT algorithm is accurate it takes a lot of simulations to converge to available optimal solutions. To boost up the algorithm performance, there are plenty of options like RAVE, Decisive move, Last good reply, and PoolRAVE, \u2026 algorithms. These algorithms will be further explained in the next articles.", "By the way, If you\u2019re looking for an experienced researcher to join your team, I\u2019m totally available.I have some experience in reinforcement learning, computer vision, and data science. You can find me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa9c412ebeff5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://m-m-moghadam.medium.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Masoud Masoumi Moghadam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9c75820911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=post_page-5c9c75820911----a9c412ebeff5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ryoji__iwata?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ryoji Iwata"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-a-case-study-along-with-implementation-part-1-ebc7753a5a3b", "anchor_text": "reinforcement learning basics"}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43", "anchor_text": "Monte Carlo Tree Search"}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "masouduut94/MCTS-agent-pythonMonte Carlo Tree Search (MCTS) is a method for finding optimal decisions in a given domain by taking random samples in\u2026github.com"}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "Github"}, {"url": "https://www.researchgate.net/profile/Kenny_Young", "anchor_text": "Kenny young"}, {"url": "https://github.com/kenjyoung/mopyhex", "anchor_text": "mopyhex"}, {"url": "https://en.wikipedia.org/wiki/Disjoint-set_data_structure", "anchor_text": "Wikipedia"}, {"url": "https://medium.com/better-programming/using-disjoint-set-union-find-to-build-a-maze-generator-7462ea3b8632", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43", "anchor_text": "here"}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/ai?source=post_page-----a9c412ebeff5---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a9c412ebeff5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a9c412ebeff5---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----a9c412ebeff5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a9c412ebeff5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=-----a9c412ebeff5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=-----a9c412ebeff5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa9c412ebeff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a9c412ebeff5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a9c412ebeff5--------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Masoud Masoumi Moghadam"}, {"url": "https://m-m-moghadam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "63 Followers"}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "https://www.linkedin.com/in/masoud-masoumi-moghadam/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9c75820911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=post_page-5c9c75820911--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F181bb37885f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-a9c412ebeff5&newsletterV3=5c9c75820911&newsletterV3Id=181bb37885f2&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}