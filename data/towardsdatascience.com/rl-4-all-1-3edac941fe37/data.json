{"url": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "time": 1683005696.2613492, "path": "towardsdatascience.com/rl-4-all-1-3edac941fe37/", "webpage": {"metadata": {"title": "Teaching machines to behave: Reinforcement Learning | by Diego Gomez Mosquera | Towards Data Science", "h1": "Teaching machines to behave: Reinforcement Learning", "description": "The purpose of this article is to provide and entry point for anyone into Reinforcement learning. I intend to provide the fundamentals necessary to comprehend what Reinforcement Learning algorithms\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Pi", "anchor_text": "greek letter pi", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman optimallity equations", "paragraph_index": 46}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "dynamic programming", "paragraph_index": 46}, {"url": "http://diegogom.com", "anchor_text": "diegogom.com", "paragraph_index": 51}], "all_paragraphs": ["The purpose of this article is to provide and entry point for anyone into Reinforcement learning. I intend to provide the fundamentals necessary to comprehend what Reinforcement Learning algorithms are, how they work. Follow up articles will go deeper in the specific algorithms and implementation. Make sure to subscribe.", "TL;DR: At the end of the article you will find a summary of all the learned material.", "Reinforcement Learning (RL) is the science that aims to create machines that learn how to behave optimally in specific contexts. This domain can be considered as a sub-field of Artificial Intelligence, if we define intelligent agents as those which act optimally. One such intelligent agent would be a system that learns how to play chess at the level of a world-champion.", "Notice that, as in all machine learning sub-fields, RL programs are not coded explicitly to perform optimally in some given task. On the contrary, the motivation behind this science is to design generic algorithms that can continuously learn from their environment, such that they can then use the learned knowledge to excel in particular tasks for which they were designed.", "RL is inspired by behavioral psychology, a field of science that focuses on how the behavior of individuals can change through reinforcement and repetition. In a scenario like this, correct behavior is rewarded and undesired behavior is punished; think of a typical conditioning method for dogs: reward a dog when it behaves well, but punish it when it chews on your shoes. With enough interactions, an individual could learn the optimal behavior that maximizes its total reward, or equivalently minimize its punishments.", "The total reward can be considered as a sum of independent rewards given at each time step during the life of an agent.", "In order to maximize its total reward, a RL agent needs to learn how to behave optimally. This means, it has to spend some of its time searching for what the best actions are at every moment. Testing for unknown action paths may waste the time an agent has (say, if the agent has a limited amount of time, like most living creatures), time which could be used to obtain higher rewards following better-known paths. This implies that a balance must be found, between exploring the environment and using the already acquired knowledge. This dilemma is named exploration vs exploitation in reinforcement learning, and it is a recurrent task that needs to be solved when engineering similar types of incumbents.", "The focus of Reinforcement Learning is the development of agents that learn to take actions in an environment, so as to maximize some notion of cumulative reward.", "To build an agent with these characteristics, we make use of mathematical frameworks that allows us to model the execution of our agent. As you will see, mathematical modeling of a problem allows engineers to implement them in computers too.", "Let\u2019s assume that the interaction between an agent and an environment goes like this: an agent observes the environment state s at a point in time t, and according to what it sees, decides to execute a given action a. At a following time t+1, say one second later, the agent receives a reward/punishment r for having executed that action; this cycle restarts at time t+1, when the agent observes the new state of the environment s.t+1. The cycle is either repeated forever or until a final time-step T.", "If the environment ever reaches a final state (e.g. reaching a check-mate in a chess game), we consider this interaction episodic: it has a start and an end. Contrarily, tasks that don\u2019t finish at any point in time are named continuous (our universe could be one of such environments). The reason we need to distinguish these two will become evident in the following sections.", "Formulating the interaction in terms of states, actions and rewards allows us to frame the problem in the mathematical framework called the Markov Decision Process (MDP, for short). You can think of the MDPs as mathematical models that allow us to simplify this high-level concept of interaction into well defined elements. Specifically, a MDP can model interactions using a three-element tuple:", "Backup diagrams are very useful tools that allow us to better visualize MDPs. We can imagine the possible interactions between an agent and an environment as a graph, where each state s in S is a node, every possible action from a given state A(s) is an action node, and the edges are the reward probabilities for each next state s\u2019 with their associated reward r.", "Let\u2019s illustrate transition probability functions through a backup diagram. Let\u2019s assume you are situated in a scenario where you could either buy a lottery ticket and win $1000 with a probability of one in a thousand (0.1%), lose $10 with a probability of 99.9% or don\u2019t buy and neither win or lose money, with a 100% chance.", "All other combinations of transitions are impossible, which means they have a zero-probability of occurring. To enumerate a few:", "State-transitions are defined stochastically (in simpler words, with a probability function), allowing random environments such as a Poker game to be modeled with MDPs. In a deterministic environment (non-stochastic, such as Tic-tac-toe), an agent starting in state s which takes action a will always transition to a state s' and receive the same reward r.", "Optimal agents themselves aim to maximize the cumulative reward, instead of individual rewards. An intelligent agent would rather earn small rewards during its trajectory and end up with a positive cumulative sum, than earn high positive and low negative rewards over time, concluding with a low cumulative sum. The sum of all rewards G after a given time t is defined as the return. This return is the value that we want our agent to learn to maximize.", "If the interaction is episodic, this sum G(t) will be bounded, as there is a maximum number of steps: T.", "Conversely, the cumulative sum G(t) will tend to +infinity or -infinity in a continuous interaction, given that the final time T does not exist. This is because there will be an infinite number of rewards.", "In general, modeling an infinite return is not useful, because it doesn\u2019t allow us to optimize the behavior of our agent. To solve this problem, a very clever idea was proposed: discounting. It consists of giving an agent the \u201ccapability\u201d of prioritizing/weighting shorter-term rewards. In this way, long-term rewards are discounted exponentially (this means that each following reward\u2019s value decreases more than the previous one). As you will see, this enables us to get rid of the infinity-reward problem.", "A useful analogy is to understand discounting as the agent becoming short-sighted in its cumulative reward calculation.", "In practical terms, we will multiply each reward by a discounting-factor (\ud835\udefe; greek word \u2018gamma\u2019) with an exponential term that becomes greater the further in time each reward is.", "Let\u2019s exemplify how discounted rewards work. Let\u2019s arbitrary decide that at each step, the reward R is 1. This is illustrated in the graphs below, where the x-axis is the time step, and the y axis is the calculated reward for the corresponding time step.", "If we apply a discounting value of \ud835\udefe=0.9, the reward at each step R * gamma becomes exponentially decreasing. This means, at each subsequent time step, the value decreases proportionally more than what it had already decreased in previous steps.", "If we were to calculate the Return G (y-axis in the graph below) only using rewards until time t (x-axis in the graph below), you will see that the reward becomes bounded to a certain value. Considering all rewards until infinity, in the example below, provides a bounded return G of 10.", "Computing this same value, without any discounting, would provide us a value of infinite, as we incorporate more and more time-steps into our return G calculation.", "It is important to have in mind:", "This technique can also be used to model episodic tasks. If the discounting-factor is set to one, and all rewards after time T are set to zero, it will model an episodic task.", "Many RL algorithms involve estimating value functions that quantitatively describe \u201chow good\u201d it is to be for an agent to position itself in a state s. The notion of value is defined in terms of the return or cumulative future rewards an agent could obtain when starting at a given state.", "Assigning rewards to states allows an agent to understand the its surrounding environment, providing insights on what the optimal strategy is. When a tic-tac-toe agent is set to play for the first time, its value function may assign zero to all states, since they are unknown. By following a predefined behavior, the agent can learn what states resulted in wins and losses, and update the value functions accordingly. A state value represents the expected reward of starting in a state s and following policy \u03c0 (greek letter pi), the agent\u2019s behavior. A state that directly leads to a check-mate in a chess board would be assigned a value of 1 for the winning player, and -1 for the losing one.", "Value functions can also be defined for a [state, action] pair. They estimate the return of taking an action a starting at state s.", "Policies, then, are the guidelines or decisions an agent takes at each state.", "A policy P can be understood as the probability of selecting an action a\u1d62 when in state s.", "Formally, a policy is defined as the probability distribution function over available actions a, given an input state s. They may not necessarily be deterministic since an agent\u2019s policy could be stochastic, meaning that it would chose a decision with certain degree of randomness.", "For instance, a random-policy will provide equal probability (equivalently use a uniform distribution) to each action available from state s, and during execution an action will be sampled from the policy. Using stochastic functions to describe a policy allows agents to handle certain degrees of randomness, a characteristic that would be valuable when positioned in environments where there is uncertainty (e.g. a poker game).", "Two types of policies can be defined in this framework:", "The ultimate objective of a Reinforcement Learning agent is to learn the policy that provides the highest reward. This is what\u2019s called an optimal policy, which is denoted by \u03c0*. For such an optimal policy, the expected return is grater or equal to the expected return of any other policy, at any state. In other words, the expected reward of following \u03c0* at every possible state is at least equal to, if not higher, than when using any other possible policy (\u03c0\u2019).", "In order to determine the unknown optimal policy \u03c0*, let\u2019s use what we have learned so far. First, let\u2019s define v*(s) as the optimal state-value function, achieved if we followed an optimal policy \u03c0*. Similarly, a* is the optimal state-action-value function, obtained if followed an optimal policy \u03c0*.", "Assuming that the optimal value functions v* and a* are known, but the optimal policy is not, it is possible to build an optimal policy in the following ways:", "There is a big benefit of using q*(s,a) v*(s). Even though v*(s) is a simpler value function, since it only depends on the possible number of states |S| (size of S), it requires knowledge of the environment. Generating an optimal policy with v* requires knowing the possible states s\u2019 reachable from an initial state s. Conversely, generating an optimal policy with q*(s, a) demands only knowledge of the initial state sand the possible actions available in that state.", "But, how can we determine the optimal value functions when we don\u2019t already know the optimal policy? The equation:", "does not provide much information on how to obtain a policy that maximizes the value functions: the optimal policy. We would have to compute the value functions for all possible policies, and then choose the one that provides the maximum value. Luckily, we can define the optimal value functions in a way that does not depend on a specific policy and that allow us to calculate them.", "Specifically, we will rephrase the value function of a given state depend on the value functions of other states. Mathematically speaking, we will define them recursively.", "By definition, the optimal value function v*(s) is the expressed as the expected return G, starting at state s and taking an action a that maximizes this value.", "The optimal value for state s is the expected value of the following reward plus the discounted optimal value at a following state s', when taking the the optimal action.", "The optimal value when taking action a at state s is the expected value of the following reward plus the discounted optimal value at a following state s' and taking the subsequent optimal action a'.", "These equations are called Bellman optimallity equations, after the American mathematician Richard Bellman, one of the first who described them. Bellman optimality equations can be solved numerically (e.g. using a computer) with optimization methods like dynamic programming.", "It is possible, then, to calculate the optimal value function for a state s, if we have an approximation of the value function for all subsequent steps s'. This should allow us to learn an optimal policy.", "Nonetheless, it would be best to leave this explanation for follow up articles. It is a lot of information to digest already, and I am happy you got to this point.", "In my next articles I will debrief the fundamental and state of the art techniques used to do this. While I write them down:", "[2] Sutton, R. and Barto, A. (n.d.). Reinforcement learning. 2nd ed. Cambridge, Massachusetts: The MIT Press.", "ML Engineer \ud83d\udc7e Googler \ud83c\udf88 Passionate about science \ud83d\udd2d Find me at: diegogom.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3edac941fe37&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/tag/rl-fundamentals", "anchor_text": "Reinforcement Learning Fundamentals"}, {"url": "https://medium.com/@diegoalejogm0?source=post_page-----3edac941fe37--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Diego Gomez Mosquera"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2ae3e490bdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=post_page-2ae3e490bdf8----3edac941fe37---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3edac941fe37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----3edac941fe37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3edac941fe37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=-----3edac941fe37---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Floating-point_arithmetic#Floating-point_numbers", "anchor_text": "rational number"}, {"url": "https://en.wikipedia.org/wiki/Pi", "anchor_text": "greek letter pi"}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman optimallity equations"}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "dynamic programming"}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f", "anchor_text": "my most recent article explaining GANs"}, {"url": "https://github.com/diegoalejogm/Reinforcement-Learning", "anchor_text": "Reinforcement Learning Github repository"}, {"url": "https://github.com/diegoalejogm/deep-q-learning", "anchor_text": "Deep-Q-learning"}, {"url": "https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ", "anchor_text": "Get started"}, {"url": "http://GitHub.com/diegoalejogm", "anchor_text": "diegoalejogm"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3edac941fe37---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----3edac941fe37---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----3edac941fe37---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----3edac941fe37---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/rl-fundamentals?source=post_page-----3edac941fe37---------------rl_fundamentals-----------------", "anchor_text": "Rl Fundamentals"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3edac941fe37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----3edac941fe37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3edac941fe37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----3edac941fe37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3edac941fe37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=post_page-----3edac941fe37--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2ae3e490bdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=post_page-2ae3e490bdf8----3edac941fe37---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F426ba88d9540&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&newsletterV3=2ae3e490bdf8&newsletterV3Id=426ba88d9540&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----3edac941fe37---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Written by Diego Gomez Mosquera"}, {"url": "https://medium.com/@diegoalejogm0/followers?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "909 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://diegogom.com", "anchor_text": "diegogom.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2ae3e490bdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=post_page-2ae3e490bdf8----3edac941fe37---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F426ba88d9540&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frl-4-all-1-3edac941fe37&newsletterV3=2ae3e490bdf8&newsletterV3Id=426ba88d9540&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----3edac941fe37---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Diego Gomez Mosquera"}, {"url": "https://medium.com/ai-society?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "AI Society"}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow\u201cThe coolest idea in deep learning in the last 20 years.\u201d \u2014 Yann LeCun on GANs."}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "\u00b717 min read\u00b7Feb 1, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fai-society%2Fcb03cdcdba0f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fai-society%2Fgans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----cb03cdcdba0f----0-----------------clap_footer----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f?source=author_recirc-----3edac941fe37----0---------------------6ecb7963_0734_4225_a779_d076cedec78c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb03cdcdba0f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fai-society%2Fgans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f&source=-----3edac941fe37----0-----------------bookmark_preview----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3edac941fe37----1---------------------6ecb7963_0734_4225_a779_d076cedec78c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----3edac941fe37----1-----------------bookmark_preview----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3edac941fe37----2---------------------6ecb7963_0734_4225_a779_d076cedec78c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3edac941fe37----2-----------------bookmark_preview----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/cyclic-generative-networks-cyclegans-9b9526b2731c?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Diego Gomez Mosquera"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/cyclic-generative-networks-cyclegans-9b9526b2731c?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "Cyclic Generative Networks: CycleGansOthers who with the help of their intelligence, transform a yellow spot into sun \u2014 Pablo Picasso"}, {"url": "https://towardsdatascience.com/cyclic-generative-networks-cyclegans-9b9526b2731c?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": "\u00b78 min read\u00b7May 10, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b9526b2731c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcyclic-generative-networks-cyclegans-9b9526b2731c&user=Diego+Gomez+Mosquera&userId=2ae3e490bdf8&source=-----9b9526b2731c----3-----------------clap_footer----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/cyclic-generative-networks-cyclegans-9b9526b2731c?source=author_recirc-----3edac941fe37----3---------------------6ecb7963_0734_4225_a779_d076cedec78c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b9526b2731c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcyclic-generative-networks-cyclegans-9b9526b2731c&source=-----3edac941fe37----3-----------------bookmark_preview----6ecb7963_0734_4225_a779_d076cedec78c-------", "anchor_text": ""}, {"url": "https://medium.com/@diegoalejogm0?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "See all from Diego Gomez Mosquera"}, {"url": "https://towardsdatascience.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----3edac941fe37----0-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&user=Andrew+Austin&userId=42d388912d13&source=-----614846383eb7----1-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&source=-----3edac941fe37----1-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich/transfer-learning-in-reinforcement-learning-45160ceb31c4?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Mauricio Fadel Argerich"}, {"url": "https://medium.com/@mauriciofadelargerich/transfer-learning-in-reinforcement-learning-45160ceb31c4?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Transfer Learning in Reinforcement LearningA quick review of how transfer learning improves the performance of RL on new, unseen tasks by exploiting learnings from past tasks."}, {"url": "https://medium.com/@mauriciofadelargerich/transfer-learning-in-reinforcement-learning-45160ceb31c4?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "\u00b78 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F45160ceb31c4&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mauriciofadelargerich%2Ftransfer-learning-in-reinforcement-learning-45160ceb31c4&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=-----45160ceb31c4----0-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich/transfer-learning-in-reinforcement-learning-45160ceb31c4?source=read_next_recirc-----3edac941fe37----0---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45160ceb31c4&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mauriciofadelargerich%2Ftransfer-learning-in-reinforcement-learning-45160ceb31c4&source=-----3edac941fe37----0-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3edac941fe37----1---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3edac941fe37----1-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----2-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----3edac941fe37----2---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----3edac941fe37----2-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/the-power-of-ai/blackjack-with-reinforcement-learning-95f588dd670c?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@artem.a.arutyunov?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/@artem.a.arutyunov?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Artem Arutyunov"}, {"url": "https://medium.com/the-power-of-ai?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "The Power of AI"}, {"url": "https://medium.com/the-power-of-ai/blackjack-with-reinforcement-learning-95f588dd670c?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "Win at Blackjack with Reinforcement LearningAs a popular casino card game, many have studied Blackjack closely in order to devise strategies for improving their likelihood of winning."}, {"url": "https://medium.com/the-power-of-ai/blackjack-with-reinforcement-learning-95f588dd670c?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": "13 min read\u00b7Dec 30, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fthe-power-of-ai%2F95f588dd670c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fthe-power-of-ai%2Fblackjack-with-reinforcement-learning-95f588dd670c&user=Artem+Arutyunov&userId=8d26b20a79d4&source=-----95f588dd670c----3-----------------clap_footer----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/the-power-of-ai/blackjack-with-reinforcement-learning-95f588dd670c?source=read_next_recirc-----3edac941fe37----3---------------------3501aea0_8f59_4754_833f_96dfa0457b82-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95f588dd670c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fthe-power-of-ai%2Fblackjack-with-reinforcement-learning-95f588dd670c&source=-----3edac941fe37----3-----------------bookmark_preview----3501aea0_8f59_4754_833f_96dfa0457b82-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3edac941fe37--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----3edac941fe37--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}