{"url": "https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491", "time": 1683008978.02971, "path": "towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491/", "webpage": {"metadata": {"title": "Cross-Attention is what you need! | by Satyam Mohla | Towards Data Science", "h1": "Cross-Attention is what you need!", "description": "Today, with recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral (MSI)\u2026"}, "outgoing_paragraph_urls": [{"url": "http://openaccess.thecvf.com/content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html", "anchor_text": "FusAtNet: Dual Attention Based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification", "paragraph_index": 22}], "all_paragraphs": ["FusAtNet: Dual Attention Based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification", "Satyam Mohla et. al, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020, pp. 92\u201393", "Today, with recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral (MSI), hyperspectral (HSI), LiDAR etc. are available.", "Effective fusion of these multi source datasets is becoming important, for these multi-modality features have been shown to generate highly accurate land-cover maps. However, fusion in the context of RS is non-trivial considering the redundancy involved in the data and the large domain differences among multiple modalities. In addition, the feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness.", "Why is a single fused representation important?", "Several advantages of combining multimodal images, including :", "Interestingly, most common methods today often just use methods like early concatenation, CNN extracted feature level concatenation or multi-stream decision level fusion methods, totally overlooking cross-domain features. Visual attention, a recent addition to the deep-learning-researchers\u2019 toolbox is largely unexplored in multi-modal domain.", "A question arises: How to best fuse these modalities for a joint, rich representation which can be used in downstream tasks?", "An ideal fusion method would synergistically combine the two modalities and ensure that the resultant product reflects the salient features of input modalities.", "In this work, we propose a new concept of \u201ccross-attention and propose attention based HSI-LiDAR fusion in the context of land-cover classification.", "Cross attention is a novel and intuitive fusion method in which attention masks from one modality (hereby LiDAR) are used to highlight the extracted features in another modality (hereby HSI). Note that this is different from self-attention where attention mask from HSI is used to highlight its own spectral features.", "We propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data in this paper. The proposed framework effectively utilizes HSI modality to generate an attention map using \u201cself-attention\u201d mechanism that highlights its own spectral features. Similarly, a \u201ccross-attention\u201d approach is simultaneously used to harness the LiDAR-derived attention map that accentuates the spatial features of HSI. These attentive spectral and spatial representations are then explored further along with the original data to obtain modality-specific feature embeddings. The modality oriented joint spectro-spatial information thus obtained, is subsequently utilized to carry out the land-cover classification task.", "Experimental evaluations on three HSI-LiDAR datasets show that the proposed method achieves the state-of-the-art classification performance, including on the largest HSI-LiDAR benchmark dataset available, Houston, opening new avenues in multimodality feature fusion classification.", "It is clearly visible for all the cases that our method outperforms all the state of the art methods with a significant margin in all the avenues, be it OA (the respective accuracies of Houston, Trento and MUUFL datasets being 89.98%, 99.06% and 91.48%), AA (respective values being 94.65%, 98.50% and 78.58%) or \u03ba. It is also easily observed that in case of classwise/producer\u2019s accuracy, the performance of our method is better than the other methods for most of the classes and only marginally exceeded by other methods for a few of them. For Houston dataset, it can be noted that the accuracy for \u2018commercial\u2019 class (92.12%) is significantly improved for our method in comparison to other methods. This can be attributed to the fact that commercial regions generally have a variable layout with frequent elevation changes that are effectively captured by LiDAR based attention maps. It is also observed in Houston classification maps, that methods such as SVM and two-branch CNN tend to classify the shadowy areas as water (in the right portion of the maps) because of their darker tone. Our approach largely mitigates this problem as well.", "Similarly, in the case of Trento dataset, the \u2018road\u2019 class shows a notable increase in accuracy (93.32%). This increment is also on account of variation in road profile with respect to its elevation.", "Also, it can be visually verified, for example, in MUUFL, that the classification maps obtained from FusAtNet tend to be less noisy and have smooth interclass transitions.", "We further carried out different ablation studies to highlight the individual aspects of our model.", "Table 4 denotes the performance in presence of various attention layers as described in the network, demonstrating the need for all three attention modules.", "Table 5 denotes the performance around data augmentation.", "Table 6 denotes performance with reduced training data.", "Besides the obvious observation, that data augmentation & increase in training data increases accuracy, it is interesting to note that our network starts outperforming existing SOTA methods with just 50% training data.", "To summarize, our work is one of the first approaches to introduce the notion of attention learning for HSI-LiDAR fusion in the context of land-cover classification. In this regard, we introduce the concept of \u201ccross-attention\u201d based feature learning among the modalities, a novel and intuitive fusion method which utilises attention from one modality (here LiDAR) to highlight features in the other modality (HSI). We demonstrated state-of-the-art classification performance on three benchmark HSI-LiDAR datasets outperforming all existing deep fusion strategies.", "FusAtNet: Dual Attention Based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification", "Satyam Mohla, Shivam Pande, Biplab Banerjee, Subhasis Chaudhuri; The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020, pp. 92\u201393", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A nomad with stories to tell."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8e6f673491&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e6f673491--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@satyammohla?source=post_page-----b8e6f673491--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyammohla?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Satyam Mohla"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb986c709b5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&user=Satyam+Mohla&userId=eb986c709b5a&source=post_page-eb986c709b5a----b8e6f673491---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://cvpr2020.thecvf.com/", "anchor_text": "New results"}, {"url": "http://openaccess.thecvf.com/content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html", "anchor_text": "FusAtNet: Dual Attention Based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b8e6f673491---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----b8e6f673491---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b8e6f673491---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/ai?source=post_page-----b8e6f673491---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/classification?source=post_page-----b8e6f673491---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&user=Satyam+Mohla&userId=eb986c709b5a&source=-----b8e6f673491---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&user=Satyam+Mohla&userId=eb986c709b5a&source=-----b8e6f673491---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb8e6f673491&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b8e6f673491---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b8e6f673491--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8e6f673491--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8e6f673491--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b8e6f673491--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b8e6f673491--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyammohla?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyammohla?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Satyam Mohla"}, {"url": "https://medium.com/@satyammohla/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb986c709b5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&user=Satyam+Mohla&userId=eb986c709b5a&source=post_page-eb986c709b5a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Feb986c709b5a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491&user=Satyam+Mohla&userId=eb986c709b5a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}