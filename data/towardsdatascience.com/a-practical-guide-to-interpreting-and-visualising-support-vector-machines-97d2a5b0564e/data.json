{"url": "https://towardsdatascience.com/a-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e", "time": 1682994615.684122, "path": "towardsdatascience.com/a-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e/", "webpage": {"metadata": {"title": "A Practical Guide to Interpreting and Visualising Support Vector Machines | by HD | Towards Data Science", "h1": "A Practical Guide to Interpreting and Visualising Support Vector Machines", "description": "In machine learning linear classifiers are any model in which there is a single hypothesis function which maps between model inputs and predicted outputs. The primary advantage of linear models over\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c", "anchor_text": "guide", "paragraph_index": 2}, {"url": "http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.41.1639&rep=rep1&type=pdf", "anchor_text": "original paper", "paragraph_index": 21}, {"url": "https://www.researchgate.net/publication/221649277_Solving_and_Interpreting_Binary_Classification_Problems_in_Marketing_with_SVMs", "anchor_text": "here", "paragraph_index": 27}, {"url": "http://www.cs.ox.ac.uk/people/vasile.palade/papers/Class-Imbalance-SVM.pdf", "anchor_text": "here", "paragraph_index": 45}, {"url": "http://www.jair.org/index.php/jair/article/view/10302.", "anchor_text": "www.jair.org/index.php/jair/article/view/10302.", "paragraph_index": 50}], "all_paragraphs": ["Hugo Dolan is an undergraduate Financial Mathematics student at University College Dublin. This is mostly based and motivated by recent data analytics and machine learning experiences in the NFL Punt Analytics Kaggle Competition and the being part of the team who won the Citadel Dublin Data Open, along with material from Stanford\u2019s CS229 online course.", "This article contains the following sections:", "This article will assume familiarity with basic ML models such as Logistic and Linear Regression. It will also assume you know how to draw some of the graphs I discuss (I have a guide for this if your stuck!). We will also assume that you know what a decision function and objective / cost function is and that you have some basic linear algebra knowledge. If not its still worth a read you can always bookmark this and come back later for a deeper pass on some of the more mathematical parts of this article.", "In machine learning linear classifiers are any model in which there is a single hypothesis function which maps between model inputs and predicted outputs.", "The primary advantage of linear models over neural networks (a non linear model) is that the feature weights directly correspond to the importance of the feature within the model. Thus it is easy to understand what the model has \u2018learned\u2019.", "At the core of any linear model is a dot product between the input example and the parameter / weight vector. In the case of linear regression this is the entire hypothesis function. Where as logistic regression feeds the dot product through a sigmoid function such that the output is between 0 and 1 and hence is suitable for binary classification problems.", "When considering classification problems the downfall of linear models is that ultimately the decision boundary is a straight line, plane or hyperplane with coefficients equal to the models weights / parameters and thus can only classify data which is linearly separable, which could be a big limitation when working on more complex analytics problems.", "The Support Vector Machine (SVM) is the only linear model which can classify data which is not linearly separable.", "You might be asking how the SVM which is a linear model can fit a linear classifier to non linear data. Intuitively with a simple linear regression model we may manually engineer x, x\u00b2, x\u00b3,\u2026 features to attempt to achieve a fit to a non linear set of data points.", "Transferring this intuition to our SVM, when we engineer the x\u00b2 feature we are essentially multiplying feature x by itself. So suppose we engineer features from our dataset by multiplying combinations of features x1,x2,x3\u2026 together, then theoretically we *could* end up with a space in which your engineered features are linearly separable. Taking the previous simple example look at how the data below is transformed to an almost linear trend in the x\u00b3 feature space.", "Unfortunately to achieve this with a complex dataset requires creating more than just a 3 dimensional space (features x, x\u00b2,x\u00b3) but in fact extremely high dimensional feature spaces which would be computationally very expensive to calculate for every example in our dataset. Below I show an example of a function \u00f8(x) which takes our original features x and combines them to create many 2nd order polynomial features.", "Before we proceed: I will use the notation of x to denote data points / training examples with superscripts to denote a particular data point and subscripts to denote a particular feature.", "Luckily for us there is a way out of this computational complexity conundrum! When we derive the optimisation problem for an SVM (The complex looking formula which tells us how to derive and update our weights for maximisation during coordinate ascent) it turns out that our feature vectors for our training input x appears in only a single place within the entire optimisation formula (highlighted in red).", "This dot product was for our original feature space so now lets replace it with our engineer feature space using our function \u00f8.", "So how does this help reduce the computational complexity? Well by definition of the dot product, we take the i-th entry of \u00f8(x(i)) and multiply it by the i-th entry of \u00f8( x(j)) and then sum all of these up to obtain a single scaler. Applying this we get:", "The kernel trick is a really simple rearrangement of the original equation we can see that we\u2019ve totally removed \u00f8(x) and only have to perform computations using our original input features, but still have the effect of computing a high dimensional space.", "All we have to do now is substitute the dot product involving \u00f8(x) with the kernel equivalent ( K(x^i, x^j) ):", "Similarly when we want to use our model to make predictions we never explicitly calculate the weights for our high dimensional space but instead use the kernel trick to make our predictions:", "In summary we can use the kernel trick to transform a non linear data set into a data set which is linearly separable, just in a higher dimensional space. Sklearn come prepackage with a number of kernels in the SVC implementation, including Radius Basis Kernel (RBF) and Polynomial Kernels, each have their own hyper parameters which can be adjusted experimentally using cross validation to achieve the best results.", "So remember how we said the great benefit of a linear model was that the weights / parameters of the model could be interpreted as the importance of the features. Well thats gone once we engineer a high or infinite dimensional feature set, the weights of the model implicitly correspond to the high dimensional space which isn\u2019t useful in aiding our understanding.", "Instead what we can do is fit a logistic regression model which estimates the probability of label y being 1, given the original features, where f(x) is the SVM decision function:", "We use maximum likelihood estimation to fit the parameters of this logistic regression model, the technique is called Platt Scaling, the original paper [3] is definitely worth reading if your curious about the inner workings.", "So how does this help us understand how the SVM works? Well we simply fit the model and select a point in our dataset to evaluate it at, then perturb one feature at a time through a range of values, whilst keeping the other features fixed. We can use this to draw a graph of the sensitivity of the model to each feature.", "SKlearn comes with this feature built into the SVC model, you just have to make sure the probability=true, when initialising and then use the clf.predict_proba(X) function to obtain the probabilities.", "In practice I found that rather than just evaluating round a single point it is often better to sample a collection of relevant points eg. 40 negative examples and average the probability distribution by feature, to get something more representative.", "Here\u2019s an example I did when I was working on the NFL Punt Analytics Kaggle Competition, examining effects of various factors on concussions:", "I took all the negative examples and averaged the probabilities across them, i\u2019ve highlighted the areas in red for each feature where players were most likely to receive a concussion. A trick if you\u2019ve a bunch of one hot encoded variables like player role, is to aggregate them into a bar chart and just look at the net change in probability between when the feature is present and not present.", "There\u2019s also a great example applied of this applied to marketing data [1] which you can find here. I\u2019d also like to thank Georgi who took the time to answer some of my questions about the paper.", "When your dealing with a high dimensional models involving an SVM, it would be nice to be able to visualise how the model is classifying data points without purely relying on metrics like F1 scores or ROC AUC.", "Whilst some may use techniques such as principle component analysis to visualise classification, in doing so we loose dimensions of our feature space and thus distorts the visual we are looking to achieve.", "A nice technique I found is called \u2018Histogram of projects\u2019 [2], it involves graphing the distribution of output of the SVM decision function for your training and test sets.", "The decision function is easy to obtain in SKlearn\u2019s SVC implementation simply call decision_function(X). You will want to keep track of your datasets labels so you can colour code your histogram of projections as below:", "A histogram of projection is fairly simple to interpret. The histogram x axis identifies the distance of a specific training example from the decision boundary of the SVM (indicated by the central dashed line).", "An SVM has a margin of separation equal to 1 either side of the the decision boundary, this is an enforced constraint of the dual optimisation problem (The \u2018Support Vectors\u2019 are data points which lie along these margins). You\u2019ll notice in the above model there is some leakage into the margin areas and indeed cross over from one class into the class on the opposite side of the decision boundary. This is because we\u2019ve set the regularisation hyper-parameter C > 0 (It allows a tradeoff between some misclassification and minimising the SVM objective function).", "Despite working with high dimensional feature spaces, this chart visualises successfully visualises the decision boundary region and all classification without loss of dimensionality. All the metrics seen in a confusion matrix (ie number of True Positives, False Positives, True Negatives and False Negatives) can also be seen through the histogram. It also enables us to observe whether a model is generalising well to the test set. If the test set has a similar distribution of decision function outputs to the training set then we might say that the model has good performance on generalisation. The model may also be used to determine whether given the selected hyper-parameters whether the data set is linearly separable.", "When a dataset has a disproportionate number of examples for one class relative to another we say it is imbalanced.", "This is a problem if we want to build a ML model to predict occurrences of the minority, as we can achieve high levels of accuracy by simply misclassifying all minority examples as the majority class.", "This occurs commonly in real world data, whether it is identifying malignant tissue, credit card fraud or concussions in a sport, due to the relative rarity of the incidents we wish to correctly identify.", "There are two commonly accepted practices for rectifying ML models working with imbalanced data:", "There are two ways in which we can resample data, either by removing existing examples (Under-sampling) or adding new examples (Over-sampling). The most commonly accepted method is to oversample the minority class using an algorithm called SMOTE (Synthetic Minority Oversampling Technique) [5]", "Its much simpler than the name suggests, for each minority point in the dataset, it selects k nearest other minority examples (typically 5) and interpolates randomly new minority examples along the line \u2018joining\u2019 existing minority examples.", "This is a reasonable thing to do as we are simply making the assumption that by interpolating between similar existing examples we will obtain new examples of the same class.", "This tends to improve the performance of the model significantly and helps to generalises the decision boundary for minority examples.", "Option 2 : Introducing weights into the objective function", "Another procedure which can be employed is to assign higher weights in the objective function for misclassification of minority examples. This will \u2018incentivise\u2019 the algorithm to correctly classify minority classes.", "I\u2019ve no personal experience using this method, but it can be use in conjunction with option 1. This is a link to a good paper here [4] which details many approaches to dealing with the class imbalance", "A reasonable rule of thumb is around 10x as many training examples as features as a minimum. If you have a large amount of training data your best off using fewer than 50,000 training examples as the SVC implementation in sklearn has O(n\u00b3) complexity, meaning the time of convergence to a solution grows cubicly with the number of training examples, it can get pretty slow on even a decent laptop or kaggle container.", "Its often worthwhile training on a smaller dataset first, and tuning your model\u2019s hyper-parameter\u2019s. You can keep a small cross validation test set for model selection. You could be surprised at how well your model generalises when you test on your remaining dataset, despite the small proportion of actual data you may have used.", "Note: If your new to this, a tip is to use sklearn\u2019s train test split module and to fix the random seed so that your results are reproducible if you happen to go back to edit some earlier code and rerun the training / model selection procedure.", "I hope this article has been helpful, if you\u2019ve any comments / questions leave them below, i\u2019ll try to answer them as best I can.", "[5] Chawla, N. V., et al. \u201cSMOTE: Synthetic Minority Over-Sampling Technique.\u201d Journal of Artificial Intelligence Research, www.jair.org/index.php/jair/article/view/10302.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F97d2a5b0564e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://hdln.medium.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": ""}, {"url": "https://hdln.medium.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "HD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18a90075b0a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&user=HD&userId=18a90075b0a1&source=post_page-18a90075b0a1----97d2a5b0564e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c", "anchor_text": "guide"}, {"url": "http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.41.1639&rep=rep1&type=pdf", "anchor_text": "original paper"}, {"url": "https://www.researchgate.net/publication/221649277_Solving_and_Interpreting_Binary_Classification_Problems_in_Marketing_with_SVMs", "anchor_text": "here"}, {"url": "http://www.cs.ox.ac.uk/people/vasile.palade/papers/Class-Imbalance-SVM.pdf", "anchor_text": "here"}, {"url": "http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639", "anchor_text": "http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639"}, {"url": "http://www.jair.org/index.php/jair/article/view/10302.", "anchor_text": "www.jair.org/index.php/jair/article/view/10302."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----97d2a5b0564e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/kernel?source=post_page-----97d2a5b0564e---------------kernel-----------------", "anchor_text": "Kernel"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----97d2a5b0564e---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/tag/visualisation?source=post_page-----97d2a5b0564e---------------visualisation-----------------", "anchor_text": "Visualisation"}, {"url": "https://medium.com/tag/non-linear?source=post_page-----97d2a5b0564e---------------non_linear-----------------", "anchor_text": "Non Linear"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&user=HD&userId=18a90075b0a1&source=-----97d2a5b0564e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&user=HD&userId=18a90075b0a1&source=-----97d2a5b0564e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F97d2a5b0564e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----97d2a5b0564e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----97d2a5b0564e--------------------------------", "anchor_text": ""}, {"url": "https://hdln.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://hdln.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "HD"}, {"url": "https://hdln.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "445 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18a90075b0a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&user=HD&userId=18a90075b0a1&source=post_page-18a90075b0a1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F18a90075b0a1%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e&user=HD&userId=18a90075b0a1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}