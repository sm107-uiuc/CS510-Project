{"url": "https://towardsdatascience.com/is-bert-the-answer-to-everything-1a1dbb52e275", "time": 1683017799.739437, "path": "towardsdatascience.com/is-bert-the-answer-to-everything-1a1dbb52e275/", "webpage": {"metadata": {"title": "Is BERT the answer to everything? | by Eram Munawwar | Towards Data Science", "h1": "Is BERT the answer to everything?", "description": "With the onset of the Transformer, there has been a rapid rise in language models. In 2018, BERT came and broke all records. However, shortly after BERT, a long list of its cousins were born, like\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://www.gwern.net/docs/psychology/writing/1953-taylor.pdf", "anchor_text": "Taylor", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "PLM", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1905.02450", "anchor_text": "MASS", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/2003.10555", "anchor_text": "Electra", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/2003.08271.pdf", "anchor_text": "paper", "paragraph_index": 24}], "all_paragraphs": ["With the onset of the Transformer, there has been a rapid rise in language models. In 2018, BERT came and broke all records. However, shortly after BERT, a long list of its cousins were born, like RoBERTa, ALBERT, StructBERT, DistilBERT, to name a few.", "BERT is essentially trained to optimise two tasks; Masked Language Model (MLM) and Next Sentence Prediction (NSP). Since there are a lot of excellent blogs available on BERT, this post\u2019s focus would be to understand why BERT might not work in some cases and what are the alternatives available.", "Before moving on to the limitations, let us briefly discuss how MLM and NSP work.", "MLM was originally introduced as a \u201cCloze task\u201d by Taylor. In BERT, it is used as an extension to a unidirectional language model. In a standard unidirectional language model, context is taken from one side only. However, the context from the opposite direction might be equally important. For example, when trying to predict the word capital in the sentence: \u201cParis is the capital of France\u201d; words to the left and right of capital are both necessary to make the correct prediction. Since context from both directions cannot be fed into a traditional language model, it is implemented as a Masked Language model in BERT.", "In MLM, some of the words or tokens are masked and the task is to predict those tokens. You can think of it as a fill-in-the-blank task.", "Given two sentences s1 and s2, NSP is the task to predict whether s1 is followed by s2 in the same document.", "For NSP, the inputs are combined as given below:", "[CLS] I am thirsty [SEP] Pour me a drink [SEP]", "[SEP] is the special token which separates the 2 sentences.", "Combining with MLM, the same input will now have masked tokens, as below:", "MLM has proven to be a very useful task for pre-trained language models because of its bidirectional nature.", "However, the main issue with it is the mismatch between pre-training and fine-tuning phases. This is because the MASK token does not appear during fine tuning.", "To deal with this, BERT used the [MASK] token 80% of the time, a random token 10% of the time, and the original token for the remaining 10% of the time to perform masking.", "However, even with this approach, some claim that the mismatch issue still remains unsolved to a large extent. This gave birth to a new pre-training task called Permuted Language Model (PLM). In short, PLM applies random permutation on input sequences and a permutation is randomly sampled from all possible permutations. Some of the tokens are masked which would finally be predicted by this task. Note that this does not affect the original order of tokens. Instead, it just defines the order of token predictions.", "Another major improvement over MLM is dynamic masking. In BERT, all mask tokens are created before training (in the preprocessing step), once and for all. Whereas, in the famous RoBERTa, tokens are masked dynamically. This means that after each epoch, the input sequence will have different tokens masked. RoBERTa has shown quite noticeable improvements over BERT on various downstream tasks.", "There are multiple researches proposing enhanced versions of MLM. Some of them are:", "Many researchers have claimed that NSP is not a necessary task and removing/altering it is a better option. Why is that so?", "As mentioned by the authors of ALBERT, NSP conflates topic prediction and coherence prediction. They say that NSP actually learns whether the two sentences belong to the same topic, which is much easier than learning whether the sentences are grammatically coherent or not.", "Even while training BERT from scratch on custom data, we notice the NSP accuracy spiking up pretty quickly. However, using the same model on a sentence pair fine tuning task gives a bad performance.", "A lot of BERT\u2019s cousins follow a pattern: Remove NSP, use MLM (with or w/o modifications) and add a new task for replacing NSP (or something that works for sentence/segment pair tasks. Some of my personal favourite alternatives are:", "SOP uses two consecutive segments from the same document as a positive example and the same sequences with their order reversed as a negative example.", "Following ALBERT\u2019s improvements over BERT on various downstream tasks, StructBERT also took SOP as a pre-training task.", "As used in MASS, Seq2Seq MLM uses an encoder decoder style training. The encoder is fed a masked sequence, and the decoder sequentially produces the masked tokens in auto-regression fashion.", "Electra is a very interesting model which uses a generator to replace some tokens of a sequence. The job of the discriminator is to identify whether the token is an actual or a replaced one.", "You would be amazed at the number of options available for language models. This paper does a perfect summarisation of many state of the art language models from all aspects: pre-training tasks, architecture types, model extensions etc. I would suggest to definitely give this a read.", "BERT and other pre-trained NLP models have undoubtedly made their mark. More importantly, it has served as a building block for several other research work. However, with so many models available, it becomes impossible to try out all of them for a given task.", "I hope this post will make you more aware of why BERT might not work on your custom data and what alternatives are already available and are handy. Personally, my vote would go to RoBERTa and ALBERT as the go-to models for most of the NLP fine tuning tasks.", "What\u2019s your take on the plethora of NLP models available out there?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An NLP and Deep Learning enthusiast who has been in this field for over 6 years. My focus areas lie in MRC, NLG and NLU. Concepts over Jargon!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a1dbb52e275&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://eram-munawwar.medium.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": ""}, {"url": "https://eram-munawwar.medium.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Eram Munawwar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7615a52ab3c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&user=Eram+Munawwar&userId=7615a52ab3c0&source=post_page-7615a52ab3c0----1a1dbb52e275---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@clarephotolover?utm_source=medium&utm_medium=referral", "anchor_text": "Clarissa Watson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://www.gwern.net/docs/psychology/writing/1953-taylor.pdf", "anchor_text": "Taylor"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "PLM"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/abs/1905.03197", "anchor_text": "UniLM"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "TLM"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT"}, {"url": "https://arxiv.org/abs/1905.02450", "anchor_text": "MASS"}, {"url": "https://arxiv.org/abs/2003.10555", "anchor_text": "Electra"}, {"url": "https://arxiv.org/pdf/2003.08271.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/2003.08271.pdf", "anchor_text": "https://arxiv.org/pdf/2003.08271.pdf"}, {"url": "https://medium.com/tag/bert?source=post_page-----1a1dbb52e275---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1a1dbb52e275---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/language?source=post_page-----1a1dbb52e275---------------language-----------------", "anchor_text": "Language"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----1a1dbb52e275---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a1dbb52e275---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&user=Eram+Munawwar&userId=7615a52ab3c0&source=-----1a1dbb52e275---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&user=Eram+Munawwar&userId=7615a52ab3c0&source=-----1a1dbb52e275---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a1dbb52e275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a1dbb52e275---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a1dbb52e275--------------------------------", "anchor_text": ""}, {"url": "https://eram-munawwar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://eram-munawwar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eram Munawwar"}, {"url": "https://eram-munawwar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "70 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7615a52ab3c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&user=Eram+Munawwar&userId=7615a52ab3c0&source=post_page-7615a52ab3c0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F41a7ac192350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-bert-the-answer-to-everything-1a1dbb52e275&newsletterV3=7615a52ab3c0&newsletterV3Id=41a7ac192350&user=Eram+Munawwar&userId=7615a52ab3c0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}