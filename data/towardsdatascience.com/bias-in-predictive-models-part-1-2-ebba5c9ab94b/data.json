{"url": "https://towardsdatascience.com/bias-in-predictive-models-part-1-2-ebba5c9ab94b", "time": 1682996612.5964239, "path": "towardsdatascience.com/bias-in-predictive-models-part-1-2-ebba5c9ab94b/", "webpage": {"metadata": {"title": "Bias in predictive models \u2014 part 1/2 | by Stas Cherkassky | Towards Data Science", "h1": "Bias in predictive models \u2014 part 1/2", "description": "AI is already playing a significant role in our lives and soon it\u2019s going to make more and more life-changing decisions. The problem of AI fairness and biases is attracting growing attention from\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G", "anchor_text": "story", "paragraph_index": 2}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "this ProPublica analysis", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "anchor_text": "from researchers", "paragraph_index": 5}, {"url": "https://www.nbcnews.com/tech/tech-news/new-bill-aims-stamp-out-bias-algorithms-used-companies-n993186", "anchor_text": "from legislators", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "anchor_text": "this article", "paragraph_index": 6}, {"url": "https://pair-code.github.io/what-if-tool/ai-fairness.html", "anchor_text": "great article", "paragraph_index": 10}, {"url": "https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/", "anchor_text": "Google Translate example", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Ricci_v._DeStefano", "anchor_text": "reached", "paragraph_index": 13}, {"url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1526&context=articles", "anchor_text": "legal discussion", "paragraph_index": 13}, {"url": "https://www.theregister.co.uk/2018/10/31/intel_diversity_report_2018/", "anchor_text": "declared", "paragraph_index": 15}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "in this case", "paragraph_index": 17}], "all_paragraphs": ["AI is already playing a significant role in our lives and soon it\u2019s going to make more and more life-changing decisions. The problem of AI fairness and biases is attracting growing attention from both research and legislators. There exist a few possible definitions of what is bias and how to measure it, each having its own merits, but also \u2014 limitations of applicability.", "Which one to use is an important (and not very simple) choice, that companies and, possibly, regulators would need to make.", "Remember this recent story about Amazon\u2019s AI hiring tool which developed a bias against women? Fortunately, their team was able to identify the problem before the model went into production. AI is already playing a significant role in our lives and soon it\u2019s going to make more and more life-changing decisions. Would you get that job offer? Get approved for that loan? Would your product be recommended? Would your social media account be blocked? Would your child get admitted to that university? Will you be \u201crandomly\u201d chosen for a deep tax audit? And even, will you get an early release from the jail? Not all decisions on this list are already done by AI, but many are, and this list is only going to grow every year.", "In this ProPublica analysis on recidivism rates in Broward County, it was shown (see table below) that African Americans that didn\u2019t re-offend, are almost twice as likely as whites to be (incorrectly) labeled as higher risk.", "Most kinds of AI, if left to their own devices, would perpetuate whatever trends (including biases) that were present in the historical data they were trained on. If women were previously a small part of the workforce that is what the AI would learn (as in Amazon hiring tool story).", "With this much at stake, it\u2019s no wonder that the topic is getting more attention, both from researchers and now \u2014 also from legislators.", "Figures are taken from this article", "The issue poses two high-level problems:", "Both questions are not easy to answer. In this post, we concentrate on the definition of bias and ways to measure it. The ways of solving the issue will be discussed in a follow-up.", "In an ideal world, everyone would want their models to be \u201cfair\u201d, and none would want them to be \u201cbiased\u201d but what does it mean, exactly? Unfortunately, there seems to be no universal and agreed definition of bias or fairness.", "In this great article about different ways to measure model fairness, David Weinberger, writer-in-residence at People + AI Research (PAIR) at Google, presents not less than five (!) different ways to define what is fair, each one with its own merits and drawbacks.", "It\u2019s interesting to note that some of the more intuitive methods, such as \u201cgroup unaware\u201d or \u201cdemographic parity\u201d sometimes turn out as not suitable for real-life problems.", "\u201cGroup unaware\u201d method states that if one totally disregards protected class information (such as gender), the results would reflect the objective merits of applicants which is fair. If, as a result, there would be a disproportional representation of some gender \u2014 this is \u201cwhat data says\u201d. However, in real life, it\u2019s often more complex. Gender information may be hidden in some other variables (proxies), and lower representation of women stemming from possible past discrimination would then creep into our new model (as happened in the Amazon case or this Google Translate example). There are problems like medical diagnosis where chances to have some condition may directly depend on sex. In other problems, the meaning of some parameters may depend on gender indirectly. For example, in CV analysis, certain gaps in employment for women may indicate maternity leave which has different meaning compared to employment gaps when a person couldn\u2019t or wouldn\u2019t get a job. In summary, sometimes it\u2019s ok, or even desirable to refer to protected variables, and not referring to them does not guarantee that there would be no bias.", "Another common method, \u201cDemographic Parity\u201d effectively postulates equality of outcome. Under this postulate, if there were 30% orange applicants, we expect to see 30% orange in approvals. The concept of \u201cDemographic Parity\u201d is very similar to \u201cDisparate Impact\u201d criteria, which is used in the US anti-discrimination legal framework that regulates areas such as Credit, Education, Employment, etc. However, the application of this method is far from obvious. Were these orange applicants as qualified as blue ones? What should be taken as 100%? Some methods to mitigate the disparity can lead to situations where similar people are treated differently, which by itself can be seen as discriminatory. One such case even reached the Supreme Court and the general topic is the subject of legal discussion which is beyond the scope of this overview.", "This assumption of equal outcome may be right in some cases, but is it always correct? Sometimes, the protected variable has a correlation with the output for reasons which have nothing to do with bias. For example, the rate of occurrence of certain medical conditions may vary a lot between different genders or ethnicities.", "Recently, Intel declared that they reached \u201cfull gender representation\u201d with only about 27% female staffers. The naive application of the \u201cDemographic Parity\u201d would, of course, fail because 27% is far less than the rate of females in the general population. Intel could make this claim because they used as reference only females that were the graduates of relevant college degrees, as only them, according to Intel, constitute the pool of qualified female candidates.", "In statistics, the bias of an estimator is defined as the difference between the estimator\u2019s expected value and the true value of the parameter being estimated. However, in real life, it\u2019s often hard to know what this \u201ctrue value\u201d is. If that declined borrower had been approved for the loan, would they actually have repaid it?", "Sometimes we have the luxury of knowing the actual outcome for all candidates, including declined ones. For example, in this case, the model was predicting the likelihood of existing offenders committing a future crime, and a couple of years after their release, we can know who actually committed another crime and who didn\u2019t. In the loan case, people who were declined loan by a particular bank might still have been able to get a loan elsewhere, and credit bureau may have information on their subsequent payments. For such cases, where the actual outcome is known, the following criteria, proposed by Moritz Hardt from Google Brain Team and teacher of \u201cFairness in Machine Learning\u201d at UC Berkeley, seems to be applicable:", "the system ought to be tuned so that the percentage of times it\u2019s wrong in the total of approvals and denials is the same for both classes", "In other words, we define as biased (and want to avoid) the situations where approval of bad applicants or decline of good applicants differ too much for different classes (e.g. genders). Note, we do not need to assume anything about the rate of correct approvals or declines. Appealing as it is, this metric is somewhat less intuitive and, of course, is only applicable when actual outcomes are known e.g. in backtesting on historical data.", "To summarize, there are multiple ways to define and measure bias, which may suit more or less for your particular problem. Which one to use is an important (and not very simple) choice, that companies and, possibly, regulators would need to make.", "After some way of detecting bias is chosen, the next challenge is to mitigate it. Some ways to mitigate the bias, both existing and novel ones, will be discussed in the next post.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Febba5c9ab94b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@scherkas?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scherkas?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Stas Cherkassky"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F115ea6ba739f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&user=Stas+Cherkassky&userId=115ea6ba739f&source=post_page-115ea6ba739f----ebba5c9ab94b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G", "anchor_text": "story"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "this ProPublica analysis"}, {"url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "anchor_text": "from researchers"}, {"url": "https://www.nbcnews.com/tech/tech-news/new-bill-aims-stamp-out-bias-algorithms-used-companies-n993186", "anchor_text": "from legislators"}, {"url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "anchor_text": "this article"}, {"url": "https://pair-code.github.io/what-if-tool/ai-fairness.html", "anchor_text": "great article"}, {"url": "https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/", "anchor_text": "Google Translate example"}, {"url": "https://en.wikipedia.org/wiki/Ricci_v._DeStefano", "anchor_text": "reached"}, {"url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1526&context=articles", "anchor_text": "legal discussion"}, {"url": "https://www.theregister.co.uk/2018/10/31/intel_diversity_report_2018/", "anchor_text": "declared"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "in this case"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ebba5c9ab94b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bias?source=post_page-----ebba5c9ab94b---------------bias-----------------", "anchor_text": "Bias"}, {"url": "https://medium.com/tag/fairness?source=post_page-----ebba5c9ab94b---------------fairness-----------------", "anchor_text": "Fairness"}, {"url": "https://medium.com/tag/ai?source=post_page-----ebba5c9ab94b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----ebba5c9ab94b---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&user=Stas+Cherkassky&userId=115ea6ba739f&source=-----ebba5c9ab94b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&user=Stas+Cherkassky&userId=115ea6ba739f&source=-----ebba5c9ab94b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Febba5c9ab94b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ebba5c9ab94b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ebba5c9ab94b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scherkas?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scherkas?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Stas Cherkassky"}, {"url": "https://medium.com/@scherkas/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "42 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F115ea6ba739f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&user=Stas+Cherkassky&userId=115ea6ba739f&source=post_page-115ea6ba739f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F115ea6ba739f%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbias-in-predictive-models-part-1-2-ebba5c9ab94b&user=Stas+Cherkassky&userId=115ea6ba739f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}