{"url": "https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31", "time": 1683011005.9312901, "path": "towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31/", "webpage": {"metadata": {"title": "Explaining DBSCAN Clustering. Using DBSCAN to identify employee\u2026 | by Kamil Mysiak | Towards Data Science", "h1": "Explaining DBSCAN Clustering", "description": "Density-based spatial clustering of applications with noise (DBSCAN) is an unsupervised clustering ML algorithm. Unsupervised in the sense that it does not use pre-labeled targets to cluster the data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK", "paragraph_index": 45}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "paragraph_index": 47}], "all_paragraphs": ["Density-based spatial clustering of applications with noise (DBSCAN) is an unsupervised clustering ML algorithm. Unsupervised in the sense that it does not use pre-labeled targets to cluster the data points. Clustering in the sense that it attempts to group similar data points into artificial groups or clusters. It is an alternative to popular clustering algorithms such as KMeans and hierarchical clustering.", "In our example, we will be examining a human resources dataset consisting of 15,000 individual employees. The dataset contains employee job characteristics such as job satisfaction, performance score, workload, years of tenure, accidents, number of promotions.", "In my previous article, we examined the KMeans clustering algorithm which I strongly suggest you read (LINK). You will be able to orient yourself to how KMeans clusters the data.", "KMeans is especially vulnerable to outliers. As the algorithm iterates through centroids, outliers have a significant impact on the way the centroids moves before reaching stability and convergence. Furthermore, KMeans has problems accurately clustering data where the clusters are of different sizes and densities. K-Means can only apply spherical clusters and its accuracy will suffer if the data is not spherical. Last but not least, KMeans requires us to first select the number of clusters we wish to find. Below is an example of how KMeans and DBSCAN would individually cluster the same dataset.", "On the other hand, DBSCAN does not require us to specify the number of clusters, avoids outliers, and works quite well with arbitrarily shaped and sized clusters. It does not have centroids, the clusters are formed by a process of linking neighbor points together.", "First, let\u2019s define Epsilon and Minimum Points, two required parameters when applying the DBSCAN algorithm, and some additional terminology.", "If \u201cminimum points\u201d = 4, any 4 or more points within the epsilon distance away from each other will be considered a cluster.", "Core Points: Core data points have at least minPts number of data points within their epsilon distance.", "I had always believed DBSCAN needed a third parameter named \u201ccore_min\u201d which would determine the minimum number of core points before a cluster of neighborhood points can be considered a valid cluster.", "Border Points: Border data points are on the outskirts as they are in the neighborhood (ie. w/in epsilon distance of core point) but have less than the required minPts.", "Outlier Points: These points are not part of a neighborhood (ie. more than epsilon distance) and are not border points. These are points located in low-density areas.", "First, a random point is selected which has at least minPts within its epsilon radius. Then each point that is within the neighborhood of the core point is evaluated to determine if it has the minPts nearby within the epsilon distance (minPts includes the point itself). If the point does meet the minPts criteria it becomes another core point and the cluster expands. If a point does not meet the minPts criteria it becomes a border point. As the process continues a chain begins to develop as core point \u201ca\u201d is a neighbor of \u201cb\u201d which is a neighbor or \u201cc\u201d and so on. The cluster is complete as it becomes surrounded by border points because there are no more points within the epsilon distance. A new random point is selected and the process repeats to identify the next cluster.", "One method used to estimate the optimal epsilon value is to use nearest neighbor distances. If you recall, nearest neighbors is a supervised ML clustering algorithm which clusters new data points based on their distance from other \u201cknown\u201d data points. We train a KNN model on labeled training data to determine which data points belong to which cluster. Then when we apply the model to new data, the algorithm determines which cluster the new data point belongs to based on the distance to trained clusters. We do have to determine the \u201ck\u201d parameter a-priori which specifies how many closest or nearest neighboring points the model will consider before assigning the new data point to a cluster.", "To determine the best epsilon value, we calculate the average distance between each point and its closest/nearest neighbors. We then plot a k-distance and choose the epsilon value at the \u201celbow\u201d of the graph. On the y-axis, we plot the average distances and the x-axis all the data points in your dataset.", "if epsilon is chosen much too small, a large part of the data will not be clustered, whereas a high epsilon value clusters will merge and the majority of data points will be in the same cluster. In general, small values of epsilon are preferable, and as a rule of thumb, only a small fraction of points should be within this distance of each other.", "Typically, we should set minPts to be greater or equal to the number of dimensionality of our dataset. That said, we often see folks multiplying the number of features X 2 to determine their minPts value.", "Much like the \u201cElbow Method\u201d used to determine the optimal epsilon value the minPts heuristic isn\u2019t correct 100% of the time.", "Silhouette Method: This technique measures the separability between clusters. First, an average distance is found between each point and all other points in a cluster. Then it measures the distance between each point and each point in other clusters. We subtract the two average measures and divide by whichever average is larger.", "We ultimately want a high (ie. closest to 1) score which would indicate that there is a small intra-cluster average distance (tight clusters) and a large inter-cluster average distance (clusters well separated).", "Visual Cluster Interpretation: Once you have obtained your clusters it is very important to interpret each cluster. This is typically done by merging the original dataset with the clusters and visualizing each cluster. The more clear and distinct each cluster is the better. We will review this process below.", "Since the features in our dataset are not on the same scale we need to standardize the entire dataset. In other words, each feature in our dataset has unique magnitudes and range for their data. A one-point increase in Satisfaction_level does not equal a one-point increase in Last_evaluation and vice versa. Since DBSCAN utilizes the distance (Euclidean) between points to determine similarity, unscaled data creates a problem. If one feature has higher variability in its data the distance calculation will be more affected by that feature. By scaling our features we align all the features to a mean of zero and a standard deviation of one.", "Some algorithms such as KMeans find it difficult to accurately construct clusters if the dataset has too many features (ie. high dimensionality). High dimensionality does not necessarily mean hundreds or even thousands of features. Even 10 features can create accuracy issues.", "The theory behind feature or dimensionality reduction is to convert the original feature set into fewer artificially derived features which still maintain most of the information encompassed in the original features.", "One of the most prevalent feature reduction techniques is Principal Component Analysis or PCA. PCA reduces the original dataset into a specified number of features which PCA calls principal components. We have to select the number of principal components we wish to see. We discuss feature reduction in my article on KMeans clustering and I strongly advise you to take a look (LINK).", "First, we need to determine the appropriate number of principal components. It would seem that 3 principal components account for roughly 75% of the variance.", "Now that we know the number of principal components needed to maintain a specific percentage of variance let\u2019s apply a 3-component PCA to our original dataset. Notice that the first principal component accounts for 26% of the variance from the original dataset. We will utilize the \u201cpca_df\u201d data frame for the remainder of this article.", "Plotting our data in a 3D space we can see some potential problems for DBSCAN. If you recall one of the main cons for DBSCAN is its inability to accurately cluster data of varying density and from the plot below we can see two separate clusters of very different density. Upon applying the DBSCAN algorithm we might be able to find clusters in the lower cluster of data points but many of the data points in the upper cluster might be classified as outliers/noise. This is of course all dependent on our selection of epsilon and minimum point values.", "Before we apply the clustering algorithm we have to determine the appropriate epsilon level using the \u201cElbow Method\u201d we discussed above. It would seem the optimal epsilon value is around 0.2. Finally, since we have 3 principal components to our data we\u2019ll set our minimum points criteria to 6.", "Setting the epsilon to 0.2 and min_samples to 6 has resulted in 53 clusters, a Silhouette score of -0.521, and over 1500 data points which are considered outliers/noise. There may be some research areas where 53 clusters might be considered informative but we have a dataset of 15,000 employees. From a business perspective, we need a manageable number of clusters (ie 3\u20135) which can be used to better understand the workplace. Also, a silhouette score of -0.521 would indicate data points are incorrectly clustered.", "Looking at the 3D plot below we can see one cluster encompassing the majority of the data points. There is a smaller but significant cluster which emerged but that leaves 52 remaining clusters of much smaller size. From a business perspective these clusters are not very informative as most employees belong to just two clusters. The organization would want to see a few large clusters in order to be certain of their validity but also be able to engage in some organization initiatives against the clustered employees (ie. increased training, compensation changes, etc.).", "Instead of using the \u201cElbow Method\u201d and the minimum value heuristic let\u2019s take an iterative approach to fine-tuning our DBSCAN model. We are going to iterate through a range of epsilon and minimum point values as we apply the DBSCAN algorithm to our data.", "In our example, we are going to iterative through epsilon values ranging from 0.5 to 1.5 at 0.1 intervals and minimum point values ranging from 2\u20137. The for-loop will run the DBSCAN algorithm using the set of values and produce the number of clusters and silhouette score for each iteration. Keep in mind you will need to adjust your parameters according to your data. You might run this code on one set of parameters and find that the best silhouette score produced is 0.30. You might want to increase the epsilon value in order to encompass more points into a cluster.", "We can see that iterating through our epsilon and minimum values we had obtained a wide range of number of clusters and silhouette scores. Epsilon scores between 0.9 and 1.1 began to produce a manageable number of clusters. Increasing epsilon to 1.2 and above creates too few clusters to make much business sense. Furthermore, some of those clusters may be just noise (ie. -1) which we\u2019ll get to in a bit.", "It is also important to understand that increasing epsilon decreases the number of clusters but each cluster will also begin to encompass more outlier/noise data points. There is a certain level of diminishing returns.", "For the sake of simplicity let\u2019s choose 7 clusters and examine what the cluster distribution looks like. (epsilon: 1.0 & minPts: 4).", "It is also important to point out a frequent error you will surely encounter running this string of code. Sometimes when you have set your parameters (ie. eps_values and min_samples) too broad the for-loop will eventually come to a combination of eps_values and min_samples which only produces one cluster. However, the Silhouette_score function requires at least two clusters to be defined. You will need to restrict your parameters to avoid this issue.", "In our example above, if we had set our epsilon parameter range from 0.2 to 2.5 that would most likely result in one cluster and ultimately elicit the error.", "You might be asking yourself \u201cweren\u2019t we supposed to obtain 7 clusters?\u201d. The answer is \u201cYes\u201d and if we look at the unique labels/clusters we see 7 labels for each data point. Per Sklearn documentation, a label of \u201c-1\u201d equates to a \u201cnoisy\u201d data point it hasn\u2019t been clustered into any of the 6 high-density clusters. We naturally don\u2019t want to consider any labels of \u201c-1\u201d as a cluster, therefore, they are removed from the calculation.", "Looking at the 3D plot of the six DBSCAN derived clusters it seems the less dense cluster towards the top of the plot did not pose much of a problem for DBSCAN despite it being less dense. If you recall, DBSCAN has a difficult time properly clustering data of various density. The distance between the top cluster and the much larger bottom cluster was most likely larger than our epsilon value of 1.0.", "That said, the dataset contains additional high-density clusters but our epsilon and minimum values are too large. The bottom cluster contains at least two high-density clusters, however, due to the strong density of the bottom cluster lowering the epsilon and minimum point values would simply create many smaller clusters. Once again this is the major downside of DBSCAN. I always believed DBSCAN needed a 3rd parameter, \u201cmin_core\u201d, which would determine the minimum number of core points before a cluster can be considered a valid cluster.", "Before we get ahead of ourselves let\u2019s quickly get a count of the number of employees in each cluster. Well it seems cluster 0 contains most of the data points which isn\u2019t very informative. In fact, if we had ran the algorithm using epsilon values of 0.5 and minimum points of 5 which would have produced 63 clusters, cluster 0 still would have contained 99% of the employee population.", "DBSCAN, a density clustering algorithm which is often used on non-linear or non-spherical datasets. Epsilon and Minimum Points are two required parameters. Epsilon is the radius within nearby data points that need to be in to be considered \u2018similar\u2019 enough to begin a cluster. Finally, Minimum Points is the minimum number of data points that need to be inside the radius (ie. epsilon) before they can be considered a cluster.", "In our example, we attempted to cluster a dataset of 15,000 employees based on their job characteristics. We first standardized the dataset to scale the features. Next, we applied PCA in order to reduce the number of dimensions/features to 3 principal components. Using the \u201cElbow Method\u201d we estimated an epsilon value of 0.2 and a minimum point value of 6. Using these parameters we were able to obtain 53 clusters, 1,500 outliers, and a silhouette score of -0.52. Needless to say, the results were not very promising.", "Next, we attempted an iterative approach to fine-tuning the epsilon and minimum points parameters. We had decided upon an epsilon value of 1.0 and a minimum points value of 4. The algorithm returned 6 valid clusters (one -1 cluster), only 7 outliers, and a respectable silhouette score of 0.46. However, upon plotting the derived clusters it was discovered the first cluster contained 99% of the employees. Once again from a business perspective, we would want our clusters to be more equally distributed to provide us with good insights about our employees.", "It would seem DBSCAN is not the optimal clustering algorithm for this particular dataset.", "This same dataset was clustered using KMeans and produced much more equal and defined clusters (LINK).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | I/O Psychologist | Motorcycle Enthusiast | On a Search for my Personal Legend/ https://www.linkedin.com/in/kamil-mysiak-b789a614/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F18eaf5c83b31&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558----18eaf5c83b31---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@seefromthesky?utm_source=medium&utm_medium=referral", "anchor_text": "Ishan @seefromthesky"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK"}, {"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK"}, {"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "anchor_text": "LINK"}, {"url": "https://medium.com/tag/dbscan?source=post_page-----18eaf5c83b31---------------dbscan-----------------", "anchor_text": "Dbscan"}, {"url": "https://medium.com/tag/clustering-algorithm?source=post_page-----18eaf5c83b31---------------clustering_algorithm-----------------", "anchor_text": "Clustering Algorithm"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----18eaf5c83b31---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/employees?source=post_page-----18eaf5c83b31---------------employees-----------------", "anchor_text": "Employees"}, {"url": "https://medium.com/tag/data-mining-techniques?source=post_page-----18eaf5c83b31---------------data_mining_techniques-----------------", "anchor_text": "Data Mining Techniques"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----18eaf5c83b31---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----18eaf5c83b31---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F18eaf5c83b31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----18eaf5c83b31---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----18eaf5c83b31--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://kamilmysiak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "348 Followers"}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F63448b4832be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-dbscan-clustering-18eaf5c83b31&newsletterV3=98fe4fecb558&newsletterV3Id=63448b4832be&user=Kamil+Mysiak&userId=98fe4fecb558&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}