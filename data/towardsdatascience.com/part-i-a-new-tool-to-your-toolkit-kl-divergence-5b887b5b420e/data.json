{"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "time": 1682996625.15793, "path": "towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e/", "webpage": {"metadata": {"title": "A new Tool to your Toolkit, Intro to KL Divergence | by Abhishek Mungoli | Towards Data Science", "h1": "A new Tool to your Toolkit, Intro to KL Divergence", "description": "Being a Data Science practitioner, how often have you used the concept of KL Divergence at work? How much clarity and confidence you have with the concepts of Entropy, Cross-Entropy, or KL\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL Divergence", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "Entropy", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross-Entropy", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL Divergence", "paragraph_index": 0}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium", "paragraph_index": 34}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "paragraph_index": 36}], "all_paragraphs": ["Being a Data Science practitioner, how often have you used the concept of KL Divergence at work? How much clarity and confidence you have with the concepts of Entropy, Cross-Entropy, or KL Divergence? A little, just theoretical or read once but forgotten-type.", "Whatever it is, it\u2019s fine. You must have read some articles about the topic over the Internet, even I did but it's so theoretical and boring that we forget with time. But, hold on, what if I present a completely different view of it, the view which enabled me to grasp the concept very well and make it a strong weapon in my armory. And believe me, once you are through this, you will have an idea of how to utilize these concepts in every small classification, clustering, or other day-to-day machine learning problems.", "This post will be followed by another post. In the part, we will start with understanding about Entropy and then Cross-Entropy and finally KL Divergence. We will then use the learned concepts and apply them in a dataset that will make things crystal clear in the next post. I will try my level best to keep things simple and intuitive but please feel free to jump to the references section and research more about the topics. Let's get started on the journey.", "You can also find my Youtube video on the same topic.", "The Internet says Entropy is the amount of Information Gain. Let me make it simple and clear, Entropy is the nothing but the amount of disorder, uncertainty, surprise, or unpredictability associated with the event. What does it mean?", "Let\u2019s take an example, seeing today\u2019s weather I predict,", "Which case gives us more information gain or knowledge?", "When said, 50\u201350 chances, it means more uncertainty, anything can possibly happen and hence more randomness. While the second case, 75% chances of raining means it\u2019s more probable to rain, less randomness, or new-information gain. Hence, the first case has a higher entropy than the second.", "Let\u2019s take another example, what will be the possible outcome of a coin toss, given", "Same as before, in the first case, there are 50\u201350 chances of heads or tails. It means more uncertainty, anything can possibly happen and hence more randomness. While in the second case, 75% chances of getting head mean it\u2019s more probable that coin\u2019s outcome will be head, less randomness, or new-information gain. Hence, the first case has a higher entropy than the second.", "The entropy of the system \u2018H(p)\u2019 is calculated as,", "where p\u1d62 is the probability of the i-th event.", "Let\u2019s calculate the Entropy of the above example and show that unbiased coin has higher entropy than the biased one.", "Case I: Entropy for Unbiased coin with equal head\u2019s and tail\u2019s probability, 0.5", "Case II: Entropy for Biased coin with head\u2019s probability 0.75", "Cross-Entropy, according to me, is the goodness of fit, the closeness of the predicted distribution to the actual one. Let me simplify it more,", "Let say we have been provided a biased coin with a probability of head being 0.75. We are unaware of the biases of the coin, so", "Case I: We tossed the coin a hundred times and get 70 heads and 30 tails. Our estimate is coin is biased with a probability of head being 0.70.", "Case II: We tossed the coin a hundred times and get 72 heads and 28 tails. Our estimate is coin is biased with a probability of head being 0.72.", "We are closer to the actual distribution in Case II, hence our cross-entropy is lower for Case II than Case I. If by chance we are able to mimic the true distribution without any error, the cross-entropy will be same as Entropy. Thus, cross-Entropy is always greater or equal to entropy.", "where p\u1d62 is the actual probability of the i-th event and q\u1d62 is the estimated probability of the i-th event. And logarithm used is Base-2.", "Let\u2019s calculate the Cross-Entropy of the above example and show that Case II has lower cross-entropy than Case I as it is more closer to the actual distribution.", "Case I: Coin is found to be biased with a probability of head being 0.70.", "Case II: Coin is found to be biased with a probability of head being 0.72.", "Clearly, the cross-entropy is smaller for Case II than Case I as it is closer to the actual distribution.", "What if the estimation was, the coin is biased with a probability of head being 0.75,", "KL Divergence is just the difference between a fitted distribution and actual distribution, i.e. the difference between cross-entropy and entropy. It can also be looked at as to how much the two distributions differ.", "Let\u2019s take the same example where we have been provided a biased coin with a probability of head being 0.75. We are unaware of the biases of the coin, so", "Case I: We tossed the coin a hundred times and got 70 heads and 30 tails. Our estimate is coin is biased with a probability of head being 0.70.", "Case II: We tossed the coin a hundred times and got 72 heads and 28 tails. Our estimate is coin is biased with a probability of head being 0.72.", "Clearly, the distance is less in Case II than the Case I, as the second distribution is closer to the actual one.", "What if the estimation was, the coin is biased with a probability of head being 0.75,", "This was the first post where we got a thorough understanding of Entropy, Cross-Entropy, and KL-Divergence. We also understood the terms both intuitively way and by calculating and seeing their value through examples. I hope that clarified the topic and gave it a different view than what's traditionally available over the Internet. In the next post, I intend to use these concepts in a real data-set. It will give everyone an idea/intuition of how to utilize these learned concepts in every small classification, clustering or other day-to-day machine learning problems. Don\u2019t miss it guys, mark my words, it\u2019s just gonna get better.", "My Youtube channel for more content:", "It takes a lot of effort to write a good post with clarity and easy understandability for the audience. I will keep trying to do justice with my work. Follow me up at Medium and check out my previous posts. I welcome feedback and constructive criticism.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Lead Data Scientist at Meesho | Ex-Walmart | IIIT-Hyderabad | NERIST | Insta: simplyspartanx | Youtube: https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b887b5b420e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de----5b887b5b420e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.shoezone.com/Boys/Size-Guide", "anchor_text": "Image Source"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL Divergence"}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "Entropy"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross-Entropy"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL Divergence"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "Abhishek MungoliHi Guys, Welcome to the channel. The channel aims to cover various topics from Machine Learning, Data Science\u2026www.youtube.com"}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium"}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "https://en.wikipedia.org/wiki/Entropy"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "https://en.wikipedia.org/wiki/Cross_entropy"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"}, {"url": "https://towardsdatascience.com/demystifying-entropy-f2c3221e2550", "anchor_text": "https://towardsdatascience.com/demystifying-entropy-f2c3221e2550"}, {"url": "https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8", "anchor_text": "https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8"}, {"url": "https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68", "anchor_text": "https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b887b5b420e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5b887b5b420e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----5b887b5b420e---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5b887b5b420e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----5b887b5b420e---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----5b887b5b420e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----5b887b5b420e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b887b5b420e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b887b5b420e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b887b5b420e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b887b5b420e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b887b5b420e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/@mungoliabhishek81/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8d41b74042bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e&newsletterV3=5f15354d23de&newsletterV3Id=8d41b74042bf&user=Abhishek+Mungoli&userId=5f15354d23de&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}