{"url": "https://towardsdatascience.com/building-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb", "time": 1683003830.078945, "path": "towardsdatascience.com/building-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb/", "webpage": {"metadata": {"title": "Building a real-time prediction pipeline using Spark Structured Streaming and Microservices | by Bogdan Cojocar | Towards Data Science", "h1": "Building a real-time prediction pipeline using Spark Structured Streaming and Microservices", "description": "We will build a real-time pipeline for machine learning prediction. The main frameworks that we will use are: In a realtime ML pipeline we embed a model in two ways: by using the model directly into\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/BogdanCojocar/medium-articles/tree/master/realtime_kafka", "anchor_text": "GitHub", "paragraph_index": 1}], "all_paragraphs": ["We will build a real-time pipeline for machine learning prediction. The main frameworks that we will use are:", "TL;DR: The code is on GitHub.", "In a realtime ML pipeline we embed a model in two ways: by using the model directly into the framework that is doing the processing or by decoupling the model separately into a microservice. By building the wrapper for the ML model we require extra effort, so why bother? There are two major advantages. Firstly, when we want to deploy a new model we don\u2019t need to deploy the whole pipeline, we just need to expose a new microservice version. Secondly, it gives you more power into testing different versions of that ML model. For example we are able to use canary deployments and use 80% of the stream of data on the version1 of the model and 20% on version2 . Once we are happy with the quality of version2 , we shift more and more traffic towards it.", "Now let\u2019s deep dive into the development of the application.", "To build the cluster we will use a docker-compose file that will start all the docker containers needed: zookeeper and a broker.", "Now very briefly, kafka is a distributed streaming platform capable of handling a large number of messages, that are organized or grouped together into topics. In order to be able to process a topic in parallel, it has to be split into partitions, and the data from these partitions are stored into separate machines called brokers. And finally, zookeeper is used to manage the resources of the brokers in the clusters.To read or write into a kafka cluster we need a broker address and a topic.", "The docker-compose will start zookeper on port 2181 , a kafka broker on port 9092. Besides that we use another docker container kafka-create-topic for the sole purpose to create a topic (called test) in the kafka broker.", "To start the kafka cluster, we have to run the following command line instruction in the same folder where we have defined the docker compose file:", "This will start all the docker containers with logs. We should see something like this in the console:", "We are using the REST protocol for our web service. We will do sentiment analysis using NLTK\u2019s Vader algorithm. This is a pre-trained model, so we can only focus on the prediction part:", "We are creating a POST request that received a JSON message in the form {\"data\": \"some text\"} , where the field data contains a sentence. We will apply the algorithm and send the response back as another JSON .", "To run the app simply run:", "After we start the Jupyter lab notebook we need to make sure that we have the kafka jar as a dependency for spark to be able to run the code. Add the following in the first cell of the notebook:", "Following that we can start pySpark using the findspark package:", "To be able to consume data in realtime we first must write some messages into kafka. We will use the confluent_kafka library in python to write a producer:", "We will send the sameJSON messages {\"data\": value} as previously, where value is a sentence from a predefined list. For each message we write into the queue we also need to assign a key. We will assign a random one based on the uuid to achieve a good distribution into the cluster. In the end, we also run a flush command to ensure that all the messages are sent.", "Once we run the confluent_kafka_producer we should receive a log telling us that the data has been sent correctly:", "As stated previously we will use Spark Structured Streaming to process the data in real-time. This is an easy to use API that treats micro batches of data as data frames. We first need to read the input data into a data frame:", "The startingOffset is earliest indicating that each time we run the code we will read all the data present in the queue.", "This input will contain different columns that represent different metrics from kafka like keys, values, offsets, etc. We are only interested in the values, the actual data and we can run a transformation to reflect that:", "In Structured Streaming we can use user defined functions, that can be applied to each row in the data frame.", "We need to make our imports in the function as this is a piece of code that can be distributed on multiple machines. We post a request to our endpoint and return the response.", "We will call our udf as vader_udf and it will return a new string column.", "In this final step, we get to see our results. The format of the input data is in JSON and we can transform it into a string . For that, we will use the helper function from_json . The same thing we can do to the output column from the sentiment analysis algorithm that has also the JSON format:", "We can display our results in the console. Because we are using the notebook, you will only be able to visualise it from the terminal you have started in Jupyter. The command trigger(once=True) , will only run the stream processing for a short period and show the output.", "That was it folks, I hope you enjoy this tutorial and find it useful. We saw how by using Structured Streaming API together with a microservice calling the ML model we can construct a powerful pattern that can be the backbone of our next real-time application.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Big data consultant. I write about the wonderful world of data."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F626dc20899eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----626dc20899eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bogdan.cojocar?source=post_page-----626dc20899eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Bogdan Cojocar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5d35fbaadc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=post_page-a5d35fbaadc8----626dc20899eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ikukevk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kevin Ku"}, {"url": "https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html", "anchor_text": "Spark Structured Streaming"}, {"url": "https://www.confluent.io", "anchor_text": "Kafka"}, {"url": "https://opensource.com/article/18/4/flask", "anchor_text": "Flask"}, {"url": "https://docs.confluent.io/current/quickstart/ce-docker-quickstart.html?utm_medium=sem&utm_source=google&utm_campaign=ch.sem_br.brand_tp.prs_tgt.confluent-brand_mt.xct_rgn.emea_lng.eng_dv.all&utm_term=confluent%20kafka%20docker&creative=&device=c&placement=&gclid=CjwKCAiAvonyBRB7EiwAadauqSDXLG4BUDfIH8weo9PodnlxrIBKdxPd76NrD0a4zL2eZLOar1XfPBoCc6AQAvD_BwE", "anchor_text": "Docker"}, {"url": "https://jupyterlab.readthedocs.io/en/stable/", "anchor_text": "Jupyter lab"}, {"url": "https://www.nltk.org", "anchor_text": "NLTK"}, {"url": "https://github.com/BogdanCojocar/medium-articles/tree/master/realtime_kafka", "anchor_text": "GitHub"}, {"url": "http://twitter.com/app", "anchor_text": "@app"}, {"url": "http://127.0.0.1:9000/predict", "anchor_text": "http://127.0.0.1:9000/predict"}, {"url": "http://localhost:9000/predict'", "anchor_text": "http://localhost:9000/predict'"}, {"url": "https://medium.com/tag/python?source=post_page-----626dc20899eb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/spark?source=post_page-----626dc20899eb---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/kafka?source=post_page-----626dc20899eb---------------kafka-----------------", "anchor_text": "Kafka"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----626dc20899eb---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----626dc20899eb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=-----626dc20899eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=-----626dc20899eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F626dc20899eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----626dc20899eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----626dc20899eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----626dc20899eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----626dc20899eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----626dc20899eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----626dc20899eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bogdan Cojocar"}, {"url": "https://medium.com/@bogdan.cojocar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "566 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5d35fbaadc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=post_page-a5d35fbaadc8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffaedcd759ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb&newsletterV3=a5d35fbaadc8&newsletterV3Id=faedcd759ecc&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}