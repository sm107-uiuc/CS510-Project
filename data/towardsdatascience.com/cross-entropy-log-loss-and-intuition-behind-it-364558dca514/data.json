{"url": "https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514", "time": 1683005480.722301, "path": "towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514/", "webpage": {"metadata": {"title": "Cross-Entropy, Log-Loss, And Intuition Behind It | by Ritesh Ranjan | Towards Data Science", "h1": "Cross-Entropy, Log-Loss, And Intuition Behind It", "description": "You need to know nothing before going through this blog as I will start with the basics. The following content will be covered and explained in detail in this blog. This is defined as a variable that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555", "anchor_text": "blog", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-89738c58f610", "anchor_text": "blog", "paragraph_index": 24}, {"url": "https://www.linkedin.com/in/riteshranjan11055/", "anchor_text": "https://www.linkedin.com/in/riteshranjan11055/", "paragraph_index": 26}], "all_paragraphs": ["You need to know nothing before going through this blog as I will start with the basics. The following content will be covered and explained in detail in this blog.", "This is defined as a variable that takes the output of a random event. For example, we can define a variable X which takes the value of the output of rolling a dice. Here X can take values from 1 to 6.", "We sometimes also calculate the probability that the random variable will take a particular value. For example, P(X=1) is 1/6 and this is the same for other values well, as they are equally likely to occur.", "When we take about information content we do in the context of the random variable. You can think it in this way if you know that an event is going to occur and that event occurs frequently then the information content is very less. Now if you know that an event is going to occur and this event occurs very rarely then the information content is very high. For example, there are two random variables X and Y. Here X tells whether the sun will rise or not and Y tells whether there will be an earthquake (surprise element) today. You can easily come to the conclusion that Y contains more information.", "Information content is directly proportional to the surprise element.", "Therefore, we can conclude that the information content of a random variable depends on the probability of an event occurring. If the probability is very low the information content is very high. We can write it mathematically as follows:", "Hence IC is a function of probability. Here P(X=S) means the probability that X will take value S. This will be used in the rest of the blog. Information content shows the following property.", "The second property on the above diagram is seen in the logarithmic family of functions. This gives us the intuition of using the log as a function to calculate IC. Hence now we can define IC mathematically as follows:", "The entropy of a random variable is defined as the expected information content of the random variable. We will use the information theory of the electronics branch to explain it in a more intuitive way.", "In information theory, the number of bits to send the value of X= some value is called the information content. Now if the random variable takes N values then we say it is a signal. In fact, we define a signal as a random variable taking N values. A signal takes different values at different intervals and hence we can define a random variable that will take N possible values that a signal can take. Now we find the number of bits taken to send the signal and this is the expected information content of the random variable. In other words, we call it entropy. The following example will make it more clear.", "The entropy/expected IC is calculated using the following formula", "Here at the receiver, we don\u2019t know the actual distribution of the random variable. We will see say 100 signals received and then estimate the distribution of the random variable. Now let us assume that the actual distribution at the sender is \u2018y\u2019 and estimated is \u2018y^\u2019. Here distribution means the probability that a random variable will take a particular value. The following are the value of the number of bits required to send the data.", "Now we already know the number of bits is the same as entropy. The entropy at the sender is called entropy and the estimated entropy at the receiver is called cross-entropy. Now, this is called cross-entropy because we are using the actual distribution and estimated distribution to calculate the estimated entropy at the receiver end. Another question that must be popping in your mind is why we are using actual distribution (y) at the receiver end. The answer to that is, we are estimating the number of bits required for each value of the random variable received which is -log(yi^). The number of bits used will depend on the distribution of the random variable received at the receiver. If this is not clear even after this then let's take an example.", "We estimate P(X=A) to be 1/4 and the actual is 1/8 then the estimated number of bits for this will be -(log(1/4)) = 2 but the contribution to the final answer will be 1/8*(2) = 1/4 as we will receive this value at the receiver 1/8 times. Now I think this will be clear by now.", "K-L divergence is equal to the difference between cross-entropy and entropy.", "Now we will move to the machine learning section. I am assuming that we know what y^ means. If you don't know then Y^ is the predicted probability of a given data point belonging to a particular class/label. For example, we can have a model in machine learning which will tell whether a text is abusive or not. The formula to calculate y^ is given below", "Here \u2018w\u2019 is the weight vector and \u2018x\u2019 is d dimensional representation of the data point and \u2018b\u2019 is the bias term.", "Our main objective in all of machine learning is to properly estimate the distribution of data points in the training dataset.", "Here training set can be treated as the sender and the model as the receiver which tries to estimate the distribution.", "The best estimation will happen when K-L divergence will be minimum. Hence we will find the (w, b) corresponding to min K-L divergence. While updating (w, b) we ignore the entropy term as this is a constant and only cross-entropy term varies. Hence our loss equation looks as below.", "This is the loss term which we generally call as log-loss as this contains log term.", "For binary classification where \u2018yi\u2019 can be 0 or 1. This loss looks will look like loss = -(y * log (y) + (1- y) * log (1 \u2014 y)). This is what most of us are familiar with. This is all for now. I hope you got all of this.", "If you loved the content then let me know in the comments below.", "If you want to learn what is calibration then go through this blog.", "Want to learn how to prevent your model from underfitting and underfitting, then go through this blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Enthusiast. Writer in Towards Data Science, Analytics Vidhya, and AI In Plain English. LinkedIn: https://www.linkedin.com/in/riteshranjan11055/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F364558dca514&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://riteshk981.medium.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": ""}, {"url": "https://riteshk981.medium.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": "Ritesh Ranjan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6b2556387fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&user=Ritesh+Ranjan&userId=b6b2556387fc&source=post_page-b6b2556387fc----364558dca514---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@freegraphictoday?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "AbsolutVision"}, {"url": "https://unsplash.com/s/photos/compass?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "https://en.wikipedia.org/wiki/Cross_entropy"}, {"url": "https://en.wikipedia.org/wiki/Information_content", "anchor_text": "https://en.wikipedia.org/wiki/Information_content"}, {"url": "https://en.wikipedia.org/wiki/Random_variable", "anchor_text": "https://en.wikipedia.org/wiki/Random_variable"}, {"url": "https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555", "anchor_text": "blog"}, {"url": "https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555", "anchor_text": "Calibration in Machine LearningIn this blog, we will learn what is calibration and why and when we should use it.medium.com"}, {"url": "https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-89738c58f610", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-89738c58f610", "anchor_text": "Overfitting And Underfitting In Machine LearningIn this article, you will learn what overfitting and underfitting are. You will also learn how to prevent the model\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----364558dca514---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----364558dca514---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/entropy?source=post_page-----364558dca514---------------entropy-----------------", "anchor_text": "Entropy"}, {"url": "https://medium.com/tag/log-loss?source=post_page-----364558dca514---------------log_loss-----------------", "anchor_text": "Log Loss"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----364558dca514---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&user=Ritesh+Ranjan&userId=b6b2556387fc&source=-----364558dca514---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&user=Ritesh+Ranjan&userId=b6b2556387fc&source=-----364558dca514---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F364558dca514&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----364558dca514---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----364558dca514--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----364558dca514--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----364558dca514--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----364558dca514--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----364558dca514--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----364558dca514--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----364558dca514--------------------------------", "anchor_text": ""}, {"url": "https://riteshk981.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://riteshk981.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ritesh Ranjan"}, {"url": "https://riteshk981.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://www.linkedin.com/in/riteshranjan11055/", "anchor_text": "https://www.linkedin.com/in/riteshranjan11055/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6b2556387fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&user=Ritesh+Ranjan&userId=b6b2556387fc&source=post_page-b6b2556387fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fad539cca55bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-log-loss-and-intuition-behind-it-364558dca514&newsletterV3=b6b2556387fc&newsletterV3Id=ad539cca55bd&user=Ritesh+Ranjan&userId=b6b2556387fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}