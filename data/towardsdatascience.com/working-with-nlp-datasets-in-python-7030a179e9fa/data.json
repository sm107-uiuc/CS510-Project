{"url": "https://towardsdatascience.com/working-with-nlp-datasets-in-python-7030a179e9fa", "time": 1683015406.049748, "path": "towardsdatascience.com/working-with-nlp-datasets-in-python-7030a179e9fa/", "webpage": {"metadata": {"title": "Working with NLP datasets in Python | by Gergely D. N\u00e9meth | Towards Data Science", "h1": "Working with NLP datasets in Python", "description": "In the field of Deep Learning, datasets are an essential part of every project. To train a neural network that can handle new situations, one has to use a dataset that represents the upcoming\u2026"}, "outgoing_paragraph_urls": [{"url": "http://paperswithcode.com", "anchor_text": "Papers With Code", "paragraph_index": 1}, {"url": "https://huggingface.co/docs/datasets/index.html", "anchor_text": "HuggingFace Dataset", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/datasets/catalog/overview?hl=en", "anchor_text": "TensorFlow Datasets", "paragraph_index": 2}, {"url": "https://colab.research.google.com/drive/1H2ikwYVxHlk2ikFZDO4zGkvMVjwnHI9A?usp=sharing", "anchor_text": "All codes are available on Google Colab.", "paragraph_index": 3}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "Large Movie Review Dataset", "paragraph_index": 4}, {"url": "http://datasetsearch.research.google.com", "anchor_text": "datasetsearch.research.google.com", "paragraph_index": 6}, {"url": "https://datasetsearch.research.google.com/search?query=IMDB%20Large%20Movie%20Reviews%20Sentiment%20Dataset&docid=exXzLiK1WI6YWyHzAAAAAA%3D%3D", "anchor_text": "the result", "paragraph_index": 6}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 6}, {"url": "https://www.kaggle.com/product-feedback/43505#post246047", "anchor_text": "limit of 10GB", "paragraph_index": 7}, {"url": "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews", "anchor_text": "IMDB Sentiment dataset on Kaggle", "paragraph_index": 8}, {"url": "https://scikit-learn.org/stable/datasets/index.html", "anchor_text": "Here", "paragraph_index": 12}, {"url": "https://keras.io/api/datasets/imdb/", "anchor_text": "only text dataset included in Keras", "paragraph_index": 12}, {"url": "https://www.tensorflow.org/datasets/overview", "anchor_text": "TensorFlow", "paragraph_index": 13}, {"url": "https://huggingface.co/docs/datasets/index.html", "anchor_text": "HuggingFace Datasets", "paragraph_index": 14}, {"url": "https://huggingface.co/nlp/viewer/?dataset=imdb", "anchor_text": "dataset viewer site", "paragraph_index": 17}, {"url": "https://huggingface.co/datasets/imdb", "anchor_text": "dataset description site", "paragraph_index": 17}, {"url": "https://www.tensorflow.org/datasets/catalog/imdb_reviews", "anchor_text": "dataset description site", "paragraph_index": 18}, {"url": "https://keras.io/api/datasets/imdb/", "anchor_text": "Keras version", "paragraph_index": 19}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset", "anchor_text": "start here", "paragraph_index": 23}, {"url": "https://www.tensorflow.org/tutorials/load_data/text", "anchor_text": "this tutorial", "paragraph_index": 26}, {"url": "https://huggingface.co/docs/datasets/beam_dataset.html", "anchor_text": "read the tutorial", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955", "anchor_text": "read it here", "paragraph_index": 34}, {"url": "https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.cleanup_cache_files", "anchor_text": "datasets.Dataset.cleanup_cache_files()", "paragraph_index": 39}, {"url": "http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf", "anchor_text": "Learning Word Vectors for Sentiment Analysis.", "paragraph_index": 44}], "all_paragraphs": ["In the field of Deep Learning, datasets are an essential part of every project. To train a neural network that can handle new situations, one has to use a dataset that represents the upcoming scenarios of the world. An image classification model trained on animal images will not perform well on a car classification task.", "Alongside training the best models, researchers use public datasets as a benchmark of their model performance. I personally think that easy-to-use public benchmarks are one of the most useful tools to help facilitate the research process. A great example of this is the Papers With Code state-of-the-art charts.", "Another great tool is the ready-to-use dataset libraries. In this post, I will review the new HuggingFace Dataset library on the example of IMBD Sentiment analysis dataset and compare it to the TensorFlow Datasets library using a Keras biLSTM network. The story can also work as a tutorial for using these libraries.", "All codes are available on Google Colab.", "When someone publishes a new dataset library, the most straightforward thing to do is to share it in the research team\u2019s webpage. For example, the IMDB Sentiment analysis dataset is published by a team of Stanford researchers and available at their own webpage: Large Movie Review Dataset. In case of a scientific publication, it usually comes with a published article: see Maas et al. [1] for example.", "I think, the two major problems with this are: 1) it is hard to find, especially, if you are an early-carrier scientist; 2) there is no standardised format to store the data and using a new dataset must come with a specific preprocessing step.", "To make a dataset accessible, one should not only make it available but also make it sure that users will find it. Google realised the importance of it when they dedicated a search platform for datasets at datasetsearch.research.google.com. However, searching IMDB Large Movie Reviews Sentiment Dataset the result does not include the original webpage of the study. Browsing the Google results for dataset search, one will find that Kaggle is one of the greatest online public dataset collection.", "Kaggle is the world\u2019s largest online machine learning community with various competition tasks, dataset collections and discussion topics. If you never heard of Kaggle but interested in deep learning, I strongly recommend taking a look at it. In Kaggle, anyone can upload new datasets (with a limit of 10GB) and the community can rate the dataset based on its documentation, machine-readability and existence of code examples to work with it.", "The IMDB Sentiment dataset on Kaggle has an 8.2 score and 164 public notebook examples to start working with it. The user can read the documentation of the dataset and preview it before downloading it.", "It is important to note that this dataset does not include the original splits of the data. This does not help the reproducibility of the models unless the builders describe their split function.", "Working with Kaggle datasets, the most important precautions are 1) make sure you use the exact dataset as many users share an altered/improved version of the datasets, 2) make sure that you have the license to work with it and the right person takes credit for it. Many datasets on Kaggle are not shared by the original creator.", "While the main reason for dataset collections is to store all datasets in one place, the dataset libraries focus on ready-to-use accessibility and performance.", "The machine learning libraries often come with a few dataset examples. Here is a list of the Scikit-learn datasets. I chose the IMDB dataset because this is the only text dataset included in Keras.", "The TensorFlow team dedicated a package for datasets. It includes several datasets and compatible with the TensorFlow and Keras neural networks.", "In the following, I will compare the TensorFlow Datasets library with the new HuggingFace Datasets library focusing on NLP problems.", "Currently, the TensorFlow Datasets list 155 entries from various fields of machine learning while the HuggingFace Datasets contains 165 entries focusing on Natural Language Processing. Here is the list of datasets sharing the same name (39):", "Note that the IMDB dataset is not on the list! In the TensorFlow Datasets, it is under the name imdb_reviews while the HuggingFace Datasets refer to it as the imdb dataset. I think it is quite unfortunate and the library builders should strive to keep the same name.", "The HuggingFace Datasets has a dataset viewer site, where samples of the dataset are presented. This site shows the splits of the data, link to the original website, citation and examples. Along with this, they have another dataset description site, where import usage and related models are shown.", "The TensorFlow Datasets has a single dataset description site, where the previously mentioned metadata information is available. Compared to the HugginFace site, TensorFlow offers multiple download options: plain text and encoded numeric word tokens.", "In the Keras version of the IMDB dataset, the plain text is already preprocessed. I use the same processing steps to illustrate the use-cases of the other libraries. The steps are as follows:", "The Keras network will expect 200 tokens long integer vectors with a vocabulary of [0,20000). The words of the vocabulary are based on the dataset\u2019s word frequency. The network consists of an Input layer for the input read, an Embedding layer to project the word representations from the integers to a 128 dimension vector space, two bidirectional LSTM layers and a Dense layer to match the output dimension.", "As the data is already preprocessed, the early steps are only a few lines:", "The epochs trained on the Colab computer are 450s long on CPU and 55s long on GPU, the final accuracy on the test data is 0.8447.", "TensorFlow offers really good tutorials, they are detailed and yet have a relatively short length to read. If you want to read more about it, start here.", "The first parameter specifies the dataset by name. Next, the split parameter tells the library which data splits should be included. It can be a percentage of a split too: train[:10%]. The as_supervised parameter specifies the format, this one allows the Keras model to train from the TensorFlow dataset. The with_info adds a new return value to the list, it contains various information from the data. My favourite is the citation to show the credit of the original data builder team.", "I think the most powerful tool of the TensorFlow Datasets library is that you don't have to load in the full data at once but only as batches of the training. Unfortunately, to build a vocabulary based on the word frequency we have to load the data before the training.", "The tokenizer building is based on this tutorial. However, I added a Counter to count the frequency of the words in the training dataset. The code first splits the sentences by whitespace. Then, the Counter counts the word frequency. To match the Keras model\u2019s vocabulary size the counter keeps only the top max_features-2 . The additional two tokens are the padding token (0) and the Out-of-vocabulary (OOV) token for words not included in the most common list (max_features-1). The next lines of code build an encoder to ensure that every word in the vocabulary has a unique integer value. The encode_map_fn function wraps the encoder in a TensorFlow function so the Datasets objects can work with it. This code snippet also includes the truncate function: it makes sure that the lengths of the sentences are not too long by cutting off the end of them.", "The next part of the code builds the pipeline for the processing steps. Firstly, it reads the (sentence,label) pairs, then encodes it to (integer_list,label) pairs and truncates the long sentences (lists). Then the cache can speed-up the work if the data can be stored in the memory. The shuffle part is only necessary for the training data: the buffer_size=1024 parameter specifies that the program randomizes on smaller windows of the data and not the whole at once. It is useful if the data is too large to fit in the memory, however, true random can be only achieved if this buffer_size is greater than the number of data samples. The padded_batch step of the pipeline batch the data into groups of 32 and pad the shorter sentences to 200 tokens. After this step the input shape is (32,200) and the output is (32,1). Lastly, the prefetch step works with multiprocessing: while the model is training on a batch, the algorithm loads in the next batches so they will be ready when the model finishes the previous one.", "Finally, the data is ready to train the model:", "The first epoch is slower on GPU (82s) but after loading the processed data into the cache, the epoch duration is similar to the Keras\u2019 (55s). The final accuracy of the test data is 0.8014. If you run the code, you can see that the Keras fit verbose can\u2019t guess the first epoch\u2019s duration: the pipeline reads the data without knowing it\u2019s final length!", "Firstly, I want to talk about the HugginFace package names: the company has 3 pip packages, transformers, tokenizers and datasets. While I understand the PR value of securing these short names and maybe the transformers and tokenizers are first of their kind, I do not like their names because it can be confusing. For example, if I use the TensorFlow Datasets library and the HuggingFace datasets library, if one\u2019s name is datasets, I can\u2019t decide which one is it. I think if Tensorflow names their libraries with mention to their name (like tensorflow_datasets), it would be nice if HuggingFace do it so.", "Secondly, working with both the tokenizers and the datasets, I have to note that while transformers and datasets have nice documentations, the tokenizers library lacks it. Also, I came across an issue during building this example following the documentation \u2014 and it was reported to them in June.", "The HuggingFace Datasets can build a pipeline similar to the TensorFlow\u2019s one to work with large data. In this experiment, I do not use it, please read the tutorial if you need it!", "The data loading works similarly to the previous one. The HuggingFace library can handle percentages as well as the TensorFlow. The Dataset object has information about the data as properties like the citation info.", "Here comes a powerful feature of the HuggingFace libraries: as the company focuses on Natural Language Processing, it has more and better-suited features for the field than TensorFlow. If we want to work with a specific transformer model, we can import its tokenizer from the corresponding package. Or we can train a new one from scratch. I talked about the differences between tokenizers in a previous post, read it here if you are interested.", "In this experiment, I built a WordPiece [2] tokenizer based on the training data. This is the tokenizer used by the famous BERT model [3]. Also, I show how to use the vocabulary from the previous part as the data of the tokenizer to achieve the same functionality.", "This code sample shows how to build a WordPiece based on the Tokenizer implementation. Unfortunately, the trainer works with files only, therefore I had to save the plain texts of the IMDB dataset temporarily. The size of the vocabulary is customizable at the train function.", "To build the tokenizer with the most frequent words, one should update the vocabulary. The only trick is to keep track of the special tokens. To see the data inside the tokenizer, a possible way is to save it to a JSON file: it is readable and contains all the information needed.", "The difference is clear if called on the sentence \u201cThis is a hideout.\u201d. The first version splits the hideout and recognizes the \u2018.\u2019 character but the second one has the whole word as a token but does not include punctuation characters. By default, the Tokenizer makes this data lowercase, I did not use this step in the previous version.", "To make sure that I don\u2019t use the same tokens, I called datasets.Dataset.cleanup_cache_files() between the two runs.", "Implementing the example in the Dataset tutorial, we can load the data to the TensorFlow Dataset format and train the Keras model with it.", "This code snippet is similar to the one in the HuggingFace tutorial. The only difference comes from the use of different tokenizers. The tutorial uses the tokenizer of a BERT model from the transformers library while I use a BertWordPieceTokenizer from the tokenizers library. Unfortunately, these two logically similar class from the same company in different libraries are not entirely compatible.", "The final step is almost identical to the one with the TensorFlow data. The only difference is the shuffle buffer. The samples in the IMDB database of the HuggingFace Datasets are sorted by label. In this case, it is not a problem but it disables the features of the TensorFlow that allowed to load only portions of the data at once. If we shuffle only with a small window in this data, in almost all cases the window contains only one of the label value. Hopefully, this is not true for the other datasets.", "In this story, I showed the use of the TensorFlow\u2019s and the HuggingFace\u2019s dataset library. I talked about why I think that building dataset collections is important for the research field. Overall, I think that HuggingFace focusing on the NLP problems will be a great facilitator of the field. The library already has more NLP dataset than the TensorFlow\u2019s. I think it is important for them to work closely with TensorFlow (as well as PyTorch) to ensure that every feature of both libraries could be utilized properly.", "[1] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7030a179e9fa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@neged.ng?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Gergely D. N\u00e9meth"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b----7030a179e9fa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://paperswithcode.com", "anchor_text": "Papers With Code"}, {"url": "https://huggingface.co/docs/datasets/index.html", "anchor_text": "HuggingFace Dataset"}, {"url": "https://www.tensorflow.org/datasets/catalog/overview?hl=en", "anchor_text": "TensorFlow Datasets"}, {"url": "https://colab.research.google.com/drive/1H2ikwYVxHlk2ikFZDO4zGkvMVjwnHI9A?usp=sharing", "anchor_text": "All codes are available on Google Colab."}, {"url": "https://paperswithcode.com/sota/sentiment-analysis-on-imdb", "anchor_text": "IMDb Sentiment Analysis chart on PapersWithCode"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "Large Movie Review Dataset"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "The original publication page of the IMDB Sentiment Dataset"}, {"url": "http://datasetsearch.research.google.com", "anchor_text": "datasetsearch.research.google.com"}, {"url": "https://datasetsearch.research.google.com/search?query=IMDB%20Large%20Movie%20Reviews%20Sentiment%20Dataset&docid=exXzLiK1WI6YWyHzAAAAAA%3D%3D", "anchor_text": "the result"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "https://www.kaggle.com/product-feedback/43505#post246047", "anchor_text": "limit of 10GB"}, {"url": "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews", "anchor_text": "IMDB Sentiment dataset on Kaggle"}, {"url": "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews", "anchor_text": "The IMDB Dataset on Kaggle"}, {"url": "https://scikit-learn.org/stable/datasets/index.html", "anchor_text": "Here"}, {"url": "https://keras.io/api/datasets/imdb/", "anchor_text": "only text dataset included in Keras"}, {"url": "https://www.tensorflow.org/datasets/overview", "anchor_text": "TensorFlow"}, {"url": "https://huggingface.co/docs/datasets/index.html", "anchor_text": "HuggingFace Datasets"}, {"url": "https://huggingface.co/nlp/viewer/?dataset=imdb", "anchor_text": "dataset viewer site"}, {"url": "https://huggingface.co/datasets/imdb", "anchor_text": "dataset description site"}, {"url": "https://www.tensorflow.org/datasets/catalog/imdb_reviews", "anchor_text": "dataset description site"}, {"url": "https://keras.io/api/datasets/imdb/", "anchor_text": "Keras version"}, {"url": "https://keras.io/examples/nlp/bidirectional_lstm_imdb/", "anchor_text": "originally by fchollet"}, {"url": "https://keras.io/examples/nlp/bidirectional_lstm_imdb/", "anchor_text": "originally by fchollet"}, {"url": "https://keras.io/examples/nlp/bidirectional_lstm_imdb/", "anchor_text": "originally by fchollet"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset", "anchor_text": "start here"}, {"url": "https://www.tensorflow.org/tutorials/load_data/text", "anchor_text": "this tutorial"}, {"url": "https://huggingface.co/docs/datasets/beam_dataset.html", "anchor_text": "read the tutorial"}, {"url": "https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955", "anchor_text": "read it here"}, {"url": "https://github.com/huggingface/tokenizers/tree/91f602f744f7fee72f2f5fa8d6c7c48bb1d72d3b/bindings/python", "anchor_text": "based on this by HuggingFace"}, {"url": "https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.cleanup_cache_files", "anchor_text": "datasets.Dataset.cleanup_cache_files()"}, {"url": "https://huggingface.co/docs/datasets/quicktour.html", "anchor_text": "based on this Tutorial"}, {"url": "https://unsplash.com/@tofi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tobias Fischer"}, {"url": "https://unsplash.com/s/photos/database?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf", "anchor_text": "Learning Word Vectors for Sentiment Analysis."}, {"url": "https://arxiv.org/abs/1609.08144", "anchor_text": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation."}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Bert: Pre-training of deep bidirectional transformers for language understanding."}, {"url": "https://medium.com/tag/nlp?source=post_page-----7030a179e9fa---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----7030a179e9fa---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----7030a179e9fa---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7030a179e9fa---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7030a179e9fa---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----7030a179e9fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----7030a179e9fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7030a179e9fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7030a179e9fa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7030a179e9fa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7030a179e9fa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7030a179e9fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gergely D. N\u00e9meth"}, {"url": "https://medium.com/@neged.ng/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "190 Followers"}, {"url": "https://ellisalicante.org", "anchor_text": "https://ellisalicante.org"}, {"url": "https://www.linkedin.com/in/gergely-nemeth-092b10137/", "anchor_text": "https://www.linkedin.com/in/gergely-nemeth-092b10137/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb43768ab187d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-nlp-datasets-in-python-7030a179e9fa&newsletterV3=71eefc6da84b&newsletterV3Id=b43768ab187d&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}