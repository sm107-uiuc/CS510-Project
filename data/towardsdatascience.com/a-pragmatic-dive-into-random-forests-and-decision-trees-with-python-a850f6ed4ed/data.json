{"url": "https://towardsdatascience.com/a-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed", "time": 1683005732.128931, "path": "towardsdatascience.com/a-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed/", "webpage": {"metadata": {"title": "A pragmatic dive into Random Forests and Decision Trees with Python | by Ben Fraser | Towards Data Science", "h1": "A pragmatic dive into Random Forests and Decision Trees with Python", "description": "A random forest is undeniably one of the best models for obtaining a quick and reasonable solution to most structured data problems. They can adapt to both regression and classification problems, are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "HERE", "paragraph_index": 3}, {"url": "https://www.kaggle.com/aungpyaeap/fish-market", "anchor_text": "Fish Market Kaggle dataset", "paragraph_index": 5}, {"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "this notebook", "paragraph_index": 10}, {"url": "http://course18.fast.ai/ml", "anchor_text": "FastAI Machine Learning For Coders Course", "paragraph_index": 22}, {"url": "http://course18.fast.ai/ml", "anchor_text": "FastAI Machine Learning For Coders Course", "paragraph_index": 77}, {"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "here", "paragraph_index": 86}], "all_paragraphs": ["A random forest is undeniably one of the best models for obtaining a quick and reasonable solution to most structured data problems. They can adapt to both regression and classification problems, are resistant to over-fitting (with sufficient estimators), and they can work without any data standardisation or creation of dummy variables. You won't get the best performance with this ensemble technique, however, their simplicity and value in interpreting our data make them extremely useful.", "Tree-based models are also effective at capturing complex relationships between data that linear models would fail miserably at, as the simple example below illustrates:", "In addition, tree-based methods offer brilliant interpretability and analysis functions to help explain how our models came to a particular result. For example, we can visualise the individual decision tree used above:", "Throughout this article, we\u2019ll be exploring Random Forests and Decision Trees in detail \u2014 in fact, we\u2019ll be coding both entirely from scratch in Python in order to fully appreciate their inner workings. This article supplements the Jupyter Notebook provided HERE, which I recommend having open as you follow along. This provides all the code and examples used throughout this article.", "The key to forming a random forest model is a decision tree\u2014 we combine many of them together to obtain a better overall performance. Individual trees suffer with high variance, but we significantly reduce this and simultaneously increase predictive performance by averaging many trees together.", "Let\u2019s take an example \u2014 a Random Forest with 6 individual estimators (decision trees) will form a total of 6 separate decision tree models. If we use the Fish Market Kaggle dataset as an example, each individual decision tree produces the following regression lines:", "Our Random Forest takes these individual predictions and averages them all, which produces a better fitting regression line overall:", "You\u2019ll notice the predictions are better in the Random Forest model when compared to any of the individual decision trees. This can be seen by the more optimal regression line and the higher R\u00b2 score. This is the benefit of ensembles; The combination of models together often results in better performances than any of the models on their own.", "If we just combine many decision trees together, we have a bagging model. For a random forest, there is a key difference; decorrelation. Rather than using all our features (columns of data) each time a split is made in a tree, a random forest only considers a random proportion of these features. This prevents one or two dominating features from making all individual trees similarly correlated. This reduces the model variance compared to a bagging model.", "As an overview, a random forest algorithm performs the following:", "In this blog, we\u2019ll create two classes. One will be a decision tree model, whilst the other will be a random forest. The Random forest model will operate by forming a large number of individual decision tree models and aggregating their individual predictions to form the overall predicted output. As highlighted previously, all code relating to this article is available in this notebook, so please check it out if you haven\u2019t already done so!", "We\u2019ll start by creating an individual decision tree. Before that, a quick background on decision tree fundamentals.", "A decision tree conducts a series of greedy recursive binary splits, each of which is calculated based on an objective function for Information Gain. For classification trees, common objective functions include Gini index, entropy and classification error:", "For regression trees, common objective functions include Mean Squared Error (MSE), Mean Absolute Error (MAE), or Standard Deviation.", "For this blog, we\u2019ll be using Standard Deviation Reduction to build a regression model. This function gives results equivalent to MSE:", "Conveniently, this formula can be re-arranged into a much better form for Python and Numpy vectorised calculations:", "This derivation allows us to determine the standard deviation using the square root of the mean sum of squares, minus the mean squared. Quite a mouthful \u2014 but in essence, this makes our Python implementation much faster and more efficient in terms of O(n) complexity.", "This formula (or any of the criteria above) are used to form a cost function, J, which is minimised during training. The decision tree algorithm used to perform splits is referred to as the Classification and Regression Tree (CART) algorithm (G\u00e9ron, 2019, p. 179), which uses the following cost function:", "This is applied recursively on every single split, until the maximum depth is reached. One of the most difficult concepts with a decision tree model is understanding how recursion works. Our tree model will be called many times from within each tree model produced. Each tree node performs a binary split (unless it is a leaf), which means each node creates two splits (left and right). Each split creates a new decision tree, and in turn, each new tree performs further left and right splits. This process repeats until our decision tree is of sufficient depth.", "Basically we form many trees within trees for each Random Forest Decision Tree, and this continues until we reach leaf nodes for all left and right splits.", "The maximum depth choice is very important and significantly impacts how our model can under or over-fit the data. For example, we can see it impacts the fitment to the Fish Market data in the following plots:", "Now that we\u2019ve covered the basics, let\u2019s move on to the practical elements of implementing a decision tree model as code.", "Whilst forming this model, a huge amount of credit needs to go to the Jeremy Howard and the FastAI Machine Learning For Coders Course. This course builds a foundational random forest and decision tree model, which I have largely based my Python implementations on.", "We\u2019ll start by showing the entire code for the decision tree, and sequentially talking through the main aspects.", "Let\u2019s get started \u2014 first, we need to initialise our decision tree with the required parameters, which include:", "When we form each a decision tree, we want it to immediately start binary splitting. This is performed by calling self.binary_split() in the class init method, as shown above.", "The purpose of this function is to iterate through our selected feature columns and find the best binary split in each. The code above finds the best split for a given column using the follow-up function best_binary_split, which is called iteratively for each feature.", "The model does this with all columns and finds the best score for a split. This gives us the best feature and index value at which the optimal left and right splits of data samples were found. As an example, this will give us a split in the Fish Market data like so:", "Above, all data samples with a width less than or equal to (\u2264) 5.167 make the first split, whilst the remaining samples form the other. We can see that 81 samples had a width \u2264 5.167, and 38 with a width > 5.167. Therefore, in this case, our model selected Width as the best feature for making a split, which was deduced using our standard deviation cost function.", "We then form further splits with these using more Decision Trees to repeat the same process (except with the smaller sub-sets of data):", "As we can see, \u2018Length3\u2019 was chosen as the best feature for making the follow-up splits in each case. We\u2019ll briefly discuss how this is done in the code implementation.", "Within the best_binary_split function we start by sorting our training data, x, and output labels, y. The argument passed to this function feat_idx corresponds to the index of the current column (remember this function is done for all columns iteratively).", "We then form initial variables for the counts, sums, and squared sums of our right and left splits of data for the current column. The idea is we start at zero for the left side, and at maximum (number of rows) for the right side. As we iterate through each row of the column data we can then update these values dynamically like so:", "After this update of values, we perform two basic checks for each iteration before going any further. The first is whether our left split currently has less samples than that chosen in the min_leaf hyper-parameter. The second is whether we have a current value that matches the next (duplicate). In either of these cases being true, we skip the current iteration and move to the next:", "Now we perform the main function \u2014 finding the current score using our standard deviation cost function:", "We need to find the score for both left and right splits. This is computed using the formula defined previously, but to recap the python implementation is as follows:", "Once we\u2019ve calculated each score, we need to form a weighted cost function like that defined in Equation 3 previously. We can do this in many ways, but one simple way is to just use the current left-hand and right-hand counts and multiple them by their respective standard deviation scores:", "Finally, after doing this we check whether the current score of this iteration is better than the previous best. Since this is a cost function the best score is the lowest value, so we seek to minimise it. If we find a better score, we save the current feature index (feat_idx), score and sample index (x_i) to our decision tree model.", "Once this iterative process has occurred for all of our features (as called from original function binary_split), we obtain the left and right splits of data corresponding to the best score, and form new decision trees for each split. This entire process repeats recursively until all splits from the original data become leaf nodes.", "To make sense of this better, let\u2019s quickly review the entire binary_split function:", "We start by selecting only a sub-sample of features to iteratively search for the best binary split. This is to decorrelate our large number of decision trees within the overall random forest model (more on this later). We iteratively call best_binary_split on each feature, which updates the values in self.score, self.feat_idx, and self.split.", "If the score in self.score equals infinity, then we know we are currently at a leaf node and therefore want to stop the recursive process on this node. This is done by returning from the function. If it is not a leaf node however, we carry on and form new decision trees for the best left and right splits found. We store these new decision trees as variables in the current decision tree as self.left_split and self.right_split, followed by repeating the same recursive process on each of these until leaf nodes are reached.", "That concludes the binary split process! You\u2019ll be thankful to know that this is by far the hardest aspect of understanding how a decision tree works, with everything else being relatively straightforward.", "For making predictions on a sample value with our model, we need to recursively compare that value to the tree split values in our model. We can visualise the typical path a sample goes through during the prediction phase in the representation below.", "In the representation above, the sample Width and Length3 features are compared against the split criteria of our Decision Tree model. By following the red arrows we arrive at the predicted value for this sample, which is a weight of 42.15 (as shown highlighted in green). This predicted value is obtained by simply finding the mean weight of all 22 samples in this leaf node.", "To achieve this using code, we use two functions:", "The first function (predict) iteratively calls the second (predict_sample) using a list comprehension. It does this in order to generate a set of predictions for a an input array, X.", "The second function is more detailed, and is called recursively through the tree splits until a leaf node is reached. We first check whether we are at a leaf node \u2014 and if so, then the predicted value is simply the mean of samples in that leaf, which is equal to self.val.", "Conversely, if it is not yet a leaf node we compare our predict sample to the feature split value for that node. If predict_sample \u2264 split_value, then we progress on to the left split, otherwise we choose the right split. We then repeat this process until a leaf node is reached in order to make our prediction.", "Thankfully, the Random Forest implementation is shorter and easier work. It functions as a higher level class that instantiates a large number of our decision trees.", "We\u2019ll start by looking at the code, and then progress by talking through the key features.", "The Random Forest model is actually quite simple when compared to the decision tree code. To instantiate the model we take in a range of parameters:", "As soon as our model is initialised it creates a number of decision trees equal to num_trees and stores these as self.trees:", "This calls our create_tree method a total of num_tree times, where the create_tree method is given as:", "This generates a set of random indexes of size sample_size, which represents a sample of data to use for our decision tree model. Providing our total training set is much larger than our sample size, each time this function is called it should produce a unique set of samples for each decision tree.", "In addition, if we have instantiated our Random Forest with bootstrap=True, then this function will generate sets of training samples with replacement. This will be explained more later on.", "Finally, we instantiate a decision tree for the given set of training samples and labels. We also pass some further arguments to each decision tree, including:", "For making predictions we have the predict function, which is short and simple:", "We pass our input prediction samples to every single Decision Tree predict function in our Random Forest model. This will give us a set of values corresponding to the predictions from each decision tree. To form our overall random forest prediction, we just calculate the mean of these values.", "That concludes the main bulk of our model, however, some key points relevant to Random Forests and how they relate to the above code still need to be explained. These will be covered in the remaining sections.", "A random forest decorrelates the similarity of its decision trees by selecting a random subset of features to choose from at each binary split. This is an absolutely essential component of a random forest, and is what sets it apart from a basic bagging model.", "To implement this in Python, all we need to do is ensure each time we perform a binary split in a decision tree we only select a subset of the features, rather than considering all of them.", "This is performed using the feat_proportion argument passed to the Decision Tree, as shown in the create_tree code above.", "Bootstrapping is the process of randomly sampling data from a population of data, with replacement. This means we will have duplicates, and also some samples that are not included at all within each sample.", "The samples not included in each bootstrapped sample are known as the Out-Of-Bag (OOB) samples.", "For example, say we want a sample size of 500 formed from a range of 500 training samples, then we can select 500 random indices using:", "For these samples, not all 500 original indices will be present, since many indices are duplicated. We can find the samples not included (out-of-bag) like so:", "For interest, if we form 500 bootstrapped samples from a dataset of 500 training examples, let's see on average how many OOB samples we get:", "As shown, we have a mean OOB sample size of around 183 for a training set of 500. This means for each bootstrapped sample we create, we have around 183 unseen samples that can be used to test the performance of our tree model formed.", "For our random forest model, we support bootstrapping by including a function to randomly sample a selected number of samples (with replacement).", "You\u2019ll notice that the function expects an \u2018idxs\u2019 argument, which is the range of training index values to select our bootstrapped sample from. We\u2019ve included this so that our model works proportionately with the selected sample size. If we didn\u2019t include this, our bootstrapped samples would have a very low probability of containing duplicates when using large datasets and a small sample size (dataset size >> sample size). In this case our OOB samples become much harder to manage, and this is something avoided by the method we adopted in the above code.", "With our function for creating bootstrapped samples with OOB created, all we need to do is call this prior to initialisation of each decision tree:", "As shown, we only obtain bootstrapped and OOB samples if we have initialised our Random Forest model with bootstrap=True. After initialising each decision tree, we keep track of what OOB samples were assigned to it by storing the original index values (of the dataset x) in an attribute named \u2018oob_idxs\u2019. As you\u2019ll see next, this allows us to determine the OOB score for each decision tree, and hence work out the overall average OOB score.", "With the foundational work conducted previously, finding the OOB scores for our model isn\u2019t too difficult. All we need to do is iterate through each individual decision tree and make predictions on the associated OOB samples, which can then be compared against the actual y labels to work out our scores.", "For this example we used the R2 Score, however you can implement any metric you like. This will ultimately depend on the nature of task we\u2019re doing, e.g. regression or classification.", "Notice that we iterate through each tree in sequence, calculate predictions for the OOB samples for that tree only, and then find the associated R\u00b2 score. We then append this score to our tree_oob_scores array, which then allows us to find the overall overage OOB score across all decision trees \u2014 this corresponds to our Random Forest OOB score.", "A last Random Forest feature worth mentioning is the ability to compute feature importances. We can grade this feature importance in multiple ways. The popular way (used by Scikit-Learn) is to measure how much a feature reduces the impurity on average across all trees in the forest. For our model, this would equate to measuring on average how strongly each feature reduces the standard deviation cost function across all our tree models.", "Another way to calculate importances, as highlighted within the FastAI Machine Learning For Coders Course, is to randomly shuffle each feature column and measure how much it impacts the evaluation score. This is done after training of a model on the dataset, and is performed on each feature one at a time in order to compare how much it reduces (or even improves) the evaluated score. Those features that cause the largest drop represent the most important, whilst those that have minimal impact are the least important.", "For our model, we implemented feature importance using the latter approach:", "We first find a baseline R2 score using the original data, and then iteratively shuffle each column individually and find the score change as a result. At any one time only one feature column is randomly shuffled; all other columns remain in their original non-shuffled state. We end the function by forming these importances into a DataFrame, and returning this sorted in terms of importance for convenience.", "By calling this function on our Random Forest instance, we can easily plot our relative feature importances like so:", "Which gives us a nice representation of the importances like so:", "Voila! I especially like this approach, since its a less-complicated, equally effective method of calculating the feature importance of a dataset. However, this does rely on usage of a good, representative dataset for obtaining these importances, otherwise our results will be unreliable. It should also be noted that since it uses a different method, it may give slightly varying results to that obtained through the Scikit-Learn Random Forest model too.", "And with that now complete\u2026 that pretty much wraps up everything we needed to cover in this article. There\u2019s a wide variety of other things you can do with Random Forests, however I think we\u2019ve managed to scratch the surface with the essentials! I hope you enjoyed it!", "As we\u2019ve seen in this article, even though Random Forests and Decision Trees are conceptually straightforward, they\u2019re still difficult to explain with a full code implementation and walkthrough of the major points. Although this model is by no means a replacement of a professional Random Forest model, it should serve as a useful one for learning purposes. I\u2019ve no doubt there are many underlying inefficiencies and use-case issues with our models, but for gaining an understanding of Random Forests it should be sufficient!", "Hopefully, you\u2019ve enjoyed this content, and in the process managed to learn or revise the key concepts. Just like neural networks and other machine learning algorithms, you should never have to actually implement a Random Forest or Decision Tree from scratch. Nevertheless, a solid understanding of the underlying concepts helps massively with using these tools in practice, and more importantly, it helps you gain the skills to interpret and tune the hyper-parameters to best suit your own projects.", "For a full code implementation of everything seen in this article, you can find it at my GitHub repo here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Aerospace and electronic systems engineer. Ministry of Defence, United Kingdom."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa850f6ed4ed&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@benjamindavidfraser?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Ben Fraser"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe64b86a0996&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&user=Ben+Fraser&userId=be64b86a0996&source=post_page-be64b86a0996----a850f6ed4ed---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "HERE"}, {"url": "https://www.kaggle.com/aungpyaeap/fish-market", "anchor_text": "Fish Market Kaggle dataset"}, {"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "this notebook"}, {"url": "http://course18.fast.ai/ml", "anchor_text": "FastAI Machine Learning For Coders Course"}, {"url": "http://course18.fast.ai/ml", "anchor_text": "FastAI Machine Learning For Coders Course"}, {"url": "https://github.com/BenjaminFraser/data-analysis/blob/master/Random_forests_and_decision_trees.ipynb", "anchor_text": "here"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----a850f6ed4ed---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----a850f6ed4ed---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a850f6ed4ed---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----a850f6ed4ed---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a850f6ed4ed---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&user=Ben+Fraser&userId=be64b86a0996&source=-----a850f6ed4ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&user=Ben+Fraser&userId=be64b86a0996&source=-----a850f6ed4ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa850f6ed4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a850f6ed4ed---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a850f6ed4ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ben Fraser"}, {"url": "https://medium.com/@benjamindavidfraser/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "57 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe64b86a0996&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&user=Ben+Fraser&userId=be64b86a0996&source=post_page-be64b86a0996--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8eb7d91aff82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pragmatic-dive-into-random-forests-and-decision-trees-with-python-a850f6ed4ed&newsletterV3=be64b86a0996&newsletterV3Id=8eb7d91aff82&user=Ben+Fraser&userId=be64b86a0996&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}