{"url": "https://towardsdatascience.com/gpt-3-primer-67bc2d821a00", "time": 1683012824.4455092, "path": "towardsdatascience.com/gpt-3-primer-67bc2d821a00/", "webpage": {"metadata": {"title": "GPT-3 Primer. Understanding OpenAI\u2019s cutting-edge\u2026 | by Scott Huston | Towards Data Science", "h1": "GPT-3 Primer", "description": "GPT-3 is likely the most computationally-expensive machine learning model. The neural network\u2019s 175 billion parameters make it about ten times larger than the previous largest language model (Turing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "Turing NLG", "paragraph_index": 0}, {"url": "https://twitter.com/sharifshameem/status/1282676454690451457?s=20", "anchor_text": "video here", "paragraph_index": 9}, {"url": "https://twitter.com/SamanyouGarg/status/1295039749221097472?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1295039749221097472%7Ctwgr%5E&ref_url=https%3A%2F%2Fwww.scotthuston.com%2F2020-08-16-GPT-3_primer%2F", "anchor_text": "video here", "paragraph_index": 10}, {"url": "https://beta.openai.com/?app=productivity&example=4_4_0", "anchor_text": "video here", "paragraph_index": 11}, {"url": "https://vimeo.com/427957683/10634d1706", "anchor_text": "video here", "paragraph_index": 12}, {"url": "https://twitter.com/paraschopra/status/1284801028676653060?s=20", "anchor_text": "video here)", "paragraph_index": 13}, {"url": "https://play.aidungeon.io/", "anchor_text": "AI Dungeon", "paragraph_index": 14}, {"url": "https://beta.openai.com/", "anchor_text": "API", "paragraph_index": 15}, {"url": "https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u", "anchor_text": "sign up for here", "paragraph_index": 15}, {"url": "https://play.aidungeon.io/", "anchor_text": "AI Dungeon", "paragraph_index": 16}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "One estimate", "paragraph_index": 21}, {"url": "https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/", "anchor_text": "Another", "paragraph_index": 21}, {"url": "https://www.businessinsider.com/2008/4/googles-ginormous-food-budget-7530-per-googler", "anchor_text": "more on food", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/2001.08361.pdf", "anchor_text": "This paper", "paragraph_index": 27}, {"url": "https://lambdalabs.com/blog/gpt-3/", "anchor_text": "Lambda Labs aggregates and summarizes other content", "paragraph_index": 31}, {"url": "https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang", "anchor_text": "Analysis of potential constraints to scaling future models", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab", "anchor_text": "Examples of uses, and details about API parameters", "paragraph_index": 33}, {"url": "https://github.com/elyase/awesome-gpt3", "anchor_text": "Collection of more demos and articles", "paragraph_index": 34}], "all_paragraphs": ["GPT-3 is likely the most computationally-expensive machine learning model. The neural network\u2019s 175 billion parameters make it about ten times larger than the previous largest language model (Turing NLG, 17 billion parameters, released by Microsoft in February 2020). The 430GB of text GPT-3 was trained on was drawn widely from the internet and supplemented with text from books. The model works by seeing some amount of text that has come previously (up to a maximum of about 2,000 words) and predicting the next word to generate novel text.", "Users interact with the model by providing a prompt. An example prompt for a chatbot-style interaction from OpenAI (the organization that created GPT-3) is \u201cThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly\u201d. In addition to supplying a prompt, users are able to specify certain parameters for things like how long the output should be, how likely words are to be repeated, or the randomness of the output.", "GPT-3 demonstrates reasonable proficiency on almost all standard natural language processing benchmarks, including state-of-the-art performance on a few of them. The benchmarks include challenges such as using a paragraph of context to predict the last word of a related sentence and determining which noun a grammatically ambiguous but contextually unambiguous pronoun refers to. Other benchmarks involve translating between languages and answering general knowledge questions. This proficiency was achieved without the task-specific fine-tuning that most cutting-edge models use. GPT-3 is capable of being fine-tuned, and further fine-tuning would almost certainly improve the results of the model on each of the specific benchmarks (at the expense of worse performance outside of the task it was fine-tuned on).", "OpenAI also tested GPT-3 on some non-standard tasks:", "A sample of around 80 people was asked to distinguish between real articles and articles with the last 200 words generated by GPT-3. The participants were unable to reliably distinguish between the real articles and those completed by GPT-3 (participants correctly categorized 52% of the articles they saw, 50% was within the 95% confidence interval). The participants did not improve their accuracy when the amount of text generated by the model was increased to 500 words (accuracy stayed at 52%).", "When asked to complete SAT analogy problems, the model correctly answered 14% more problems than an average college applicant.", "The chart below shows the accuracy of the model when it is prompted with several example math problems and then asked to answer one. The results for the model I\u2019ve been referring to as GPT-3 is on the far right (175B). OpenAI created several versions of the model to test how performance varied across different model sizes. Larger models show a marked improvement.", "Overall, the model is able to successfully answer two-digit addition and subtraction problems reliably. For all other problem types, the model is not able to consistently give the correct answer but is significantly better than chance.", "Metrics are one thing, but the best way to get a feel for the capabilities of the model is to see the outputs. Many people are demonstrating potential use cases for GPT-3. Here are some highlights:", "Creating layouts in JavaScript (video here)", "Creating an API in Python (video here)", "Creating functions in Python (video here)", "Summarizing an NDA for a second grader (video here)", "\u201cSearch engine\u201d\u2026 that doesn\u2019t actually search (video here)", "Of course, it\u2019s hard to judge the model based solely on a few cherry-picked examples. It seems to be relatively easy to demonstrate impressive capabilities. Generating results that are reliably good enough to use in some sort of production setting (ie. as a customer service bot) is a very different story. It\u2019s likely that the model will be most useful in either systems with a human in the loop (perhaps generating a suggested response for a human to approve or edit), or for applications that don\u2019t require consistently good results (such as generating fun fictional stories like AI Dungeon).", "The model will be available through an API. OpenAI currently has a private beta release of the API with a waitlist you can sign up for here. Pricing information for the API hasn\u2019t been announced yet, but we know that the electricity costs of generating 100 pages of content from the model are a few cents. A cost of using the API in the range of $0.50 to $5 per 100 pages generated would seem to be reasonable in order to pay back the initial costs of creating the model, but it\u2019s hard to say.", "Alternatively, you can access the model through AI Dungeon. Note that the free tier of AI Dungeon uses text generated by GPT-2, not GPT-3. In order to use GPT-3, you will need to sign up for the paid version (though the first 7 days are free). After signing up, you will need to change the settings to use the \u201cDragon\u201d model (aka GPT-3) as opposed to the \u201cGriffin\u201d model (aka GPT-2). The paid version also includes an option for custom prompts (\u201cscenarios\u201d) which means you don\u2019t need to use the standard story prompts.", "First, the wide-ranging capabilities of the model exceed what is publicly available. It\u2019s difficult to predict what people will be able to make with the model, but it\u2019s likely the model will be used in new ways and improve results in areas where language models are already used.", "In addition to the practical new uses of the model, there are some interesting takeaways from the research:", "Perhaps the most important point is that larger models continue to perform better. Prior to GPT-3, researchers had observed a power-law relationship between model size and performance. They saw that there were diminishing returns to using additional computational power during the training of models, but still significant performance gains for more expensive models. Despite the trend at lower levels of computation, there was some debate about how far that trend could be extrapolated. After GPT-3 it\u2019s still not clear where the limits of that trend may be, but we haven\u2019t reached them yet. Despite GPT-3 being ten times larger than the previous largest model, it\u2019s performance is what would be expected from the previously observed trend.", "The above graph shows model performance (lower is better) across a range of model sizes and computational expenditure. GPT-3 is the yellow line, and the power-law represented by the dotted line seems to be holding at all the model sizes OpenAI tested.", "There are multiple estimates for how much it cost OpenAI to train GPT-3. One estimate says $4.6 million. Another says $12 million. Neither includes researcher compensation. Regardless of the true number, the takeaway doesn\u2019t change. GPT-3 was extraordinarily cheap to produce given its potential applications, and larger models will likely follow. Google spent much more on food in 2008 than OpenAI just spent to create a state-of-the-art language model with commercial applications. There\u2019s plenty of money to push towards larger models if that direction is deemed promising enough. After GPT-3 it\u2019s hard to argue against larger models being significantly more effective. Funding is not the only constraint towards creating more powerful models. There is a significant amount of novel engineering that needs to be done to train this kind of model, but OpenAI is not the only organization with the talent to accomplish that.", "The fact that GPT-3 has the ability to do arithmetic, when only very few of the specific problems it was tested on were likely to be in the training data, implies the model is somehow actually learning how to do the mathematical operations. That point is further supported by the authors of the paper stating, \u201cinspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \u20181\u2019, suggesting it is actually attempting to perform the relevant computation rather than memorizing a table\u201d. GPT-3 also correctly answers about 20% of single digit combined operations (for example, 9*(7+5)) \u2014 a rate much better than random chance. It is remarkable that a model trained simply to predict the next word in a text appears to be learning how to do math in order to better predict the next word. These results raise questions about what new capabilities models might acquire at a significantly larger scale. For example, could a sufficiently powerful language model read thousands of scientific papers and use that data to successfully predict the results of novel experiments?", "Most large, publicly available machine learning systems take the approach of doing a large amount of training on some sort of generalized data and then fine-tuning the model on domain-specific data. GPT-3 demonstrates proficiency in many domains by replacing the fine-tuning step with what OpenAI has dubbed \u201cfew-shot learning\u201d. Few-shot learning is simply showing the model a few successful examples of what you want it to do in the prompt the model is given. For example, a prompt to get the model to successfully answer general-knowledge questions might look like this, with the last question being the one you want GPT-3 to answer.", "It is also possible to use the model by providing a prompt with no examples (\u201czero-shot\u201d) or one example (\u201cone shot\u201d), but the model generally performs better the more examples it sees.", "The few-shot learning approach has several benefits:", "As the graph below shows, few-shot learning works better the larger the model is. Few-shot learning is not just a viable alternative to fine-tuning with the current state of machine learning, it will continue to get more effective with larger future models. The increasing effectiveness of few-shot learning combined with the direct performance gains from increasing model size will likely cause a trend towards larger models that use few-shot learning.", "This paper by OpenAI investigates the scaling of language models. The researchers treat model performance as a function of the size of the model, the amount of training data, and the computational power used to train the model. They find a power-law relationship where more scaling the inputs reliably leads to better performance. Although the paper was written prior to GPT-3, the new model is consistent with the relationship they found even though it is at a scale much greater than they were able to test. The researchers extrapolate the trend to find the point at which a model (using the optimal ratio of inputs) would reach the theoretical maximum performance of a similar language model \u2014 a point where all of the information has been extracted from text. It\u2019s entirely possible that this pattern will break for unforeseen reasons before reaching that point. If the trend holds however, the researchers estimate that maximum performance would be reached by a model with about 1 trillion parameters, trained on 1 trillion tokens (about 1.4 terabytes), and using about 10,000 petaflop/s-days of compute (pg. 17).", "The paper cautions, \u201cthe numerical values are highly uncertain, varying by an order of magnitude in either direction depending on the precise values of the exponents from the power-law fits. The most obvious interpretation is that our scaling laws break down at or before we reach this point, which is still many orders of magnitude away in both compute and model size\u201d. That was written before GPT-3, and GPT-3 is within an order of magnitude now. The equation from that paper predicts training loss to be 1.75 with 10,000 petaflop/s-days of compute, while the updated equation from the GPT-3 paper predicts a training loss of 1.65. After updating the trend line with the newest data from GPT-3, the theoretical best language model appears more achievable than the previous paper (and the numbers here) show.", "It\u2019s worth noting that, assuming the trend doesn\u2019t break down, it likely underestimates future performance. The relationship does not account for future improvements in training techniques. OpenAI has used a consistent process for training various versions of their GPT model, but other researchers have continued to improve the training process of similar models. GPT-3 was not trained in a cutting-edge way.", "If a next-generation model scales as much as GPT-3 did, it will be well beyond the theoretical best model predicted by the power-law that has been observed so far. If the trend breaks, we\u2019ll get important information about the limits of current approaches. If the trend doesn\u2019t break, we\u2019ll be living in a very different world.", "Lambda Labs aggregates and summarizes other content", "Analysis of potential constraints to scaling future models", "Examples of uses, and details about API parameters", "Collection of more demos and articles", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist interested in the future of machine learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F67bc2d821a00&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@scottphuston?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottphuston?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Scott Huston"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F845aae6a813a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&user=Scott+Huston&userId=845aae6a813a&source=post_page-845aae6a813a----67bc2d821a00---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "Turing NLG"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 paper"}, {"url": "https://twitter.com/sharifshameem/status/1282676454690451457?s=20", "anchor_text": "video here"}, {"url": "https://twitter.com/SamanyouGarg/status/1295039749221097472?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1295039749221097472%7Ctwgr%5E&ref_url=https%3A%2F%2Fwww.scotthuston.com%2F2020-08-16-GPT-3_primer%2F", "anchor_text": "video here"}, {"url": "https://beta.openai.com/?app=productivity&example=4_4_0", "anchor_text": "video here"}, {"url": "https://vimeo.com/427957683/10634d1706", "anchor_text": "video here"}, {"url": "https://twitter.com/f_j_j_/status/1283349995144359937?s=20", "anchor_text": "Writing like an attorney"}, {"url": "https://twitter.com/paraschopra/status/1284801028676653060?s=20", "anchor_text": "video here)"}, {"url": "https://openai-public.s3-us-west-2.amazonaws.com/beta/looped/poetry.mp4", "anchor_text": "Writing poetry"}, {"url": "https://github.com/elyase/awesome-gpt3", "anchor_text": "here"}, {"url": "https://play.aidungeon.io/", "anchor_text": "AI Dungeon"}, {"url": "https://beta.openai.com/", "anchor_text": "API"}, {"url": "https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u", "anchor_text": "sign up for here"}, {"url": "https://play.aidungeon.io/", "anchor_text": "AI Dungeon"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 paper"}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "One estimate"}, {"url": "https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/", "anchor_text": "Another"}, {"url": "https://www.businessinsider.com/2008/4/googles-ginormous-food-budget-7530-per-googler", "anchor_text": "more on food"}, {"url": "https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab", "anchor_text": "this post"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 paper"}, {"url": "https://arxiv.org/pdf/2001.08361.pdf", "anchor_text": "This paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 paper"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2 paper"}, {"url": "https://arxiv.org/pdf/2001.08361.pdf", "anchor_text": "the scaling paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 paper"}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "OpenAI blogpost"}, {"url": "https://www.gwern.net/newsletter/2020/05#gpt-3", "anchor_text": "Gwern\u2019s post"}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "Lambda Labs post"}, {"url": "https://lambdalabs.com/blog/gpt-3/", "anchor_text": "Lambda Labs aggregates and summarizes other content"}, {"url": "https://minimaxir.com/2020/07/gpt3-expectations/", "anchor_text": "Good overview"}, {"url": "https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/", "anchor_text": "Slatestarcodex post"}, {"url": "https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang", "anchor_text": "Analysis of potential constraints to scaling future models"}, {"url": "https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab", "anchor_text": "Examples of uses, and details about API parameters"}, {"url": "https://www.youtube.com/watch?v=_8yVOC4ciXc", "anchor_text": "Computerfile video"}, {"url": "https://github.com/elyase/awesome-gpt3", "anchor_text": "Collection of more demos and articles"}, {"url": "https://medium.com/tag/gpt-3?source=post_page-----67bc2d821a00---------------gpt_3-----------------", "anchor_text": "Gpt 3"}, {"url": "https://medium.com/tag/nlp?source=post_page-----67bc2d821a00---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/openai?source=post_page-----67bc2d821a00---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/ai?source=post_page-----67bc2d821a00---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----67bc2d821a00---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&user=Scott+Huston&userId=845aae6a813a&source=-----67bc2d821a00---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&user=Scott+Huston&userId=845aae6a813a&source=-----67bc2d821a00---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F67bc2d821a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----67bc2d821a00---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----67bc2d821a00--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----67bc2d821a00--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----67bc2d821a00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottphuston?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottphuston?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Scott Huston"}, {"url": "https://medium.com/@scottphuston/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "23 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F845aae6a813a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&user=Scott+Huston&userId=845aae6a813a&source=post_page-845aae6a813a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56ecec952d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-primer-67bc2d821a00&newsletterV3=845aae6a813a&newsletterV3Id=56ecec952d8e&user=Scott+Huston&userId=845aae6a813a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}