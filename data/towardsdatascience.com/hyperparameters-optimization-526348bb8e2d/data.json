{"url": "https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d", "time": 1683000612.940714, "path": "towardsdatascience.com/hyperparameters-optimization-526348bb8e2d/", "webpage": {"metadata": {"title": "Hyperparameters Optimization. An introduction on how to fine-tune\u2026 | by Pier Paolo Ippolito | Towards Data Science", "h1": "Hyperparameters Optimization", "description": "The model parameters define how to use input data to get the desired output and are learned at training time. Instead, Hyperparameters determine how our model is structured in the first place\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "Credit Card Fraud Detection Kaggle Dataset", "paragraph_index": 5}, {"url": "https://github.com/pierpaolo28/Kaggle-Challenges/blob/master/credit-card-fraud-model-tuning.ipynb", "anchor_text": "GitHub repository", "paragraph_index": 7}, {"url": "https://www.kaggle.com/pierpaolo28/credit-card-fraud-model-tuning", "anchor_text": "Kaggle Profile", "paragraph_index": 7}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Documentation", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/interactive-data-visualization-167ae26016e8", "anchor_text": "Plotly", "paragraph_index": 25}, {"url": "https://www.kaggle.com/kernels/scriptcontent/20590929/download", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9", "anchor_text": "Xoel L\u00f3pez Barata", "paragraph_index": 25}, {"url": "https://epistasislab.github.io/tpot/", "anchor_text": "TPOT Auto Machine Learning library", "paragraph_index": 43}, {"url": "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/", "anchor_text": "here", "paragraph_index": 46}, {"url": "https://medium.com/@pierpaoloippolito28?source=post_page---------------------------", "anchor_text": "follow me on Medium", "paragraph_index": 51}, {"url": "http://eepurl.com/gwO-Dr?source=post_page---------------------------", "anchor_text": "mailing list", "paragraph_index": 51}, {"url": "https://linktr.ee/pierpaolo28", "anchor_text": "https://linktr.ee/pierpaolo28", "paragraph_index": 53}], "all_paragraphs": ["Machine Learning models are composed of two different types of parameters:", "The model parameters define how to use input data to get the desired output and are learned at training time. Instead, Hyperparameters determine how our model is structured in the first place.", "Machine Learning models tuning is a type of optimization problem. We have a set of hyperparameters and we aim to find the right combination of their values which can help us to find either the minimum (eg. loss) or the maximum (eg. accuracy) of a function (Figure 1).", "This can be particularly important when comparing how different Machine Learning models performs on a dataset. In fact, it would be unfair for example to compare an SVM model with the best Hyperparameters against a Random Forest model which has not been optimized.", "In this post, the following approaches to Hyperparameter optimization will be explained:", "In order to demonstrate how to perform Hyperparameters Optimization in Python, I decided to perform a complete Data Analysis of the Credit Card Fraud Detection Kaggle Dataset. Our objective in this article will be to correctly classify which credit card transactions should be labelled as fraudulent or genuine (binary classification). This Dataset has been anonymized before being distributed, therefore, the meaning of most of the features has not been disclosed.", "In this case, I decided to use just a subset of the dataset, in order to speed up training times and make sure to achieve a perfect balance between the two different classes. Additionally, just a limited amount of features has been used to make the optimization tasks more challenging. The final dataset is shown in the figure below (Figure 2).", "All the code used in this article (and more!) is available in my GitHub repository and Kaggle Profile.", "First of all, we need to divide our dataset into training and test sets.", "Throughout this article, we will use a Random Forest Classifier as our model to optimize.", "Random Forest models are formed by a large number of uncorrelated decision trees, which joint together constitute an ensemble. In Random Forest, each decision tree makes its own prediction and the overall model output is selected to be the prediction which appeared most frequently.", "We can now start by calculating our base model accuracy.", "Using the Random Forest Classifier with the default scikit-learn parameters lead to 95% overall accuracy. Let\u2019s see now if applying some optimization techniques we can achieve better accuracy.", "When using Manual Search, we choose some model hyperparameters based on our judgment/experience. We then train the model, evaluate its accuracy and start the process again. This loop is repeated until a satisfactory accuracy is scored.", "The main parameters used by a Random Forest Classifier are:", "More information about Random Forest parameters can be found on the scikit-learn Documentation.", "As an example of Manual Search, I tried to specify the number of estimators in our model. Unfortunately, this didn\u2019t lead to any improvement in accuracy.", "In Random Search, we create a grid of hyperparameters and train/test our model on just some random combination of these hyperparameters. In this example, I additionally decided to perform Cross-Validation on the training set.", "When performing Machine Learning tasks, we generally divide our dataset in training and test sets. This is done so that to test our model after having trained it (in this way we can check it\u2019s performances when working with unseen data). When using Cross-Validation, we divide our training set into N other partitions to make sure our model is not overfitting our data.", "One of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our training set into N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times the model we then average the training results obtained in each iteration to obtain our overall training performance results (Figure 3).", "Using Cross-Validation when implementing Hyperparameters optimization can be really important. In this way, we might avoid using some Hyperparameters which works really good on the training data but not so good with the test data.", "We can now start implementing Random Search by first defying a grid of hyperparameters which will be randomly sampled when calling RandomizedSearchCV(). For this example, I decided to divide our training set into 4 Folds (cv = 4) and select 80 as the number of combinations to sample (n_iter = 80). Using the scikit-learn best_estimator_ attribute, we can then retrieve the set of hyperparameters which performed best during training to test our model.", "Once trained our model, we can then visualize how changing some of its Hyperparameters can affect the overall model accuracy (Figure 4). In this case, I decided to observe how changing the number of estimators and the criterion can affect our Random Forest accuracy.", "We can then take this a step further by making our visualization more interactive. In the chart below, we can examine (using the slider) how varying the number of estimators in our model can affect the overall accuracy of our model considered the selected min_split and min_leaf parameters.", "Feel free to play with the graph below by changing the n_estimators parameters, zooming in and out of the graph, changing it\u2019s orientation and hovering over the single data points to get additional information about them!", "If you are interested in finding out more about how to create these animations using Plotly, my code is available here. Additionally, this has also been covered in an article written by Xoel L\u00f3pez Barata.", "We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model.", "In Grid Search, we set up a grid of hyperparameters and train/test our model on each of the possible combinations.", "In order to choose the parameters to use in Grid Search, we can now look at which parameters worked best with Random Search and form a grid based on them to see if we can find a better combination.", "Grid Search can be implemented in Python using scikit-learn GridSearchCV() function. Also on this occasion, I decided to divide our training set into 4 Folds (cv = 4).", "When using Grid Search, all the possible combinations of the parameters in the grid are tried. In this case, 128000 combinations (2 \u00d7 10 \u00d7 4 \u00d7 4 \u00d7 4 \u00d7 10) will be used during training. Instead, in the Grid Search example before, just 80 combinations have been used.", "Grid Search is slower compared to Random Search but it can be overall more effective because it can go through the whole search space. Instead, Random Search can be faster fast but might miss some important points in the search space.", "When using Automated Hyperparameter Tuning, the model hyperparameters to use are identified using techniques such as: Bayesian Optimization, Gradient Descent and Evolutionary Algorithms.", "Bayesian Optimization can be performed in Python using the Hyperopt library. Bayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can give us the lowest possible output value.", "Bayesian optimization has been proved to be more efficient than random, grid or manual search. Bayesian Optimization can, therefore, lead to better performance in the testing phase and reduced optimization time.", "In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin().", "Additionally, can also be defined in fmin() the maximum number of evaluations to perform.", "Bayesian Optimization can reduce the number of search iterations by choosing the input values bearing in mind the past outcomes. In this way, we can concentrate our search from the beginning on values which are closer to our desired output.", "We can now run our Bayesian Optimizer using the fmin() function. A Trials() object is first created to make possible to visualize later what was going on while the fmin() function was running (eg. how the loss function was changing and how to used Hyperparameters were changing).", "We can now retrieve the set of best parameters identified and test our model using the best dictionary created during training. Some of the parameters have been stored in the best dictionary numerically using indices, therefore, we need first to convert them back as strings before input them in our Random Forest.", "The classification report using Bayesian Optimization is shown below.", "Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts. They are inspired by the Darwinian process of Natural Selection and they are therefore also usually called as Evolutionary Algorithms.", "Let\u2019s imagine we create a population of N Machine Learning models with some predefined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that perform best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that to get again a population of N models. At this point, we can again calculate the accuracy of each model and repeat the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.", "In order to implement Genetic Algorithms in Python, we can use the TPOT Auto Machine Learning library. TPOT is built on the scikit-learn library and it can be used for either regression or classification tasks.", "The training report and the best parameters identified using Genetic Algorithms are shown in the following snippet.", "The overall accuracy of our Random Forest Genetic Algorithm optimized model is shown below.", "Using KerasClassifier wrapper, it is possible to apply Grid Search and Random Search for Deep Learning models in the same way it was done when using scikit-learn Machine Learning models. In the following example, we will try to optimize some of our ANN parameters such as: how many neurons to use in each layer and which activation function and optimizer to use. More examples of Deep Learning Hyperparameters optimization are available here.", "The overall accuracy scored using our Artificial Neural Network (ANN) can be observed below.", "We can now compare how all the different optimization techniques performed on this given exercise. Overall, Random Search and Evolutionary Algorithms performed best.", "The results obtained, are highly dependent on the chosen grid space and dataset used. Therefore, in different situations, different optimization techniques will perform better than others.", "I hope you enjoyed this article, thank you for reading!", "If you want to keep updated with my latest articles and projects follow me on Medium and subscribe to my mailing list. These are some of my contacts details:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Analytics @ Swiss Re, TDS Associate Editor and Freelancer. https://linktr.ee/pierpaolo28"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F526348bb8e2d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pierpaoloippolito28.medium.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": ""}, {"url": "https://pierpaoloippolito28.medium.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Pier Paolo Ippolito"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb8391a6a5f1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=post_page-b8391a6a5f1a----526348bb8e2d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "Credit Card Fraud Detection Kaggle Dataset"}, {"url": "https://github.com/pierpaolo28/Kaggle-Challenges/blob/master/credit-card-fraud-model-tuning.ipynb", "anchor_text": "GitHub repository"}, {"url": "https://www.kaggle.com/pierpaolo28/credit-card-fraud-model-tuning", "anchor_text": "Kaggle Profile"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Documentation"}, {"url": "https://towardsdatascience.com/interactive-data-visualization-167ae26016e8", "anchor_text": "Plotly"}, {"url": "https://www.kaggle.com/kernels/scriptcontent/20590929/download", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9", "anchor_text": "Xoel L\u00f3pez Barata"}, {"url": "https://epistasislab.github.io/tpot/", "anchor_text": "TPOT Auto Machine Learning library"}, {"url": "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/", "anchor_text": "here"}, {"url": "https://medium.com/@pierpaoloippolito28?source=post_page---------------------------", "anchor_text": "follow me on Medium"}, {"url": "http://eepurl.com/gwO-Dr?source=post_page---------------------------", "anchor_text": "mailing list"}, {"url": "https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------", "anchor_text": "Linkedin"}, {"url": "https://pierpaolo28.github.io/blog/?source=post_page---------------------------", "anchor_text": "Personal Blog"}, {"url": "https://pierpaolo28.github.io/?source=post_page---------------------------", "anchor_text": "Personal Website"}, {"url": "https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------", "anchor_text": "Medium Profile"}, {"url": "https://github.com/pierpaolo28?source=post_page---------------------------", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/pierpaolo28?source=post_page---------------------------", "anchor_text": "Kaggle"}, {"url": "https://dkopczyk.quantee.co.uk/hyperparameter-optimization/", "anchor_text": "https://dkopczyk.quantee.co.uk/hyperparameter-optimization/"}, {"url": "http://ethen8181.github.io/machine-learning/model_selection/model_selection.html", "anchor_text": "http://ethen8181.github.io/machine-learning/model_selection/model_selection.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----526348bb8e2d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----526348bb8e2d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----526348bb8e2d---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/programming?source=post_page-----526348bb8e2d---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----526348bb8e2d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=-----526348bb8e2d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=-----526348bb8e2d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F526348bb8e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----526348bb8e2d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----526348bb8e2d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----526348bb8e2d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----526348bb8e2d--------------------------------", "anchor_text": ""}, {"url": "https://pierpaoloippolito28.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pierpaoloippolito28.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pier Paolo Ippolito"}, {"url": "https://pierpaoloippolito28.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.1K Followers"}, {"url": "https://linktr.ee/pierpaolo28", "anchor_text": "https://linktr.ee/pierpaolo28"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb8391a6a5f1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=post_page-b8391a6a5f1a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e32d8fa9b0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-optimization-526348bb8e2d&newsletterV3=b8391a6a5f1a&newsletterV3Id=4e32d8fa9b0e&user=Pier+Paolo+Ippolito&userId=b8391a6a5f1a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}