{"url": "https://towardsdatascience.com/the-intuition-behind-markov-chains-713e6ec6ce92", "time": 1683006446.347471, "path": "towardsdatascience.com/the-intuition-behind-markov-chains-713e6ec6ce92/", "webpage": {"metadata": {"title": "The Intuition Behind Markov Chains | by Kyle Chan | Towards Data Science", "h1": "The Intuition Behind Markov Chains", "description": "A simple introduction to Markov chains, stationary distribution, irreducible, aperiodic, ergodic Markov chains, and the ergodic theorem."}, "outgoing_paragraph_urls": [{"url": "http://www.kyleichan.com", "anchor_text": "www.kyleichan.com", "paragraph_index": 39}], "all_paragraphs": ["Most explanations of Markov chains dive right into the math and statistics without offering any intuitive sense of the concept. Textbooks and lectures often emphasize precision and efficiency over actual comprehensibility. In this article, I\u2019ll try to provide the basic intuition behind Markov chains using a simple example and as little math or statistics as possible. This article is not meant to be comprehensive but rather aims to complement the more formal explanations you see in textbooks and elsewhere.", "A Markov chain is a series of events where the probability of something happening depends only on what happened right before it. For musicians, it\u2019s like having the success of your next album depend only on the success of your latest album. Whatever happened in the distant past doesn\u2019t matter.", "Let\u2019s think about 4 time periods. In a Markov chain, whatever happens in Time 4 depends only on the situation in Time 3. Anything that happened in Time 1 or Time 2 doesn\u2019t really matter for Time 4.", "There are two general reasons why we should care about Markov chains.", "First, many things in the real world behave sort of like Markov chains. We can think of examples of things where the immediate future mostly depends on the recent past rather than on all of history.", "Weather is a common example. If it\u2019s rainy today, then there\u2019s a decent chance it\u2019ll be rainy tomorrow. Tomorrow\u2019s weather will probably be like today\u2019s weather. Whether it rained or not a month ago doesn\u2019t really affect tomorrow\u2019s weather forecast. This isn\u2019t a perfect example of a Markov chain because we have things like seasons, but it fits pretty well.", "Second, Markov chains allow us to reduce complex problems to a set of simple steps that can be easily programmed and run on a computer. When we have a computer execute these same steps thousands of times, we can produce very close answers to problems that we wouldn\u2019t be able to solve otherwise. Later, I\u2019ll touch on how Markov chains allow us to use a set of machine learning techniques called Markov chain Monte Carlo methods.", "So how do Markov chains work? It\u2019s helpful to walk through an example.", "Let\u2019s think about smartphone users, which we can divide into two types: iPhone users and Android users. These are our possible states. Being an iPhone user is one state. Being an Android user is another state. The full set of possibilities is the state space. In this case, the state space is only made up of two possible states: being an iPhone user or being an Android user.", "Let\u2019s say each year, everyone buys a new smartphone. You can stick with your current type of smartphone or switch. Let\u2019s say users tend to stick with their current type of smartphone but may occasionally switch.", "So if you currently use an iPhone, the next phone you buy will probably be another iPhone. Let\u2019s say this probability is 80%. But that there\u2019s also a chance you\u2019ll switch to Android. Let\u2019s say this probability of switching is 20%.", "And if you currently use an Android phone, you\u2019ll probably buy an Android phone again \u2014 say with 70% probability \u2014 but there\u2019s also a chance you\u2019ll switch to an iPhone \u2014 say with 30% probability.", "Each of these probabilities of staying with your current type of smartphone or switching is called a transition probability. Transition probabilities are \u201cconditional probabilities\u201d where the probability of being an iPhone or Android user in one year depends on (\u201cis conditional on\u201d) whether you were an iPhone or Android user the previous year.", "All of these transition probabilities put together form a transition matrix. A transition matrix allows us to map out the probabilities for every possible future state\u2014 whether you\u2019re iPhone user or Android user next year\u2014 for each possible current state\u2014 whether you\u2019re an iPhone user or Android user this year.", "We can think about what will happen not just for this year and next but the year after as well.", "If we start with an iPhone in Year 1, then in Year 2 we have an 80% chance of having an iPhone again and a 20% of having an Android phone. We can do the same thing again for Year 3, but this time we apply our transition probabilities to our situation in Year 2. If we had an iPhone in Year 2, then in Year 3 we\u2019d have an 80% chance of having an iPhone and a 20% chance of having an Android phone. But if we had an Android phone in Year 2, then in Year 3 we\u2019d have a 30% chance of having an iPhone and a 70% chance of having an Android phone.", "We can think about this at the group level across many smartphone users. Let\u2019s say our users start off being a mix of 90% iPhone users and 10% Android users. This is called the initial distribution.", "If we apply our initial distribution to a group of 100 smartphone users, that means in Year 1 we\u2019d start with 90 iPhone users and 10 Android users. In Year 2, some people will switch and some will stay with their existing smartphone platforms. How many people will switch and how many will stay is determined by the transition probabilities we set earlier.", "Of the 10 original Android users:", "So in Year 2, if we add these all up, we end up with:", "For Year 3, we would apply our same transition probabilities again but to Year 2 instead. A percentage of our iPhone users in Year 2 would stay iPhone users in Year 3 (80%) while the rest would switch to Android (20%). A percentage of our Android users in Year 2 would stay Android users in Year 3 (70%) while the rest would switch to using an iPhone (30%). For Year 3, it doesn\u2019t matter whether you were originally an iPhone user back in Year 1. All that matters is whether you were an iPhone user in Year 2.", "For Year 4 and Year 5 and every year after that, we\u2019d apply our transition probabilities to the distribution of smartphone users in the previous year. We can do this for as many years as we\u2019d like.", "If we keep going for many years, the proportion of iPhone users and Android users will eventually converge to a steady-state equilibrium. You can see below that after around 6 years, our group of users ends up stabilizing at around 60% iPhone users and 40% Android users. After 100 years or 1,000 years or a million years, our proportion of iPhone and Android users basically stays the same.", "When we look at what happens to a distribution over the very long run, we\u2019re finding the limiting distribution. And when our limiting distribution converges to something stable, we call this a stationary distribution (it\u2019s also called an invariant distribution or an equilibrium distribution). So for our example, the 60/40 split of iPhone users and Android users over the long run is our stationary distribution.", "It turns out that the exact proportion of iPhone and Android users that we converge to over the long run depends only on our transition probabilities. If we change any of our transition probabilities, that would change our stationary distribution. For example, if we increased our probability that an iPhone user this year sticks with an iPhone again next year from 80% to 90%, we would end up with a 75/25 split of iPhone users vs. Android users in the long run. These long-run stationary distributions can be found mathematically using linear algebra.", "It turns out that our initial distribution doesn\u2019t matter in the long run. It doesn\u2019t matter if we started out with 90% iPhone users and 10% Android users or if we started out with 50% iPhone users and 50% Android users. The amount of time it would take might vary, but eventually we would arrive at the same steady state 60/40 split as long as our transition probabilities remained the same.", "Do all Markov chains converge in the long run to a single stationary distribution like in our example? No. It turns out only a special type of Markov chains called ergodic Markov chains will converge like this to a single distribution. An ergodic Markov chain is a Markov chain that satisfies two special conditions: it\u2019s both irreducible and aperiodic. I\u2019ll explain what these mean.", "First, we have to be able to get be able to get from any one state to any other state eventually. We can never get permanently stuck in one state or a set of states. When this is true, then the Markov chain is said to be irreducible.", "This is true for our smartphone example. If you\u2019re an iPhone user, you can become an Android user next year or sometime in the future. And if you\u2019re an Android user, you can become an iPhone user next year or sometime in the future. If we had set up our example differently so that once you became an iPhone user you could never switch to having an Android phone, then our Markov chain would no longer be irreducible.", "Keep in mind that for a Markov chain to be irreducible, you don\u2019t have to be able to reach every state right away. It could be the case that it takes several steps to reach a certain state. But the important thing is that eventually you could reach any state with enough steps.", "Second, we can\u2019t get stuck cycling back and forth between the same set of states at regular intervals. In other words, our Markov chain must be aperiodic.", "It\u2019s helpful to first explain what a periodic Markov chain is. A Markov chain is periodic when we keep ending up at the same state every 2 or 3 or more regular intervals of time. For our smartphone example, let\u2019s go back to thinking only about a single user. And this time, let\u2019s say that each year you definitely switch which type of smartphone you use (so your probability of switching is 1). So if you start with an iPhone in Year 1, then you\u2019ll have an Android phone in Year 2, and then an iPhone again in Year 3.", "You can quickly see that in odd years, you\u2019d definitely have an iPhone and in even years you\u2019d definitely have an Android phone. In this case, your Markov chain is periodic because you keep cycling back and forth between an iPhone and an Android phone at a fixed interval of time \u2014 in this case, every 2 years.", "But in our original smartphone example with our various transition probabilities, we would never end up cycling between an iPhone and an Android with such regularity. While we do go back and forth between our two possible states \u2014 iPhone and Android \u2014 we don\u2019t do this at regular time intervals. In other words, it is not the case that every 2 years or every 3 years you end up with an iPhone. There is no regular pattern where you could predict that in every odd or even year or every 3 or 5 years you would have an iPhone. Because there is no time-based pattern, this Markov chain is said to be aperiodic.", "When these two conditions are satisfied \u2014 that is, when our Markov chain is both irreducible and aperiodic \u2014 then we can say our Markov chain is ergodic. And if we put all this together, we have the ergodic theorem, which says that any Markov chain that\u2019s ergodic converges in the long run to a single stationary distribution regardless of our initial distribution. In other words, if you run one of these special types of Markov chains over many time periods, you\u2019ll get closer and closer to a certain distribution regardless of how you started.", "We can see this is the case for our group-level example of smartphone users. Because of the way we set up our transition probabilities (e.g., an iPhone user this year has an 80% of being an iPhone user next year), we created a Markov chain that happened to be ergodic. And because our Markov chain was ergodic, we could apply the ergodic theorem and know in advance that we would eventually converge to a certain mix of iPhone and Android users in the long run. The ergodic theorem allowed us to know this without ever having to play out our Markov chain over multiple years.", "Why should we care about the ergodic theorem? Because it\u2019s the basis for a powerful type of machine learning techniques called Markov chain Monte Carlo methods. Markov chain Monte Carlo methods (often abbreviated as MCMC) involve running simulations of Markov chains on a computer to get answers to complex statistics problems that are too difficult or even impossible to solve normally.", "The ergodic theorem is fundamental to using Markov chain Monte Carlo methods because it ensures convergence. As long as we use ergodic Markov chains when we set up our Markov chain Monte Carlo simulation, then we can be sure that the data points generated by our simulation will indeed converge to a single distribution. But more on Markov chain Monte Carlo methods in another article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Sociology PhD Student at Princeton University www.kyleichan.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F713e6ec6ce92&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kyle.i.chan?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.i.chan?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Kyle Chan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98f0625bb587&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&user=Kyle+Chan&userId=98f0625bb587&source=post_page-98f0625bb587----713e6ec6ce92---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/markov-chains?source=post_page-----713e6ec6ce92---------------markov_chains-----------------", "anchor_text": "Markov Chains"}, {"url": "https://medium.com/tag/statistics?source=post_page-----713e6ec6ce92---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----713e6ec6ce92---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/probability?source=post_page-----713e6ec6ce92---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/monte-carlo-method?source=post_page-----713e6ec6ce92---------------monte_carlo_method-----------------", "anchor_text": "Monte Carlo Method"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&user=Kyle+Chan&userId=98f0625bb587&source=-----713e6ec6ce92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&user=Kyle+Chan&userId=98f0625bb587&source=-----713e6ec6ce92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F713e6ec6ce92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----713e6ec6ce92---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----713e6ec6ce92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.i.chan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.i.chan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kyle Chan"}, {"url": "https://medium.com/@kyle.i.chan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "52 Followers"}, {"url": "http://www.kyleichan.com", "anchor_text": "www.kyleichan.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98f0625bb587&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&user=Kyle+Chan&userId=98f0625bb587&source=post_page-98f0625bb587--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1f5d06ef691b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-behind-markov-chains-713e6ec6ce92&newsletterV3=98f0625bb587&newsletterV3Id=1f5d06ef691b&user=Kyle+Chan&userId=98f0625bb587&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}