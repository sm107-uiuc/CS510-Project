{"url": "https://towardsdatascience.com/linear-regression-made-easy-702e5dc01f03", "time": 1683007805.422528, "path": "towardsdatascience.com/linear-regression-made-easy-702e5dc01f03/", "webpage": {"metadata": {"title": "Linear Regression made EASY!. A dummy\u2019s guide to master linear\u2026 | by Athreya Anand | Towards Data Science", "h1": "Linear Regression made EASY!", "description": "When I wanted to learn Machine Learning and began to sift through the internet in search of explanations and implementations of introductory algorithms, I was taken aback. Every site I landed on\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["When I wanted to learn Machine Learning and began to sift through the internet in search of explanations and implementations of introductory algorithms, I was taken aback. Every site I landed on explained the algorithms like I was reading some sort of research paper and was not beginner-friendly at all! All kinds of jargon and equations were thrown around assuming I was just supposed to know them \u2014 which I had absolutely no clue about.", "With this article, and the others in the series, I\u2019ll try to explain the algorithm and the intuition behind it with a down-to-earth, layman\u2019s approach. No prior knowledge needed whatsoever! If at any point, you are confused about a term or process don\u2019t worry, chances are others are in the same boat. Go ahead and leave a response and I or another person who\u2019s familiar with the term will help; like always, we\u2019re on this learning journey together :). Now without further ado, let\u2019s put our thinking caps on and dive in!", "To many, Linear Regression is considered the \u201chello world\u201d of machine learning. It is a fantastic starting point to highlight the capabilities of Machine Learning and the crossroads that exist between statistics and computer science.", "In general, Linear Regression is used to make sense of the data we have by revealing the underlying relationship between the input features and target values of the data. Once we discover this relationship, we have the power to make predictions on new data that we have not seen before. Still a bit confused? Don\u2019t worry, let\u2019s go through an example :)", "Let us jump into a hypothetical situation; me and you are in search of a brand-new house! Suppose we want to find out how much a typical house costs in a specific community, how would you go about guessing?", "Chances are you weigh a significant number of different factors. Some could be:", "The list goes on and on, can you think of more?", "After talking to some real estate agents and asking your friends, we find out that the price is determined by three core factors: size, crime, and proximity to stores/markets (remember, this is a hypothetical\u2026 I know nothing about real estate :)). In other words, using these three values, we should be able to predict the value of any house.", "Now that we know what determines the price of a house, we want to reveal the underlying relationship between these factors and the target value, which in our case is the total price of the house. With that, we are faced with this equation:", "From this equation, we can deduce that the price of the house is determined by three attributes. In ML terminology, these attributes are called features and affect the house price (target value). Obviously, every feature does not equally affect the target value/house price (i.e. Size could have a bigger effect on the price than the amount of crime in the area). Thus, we must somehow discover what percentage of the house price is reliant on each specific feature and assign a weight (indicated by a ?) to each feature.", "Now, imagine what we can do after we discover the true values of the question marks. We would be able to accurately predict the price of any house given just three numbers: size, crime rate, and proximity to stores! In other words, we would have understood the underlying relationship between the features and target value.", "Guess what? Turns out Linear Regression is used to do exactly that! Using the algorithm, we will be able to reveal the weights so that we can predict the target value of unseen data.", "Let\u2019s quickly go over the key terms we just saw:", "So how exactly do we figure out these weights/parameters? I\u2019m so glad you asked! Let me introduce you to my good friend, gradient descent.", "Now that we have a solid grasp on what linear regression is, it\u2019s time to dive into the how. Specifically, how do we figure out the weight parameters for linear regression?", "With gradient descent, parameters of the model are changed iteratively at each \u201cstep\u201d until we reach a desired accuracy.", "PITSTOP: Remember what parameters are? If not, reference the paragraph on key terms right above this section!", "Let\u2019s say, to initialize our parameters, we use random values your mom, an ex-real-estate agent, said were right.", "Let\u2019s take that previous equation and replace the question marks:", "According to your mom, the proximity to stores is the most important factor that determines house prices (as seen by the largest weight assigned to the proximity feature).", "Remember, these numbers are our initial values we chose intuitively. We could have also chosen completely random numbers and been fine. These initial values will change at every step of the algorithm and eventually converge at their optimal values.", "Going along with our analogy, let\u2019s say that the two of us go on a house tour with your mom and for each house we see we ask these questions:", "For the first question, we expect the answer to be the actual square footage and, thus, representative of size.", "For the second question, let\u2019s imagine a hypothetical scale called VPY (victims per year). This will indicate the number of victims of crimes in the area of interest within the past year and represent the crime value in our equation.", "For our third and final question, let\u2019s assume another objective hypothetical scale ranging from 1 (very far from stores) to 100 (very close). This value will represent our proximity value.", "When we see the first house and ask your mom the questions, she provides the following responses:", "Remember, since we want to predict the price of the house based on our parameters, we only ask for these values and not the actual value of the house. After providing a prediction of the house price, we ask your mom what the actual price of the house is in order to compare.", "The amount our prediction and actual value differs determines how optimal our parameters/weights are. The job of gradient descent is to minimize this difference (predicted \u2014 actual); easy right?!", "In the literature, this difference is called error since it indicates how different/wrong the prediction is compared to the actual value.", "Now, let\u2019s plug those numbers from the responses into our equation:", "After calculating, our prediction turns out to be:", "Now, after predicting, we ask your mom, get the actual price of the house and calculate the error between the two values.", "Our error turns out to be 291,000. Logically, our goal is to make this value as small as possible by altering our weights/parameters.", "This error value also goes by another name in machine learning: cost. The value is called cost since it costs us to miss the actual value by a certain amount. In this housing example, if we went with our prediction, it would have literally cost us an extra $291,000 if we decided to buy the house with that price!", "PITSTOP: To make sure you understand, what would an error/cost of 0 mean?", "It means that our prediction is exactly the actual value and that our parameters are optimal!", "Now the question is, how in the world are we supposed to change the weights to minimize our cost? How do we know which one to increase or decrease?", "Sure, in this case, we can just intuitively increase and decrease the numbers until we reach a low error/cost; but, imagine if we had 100 features. I don\u2019t know about you, but I sure do not want to randomly fidget around the weights of a hundred values! There should be an easier way\u2026", "Similarly, we have another problem: The error we have cannot only represent one house. Since we want our world-changing formula to be representative of all houses, we want our parameters to make an accurate prediction when given our three values of any house \u2014 not just one.", "Due to this, we have to get our desired values (size, crime, proximity) for a whole lot of houses, plug those into the equation, find the error, and change the parameters respectively. The more houses and data we get, the more our formula will be able to generalize!", "The core purpose of Gradient Descent is to minimize the cost function. By minimizing the cost function (pred \u2014 actual), we also ensure the lowest error and highest accuracy! We go over our dataset iteratively (value by value / house by house) while updating our parameters at each step.", "Going back to our house tour analogy, recall that we had three parameters: size, crime, and proximity. Like aforementioned, we want them to change in the direction that minimizes the cost function. Going over each data point translates to asking the same three questions about every house we see, plugging in the values to extract the error/cost, and deciding which direction the next step should be in order to minimize the cost function.", "Now, the ultimate question. How do we decide which direction we should move towards to minimize our cost?", "Calculus saves the day here. By taking the derivative of the cost function with respect to a specific variable, we can get the direction we should change our variable.", "I know this sounds extremely complicated at first glance, but don\u2019t worry! It\u2019s actually quite easy. By taking the derivative of a specific function, we are able to get the slope of the error. This slope represents the direction of the error and we simply take a small step in that direction in order to reduce the total error. We do this process of finding the direction to take a step in, and take a step for each of our variables (three in this case). The direction/slope represents the gradient and our step represents our descent, thus Gradient Descent!", "Imagine we have the parabola below that maps cost and weight for each of our three functions (three parabolas total). This means that, at each step, we get closer to the optimal value of each weight!", "PITSTOP: To make sure you understand, what would a slope of 0 mean?", "It means that our weight for a specific variable is optimal and that we don\u2019t have to take any steps to correct our error! In other words, we are at the vertex of the parabola (look at figure!). This point is also known as the local minima", "So to recap, we now have the derivative of the cost function representing the direction we need to move in. Now all we have to do is update each parameter! This is done by the following:", "Size updated = Size old \u2014 (Learning Rate x Derivative of Cost Function w/ Size)", "Crime updated = Crime old \u2014 (Learning Rate x Derivative of Cost Function w/ Crime)", "Proximity updated = \u2026 (can you figure this one out?)", "This is called the update rule and is applied to all of our parameters using their unique partial derivatives. We apply this update rule for all parameters at each step (every data point we see).", "You might be wondering what Learning Rate is. This represents the amount the parameters are updated at each step. The learning rate is configurable and often chooses a value between 0 and 1; in simpler words, it determines the speed at which the model learns:", "Just like Goldilocks: not too hot, not too cold, just right. Tuning this hyperparameter is very important to machine learning!", "So just to review, one iteration means asking the three questions about a single house and updating our parameters respectively.", "After iterating over our dataset many times, we come to a halt when we reach a point where the cost is low enough (i.e. the vertex of the cost parabola above).", "Once the cost is low, we know that the parameters are optimized. Using these optimized parameters for our features (size, crime, and proximity), we are now able to accurately guess the price of any house without seeing the price itself! All we need to do is ask those three questions, multiply them with our optimal weights \u2014 and viola!", "This is the what the machine learns in machine learning: the optimal parameters to accurately predict anything the machine is given.", "That\u2019s it! Now you\u2019re a pro in machine learning and linear regression \u2014 maybe not, but this is a huge step in the right direction!", "So, now that we finally finished, let\u2019s go over all the terms you learned:", "If you can answer all these questions, you know everything there is about linear regression and the underlying theory behind gradient descent!", "If you couldn\u2019t answer one of the questions, go ahead and scroll up and find the answer! The terms are listed in the order they show up within the article and are also bolded when they appear. Feel free to highlight important sentences too in order to help your fellow readers find relevant info easier.", "Now that we understand linear regression, we can code it! Stay tuned for how to code linear regression completely from scratch and shoot a follow so you know exactly when it comes out. For now, I hope you learned something new and hope to see you fairly soon :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A wannabe writer playing at the crossroads between life, technology, art, programming and intelligence | Software Engineer @ Google | ML Masters\u2019 @ Georgia Tech"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F702e5dc01f03&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://athreyaanand.medium.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": ""}, {"url": "https://athreyaanand.medium.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Athreya Anand"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7947b7445b03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&user=Athreya+Anand&userId=7947b7445b03&source=post_page-7947b7445b03----702e5dc01f03---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mattragland?utm_source=medium&utm_medium=referral", "anchor_text": "Matt Ragland"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@heftiba?utm_source=medium&utm_medium=referral", "anchor_text": "Toa Heftiba"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@dmtors?utm_source=medium&utm_medium=referral", "anchor_text": "Derek Torsani"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@yer_a_wizard?utm_source=medium&utm_medium=referral", "anchor_text": "Fleur"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----702e5dc01f03---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----702e5dc01f03---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----702e5dc01f03---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----702e5dc01f03---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----702e5dc01f03---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&user=Athreya+Anand&userId=7947b7445b03&source=-----702e5dc01f03---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&user=Athreya+Anand&userId=7947b7445b03&source=-----702e5dc01f03---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F702e5dc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----702e5dc01f03---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----702e5dc01f03--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----702e5dc01f03--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----702e5dc01f03--------------------------------", "anchor_text": ""}, {"url": "https://athreyaanand.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://athreyaanand.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Athreya Anand"}, {"url": "https://athreyaanand.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "142 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7947b7445b03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&user=Athreya+Anand&userId=7947b7445b03&source=post_page-7947b7445b03--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a01cb1730ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-made-easy-702e5dc01f03&newsletterV3=7947b7445b03&newsletterV3Id=8a01cb1730ad&user=Athreya+Anand&userId=7947b7445b03&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}