{"url": "https://towardsdatascience.com/deduplication-deduplication-1d1414ffb4d2", "time": 1683000811.160256, "path": "towardsdatascience.com/deduplication-deduplication-1d1414ffb4d2/", "webpage": {"metadata": {"title": "Deduplication Deduplication. Deduplication of text is an application\u2026 | by Abhijith C | Towards Data Science", "h1": "Deduplication Deduplication", "description": "Deduplication of text is an application of the domain \u2014 Semantic Text Similarity (STS). We show how an averaging ensemble of word2vec and Transformer based Universal Sentence Encoder performed well."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "Jaccard Similarity", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Earth_mover%27s_distance", "anchor_text": "Earth Mover Distance", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence", "anchor_text": "Jensen-Shannon distance", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Measure_of_similarity", "anchor_text": "measure of similarity", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Inner_product_space", "anchor_text": "inner product space", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Cosine", "anchor_text": "cosine", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1803.11175.pdf", "anchor_text": "series of models", "paragraph_index": 24}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer", "paragraph_index": 25}, {"url": "http://mlexplained.com/2018/05/11/paper-dissected-deep-unordered-composition-rivals-syntactic-methods-for-text-classification-explained/", "anchor_text": "Deep Averaging Network", "paragraph_index": 25}], "all_paragraphs": ["nounthe elimination of duplicate or redundant information, especially in computer data.", "\"deduplication removes the repetitive information before storing it\"", "As the definition says, the task we are trying to do is to remove the duplicate texts/sentences and so on. This is nothing but the act of checking how similar the text is to one another. They can be exactly identical like: Deep Learning is Awesome! and Deep Learning is Awesome!. Or, they could be quite similar to one another in terms of what the sentence tries to convey, like: Deep Learning is Awesome! and Deep Learning is so cool. We know that these two sentences convey the same thing, and this is what we want our machines to capture.", "Such a task in literature is referred to as Semantic Text Similarity(STS). It deals with determining how similar two pieces of texts are. This would include not just the syntactic similarity, that is how similar or same are the words that are used in the two sentence, but also the semantic similarity that captures the similarity in what is being conveyed using the two sentences, i.e the meaning of the text plays an important role in determining what is similar and not similar.", "The problem. Yes, that\u2019s our main goal. To solve the problem. Let me give you an example. Say, you have to send really funny jokes (your joke could be a sentence or a bunch of sentences) to a group of people over e-mail (LOL!), and your boss asks you to make sure that people don\u2019t receive same kind of jokes. So you have to make sure that all the jokes you have are unique and people don\u2019t get bored with the content. What a job, seriously?", "You, being a kick-ass coder, decide to automate this task. You have this magical API that gives you a lot of jokes for free and you write a script to mail the jokes to that group of people your boss loves. But, we can\u2019t really trust this magical API, can we? It\u2019s magical. What if the API gives you similar jokes? You can\u2019t risk upsetting your boss.This is where you can use a deduplication engine that makes sure that no joke sent is similar to any that was sent in the past.", "My primary aim here is not to talk a lot about these models. But to help you use them for a practical task, something like the one that is stated above. I admit that sending jokes to people in order to impress your boss isn\u2019t really practical.", "Let\u2019s try to break this down into how this similarity measurement is defined and between what two entities we are trying to find the similarity (literally the text as is, or something else?).", "Firstly, talking about the similarity measurement, there\u2019s quite a few that can be used. Just for the sake of completeness, listing a few:1. Jaccard Similarity2. Cosine Similarity3. Earth Mover Distance4. Jensen-Shannon distance", "But to cut to the chase, we\u2019ll be using Cosine Similarity. Mathematically, Cosine similarity is a measure of similarity between two vectors (non-zero) of an inner product space that measures the cosine of the angle between them.", "If two documents are similar and if they are far apart in the Euclidean space, they could still be pretty close to each other. This is captured by cosine distance and hence is advantageous.", "Secondly, where do we use this cosine distance? Between pairs of sentence strings? Nope! This is were we use the power of Natural Language Processing and Deep learning. We use vectors.", "A word/sentence vector is a row of real valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word\u2019s/sentence\u2019s meaning and where semantically similar words/sentences have similar vectors.", "And, again, there are plenty of methods to get these vectors. To name a few:Word Embedding: word2vec, GloVe, BERT word embeddings, ELMo and so on.Sentence Embedding: BERT sentence embedding, Universal Sentence Encoder, etc.", "I\u2019ll jump straight into the methods that I personally experimented with and that worked beautifully for me.", "To prevent this from being a pure implementation-oriented article (which it is intended to be), I\u2019ll try to explain what these models are, very briefly.", "word2vec comes in two variants: Skip-Gram and Continuous Bag of Words model (CBOW). There\u2019s plenty of material on both these variants, if you are looking for a detailed explanation. I\u2019ll be very crisp here. The skip-gram model is a bit slower but usually does a better job with infrequent words. Hence, this is what is used often. We\u2019ll talk briefly about this.", "You\u2019ll find this diagram in almost every word2vec (Skip-Gram model) blog and tutorial. In this architecture, the model uses the current word to predict the surrounding window of context words. It weighs the nearby context words more heavily than the distant context words.", "Here, we look at a window of context words (2 words on each side, in this case) and try to predict the center word.", "Consider w(t) is the input word, the usual dot product between the weight matrix and the input vector w(t) is done by the single hidden layer. We apply the softmax function to the dot product between the output vector of the hidden layer and the weight matrix. This gives us the probabilities of the words that appear in the context of w(t) at the current word location.", "It\u2019s the vectors that are present in the hidden layers that become the vector representation of that word. But these are \u2018word\u2019 embedding, and we have to find similar \u2018sentences\u2019. So, how do we get the vector representation of the sentence instead of just word embedding?", "One simple and trivial way (the one that I will show today) is by simply averaging the word embedding of all the words of that sentence. Simple, isn\u2019t it?", "Now for the main part, let\u2019s code this in.", "That\u2019s it! \ud83e\udd37\u200d\u2642 You have your sentence embedding using word2vec.", "Google presented a series of models for encoding sentences into vectors. The authors have specifically targeted this for downstream tasks, i.e for transfer learning tasks. STS is one such task.", "It comes in two variants:1. One with a Transformer Encoder2. One with a Deep Averaging Network", "Each of them have different design goals:1. Targets high accuracy at the cost of greater model complexity and resource consumption.2. Targets efficient inference with slightly reduced accuracy.", "I\u2019ve hyperlinked both these architectures with my favorite blogs that explain each of them very well. Let\u2019s focus more on how to implement them.", "Now we have the sentence embedding for your jokes from two different models.", "Now we need the cosine similarities!", "When I was doing this job before you, I had a bunch of jokes that were non-duplicates and few that were duplicates. Specifically, I had 385K non-duplicate pairs and 10K duplicate pairs. I plotted an AUC-ROC for this task using only the word2vec model.", "Nice! The curve looks real nice. (I\u2019m omitting the confusion matrices on purpose).", "Let\u2019s see how the Universal Sentence Encoder fared.", "The area under the curve is slightly better, isn\u2019t it?", "Let\u2019s see what happens when we combine them. By combine what I mean is average the cosine similarities from both the approaches and check the metrics. \u201cAn averaging ensemble of the models\u201d.", "Wohooo! \ud83c\udf89\ud83c\udf89 We have a clear winner!", "This is one quick way of finding duplicates. No training involved, just downloading existing brilliant models and using them for your STS task!", "With this piece in place, you can easily build your deduplication engine. Just store all the jokes that you send to your boss\u2019s group on the first day and then for every new incoming joke, pair them up with all the previously seen jokes and use this ensemble model to find whether they are duplicates or not. If they are, throw them away.", "And in this way, keep your boss\u2019s friends happy, boss happy and get a nice paycheck and keep yourself happy :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Glance (An InMobi Group Company)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d1414ffb4d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhijith0505.medium.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": ""}, {"url": "https://abhijith0505.medium.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Abhijith C"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F266c18c429cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&user=Abhijith+C&userId=266c18c429cc&source=post_page-266c18c429cc----1d1414ffb4d2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "Jaccard Similarity"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity"}, {"url": "https://en.wikipedia.org/wiki/Earth_mover%27s_distance", "anchor_text": "Earth Mover Distance"}, {"url": "https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence", "anchor_text": "Jensen-Shannon distance"}, {"url": "https://en.wikipedia.org/wiki/Measure_of_similarity", "anchor_text": "measure of similarity"}, {"url": "https://en.wikipedia.org/wiki/Inner_product_space", "anchor_text": "inner product space"}, {"url": "https://en.wikipedia.org/wiki/Cosine", "anchor_text": "cosine"}, {"url": "https://github.com/mmihaltz/word2vec-GoogleNews-vectors", "anchor_text": "word2vec"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder-large/3", "anchor_text": "Universal Sentence Encoder"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "https://arxiv.org/pdf/1301.3781.pdf"}, {"url": "https://arxiv.org/pdf/1803.11175.pdf", "anchor_text": "series of models"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer"}, {"url": "http://mlexplained.com/2018/05/11/paper-dissected-deep-unordered-composition-rivals-syntactic-methods-for-text-classification-explained/", "anchor_text": "Deep Averaging Network"}, {"url": "http://twitter.com/param", "anchor_text": "@param"}, {"url": "http://twitter.com/param", "anchor_text": "@param"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d1414ffb4d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/semantic-text-similarity?source=post_page-----1d1414ffb4d2---------------semantic_text_similarity-----------------", "anchor_text": "Semantic Text Similarity"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1d1414ffb4d2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1d1414ffb4d2---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----1d1414ffb4d2---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&user=Abhijith+C&userId=266c18c429cc&source=-----1d1414ffb4d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&user=Abhijith+C&userId=266c18c429cc&source=-----1d1414ffb4d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d1414ffb4d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d1414ffb4d2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d1414ffb4d2--------------------------------", "anchor_text": ""}, {"url": "https://abhijith0505.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://abhijith0505.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijith C"}, {"url": "https://abhijith0505.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "62 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F266c18c429cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&user=Abhijith+C&userId=266c18c429cc&source=post_page-266c18c429cc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcefb24c3c745&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeduplication-deduplication-1d1414ffb4d2&newsletterV3=266c18c429cc&newsletterV3Id=cefb24c3c745&user=Abhijith+C&userId=266c18c429cc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}