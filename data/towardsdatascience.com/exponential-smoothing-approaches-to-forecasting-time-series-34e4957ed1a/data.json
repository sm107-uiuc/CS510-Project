{"url": "https://towardsdatascience.com/exponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a", "time": 1683013819.773479, "path": "towardsdatascience.com/exponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a/", "webpage": {"metadata": {"title": "Exponential Smoothing Approaches In Time Series Forecasting | by Arun Jagota | Towards Data Science", "h1": "Exponential Smoothing Approaches In Time Series Forecasting", "description": "Covered for these features. Simple to implement. Effective for short-term forecasting. Adaptive to non-stationary, i.e. rapidly changing time series. Lend themselves to incremental learning."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this post, we describe and explain certain classical algorithms that forecast future values of time series. These algorithms, called exponential smoothers, have been in wide use for many decades. We convey intuition with examples, some augmented with Python code in an appendix", "Exponential smoothers continue to remain attractive in the modern age of deep machine learning. They are simple and easy to implement. They are also surprisingly effective for short-term forecasting. Especially on rapidly-changing time series.", "Exponential smoothers also lend themselves to incremental learning, often a must-have on rapidly-changing time series. This is so that the algorithm can keep up with the changing characteristics. Such as a trending time series suddenly starting to oscillate.", "A time series is a sequence of values over time. Such as daily average temperature in a city, the daily closing price of a particular stock, monthly sales of iPhone 11.", "Our interest here is in forecasting future values of a time series from its historical values. (In a broader formulation, which we won\u2019t cover here, we might include additional predictors, even additional time series.)", "This problem is of immense interest. Businesses want to forecast future sales. Traders would like to forecast stock or index prices when possible. We all want to forecast the weather.", "Next, a key concept. The forecast horizon, a positive integer h, is the number of steps ahead we\u2019d like to forecast. So for a daily time series, a forecast horizon of 1 would forecast the next day\u2019s value, a forecast horizon of 7 would forecast its value 7 days into the future.", "We start with the naive forecaster (NF). While it\u2019s too naive to use, it serves as a useful baseline to compare against the various exponential smoothers.", "The naive forecaster forecasts x(t+h) to be x(t). Note that, regardless of what h is, the forecast is always x(t).", "Now, on to our first exponential smoother.", "Let\u2019s model our time series as follows:", "Here f(t) is a deterministic function of t, and noise is independently generated at each time step by sampling from a suitable distribution, e.g. standard normal. This model is both rich and intuitively appealing. f(t) models the deterministic component of the time series. This can be pretty elaborate, including multiple trends and multiple seasonalities if we like. noise models random fluctuations and other unmodeled effects.", "The main idea in SES is to estimate f(t) from x(t), x(t-1), \u2026, and then use this estimate to forecast future values of x.", "SES\u2019s estimate of f(t) is the exponentially-weighted average of x(t), x(t-1), x(t-2), etc. x(t) contributes the most, x(t-1) the second most, x(t-2) the third most, and so on. The contributions decay exponentially, at a rate controlled by a decay parameter a.", "This may be expressed recursively as", "The ^ is there to remind us that this is an estimate of f. Not f itself, which is hidden from us.", "For its simplicity, SES works surprisingly well for 1-step forecasts, i.e. h=1. The reason for this is that SES not only smooths out noise, but it also fits f(t) locally, i.e. at time t, which is important when the time series is changing rapidly. NF also fits f(t) locally, however, it does not smooth out noise. SES is ineffective for longer-term forecasts, i.e. h > 1.", "Simple Exponential Smoothing + Trend (TrES)", "As mentioned earlier, SES is ineffective for longer-term forecasts. By adding a trending component to SES, we can improve the situation. The intuition is simple. If there is a local trend at x(t), a trend component can estimate it, and use it to forecast for somewhat longer horizons. (This is similar to the reasoning: if you know an object\u2019s current position and velocity, you can predict where it will be a little later.)", "Here is a simple example. Consider x(t) = t + noise. Say somehow we figure out that x(t) is growing by 1 in every time unit (noise aside). Knowing this we can forecast that x\u2019s value h time units later will be its current value plus h.", "Okay, let\u2019s describe the algorithm more thoroughly. For this, let\u2019s reexpress our time series as", "Here f(t\u2019) is a function of t\u2019 and df(t\u2019) is the local trend at time t\u2019. We have written it as df(t\u2019) because it is the discrete analog of the first derivative of f at t\u2019.", "This model is equivalent to our first model. We\u2019ve just factored f(t) as f(t-1) + df(t-1).", "What\u2019s the point of this factoring? We can estimate the two terms f(t-1) and df(t-1) separately. This allows us to make sensible longer term forecasts on series in which df(t) can be accurately estimated. Such as in x(t) = t + noise. Under the factoring x(t) = (t-1) + 1 + noise we see that df(t) equals 1. Using this estimate lets us make sensible forecasts further out into the future.", "Let\u2019s call these estimates f^(t) and df^(t) respectively. The forecast is now", "Note that f^(t) and df^(t) are both local estimates. So the algorithm adapts to changes in f(t) as well as df(t). We can use a and b as the exponential-decay parameters for estimating f and df respectively. This gives us more knobs to tweak. (Following the example below, we will discuss tuning these knobs a bit.)", "f^(2) is 2 because we have initialized it to x(2). df^(2) is 1 because we have initialized it to x(2)-x(1). We see that the algorithm has deduced the trend that the series grows by 1 in every time step.", "Let\u2019s calculate x^(t+1) at time t=4. It is f^(4) + df^(4) = 3.25 + 1 = 4.25. Close to the actual value x(5) = 5 though lagging a bit. The lag is a consequence of the exponential smoothing. We could reduce the lag by weighing recency higher, but that may incur a cost elsewhere. We discuss trade-offs involving this in the next paragraph.", "Next, let\u2019s calculate x^(t+3) at time t = 4. It is f^(4) + 3*df^(4) = 3.25 + 3*1 = 6.25. Clearly we have been able to exploit the trend to forecast further into the future! x(7) is 7. The longer horizon forecast is as accurate as the 1-step one in this case! The lag hasn\u2019t increased.", "The smoothing parameters offer us ways to control the lags. In our example, increasing the value of a will reduce the lag. On the other hand, on a noisy version of our example, an overly large value of a will not be able to smooth out the noise. So yes, the lag will reduce, but not the forecast quality. It will be as if we are chasing the most recent value, moving with the noise in it.", "Similar reasoning applies to the parameter b. Except that it applies to df, i.e. to the slope. In our example, the slope was always the same, 1. So changing b would not have any effect. However, if the slope was greater than 1 and had multiplicative noise in it, e.g. df was 5*noise where noise has a mean of 1 and fluctuates around it, changing b would likely have an impact on how much df^ lags df versus how much df^ chases the noise in df.", "The way to go here is to auto-tune these parameters on suitable slices of the input data. That is, machine-learn them to minimize a suitable loss function. We won\u2019t go into the details here.", "Simple Exponential Smoothing + Trend + Seasonality (TrSeES)", "SeES, while effective on series that exhibit locally linear trends, is not always effective on series with cyclic structure.", "SeES will predict x^(3) to be close to 4. In reality, x(3) is 1. That\u2019s a big difference. If we somehow knew that the time series repeats itself every three time steps we could use this information to improve the forecast.", "How can we improve the TrES? Let\u2019s assume the series has a single seasonality, i.e. a single repeating component, and we know its order, i.e. k. We can model such a time series as follows.", "Here s(t) is a repeating time series. x(t) is obtained by adding s(t) to another time series which we are calling tr(t). We are calling it tr(t) because we think of it as modeling x(t)\u2019s trend component. That said, in reality, tr(t) models whatever is left over after removing s(t) from x(t). It may yet have other repeating components. tr(t) is just less seasonable than x(t). As will become clear when we see the algorithm below, even removing a single seasonality will be progress.", "Let\u2019s see an example expressed this way.", "How do we estimate tr(t) and s(t)? First, we assume we know the order k. One sensible approach is to obtain a new time series yk(t) = x(t) - x(t - k) with the influence of the seasonality removed. We can then apply TrES to forecast yk^(t+h). We then add x(t+h - k) to yk^(t+h) which gives us a sensible forecast x^(t+h) of x(t + h).", "Let\u2019s see how this plays out in the example above.", "Great, y3 is easy to forecast on. Its value is always 3. Removing the seasonality\u2019s influence helped a lot! TrES will quickly figure out that y3 is constant.", "We can do this so long as h is less than or equal to k. (If h were greater than k, t+h - k would be greater than t, i.e. into the future!)", "We can improve on TrSeES further. In deriving x^(t+h) from y^k(t+h) instead of adding x(t+h - k) we can add a smoothed version of it. Smoothed in what way? By taking the exponentially-weighted average of x(t+h - k), x(t+h - 2k), x(t+h - 3k), \u2026", "Why do we think smoothing the seasonality improves TrSeES further? Consider the case when the seasonality component has multiplicative noise in it. I.e.,", "where, for illustration purposes, let\u2019s say this noise is a Gaussian with a mean of 1 and a standard deviation greater than 0 but much less than 1. Such a seasonality model is realistic.", "In this case, x(t+h - k), x(t+h - 2k), x(t+h - 3k), \u2026 will also be influenced by such multiplicative noise. TrES will address the additive noise we previously had. But not this multiplicative noise. Using an exponentially-smoothed estimate of x(t+h - k) from x(t+h - k), x(t+h - 2k), x(t+h - 3k), \u2026, rather than x(t+h - k) will help alleviate the effect of this noise on the forecast.", "I included it here so people can see some actual code. That said, it\u2019s incomplete in parts. There might also be bugs. So be prepared to do some additional work if you want to get it all working.", "By choosing f suitably we can generate a variety of time series. Including ones with trends, ones with cycles, and ones with both.", "Next, we show the code for the various exponential smoothers. As the first statement in all of them, add", "Or, better, use the time series generator to generate x.", "This needs more work to get it going end-to-end. It also does not smoothen the seasonality.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F34e4957ed1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----34e4957ed1a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@oceanng?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ocean Ng"}, {"url": "https://unsplash.com/s/photos/clock?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://otexts.com/fpp2/", "anchor_text": "https://otexts.com/fpp2/"}, {"url": "https://medium.com/tag/forecast-horizon?source=post_page-----34e4957ed1a---------------forecast_horizon-----------------", "anchor_text": "Forecast Horizon"}, {"url": "https://medium.com/tag/non-stationarity?source=post_page-----34e4957ed1a---------------non_stationarity-----------------", "anchor_text": "Non Stationarity"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&user=Arun+Jagota&userId=ef9ed921edad&source=-----34e4957ed1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&user=Arun+Jagota&userId=ef9ed921edad&source=-----34e4957ed1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F34e4957ed1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----34e4957ed1a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----34e4957ed1a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----34e4957ed1a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----34e4957ed1a--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "685 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexponential-smoothing-approaches-to-forecasting-time-series-34e4957ed1a&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}