{"url": "https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451", "time": 1683007836.757296, "path": "towardsdatascience.com/cross-entropy-for-classification-d98e7f974451/", "webpage": {"metadata": {"title": "Cross-entropy for classification. Binary, multi-class and multi-label\u2026 | by Vlastimil Martinek | Towards Data Science", "h1": "Cross-entropy for classification", "description": "We will go over binary cross-entropy, multi-class cross-entropy, and multi-label classification, and explain the only formula needed to understand them."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "video", "paragraph_index": 29}], "all_paragraphs": ["Cross-entropy is a commonly used loss function for classification tasks. Let\u2019s see why and where to use it. We\u2019ll start with a typical multi-class classification task.", "Which class is on the image \u2014 dog, cat, or panda? It can only be one of them. Let\u2019s have an image of a dog.", "The prediction is a probability vector, meaning it represents predicted probabilities of all classes, summing up to 1.", "In a neural network, you typically achieve this prediction by having the last layer activated by a softmax function, but anything goes \u2014 it just must be a probability vector.", "Let\u2019s compute the cross-entropy loss for this image.", "Loss is a measure of performance of a model. The lower, the better. When learning, the model aims to get the lowest loss possible.", "The target represents probabilities for all classes \u2014 dog, cat, and panda.", "The target for multi-class classification is a one-hot vector, meaning it has 1 on a single position and 0\u2019s everywhere else.", "For the dog class, we want the probability to be 1. For other classes, we want it to be 0.", "We will start by calculating the loss for each class separately and then summing them. The loss for each separate class is computed like this:", "Don\u2019t worry too much about the formula, we\u2019ll cover that in a second. Just notice that if the class probability is 0 in the target, the loss for it is also 0.", "And lastly \u2014 the loss for the dog class:", "Let\u2019s see how would the loss behave if the predicted probability was different:", "What if we predict something in the middle?", "The loss gets steeper, the further away from the target we get.You can think of it as a similar concept to square error \u2014 the further away we are from the target, the faster the error grows.", "Why is the loss 0 for the cat and panda classes?", "It looks like we are rewarding the model with low loss, even if it predicts a high probability for a class that is not present in the image.", "We don\u2019t mind if the model predicts that there is a cat with an 80% probability if there is none because then it has only 20% left to predict the correct class. There, the loss will be that much bigger. In other words \u2014 we don\u2019t care on which classes the model wastes the predicted probabilities, only how correctly it identifies the only present class.", "The total loss for this image is the sum of losses for each class.", "It can be formulated as a sum over all classes.", "This is the cross-entropy formula that can be used as a loss function for any two probability vectors. That is our loss for 1 image \u2014 the image of a dog we showed at the beginning. If we wanted the loss for our batch or the whole dataset, we would just sum up the losses of the individual images.", "Suppose we have 2 different models giving the following predictions:", "In the eyes of cross-entropy, model B is better \u2014 it has a lower cross-entropy loss. If you can see why \u2014 well done! Have a panda.", "Training models by punishing big mistakes a lot more than small mistakes turned out to be a good idea in machine learning.", "Why sum up over all the classes if the loss for most of them is 0?If our target is a one-hot vector, we can indeed forget targets and predictions for all the other classes and compute only the loss for the hot class. This is the negative natural logarithm of our prediction.", "This is called categorical cross-entropy \u2014 a special case of cross-entropy, where our target is a one-hot vector.", "The thing is \u2014 the cross-entropy loss works even for distributions that are not one-hot vectors.The loss would work even for this task:", "With the cross-entropy, we would still be able to compute the loss, and it would be minimal if all the classes would be correct, and still have the property of punishing bigger mistakes much more.", "In our one-hot target example, the entropy was conveniently 0, so the minimal loss was 0. If your target is a probability vector that is not one-hot, entropy (minimal loss) will be bigger than 0, but you can still use the cross-entropy loss just fine.", "If you\u2019re more curious about what entropy is, I recommend watching this video.", "Binary cross-entropy is another special case of cross-entropy \u2014 used if our target is either 0 or 1. In a neural network, you typically achieve this prediction by sigmoid activation.", "The target is not a probability vector. We can still use cross-entropy with a little trick.", "We want to predict whether the image contains a panda or not.", "This is the same as if we would convert the target to a one-hot vector and our prediction to a probability vector \u2014 the probability of panda would be the same as the prediction and probability of not-a-panda would be 1-prediction. In other words, if we predict 0.6, that means we are saying that it\u2019s 60% a panda and 40% not-a-panda.", "This loss can be computed with the cross-entropy function since we are now comparing just two probability vectors or even with categorical cross-entropy since our target is a one-hot vector. It can also be computed without the conversion with a binary cross-entropy.", "We are just applying the natural logarithm to the difference between our prediction and our target.", "And that\u2019s all there is to binary cross-entropy.", "Cross-entropy can also be used as a loss function for a multi-label problem with this simple trick:", "Notice our target and prediction are not a probability vector. It\u2019s possible that there are all classes in the image, as well as none of them. In a neural network, you typically achieve this by sigmoid activation.", "We can look at this problem as multiple binary classification subtasks. Let\u2019s say we want to only predict if there is a dog or not.", "We know that our target is 1 and we have predicted 0.6", "We will compute the binary cross-entropy for this subtask:", "And do the same for the other classes.", "For a cat, our target is 0, so the other part of binary cross-entropy cancels out:", "And sum up the losses for each subtask:", "That\u2019s all there is to the cross-entropy loss for multi-label classification.", "Cross-entropy \u2014 the general formula, used for calculating loss among two probability vectors. The more we are away from our target, the more the error grows \u2014 similar idea to square error.", "Multi-class classification \u2014 we use multi-class cross-entropy \u2014 a specific case of cross-entropy where the target is a one-hot encoded vector. It can be computed with the cross-entropy formula but can be simplified.", "Binary classification \u2014 we use binary cross-entropy \u2014 a specific case of cross-entropy where our target is 0 or 1. It can be computed with the cross-entropy formula if we convert the target to a one-hot vector like [0,1] or [1,0] and the predictions respectively. We can compute it even without this conversion, with the simplified formula.", "Multi-label classification \u2014 Our target can represent multiple (or even zero) classes at once. We compute the binary cross-entropy for each class separately and then sum them up for the complete loss.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd98e7f974451&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d98e7f974451--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@martinekvlastimil?source=post_page-----d98e7f974451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@martinekvlastimil?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Vlastimil Martinek"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5aadbfa7b614&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&user=Vlastimil+Martinek&userId=5aadbfa7b614&source=post_page-5aadbfa7b614----d98e7f974451---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lijiangang?utm_source=medium&utm_medium=referral", "anchor_text": "Li Jiangang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@zuoanyixi?utm_source=medium&utm_medium=referral", "anchor_text": "Jeremy C"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "video"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d98e7f974451---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----d98e7f974451---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d98e7f974451---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----d98e7f974451---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/cross-entropy?source=post_page-----d98e7f974451---------------cross_entropy-----------------", "anchor_text": "Cross Entropy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&user=Vlastimil+Martinek&userId=5aadbfa7b614&source=-----d98e7f974451---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&user=Vlastimil+Martinek&userId=5aadbfa7b614&source=-----d98e7f974451---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd98e7f974451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d98e7f974451---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d98e7f974451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d98e7f974451--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d98e7f974451--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d98e7f974451--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d98e7f974451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@martinekvlastimil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@martinekvlastimil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vlastimil Martinek"}, {"url": "https://medium.com/@martinekvlastimil/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "78 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5aadbfa7b614&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&user=Vlastimil+Martinek&userId=5aadbfa7b614&source=post_page-5aadbfa7b614--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F37d0e324abda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-classification-d98e7f974451&newsletterV3=5aadbfa7b614&newsletterV3Id=37d0e324abda&user=Vlastimil+Martinek&userId=5aadbfa7b614&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}