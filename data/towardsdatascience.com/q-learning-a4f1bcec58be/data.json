{"url": "https://towardsdatascience.com/q-learning-a4f1bcec58be", "time": 1683004561.609963, "path": "towardsdatascience.com/q-learning-a4f1bcec58be/", "webpage": {"metadata": {"title": "Q-Learning. An early breakthrough in reinforcement\u2026 | by Reuben Kavalov | Towards Data Science", "h1": "Q-Learning", "description": "Welcome to my column on reinforcement learning, where I spend some time going over some very interesting concepts revolving around the nature of learning with a computational approach. As with most\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/tagged/a-journey-into-r-l", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032", "anchor_text": "Monte Carlo methods", "paragraph_index": 2}, {"url": "https://medium.com/@reubena.kavalov/my-journey-into-reinforcement-learning-part-3-dynamic-programming-3cb8a8d0815c", "anchor_text": "Bellman equation from before", "paragraph_index": 3}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI Gym", "paragraph_index": 8}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto", "paragraph_index": 12}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver", "paragraph_index": 13}], "all_paragraphs": ["Welcome to my column on reinforcement learning, where I spend some time going over some very interesting concepts revolving around the nature of learning with a computational approach. As with most learning, there is an interaction with an environment, and, as put by Sutton and Barto in Reinforcement Learning: An Introduction, \u201cLearning from interaction is a foundational idea underlying nearly all theories of learning and intelligence.\u201d", "In my last post, we went over on-policy control methods in Temporal-Difference (TD) learning, specifically \u2014 Sarsa, the case in which we update our value function towards the estimated return after one step, moving from one state-action pair to the next. Today, we will be looking at Q-Learning, an off-policy TD control method. Some concepts were explained in the previous posts, which you can find here. Links to the resources that I used to learn about this cool topic will be at the bottom of the post.", "The idea of Q-Learning is easy to grasp: We select our next action based on our behavior policy, but we also consider an alternative action that we might have taken, had we followed our target policy. This allows the behavior and target policies to improve, making use of the action-values Q(s, a). The process works similarly to off-policy Monte Carlo methods. The update looks like this:", "As you can see, our Sarsa update was replaced with Q-values Q(S\u209c, A\u209c), being updated a little bit in the direction of the reward plus the discounted value of the next state of the alternate action under the target policy. This is the Bellman equation from before, but now for Q, eliminating the need for importance sampling.", "As we did with off-policy control with MC methods, we will make the target policy \u03c0 greedy with respect to our value function Q(s, a) at every step:", "Our behavior policy b is \u03b5-greedy with respect to our value function Q(s, a), allowing for a bit of exploration while still roughly following a sensible path.", "By plugging this into our Bellman equation, we can see the Q-learning target simplifies into the following:", "This algorithm can also be called SARSAMAX, as it chooses the action with the maximum value available after the step is taken. This simple diagram visualizes this clearly.", "As usual, let\u2019s translate this example into another OpenAI Gym environment, Cliffwalking.", "As in the gridworld scenario, the aim is to reach a particular goal (G) given a particular starting point (S). The agent can move in the four cardinal directions, and for every step that the agent takes, he receives a reward of -1. Therefore, as with our gridworld example, there exists a motivation to reach the goal faster. The twist this environment includes is the addition of the cliff, which resets the agent back to the bottom left corner if he steps on any of the spaces it occupies, as well as inflicting a major punishment, a reward of -100. There are obviously multiple safe routes from start to finish, but only one optimal route(which, coincidentally, is the most \u201cdangerous\u201d). Controlling our agent using off-policy temporal-difference control provides us with some great results \u2014 We can observe the nice and quick convergence to the optimal policy, as the agent learns to maximize reward by optimizing the action-value function.", "The beautiful part of these visualizations, to me, is how you can see the \u03b5-greedy behavior policy at work even after the optimal target policy is found. Of course, the agent will continue to take off-steps and explore off the path that it came to know to be the best one, and will occasionally fall into the cliff and have to reset. But this process makes me imagine a future where the world this agent exists in experiences some kind of shattering major changes, perhaps where the goal space is moved to some other part of the environment, or the cliffs are scattered in different ways. Our agent might be confused for a short while, but would surely find the best path to take once again.", "Thanks a lot for joining me on my crusade for reinforcement learning knowledge. Next time, we\u2019ll be taking a look at value function approximation, granting us a more sophisticated and complex method of updating value functions!", "Reinforcement Learning: An Introduction by Sutton and Barto", "RL Course by David Silver on YouTube", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist and machine learning engineer with a passion for connecting people through technology and information."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa4f1bcec58be&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451----a4f1bcec58be---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/a-journey-into-r-l", "anchor_text": "A Journey Into Reinforcement Learning"}, {"url": "https://towardsdatascience.com/tagged/a-journey-into-r-l", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032", "anchor_text": "Monte Carlo methods"}, {"url": "https://medium.com/@reubena.kavalov/my-journey-into-reinforcement-learning-part-3-dynamic-programming-3cb8a8d0815c", "anchor_text": "Bellman equation from before"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI Gym"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto"}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "Reinforcement Learning Github"}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI\u2019s Gym"}, {"url": "https://medium.com/tag/a-journey-into-r-l?source=post_page-----a4f1bcec58be---------------a_journey_into_r_l-----------------", "anchor_text": "A Journey Into R L"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a4f1bcec58be---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a4f1bcec58be---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a4f1bcec58be---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----a4f1bcec58be---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&user=Reuben+Kavalov&userId=11db47030451&source=-----a4f1bcec58be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&user=Reuben+Kavalov&userId=11db47030451&source=-----a4f1bcec58be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa4f1bcec58be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a4f1bcec58be---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a4f1bcec58be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/@reubena.kavalov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "134 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce29c814d50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fq-learning-a4f1bcec58be&newsletterV3=11db47030451&newsletterV3Id=ce29c814d50b&user=Reuben+Kavalov&userId=11db47030451&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}