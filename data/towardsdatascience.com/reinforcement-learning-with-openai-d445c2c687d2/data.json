{"url": "https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2", "time": 1682993730.78127, "path": "towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2/", "webpage": {"metadata": {"title": "Introduction: Reinforcement Learning with OpenAI Gym | by ASHISH RANA | Towards Data Science", "h1": "Introduction: Reinforcement Learning with OpenAI Gym", "description": "With reinforcement learning we aim to create algorithms that helps an agent to achieve maximum result. Now, with OpenAI we can test our algorithms in an artificial environment in generalized manner."}, "outgoing_paragraph_urls": [{"url": "https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970", "anchor_text": "A 2016 Nature survey", "paragraph_index": 16}, {"url": "https://gym.openai.com/envs", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://gym.openai.com/envs/FrozenLake8x8-v0/", "anchor_text": "environment", "paragraph_index": 24}, {"url": "https://stable-baselines3.readthedocs.io/en/master/", "anchor_text": "stable-baselines3", "paragraph_index": 29}, {"url": "https://github.com/openai", "anchor_text": "github", "paragraph_index": 29}], "all_paragraphs": ["Understand the basic goto concepts to get a quick start on reinforcement learning and learn to test your algorithms with OpenAI gym to achieve research centric reproducible results.", "This article first walks you through the basics of reinforcement learning, its current advancements and a somewhat detailed practical use-case of autonomous driving. After that we get dirty with code and learn about OpenAI Gym, a tool often used by researchers for standardization and benchmarking results. When the coding section comes please open your terminal and get ready for some hands on.A time saver tip: You can directly skip to \u2018Conceptual Understanding\u2019 section if you want to skip basics and only want try out Open AI gym directly.", "Mainly three categories of learning are supervised, unsupervised and reinforcement. Let\u2019s see the basic differences between them. In supervised learning we try to predict a target value or class where the input data for training is already having labels assigned to it. Whereas unsupervised learning uses unlabelled data for looking at patterns to make clusters, or doing PCA/anomaly detection. RL algorithms are like optimization procedures to find best methods to earn maximum reward i.e. give winning strategy to attain objective.", "Within Reinforcement Learning, there are multiple paradigms that attain this winning strategy in their own way. In complex situations calculating exact winning strategy or exact reward-value function becomes really hard, especially where our agents start learning from interactions rather than prior-gained experience. For example, a dog finding its favorite toy hidden in a new house. The dog will search the house and get an approximate idea about house schematics based on its interactions. But, still there might be a very good chance that the dog will find it incredibly hard to find the hidden toy again if it\u2019s located in a new unexplored location.", "This interaction based learning is also popularly known as model-free learning where the agent doesn\u2019t have exact knowledge of the environment i.e. agent doesn\u2019t store the knowledge of state-action probability transition function. And it tries to approximate either winning strategy (i.e. policy) or reward-value gains (i.e. value function). The same dog searching for his hidden toy in his owner\u2019s house would have previous experience i.e. prior environment knowledge and would be comfortable searching for the toy in any location within the house. And this type of learning for the hidden toy finding is known as model-based learning in RL.", "By very definition in reinforcement learning an agent takes action in the given environment either in continuous or discrete manner to maximize some notion of reward that is coded into it. Sounds too profound, well it is with a research base dating way back to classical behaviorist psychology, game theory, optimization algorithms etc. But, good for us, a lot of \u2018setting the stage\u2019 work has already been done for us to kick-start us directly into the problem formulation world and discover new things.", "Essentially, the most important of them all is that reinforcement learning scenarios for an agent in a completely known/observable deterministic environment can be formulated as a dynamic programming problem. Fundamentally meaning that the agent has to perform a series of steps in a systematic manner so that it can learn the ideal solution and it will receive guidance from reward values. The equation that expresses such a scenario in mathematical terms is known as the Bellman equation which we will see in action in some time.", "Let\u2019s first qualitatively define the concept of agent and environment formally before proceeding further for understanding technical details about RL. Environment is the universe of agents which changes the state of agent with given action performed on it. Agent is the system that perceives the environment via sensors and performs actions with actuators. In the below situations Homer(Left) and Bart(right) are our agents and World is their environment. They performs actions within it and improve their state of being by getting happiness or contentment as reward.", "Starting with the most popular game series since IBM\u2019s Deep Blue v/s Kasparov which created huge hype for AI. And the inhuman-like awareness for the deep reinforcement learning agent in AlpaGo v/s Lee Sedol in Go competition series. Mastering a game with more board configuration than atoms in the Universe against a den 9 master shows the power such smart systems hold. Also recent RL research breakthroughs and wins against World Pros with Dota bots from OpenAI is also commendable. With agents getting trained to handle such complex and dynamic environments, mastering these games are an example of testing the limits of AI agents for handling very complex hierarchical-like situations. Application wise, already complex applications like driverless cars, smart drones are operating in the real world.", "Let\u2019s understand some more fundamentals of reinforcement learning and then start with OpenAI gym to make our own agent. After that I\u2019ll recommend you to move towards Deep RL and tackle more complex situations. Scope of all the RL applications is beyond imagination and can be applied to so many domains like time-series predictions, healthcare, supply-chain automation and so on. But, here we\u2019ll discuss one of the most popular application use-case of self driving autonomous vehicles and navigation tasks in general.", "The unique ability to run algorithms on the same state over and over which helps it to learn best action for that particular state to progress to the ideal next state, which essentially is equivalent to breaking the construct of time for humans to gain infinite learning experience at almost no time.", "With RL as a framework agent acts with certain actions which transform the state of the agent, each action is associated with reward value. It also uses a policy to determine its next action, which is constituted of a sequence of steps that maps states-action pairs to calculated reward values. A policy can be qualitatively defined as an agent\u2019s way of behaving at a given time. Now, policies can be deterministic and stochastic, finding an optimal policy is the key for solving a given task.", "Also, Different actions in different states will have different associated reward values. Like the \u2018Fire\u2019 command in a game of Pocket Tanks can\u2019t always have the same reward value associated with it, as sometimes it\u2019s better to retain a position which is strategically good. To handle this complex dynamic problem with such huge combinations in a planned manner we need a Q-value( or action-value ) table which stores a map of state-action pairs to rewards.", "Now, defining the environment in RL\u2019s context as a functional component, it simply takes action at a given state as input and returns a new state and reward value associated with action-state pair.", "Interestingly enough though, neural nets enter the picture with their ability to learn state-action pairs rewards with ease when the environment becomes highly complex to handle with computationally restrictive Iterative algorithms and this is known as Deep RL. Like playing those earlier games like Mario, Atari, PAC-MAN etc.", "Here, we will limit to simple Q-Learning only i.e. w/o neural networks, where Q function maps state-action pairs to a maximum with combination of immediate reward plus future rewards i.e. for new states learned value is current reward plus future estimate of rewards. Quantifying it into an equation with different parameters like learning rate and discount factor to guide agent\u2019s choice of action. We arrive at the following equation: Structurally, it holds much similarity to Bellman\u2019s equation.", "A 2016 Nature survey indicated that more than 70 percent of researchers have tried and failed to reproduce another scientist\u2019s experiments, and more than half have failed to reproduce their own experiments.", "OpenAI is created for removing this problem of lack of standardization in papers along with an aim to create better benchmarks by giving versatile numbers of environments with great ease of setting up. Aim of this tool is to increase reproducibility in the field of AI and provide tools with which everyone can learn about the basics of AI.", "Open your terminal and get ready for some CTRL+C and CTRL+V work !! But, of course I\u2019ll recommend against it.", "What is OpenAI gym ? This python library gives us a huge number of test environments to work on our RL agent\u2019s algorithms with shared interfaces for writing general algorithms and testing them. Let\u2019s get started, just type pip install gym on the terminal for easy install, you\u2019ll get some classic environment to start working on your agent. Copy the code below and run it, your environment will get loaded. You can check out other available environments like Algorithmic, Atari, Box2D and Robotics here and use the second listed code snippet component below for listing all the available environments.", "When object interacts with environment with an action then step() function returns observation which generally represents environments next state, reward a float of reward in previous action, done when it\u2019s time to reset the environment or goal achieved and info a dict for debugging, it can be used for learning if it contains raw probabilities of environment\u2019s last state. See how it works from the code snippet below. Also, observe how observation of type Space is different for different environments.", "What is action_space in above code? action-space & observation-space describes what is the valid format of action & state parameters for that particular env to work on with. Just take a look at values returned.", "Discrete is non-negative possible values, above 0 or 1 are equivalent to left and right movement for CartPole balancing. Box represent n-dim array. These standard interfaces can help in writing general codes for different environments. As we can simply check the bounds env.observation_space.high/[low] and code them into our general algorithm.", "Here, we are using Python3.x for the highlighted code sample of Q-Learning algorithm below.", "Let\u2019s start building our Q-table algorithm, which will try to solve the FrozenLake navigation environment. In this environment the aim is to reach the goal, on a frozen lake that might have some holes in it. Here is how the surface is the depicted by this Toy-Text environment.", "Q-table contains state-action pairs mapping to reward. So, we will construct an array which maps different states and actions to reward values during execution of algorithm. Its dimension will clearly |states|x|actions|. Let\u2019s write it in code for the Q-learning Algorithm.", "If you are interested in working with other environments, you can opt to work along the lines of a cleaner code snippet highlighted below.", "But do remember even with a common interface the code complexity will be different for different environments. In the above environment we only had a simple 64 state environment with few actions only to handle. We were able to store them in a two dimensional array for reward mapping very easily. Now, Let\u2019s consider more complicated environment cases like Atari envs and look at the approach that is needed.", "observation_space is needed to be represented by a 210x160x3 tensor which makes our Q-table even more complicated. Also, each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from {2,3,4}. With 33,600 pixels in RGB channels with values ranging from 0\u2013255 the environment clearly has become overly complicated. A simple Q-learning approach can\u2019t be used here. Deep learning with its CNN architecture is the solution for this problem and a topic you should focus on for follow up of this introductory article.", "Now, with the above tutorial you have the basic knowledge about the gym and all you need to get started with it. Gym is also TensorFlow & PyTorch compatible but I haven\u2019t used them here to keep the tutorial simple. After trying out the gym package you must get started with stable-baselines3 for learning the good implementations of RL algorithms to compare your implementations. To see all the OpenAI tools check out their github page. RL is an expanding field with applications in a huge number of domains and it will play an important role in future AI breakthroughs. Hope you continue with your RL journey & thanks for reading!!", "Yes, sharing with you shameful yet glorious starter project plugs, in case you are just getting started with reinforcement learning. The additional advantage of picking up these practical projects is getting complete mentor support & wonderful free reinforcement learning resources like books, videos and so on. Interestingly enough though, there are two liveProjects that I\u2019ll recommend to you pursuing further:", "Additionally, the first milestone is free practice for this liveProject. Even if you don\u2019t move forward with the liveProject, I promise that you\u2019ll learn a great deal about the autonomous driving RL task just by practically following along the first milestone. Cheers!", "Important Note: If you are here before July-26-2021 and are interested in carrying forward with the first liveProject. Please, use the code lprana and you\u2019ll get 45% off on the project. Kudos!!", "Chao for now! In case you select the first project, be seeing you there\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Mentoring @Manning liveProject: RL for Self-Driving Vehicles"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd445c2c687d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashishrana160796?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashishrana160796?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "ASHISH RANA"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7214e6e13ff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&user=ASHISH+RANA&userId=7214e6e13ff3&source=post_page-7214e6e13ff3----d445c2c687d2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://preydata.com/machine-learning-use-cases/", "anchor_text": "Preydata"}, {"url": "https://www.psychestudy.com/behavioral/learning-memory/operant-conditioning/reinforcement-punishment/positive-negative-reinforcement", "anchor_text": "psychestudy"}, {"url": "http://ajudaily.com", "anchor_text": "ajudaily"}, {"url": "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_reinforcement_learning.html", "anchor_text": "leonardoaraujosantos"}, {"url": "https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970", "anchor_text": "A 2016 Nature survey"}, {"url": "https://gym.openai.com/envs", "anchor_text": "here"}, {"url": "https://gym.openai.com/envs/FrozenLake8x8-v0/", "anchor_text": "environment"}, {"url": "https://stable-baselines3.readthedocs.io/en/master/", "anchor_text": "stable-baselines3"}, {"url": "https://github.com/openai", "anchor_text": "github"}, {"url": "https://www.manning.com/liveproject/reinforcement-learning-for-self-driving-vehicles?utm_source=ashishrana&utm_medium=affiliate&utm_campaign=liveproject_rana_reinforcement_7_6_21&a_aid=ashishrana&a_bid=8f38c809", "anchor_text": "Reinforcement Learning for Self-driving VehiclesIn this liveProject, you'll develop an AI driving agent that can simulate independent driving. Your agent should be\u2026www.manning.com"}, {"url": "https://www.manning.com/liveproject/deep-reinforcement-learning-for-self-driving-robots?utm_source=ashishrana&utm_medium=affiliate&utm_campaign=liveproject_galbraith_deep_6_2_21&a_aid=ashishrana&a_bid=e4173afc", "anchor_text": "Deep Reinforcement Learning for Self-Driving RobotsIn this liveProject, you'll investigate reinforcement learning approaches that will allow autonomous robotic carts to\u2026www.manning.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d445c2c687d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d445c2c687d2---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/openai?source=post_page-----d445c2c687d2---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----d445c2c687d2---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----d445c2c687d2---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&user=ASHISH+RANA&userId=7214e6e13ff3&source=-----d445c2c687d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&user=ASHISH+RANA&userId=7214e6e13ff3&source=-----d445c2c687d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd445c2c687d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d445c2c687d2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d445c2c687d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d445c2c687d2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d445c2c687d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashishrana160796?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashishrana160796?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "ASHISH RANA"}, {"url": "https://medium.com/@ashishrana160796/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "323 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7214e6e13ff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&user=ASHISH+RANA&userId=7214e6e13ff3&source=post_page-7214e6e13ff3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F493de0b23376&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-with-openai-d445c2c687d2&newsletterV3=7214e6e13ff3&newsletterV3Id=493de0b23376&user=ASHISH+RANA&userId=7214e6e13ff3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}