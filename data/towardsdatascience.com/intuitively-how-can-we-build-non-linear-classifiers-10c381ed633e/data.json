{"url": "https://towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e", "time": 1683006864.796405, "path": "towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e/", "webpage": {"metadata": {"title": "Intuitively, How Can We build Non-Linear Classifiers | by Angela Shi | Towards Data Science", "h1": "Intuitively, How Can We build Non-Linear Classifiers", "description": "In my article Intuitively, how can we Understand different Classification Algorithms, I introduced 5 approaches to classify data. But the toy data I used was almost linearly separable. So, in this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3", "anchor_text": "Intuitively, how can we Understand different Classification Algorithms", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136", "anchor_text": "Intuitively, How Can We (Better) Understand Logistic Regression", "paragraph_index": 17}], "all_paragraphs": ["In my article Intuitively, how can we Understand different Classification Algorithms, I introduced 5 approaches to classify data.", "But the toy data I used was almost linearly separable. So, in this article, we will see how algorithms deal with non-linearly separable data.", "Let\u2019s take some simple examples in 1D.", "LDA means Linear Discriminant Analysis. So by definition, it should not be able to deal with non-linearly separable data. But maybe we can do some improvements and make it work?", "Let\u2019s go back to the definition of LDA. The idea is to build two normal distributions: one for blue dots and the other one for red dots. Concerning the calculation of the standard deviation of these two normal distributions, we have two choices:", "The idea of LDA consists of comparing the two distributions (the one for blue dots and the one for red dots). Or we can calculate the ratio of blue dots density to estimate the probability of a new dot to be belonged to blue dots.", "When estimating the normal distribution, if we consider that the standard deviation is the same for the two classes, then we can simplify:", "In the equation above, let\u2019s note the mean and standard deviation with subscript b for blue dots, and subscript r for red dots.", "Finally, after simplifying, we end up with a logistic function.", "If we keep a different standard deviation for each class, then the x\u00b2 terms or quadratic terms will stay.", "In the graph below, we can see that it would make much more sense if the standard deviation for the red dots was different from the blue dots:", "Then we can see that there are two different points where the two curves are in contact, which means that they are equal, so, the probability is 50%. And we can use these two points of intersection to be two decision boundaries.", "It is because of the quadratic term that results in a quadratic equation that we obtain two zeros. So we call this algorithm QDA or Quadratic Discriminant Analysis.", "In conclusion, it was quite an intuitive way to come up with a non-linear classifier with LDA: the necessity of considering that the standard deviations of different classes are different.", "But the obvious weakness is that if the nonlinearity is more complex, then the QDA algorithm can't handle it. For example, if we need a combination of 3 linear boundaries to classify the data, then QDA will fail.", "Logistic regression performs badly as well in front of non-linearly separable data. We can see the results below.", "Just as a reminder from my previous article, the graphs below show the probabilities (the blue lines and the red lines) for which you should maximize the product to get the solution for logistic regression.", "We know that LDA and Logistic Regression are very closely related. They have the final model is the same, with a logistic function. But the parameters are estimated differently. You can read this article Intuitively, How Can We (Better) Understand Logistic Regression.", "We can see that to go from LDA to QDA, the difference is the presence of the quadratic term. So, why not try to improve the logistic regression by adding an x\u00b2 term? And that\u2019s why it is called Quadratic Logistic Regression.", "And the initial data of 1 variable is then turned into a dataset with two variables. Now, we can see that the data seem to behave linearly.", "We can apply Logistic Regression to these two variables and get the following results. Since we have two inputs and one output that is between 0 and 1. Then we can visualize the surface created by the algorithm.", "Then we can find the decision boundary, which corresponds to the line with probability equals 50%. Which is the intersection between the LR surface and the plan with y=0.5", "In 2D we can project the line that will be our decision boundary. And we can add the probability as the opacity of the color.", "Now, what is the relationship between Quadratic Logistic Regression and Quadratic Discriminant Analysis? In 1D, the only difference is the difference of parameters estimation (for Quadratic logistic regression, it is the Likelihood maximization; for QDA, the parameters come from means and SD estimations).", "Now for higher dimensions. QDA can take covariances into account. And as for QDA, Quadratic Logistic Regression will also fail to capture more complex non-linearities in the data.", "The trick of manually adding a quadratic term can be done as well for SVM. The result below shows that the hyperplane separator seems to capture the non-linearity of the data. (The dots with X are the support vectors.)", "Mathematicians found other \u201ctricks\u201d to transform the data. And one of the tricks is to apply a Gaussian kernel. We will see a quick justification after. But one intuitive way to explain it is: instead of considering support vectors (here they are just dots) as isolated, the idea is to consider them with a certain distribution around them.", "Let\u2019s first look at the linearly separable data, the intuition is still to analyze the frontier areas. Instead of a linear function, we can consider a curve that takes the distributions formed by the distributions of the support vectors. We can see that the support vectors \u201cat the border\u201d are more important.", "Now let\u2019s go back to the non-linearly separable case. We can apply the same trick and get the following results.", "The decision values are the weighted sum of all the distributions plus a bias.", "We can notice that in the frontier areas, we have segments of straight lines.", "In the end, we can calculate the probability to classify the dots.", "The idea of kernel tricks can be seen as mapping the data into a higher dimension space. And the new space is called Feature Space.", "In the case of polynomial kernels, our initial space (x, 1 dimension) is transformed into 2 dimensions (formed by x, and x\u00b2). In the case of the Gaussian kernel, the number of dimensions is infinite. We can use the Taylor series to transform the exponential function into its polynomial form.", "To visualize the transformation of the kernel. We can consider the dual version of the classifier.", "Applying the kernel to the primal version is then equivalent to applying it to the dual version.", "The previous transformation by adding a quadratic term can be considered as using the polynomial kernel:", "And in our case, the parameter d (degree) is 2, the coefficient c0 is 1/2, and the coefficient gamma is 1.", "So, the Gaussian transformation uses a kernel called RBF (Radial Basis Function) kernel or Gaussian kernel.", "Conclusion: Kernel tricks are used in SVM to make it a non-linear classifier. And actually, the same method can be applied to Logistic Regression, and then we call them Kernel Logistic Regression.", "By construction, kNN and decision trees are non-linear models. So they will behave well in front of non-linearly separable data.", "As a reminder, here are the principles for the two algorithms.", "For kNN, we consider a locally constant function and find the nearest neighbors for a new dot. And then the proportion of the neighbors\u2019 class will result in the final prediction.", "For a classification tree, the idea is: divide and conquer. The principle is to divide in order to minimize a metric (that can be the Gini impurity or Entropy). So the non-linear decision boundaries can be found when growing the tree.", "Here is the result of a decision tree for our toy data.", "Here is the recap of how non-linear classifiers work:", "I spent a lot of time trying to figure out some intuitive ways of considering the relationships between the different algorithms. I hope that it is useful for you too.", "And another way of transforming data that I didn\u2019t discuss here is neural networks. You can read the following article to discover how.", "For the principles of different classifiers, you may be interested in this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "a data science and machine learning enthusiast, dedicated to simplifying complex concepts in a clear way."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F10c381ed633e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----10c381ed633e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@angela.shi?source=post_page-----10c381ed633e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Angela Shi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&user=Angela+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e----10c381ed633e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3", "anchor_text": "Intuitively, how can we Understand different Classification Algorithms"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136", "anchor_text": "Intuitively, How Can We (Better) Understand Logistic Regression"}, {"url": "https://towardsdatascience.com/intuitively-how-do-neural-networks-work-d7710b602e51", "anchor_text": "Intuitively, How Do Neural Networks Work?The term \u201cNeural Networks\u201d may seem mysterious, why is an algorithm called Neural Networks? Does it really mimic real\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3", "anchor_text": "Intuitively, How Can We Understand Different Classification Algorithms PrinciplesMachine learning algorithms can be viewed sometimes as a black box, so how can we explain them in a more intuitive way?towardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----10c381ed633e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----10c381ed633e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----10c381ed633e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----10c381ed633e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----10c381ed633e---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&user=Angela+Shi&userId=2bf03e38122e&source=-----10c381ed633e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&user=Angela+Shi&userId=2bf03e38122e&source=-----10c381ed633e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F10c381ed633e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----10c381ed633e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----10c381ed633e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----10c381ed633e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----10c381ed633e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----10c381ed633e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----10c381ed633e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Angela Shi"}, {"url": "https://medium.com/@angela.shi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&user=Angela+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2aafe7bac5e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-build-non-linear-classifiers-10c381ed633e&newsletterV3=2bf03e38122e&newsletterV3Id=2aafe7bac5e3&user=Angela+Shi&userId=2bf03e38122e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}