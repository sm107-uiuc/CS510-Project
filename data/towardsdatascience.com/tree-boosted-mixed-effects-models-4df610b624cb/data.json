{"url": "https://towardsdatascience.com/tree-boosted-mixed-effects-models-4df610b624cb", "time": 1683012326.131222, "path": "towardsdatascience.com/tree-boosted-mixed-effects-models-4df610b624cb/", "webpage": {"metadata": {"title": "Tree-Boosted Mixed Effects Models | by Fabio Sigrist | Towards Data Science", "h1": "Tree-Boosted Mixed Effects Models", "description": "Combining gradient tree-boosting with mixed effects models (grouped / clustered random effects) using GPBoost (Machine Learning; Statistics; Data Science; Artificial Intelligence; XGBoost; LightGBM)."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "GPBoost algorithm", "paragraph_index": 0}, {"url": "https://github.com/fabsig/GPBoost", "anchor_text": "GPBoost library", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020),", "paragraph_index": 3}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/S0957417420308381", "anchor_text": "gradient and/or a Newton boosting", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)", "paragraph_index": 7}, {"url": "https://github.com/microsoft/LightGBM/", "anchor_text": "LightGBM library", "paragraph_index": 7}, {"url": "https://github.com/fabsig/GPBoost/blob/master/examples/python-guide/GPBoost_algorithm_blog_post_example.py", "anchor_text": "here as a Python script", "paragraph_index": 8}, {"url": "https://github.com/fabsig/GPBoost/tree/master/R-package", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman3.html", "anchor_text": "function F(X)", "paragraph_index": 9}, {"url": "https://github.com/fabsig/GPBoost/tree/master/examples/python-guide/parameter_tuning.py", "anchor_text": "Python parameter tuning demo", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Efficiency_(statistics)", "anchor_text": "efficient", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/tree-boosting-for-spatial-data-789145d6d97d", "anchor_text": "supports other types of random effects such as Gaussian processes", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)", "paragraph_index": 21}, {"url": "https://github.com/fabsig/GPBoost", "anchor_text": "github", "paragraph_index": 21}], "all_paragraphs": ["This article shows how tree-boosting (sometimes also referred to as \u201cgradient tree-boosting\u201d) can be combined with mixed effects models using the GPBoost algorithm. Background is provided on both the methodology as well as on how to apply the GPBoost library using Python. We show how (i) models are trained, (ii) parameters tuned, (iii) model are interpreted, and (iv) predictions are made. Further, we do a comparison of several alternative approaches.", "Tree-boosting with its well-known implementations such as XGBoost, LightGBM, and CatBoost, is widely used in applied data science. Besides state-of-the-art predictive accuracy, tree-boosting has the following advantages:", "Mixed effects models are a modeling approach for clustered, grouped, longitudinal, or panel data. Among other things, they have the advantage that they allow for more efficient learning of the chosen model for the regression function (e.g. a linear model or a tree ensemble).", "As outlined in Sigrist (2020), combined gradient tree-boosting and mixed effects models often performs better than (i) plain vanilla gradient boosting, (ii) standard linear mixed effects models, and (iii) alternative approaches for combing machine learning or statistical models with mixed effects models.", "Grouped data (aka clustered data, longitudinal data, panel data) occurs naturally in many applications when there are multiple measurements for different units of a variable of interest. Examples include:", "Basically, such grouped data can be modeled using four different approaches:", "For the GPBoost algorithm, it is assumed that the response variable y is the sum of a potentially non-linear mean function F(X) and so-called random effects Zb:", "The model is trained using the GPBoost algorithm, where trainings means learning the (co-)variance parameters (aka hyper-parameters) of the random effects and the regression function F(X) using a tree ensemble. The random effects Zb can be estimated (or predicted, as it is often called) after the model has been learned. In brief, the GPBoost algorithm is a boosting algorithm that iteratively learns the (co-)variance parameters and adds a tree to the ensemble of trees using a gradient and/or a Newton boosting step. The main difference to existing boosting algorithms is that, first, it accounts for dependency among the data due to clustering and, second, it learns the (co-)variance parameters of the random effects. See Sigrist (2020) for more details on the methodology. In the GPBoost library, (co-)variance parameters can be learned using (accelerated) gradient descent or Fisher scoring, and trees are learned using the LightGBM library. In particular, this means that the full functionality of LightGBM is available.", "In the following, we show how combined tree-boosting and mixed effects models can be applied using the GPBoost library from Python. The complete code used in this article can be found here as a Python script. Note that there is also an equivalent R package. More information on this can be found here.", "We use simulated data here. We adopt a well known non-linear function F(X). For simplicity, we use one grouping variable. But one could equally well use several random effects including hierarchically nested ones, crossed ones, or random slopes. The number of samples is 5'000 and the number of different groups or clusters is 500. We also generate test data for evaluating the predictive accuracy. For the test data, we include both known, observed groups as well as novel, unobserved groups.", "The following code shows how one trains a model and makes predictions. As can be seen below, the learned variance parameters are close to the true ones. Note that when making predictions, one can make separate predictions for the mean function F(X) and the random effects Zb.", "A careful choice of the tuning parameters is important for all boosting algorithms. Arguably the most important tuning parameter is the number of boosting iterations. A too large number will often result in over-fitting in regression problems and a too small value in \u201cunder-fitting\u201d. In the following, we show how the number of boosting iterations can be chosen using cross-validation. Other important tuning parameters include the learning rate, the tree-depth, and the minimal number of samples per leaf. For simplicity, we do not tune them here but use some default values.", "Update: as of version 0.4.3, GPBoost has now a function (`grid_search_tune_parameters`) which can be used for parameter tuning using a random or deterministic grid search. See this Python parameter tuning demo for more details.", "Feature importance plots and partial dependence plots are tools for interpreting machine learning models. These can be used as follows.", "SHAP values and dependence plots are another important tool for model interpretation. These can be created as follows. Note: you need shap version>=0.36.0 for this.", "In the following, we compare the GPBoost algorithm to several existing approaches using the above simulated data. We consider the following alternative approaches:", "We compare the algorithms in terms of predictive accuracy measured using the root mean square error (RMSE) and computational time (clock time in seconds). The results are shown in the table below. The code for producing these results can be found below in the appendix.", "We see that GPBoost and MERF perform best (and almost equally well) in terms of predictive accuracy. Further, the GPBoost algorithm is approximately 1000 times faster than the MERF algorithm. The linear mixed effects model (\u2018Linear_ME\u2019) and tree-boosting ignoring the grouping variable (\u2018Boosting_Ign\u2019) have clearly lower predictive accuracy. Tree-boosting with the grouping variable included as a categorical variable (\u2018Boosting_Cat\u2019) also shows lower predictive accuracy than GPBoost or MERF. Note that in this example, the test data contains both existing groups, which have already been observed in the training data, and novel groups not observed in the training data (50% each). If the test data consists only of existing groups, differences among Boosting_Cat and GPBoost / MERF are larger; see the experiments in Sigrist (2020).", "Note that, for simplicity, we do only one simulation run (see Sigrist (2020) for a much more detailed comparison). Except for MERF, all computations are done using the gpboost Python package version 0.7.6. Further, we use the MERF Python package version 0.3.", "GPBoost allows for combining mixed effects models and tree-boosting. If you apply linear mixed effects models, you should investigate whether the linearity assumption is indeed appropriate. The GPBoost model allows for relaxing this assumption. It may help you to find non-linearities and interactions and achieve higher predictive accuracy. If you are a frequent user of boosting algorithms such as XGBoost and LightGBM and you have categorical variables with potentially high-cardinality, GPBoost (which extends LightGBM) can make learning more efficient and result in higher predictive accuracy.", "To the best of our knowledge, the GPBoost library is currently unmatched in terms of computational speed and predictive accuracy. Additional advantages are that GPBoost supports a range of model interpretation tools (variable importance values, partial dependence plots, SHAP values etc.). Further, it also supports other types of random effects such as Gaussian processes in addition to grouped or clustered random effects.", "Hopefully, you have found this article useful. More information on GPBoost can be found in the companion article Sigrist (2020) and on github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Fabio Sigrist is Professor of Applied Statistics and Data Science at Lucerne University of Applied Sciences and Arts."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4df610b624cb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4df610b624cb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fabsig?source=post_page-----4df610b624cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabsig?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Fabio Sigrist"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5b503a0c329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&user=Fabio+Sigrist&userId=b5b503a0c329&source=post_page-b5b503a0c329----4df610b624cb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "GPBoost algorithm"}, {"url": "https://github.com/fabsig/GPBoost", "anchor_text": "GPBoost library"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020),"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020),"}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/S0957417420308381", "anchor_text": "gradient and/or a Newton boosting"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)"}, {"url": "https://github.com/microsoft/LightGBM/", "anchor_text": "LightGBM library"}, {"url": "https://github.com/fabsig/GPBoost/blob/master/examples/python-guide/GPBoost_algorithm_blog_post_example.py", "anchor_text": "here as a Python script"}, {"url": "https://github.com/fabsig/GPBoost/tree/master/R-package", "anchor_text": "here"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman3.html", "anchor_text": "function F(X)"}, {"url": "https://github.com/fabsig/GPBoost/tree/master/examples/python-guide/parameter_tuning.py", "anchor_text": "Python parameter tuning demo"}, {"url": "https://towardsdatascience.com/mixed-effects-random-forests-6ecbb85cb177", "anchor_text": "here"}, {"url": "https://www.tandfonline.com/doi/full/10.1080/00949655.2012.741599", "anchor_text": "Hajjem et al. (2014)"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)"}, {"url": "https://en.wikipedia.org/wiki/Efficiency_(statistics)", "anchor_text": "efficient"}, {"url": "https://towardsdatascience.com/tree-boosting-for-spatial-data-789145d6d97d", "anchor_text": "supports other types of random effects such as Gaussian processes"}, {"url": "https://arxiv.org/abs/2004.02653", "anchor_text": "Sigrist (2020)"}, {"url": "https://github.com/fabsig/GPBoost", "anchor_text": "github"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4df610b624cb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4df610b624cb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4df610b624cb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/mixed-effects?source=post_page-----4df610b624cb---------------mixed_effects-----------------", "anchor_text": "Mixed Effects"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----4df610b624cb---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&user=Fabio+Sigrist&userId=b5b503a0c329&source=-----4df610b624cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&user=Fabio+Sigrist&userId=b5b503a0c329&source=-----4df610b624cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4df610b624cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4df610b624cb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4df610b624cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4df610b624cb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4df610b624cb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4df610b624cb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4df610b624cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabsig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabsig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fabio Sigrist"}, {"url": "https://medium.com/@fabsig/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "216 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5b503a0c329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&user=Fabio+Sigrist&userId=b5b503a0c329&source=post_page-b5b503a0c329--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F31dbc0932157&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-boosted-mixed-effects-models-4df610b624cb&newsletterV3=b5b503a0c329&newsletterV3Id=31dbc0932157&user=Fabio+Sigrist&userId=b5b503a0c329&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}