{"url": "https://towardsdatascience.com/calibrating-classifiers-559abc30711a", "time": 1683011608.758834, "path": "towardsdatascience.com/calibrating-classifiers-559abc30711a/", "webpage": {"metadata": {"title": "Calibrating classifiers. Are you sure your model returns\u2026 | by Micha\u0142 Oleszak | Towards Data Science", "h1": "Calibrating classifiers", "description": "Most machine learning models for classification output numbers between 0 and 1 that we tend to interpret as probabilities of the sample belonging to respective classes. In scikit-learn, for instance\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Frequentist_probability", "anchor_text": "frequentist will tell", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Elo_rating_system", "anchor_text": "ELO ratings", "paragraph_index": 8}, {"url": "https://michaloleszak.medium.com/subscribe", "anchor_text": "subscribe for email updates", "paragraph_index": 27}, {"url": "https://michaloleszak.medium.com/membership", "anchor_text": "becoming a Medium member", "paragraph_index": 27}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles", "paragraph_index": 29}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com", "paragraph_index": 31}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal", "paragraph_index": 31}], "all_paragraphs": ["Most machine learning models for classification output numbers between 0 and 1 that we tend to interpret as probabilities of the sample belonging to respective classes. In scikit-learn, for instance, we can obtain them by calling a predict_proba() method on the model. Proba, like in \u2018probabilities\u2019, right? These numbers typically sum up to one for all classes, confirming our belief that they are probabilities. But are they? Well, usually no, and here is why.", "Well, okay, these numbers are probabilities in some sense, but not in a sense that we want them to be. They fulfill most conditions to be treated as such: they are between 0 and 1, sum up to 1, and have the property that the higher the number, the more likely the class membership of the respective sample. The last point means that if you only care about the hard classification, you can sort your samples by these pseudo-probabilities and apply thresholding to take x% of the most likely samples per class, for instance.", "However, if you also care about certainty in your classification, these numbers are no good. This is because they are not probabilities in the frequentist sense. What does this mean? A frequentist will tell you that", "\u201can event\u2019s probability is the limit of its relative frequency in many trials\u201d.", "For example, why is the probability of rolling six with a dice 1/6? Because if you rolled it 6 000 000 times, you\u2019d get approximately 1 000 000 sixes. And if you rolled it infinitely many times, then exactly 1/6 rolls will come up six.", "How does this relate to classification models? If your model would produce probabilities in the frequentist sense, then out of all situations when the model predicts 80% probability of class 1, it would be correct in approximately 80% cases.", "As we have said, this is not the case for most models. But why is it important? Let\u2019s make it practical and try to win some cash from betting on football matches.", "I have gathered a dataset containing outcomes of football games in the English Premier League for the past few years. One row is one game. It looks like this:", "The result column states who won the game: Home team, Away team, or there was a Draw. We will build a binary classifier trying to predict if the home team wins. There are 18 features in the data set: ELO ratings of both teams as well as many different statistics summing up how good each team was in the recent games with respect to attack and defense. If we can predict whether the home team wins, we will bet on them and be rich.", "There are 5588 games in the data set. I have put aside one-tenth of them as a test set and fit a random forest to the remaining training set. Since there are many features, we will reduce dimensionality first using principal components.", "We got a score of 0.63. Not good, not terrible. Certainly better than a dummy model which always predicts a home win \u2014 such a model would score the accuracy of 0.46 (as the hosts tend to win almost half of the games).", "Before you rush to the betting website, check out the calibration curve. It tells you how much off are the probabilities returned by the model from the ideal frequentist probabilities. We compute it here on the test data in the following manner: we bin all the predicted probabilities into bins of width 0.1, and for each bin, we calculate the percentage of samples in which the home team won:", "You could also get similar results quicker by using the scikit-learn\u2019s calibration_curve() function, but the manual job we\u2019ve done tells you explicitely what\u2019s happened. We can now plot the resultingdf to get the so-called calibration curve:", "Ideally, all points should be on the diagonal. That would mean the model is perfectly calibrated and its probability estimates are trustworthy. However, this is not the case.", "What is the case is that if the model predicts between 0% and 10% probability of the home team winning (first bin on the horizontal axis), in reality, the hosts win in about 40% of the cases. What if we bet only on the games for which the model is really certain about the home win (>90% probability)? We would get it right in approximately 30% of the games only. This is a terrible betting strategy!", "The calibration curve we\u2019ve just seen displays an S-shaped pattern. This is often the case for many classification models, and the consequence is usually over-forecasting low probabilities and under-forecasting high probabilities. (For our data, it seems to be the other way around, possibly due to the number of observations in the extreme bins, but the point still holds: probability estimates are rather poor.)", "So, why do many models produce such biased probability estimates, especially for very low and high values? The reasons are slightly different for each model depending on how it works. Here, we have used a random forest, which is an ensemble of decision trees. The forest\u2019s prediction is made by averaging individual trees\u2019 predictions, and there is always some variance among them. Due to the fact that probabilities are bounded by [0, 1], there will always be some bias at the borders of this interval. For instance, say a model should predict a probability of zero for some test sample. Some of the underlying trees will correctly predict zero, but some will predict a slightly higher value. Taking the average pushes the forest\u2019s prediction away from zero. This effect is similar but opposite on the other side of the [0, 1] interval.", "A lot of other classifiers, such as naive Bayes, SVMs, or decision trees will also produce an S-shaped calibration curve. The general reason for this is that most classification models optimize for and are scored by binary metrics. Accuracy only looks at whether we are right or wrong, disregarding certainty. Gini-impurity used by decision trees to decide on splits optimizes for being as accurate as possible as quickly as possible. Examples are numerous.", "Luckily, there is a way to fix, i.e. calibrate, the probability estimates. Actually, there are even two!", "One is to feed the probability estimates from, say, a random forest to a logistic regression model and train it with the original targets. This works, because logistic regression is a rare beast that actually produces calibrated probabilities. The secret behind it is that it optimizes for log-odds, which makes probabilities actually present in the model\u2019s cost function. This approach is known as Platt-scaling.", "Another one is the so-called isotonic regression. It\u2019s a non-parametric model that fits a piecewise constant, non-decreasing function to the probabilities predicted by a random forest, for instance, in order to map them onto the true probability space.", "The practical difference between using these two methods is that Platt-scaling is better at correcting S-shaped probability estimates, like the one we have obtained for out football dataset. Isotonic regression, on the other hand, can handle any bias shape but at the cost of being prone to overfitting. In practice, I\u2019d recommend going for Platt-scaling, unless you see it not working well and your data set is large.", "Note that whichever method you pick, the calibration model should be based on different data than that to which a random forest was fit. Hence, you either need a separate validation set (you fit the random forest to the train set, make probability predictions for the validation targets, and fit the calibration model predicting validation targets from random forests\u2019 predictions on these targets), or you can simply use cross-validation, as we will do now.", "Calibrating a classifier is as easy as passing it to scikit-learn\u2019s CalibratedClassiferCV. The method argument can be either sigmoid (the default, for logistic regression a.k.a. Platt-scaling) or isotonic.", "Let\u2019s now draw the calibration curve for this new, calibrated model on top of the previous one.", "There are two interesting things to see here:", "One final remark: calibrating the model does not guarantee an improvement in its class assignment capabilities. Metrics such as accuracy, precision, or recall can even go down at times. In this case, the accuracy of the calibrated model is 62%, compared to the raw random forest\u2019s 63%. However, calibration gets the probabilities right and we\u2019re not losing a fortune to a bookmaker.", "If you liked this post, why don\u2019t you subscribe for email updates on my new articles? And by becoming a Medium member, you can support my writing and get unlimited access to all stories by other authors and myself.", "Need consulting? You can ask me anything or book me for a 1:1 here.", "You can also try one of my other articles. Can\u2019t choose? Pick one of these:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer & Data Science Instructor | Top Writer in AI & Statistics | michaloleszak.com | Book 1:1 @ hiretheauthor.com/michal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F559abc30711a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaloleszak.medium.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----559abc30711a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hdsfotografie95?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "hidde schalm"}, {"url": "https://unsplash.com/s/photos/meter?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Frequentist_probability", "anchor_text": "frequentist will tell"}, {"url": "https://en.wikipedia.org/wiki/Elo_rating_system", "anchor_text": "ELO ratings"}, {"url": "https://michaloleszak.medium.com/subscribe", "anchor_text": "subscribe for email updates"}, {"url": "https://michaloleszak.medium.com/membership", "anchor_text": "becoming a Medium member"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "here"}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles"}, {"url": "https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb", "anchor_text": "Linear Classifiers: An OverviewThis article discusses the mathematical properties and practical Python applications of four popular linear\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "A Comparison of Shrinkage and Selection Methods for Linear RegressionA detailed look at 7 popular shrinkage & selection methods.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/boost-your-grasp-on-boosting-acf239694b1", "anchor_text": "Boost your grasp on boostingDemystifying the famous competition-winning algorithm.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----559abc30711a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----559abc30711a---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/classification?source=post_page-----559abc30711a---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/data-science?source=post_page-----559abc30711a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/probability?source=post_page-----559abc30711a---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----559abc30711a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----559abc30711a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F559abc30711a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----559abc30711a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----559abc30711a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----559abc30711a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----559abc30711a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----559abc30711a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----559abc30711a--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://michaloleszak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38bf302f5b56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalibrating-classifiers-559abc30711a&newsletterV3=c58320fab2a8&newsletterV3Id=38bf302f5b56&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}