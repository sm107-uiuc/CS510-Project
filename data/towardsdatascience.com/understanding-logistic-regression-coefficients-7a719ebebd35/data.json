{"url": "https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35", "time": 1683002072.387977, "path": "towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35/", "webpage": {"metadata": {"title": "Understanding Logistic Regression Coefficients | by Ravi Charan | Towards Data Science", "h1": "Understanding Logistic Regression Coefficients", "description": "Measuring probability in terms of evidence (log-odds) gives an interpretation of Logistic Regression coefficients that arises naturally in a Bayesian context and extends to the multi-class case."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Information_theory", "anchor_text": "Information Theory", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes", "anchor_text": "Jaynes", "paragraph_index": 6}, {"url": "https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99#", "anchor_text": "Probability Theory", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Hartley_(unit)", "anchor_text": "Hartley", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Ultra", "anchor_text": "Bletchley Park", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/List_of_unusual_units_of_measurement#Nines", "anchor_text": "nine", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Nat_(unit)", "anchor_text": "nat", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/E_(mathematical_constant)", "anchor_text": "Euler\u2019s Number", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Shannon_(unit)", "anchor_text": "Shannon", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Claude_Shannon", "anchor_text": "Claude Shannon", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Rule_of_72", "anchor_text": "Rule of 72", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "logistic sigmoid function", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax function", "paragraph_index": 39}, {"url": "https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html", "anchor_text": "not the same", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/multiclass.html", "anchor_text": "scikit-learn documentation", "paragraph_index": 44}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale", "anchor_text": "derive", "paragraph_index": 45}, {"url": "https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99", "anchor_text": "book", "paragraph_index": 53}], "all_paragraphs": ["Logistic Regression suffers from a common frustration: the coefficients are hard to interpret. If you\u2019ve fit a Logistic Regression model, you might try to say something like \u201cif variable X goes up by 1, then the probability of the dependent variable happening goes up by ???\u201d but the \u201c???\u201d is a little hard to fill in.", "The trick lies in changing the word \u201cprobability\u201d to \u201cevidence.\u201d In this post, we\u2019ll understand how to quantify evidence. Using that, we\u2019ll talk about how to interpret Logistic Regression coefficients.", "Finally, we will briefly discuss multi-class Logistic Regression in this context and make the connection to Information Theory.", "This post assumes you have some experience interpreting Linear Regression coefficients and have seen Logistic Regression at least once before.", "We are used to thinking about probability as a number between 0 and 1 (or equivalently, 0 to 100%). But this is just a particular mathematical representation of the \u201cdegree of plausibility.\u201d", "There is a second representation of \u201cdegree of plausibility\u201d with which you are familiar: odds ratios. For example, if I tell you that \u201cthe odds that an observation is correctly classified is 2:1\u201d, you can check that the probability of correct classification is two thirds. Similarly, \u201ceven odds\u201d means 50%.", "My goal is convince you to adopt a third: the log-odds, or the logarithm of the odds. For interpretation, we we will call the log-odds the evidence. This follows E.T. Jaynes in his post-humous 2003 magnum opus Probability Theory: The Logic of Science.", "In general, there are two considerations when using a mathematical representation. First, it should be interpretable. Second, the mathematical properties should be convenient.", "In order to convince you that evidence is interpretable, I am going to give you some numerical scales to calibrate your intuition.", "First, evidence can be measured in a number of different units. We\u2019ll start with just one, the Hartley. The Hartley has many names: Alan Turing called it a \u201cban\u201d after the name of a town near Bletchley Park, where the English decoded Nazi communications during World War II. It is also called a \u201cdit\u201d which is short for \u201cdecimal digit.\u201d", "The formula to find the evidence of an event with probability p in Hartleys is quite simple:", "Where the odds are p/(1-p). This is much easier to explain with the table below. Note that judicious use of rounding has been made to make the probability look nice. With this careful rounding, it is clear that 1 Hartley is approximately \u201c1 nine.\u201d", "Notice that 1 Hartley is quite a bit of evidence for an event. A more useful measure could be a tenth of a Hartley. A \u201cdeci-Hartley\u201d sounds terrible, so more common names are \u201cdeciban\u201d or a decibel. Here is another table so that you can get a sense of how much information a deciban is. Hopefully you can see this is a decent scale on which to measure evidence: not too large and not too small.", "I also said that evidence should have convenient mathematical properties. It turns out that evidence appears naturally in Bayesian statistics.", "Suppose we wish to classify an observation as either True or False. We can write:", "In Bayesian statistics the left hand side of each equation is called the \u201cposterior probability\u201d and is the assigned probability after seeing the data. The P(True) and P(False) on the right hand side are each the \u201cprior probability\u201d from before we saw the data. We think of these probabilities as states of belief and of Bayes\u2019 law as telling us how to go from the prior state of belief to the posterior state. If you don\u2019t like fancy Latinate words, you could also call this \u201cafter \u2190 before\u201d beliefs.", "More on what our prior (\u201cbefore\u201d) state of belief was later. The standard approach here is to compute each probability. This is a bit of a slog that you may have been made to do once. The slick way is to start by considering the odds. If we divide the two previous equations, we get an equation for the \u201cposterior odds.\u201d", "And then we will consider the evidence which we will denote Ev. So Ev(True) is the prior (\u201cbefore\u201d) evidence for the True classification. And Ev(True|Data) is the posterior (\u201cafter\u201d). We get this in units of Hartleys by taking the log in base 10:", "In the context of binary classification, this tells us that we can interpret the Data Science process as: collect data, then add or subtract to the evidence you already have for the hypothesis. By quantifying evidence, we can make this quite literal: you add or subtract the amount!", "There are three common unit conventions for measuring evidence. We have met one, which uses Hartleys/bans/dits (or decibans etc.). This choice of unit arises when we take the logarithm in base 10.", "The next unit is \u201cnat\u201d and is also sometimes called the \u201cnit.\u201d It can be computed simply by taking the logarithm in base e. Recall that e \u22482.718 is Euler\u2019s Number.", "The final common unit is the \u201cbit\u201d and is computed by taking the logarithm in base 2. It is also sometimes called a Shannon after the legendary contributor to Information Theory, Claude Shannon.", "Until the invention of computers, the Hartley was the most commonly used unit of evidence and information because it was substantially easier to compute than the other two. (Note that information is slightly different than evidence; more below.)", "With the advent computers, it made sense to move to the bit, because information theory was often concerned with transmitting and storing information on computers, which use physical bits.", "Finally, the natural log is the most \u201cnatural\u201d according to the mathematicians. For this reason, this is the default choice for many software packages. It is also common in physics.", "I believe, and I encourage you to believe:", "Note, for data scientists, this involves converting model outputs from the default option, which is the nat.", "Finally, here is a unit conversion table. I have empirically found that a number of people know the first row off the top of their head. The 0.69 is the basis of the Rule of 72, common in finance. The 3.01 \u2248 3.0 is well known to many electrical engineers (\u201c3 decibels is a doubling of power\u201d).", "Having just said that we should use decibans instead of nats, I am going to do this section in nats so that you recognize the equations if you have seen them before. Let\u2019s denote the evidence (in nats) as S. The formula is:", "Let\u2019s say that the evidence for True is S. Then the odds and probability can be computed as follows:", "If the last two formulas seem confusing, just work out the probability that your horse wins if the odds are 2:3 against. You will first add 2 and 3, then divide 2 by their sum.", "If you believe me that evidence is a nice way to think about things, then hopefully you are starting to see a very clean way to interpret logistic regression. First, remember the logistic sigmoid function:", "Hopefully instead of a complicated jumble of symbols you see this as the function that converts information to probability. It\u2019s exactly the same as the one above!", "Let\u2019s treat our dependent variable as a 0/1 valued indicator. So 0 = False and 1 = True in the language above. The logistic regression model is", "Where X is the vector of observed values for an observation (including a constant), \u03b2 is the vector of coefficients, and \u03c3 is the sigmoid function above.", "This immediately tells us that we can interpret a coefficient as the amount of evidence provided per change in the associated predictor.", "For example, suppose we are classifying \u201cwill it go viral or not\u201d for online videos and one of our predictors is the number minutes of the video that have a cat in it (\u201ccats\u201d).", "A few brief points I\u2019ve chosen not to go into depth on.", "Given the discussion above, the intuitive thing to do in the multi-class case is to quantify the information in favor of each class and then (a) classify to the class with the most information in favor; and/or (b) predict probabilities for each class such that the log odds ratio between any two classes is the difference in evidence between them.", "We can achieve (b) by the softmax function. The probability of observing class k out of n total classes is:", "Dividing any two of these (say for k and \u2113) gives the appropriate log odds.", "How do we estimate the information in favor of each class? There are two apparent options:", "In the case of n = 2, approach 1 most obviously reproduces the logistic sigmoid function from above. Approach 2 turns out to be equivalent as well.", "Warning: for n > 2, these approaches are not the same. (The good news is that the choice of class \u2b51 in option 1 does not change the results of the regression.)", "I am not going to go into much depth about this here, because I don\u2019t have many good references for it. If you want to read more, consider starting with the scikit-learn documentation (which also talks about 1v1 multi-class classification). If you have/find a good reference, please let me know! The point here is more to see how the evidence perspective extends to the multi-class case.", "This will be very brief, but I want to point towards how this fits towards the classic theory of Information. Information Theory got its start in studying how many bits are required to write down a message as well as properties of sending messages. In 1948, Claude Shannon was able to derive that the information (or entropy or surprisal) of an event with probability p occurring is:", "Given a probability distribution, we can compute the expected amount of information per sample and obtain the entropy S:", "where I have chosen to omit the base of the logarithm, which sets the units (in bits, nats, or bans). Physically, the information is realized in the fact that it is impossible to losslessly compress a message below its information content.", "The connection for us is somewhat loose, but we have that in the binary case, the evidence for True is", "The negative sign is quite necessary because, in the analysis of signals, something that always happens has no surprisal or information content; for us, something that always happens has quite a bit of evidence for it.", "Information is the resolution of uncertainty\u2013 Claude Shannon", "Probability is a common language shared by most humans and the easiest to communicate in. But it is not the best for every context. In this post:", "I hope that you will get in the habit of converting your coefficients to decibels/decibans and thinking in terms of evidence, not probability.", "I highly recommend E.T. Jaynes\u2019 book mentioned above.", "For context, E.T. Jaynes is what you might call a militant Bayesian.", "Also: there seem to be a number of pdfs of the book floating around on Google if you don\u2019t want to get a hard copy.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a719ebebd35&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----7a719ebebd35---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@franki?utm_source=medium&utm_medium=referral", "anchor_text": "Franki Chamaki"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Information_theory", "anchor_text": "Information Theory"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression"}, {"url": "https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes", "anchor_text": "Jaynes"}, {"url": "https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99#", "anchor_text": "Probability Theory"}, {"url": "https://en.wikipedia.org/wiki/Hartley_(unit)", "anchor_text": "Hartley"}, {"url": "https://en.wikipedia.org/wiki/Ultra", "anchor_text": "Bletchley Park"}, {"url": "https://en.wikipedia.org/wiki/List_of_unusual_units_of_measurement#Nines", "anchor_text": "nine"}, {"url": "https://en.wikipedia.org/wiki/Nat_(unit)", "anchor_text": "nat"}, {"url": "https://en.wikipedia.org/wiki/E_(mathematical_constant)", "anchor_text": "Euler\u2019s Number"}, {"url": "https://en.wikipedia.org/wiki/Shannon_(unit)", "anchor_text": "Shannon"}, {"url": "https://en.wikipedia.org/wiki/Claude_Shannon", "anchor_text": "Claude Shannon"}, {"url": "https://en.wikipedia.org/wiki/Rule_of_72", "anchor_text": "Rule of 72"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "logistic sigmoid function"}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "ROC curve"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax function"}, {"url": "https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html", "anchor_text": "not the same"}, {"url": "https://scikit-learn.org/stable/modules/multiclass.html", "anchor_text": "scikit-learn documentation"}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale", "anchor_text": "derive"}, {"url": "https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99", "anchor_text": "book"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a719ebebd35---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----7a719ebebd35---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a719ebebd35---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----7a719ebebd35---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/statistics?source=post_page-----7a719ebebd35---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&user=Ravi+Charan&userId=393ce2bbf82c&source=-----7a719ebebd35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&user=Ravi+Charan&userId=393ce2bbf82c&source=-----7a719ebebd35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a719ebebd35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a719ebebd35---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a719ebebd35--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a719ebebd35--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a719ebebd35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-coefficients-7a719ebebd35&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}