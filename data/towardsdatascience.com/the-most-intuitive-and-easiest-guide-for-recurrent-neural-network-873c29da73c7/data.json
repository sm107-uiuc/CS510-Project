{"url": "https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7", "time": 1682994884.65813, "path": "towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7/", "webpage": {"metadata": {"title": "The Most Intuitive and Easiest Guide for Recurrent Neural Network | by Jiwon Jeong | Towards Data Science", "h1": "The Most Intuitive and Easiest Guide for Recurrent Neural Network", "description": "Everything has its past. And sometimes the past defines us. What route we\u2019ve walked through and what choices we made along the way. They are curved as our history and tells us what kind of person we\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/DNA", "anchor_text": "DNA", "paragraph_index": 6}, {"url": "https://www.linkedin.com/in/jiwon-jeong/", "anchor_text": "https://www.linkedin.com/in/jiwon-jeong/", "paragraph_index": 31}], "all_paragraphs": ["Everything has its past. And sometimes the past defines us. What route we\u2019ve walked through and what choices we made along the way. They are curved as our history and tells us what kind of person we were and where we\u2019re headed. This is also applicable to data. They can have their past, and that history can be used for predicting what\u2019s coming next, the future. As we have a fortune teller, likewise data, which is called the sequence models.", "This is the third part of \u2018The Most Intuitive and Easiest Guide\u2019 series to neural networks. The complete set of this post is as follows:", "This post assumes that you have pre-knowledge on the basics of neural networks. If you are completely new to this, feel free to check out the first series. Today\u2019s keywords are sequential, vanishing gradients, memory and gates. It can be hard to understand the structures and the mathematical expressions of recurrent neural networks. So this post will focus on the basic intuition with least amount of maths, just like what we did with ANN in the previous post.", "If you\u2019ve ever heard of sequence models, then you\u2019d probably have seen this kind of diagrams shown below. And It\u2019s so natural that you don\u2019t get the concept portrayed in the diagram at one sight. This is because the sequence models are a bit different from the other neural networks discussed previously.", "So before jumping deep into this neural network, why don\u2019t we talk about where the sequence models are used in the real world. This is because it can help us to understand the concept by getting a hint from those examples. So where RNN get the spotlight by data scientists?", "Speech recognition can be one typical example we can find in the closest distance. Alexa, Siri, and Google home. Those devices response to our call and can do some simple tasks such as setting the alarm or sending a message to someone else. We can also find it with google translation or chatbots as well.", "The sequence model is also popular in the field of genetics. If you have a little background knowledge on DNA, it can be much easier to understand the meaning of sequence here. In human molecules, DNA(deoxyribonucleic acid) is composed of a chain of 4 different types of nucleotides: A, C, G, and T. Genetic information exists in the successive combination of the nucleotides. Therefore these orders are essential for all kinds of living things and the sequence model has high usage in predicting the uncovered part of genes.", "What about time series analysis? This neural network is also able to predict what value will come next in stock market price or temperature data. For example, we gather 30 days of temperature data and use this series of data to predict what will be the temperature of 31st.", "Did you notice the common characteristic of these examples? Speech, DNA chains, and time series. The answer is \u201csequential.\u201d The data is not static or discontinuous, but forgoing and successive. But what do we mean by sequential or consecutive when it comes to data?", "Let\u2019s recall what we\u2019ve been doing with other models like ANN. We had sample data and passed them through layers from left to right. This means that we will input the data at a time and then they will travel toward the output layer. It was feedforward propagation and had only one direction of the flow.", "In the case of RNN, however, the data aren\u2019t inputted at the same time. As you can see the picture on the right, we will input X1 first and then input X2 to the result of X1 computation. So in the same way, X3 is computed with the result from X2 computation stage.", "Therefore when it comes to data, \u2018sequential\u2019 means we have an order in time between the data. When it\u2019s ANN, there isn\u2019t any concept of order in X1, X2 and X3. We just input them at once. In the case of RNN, however, they are inputted at different times. Therefore if we change the order, it becomes significantly different. A sentence will lose its meaning. And when it comes to DNA, this change might create\u2026 a mutant.", "So the RNN has its appeal in that we can connect the data with the previous data. This means that the model starts to care about the past and what is coming next. As the recurrent units hold the past values, we can refer to this as memory. We are now able to consider a real meaning of \u2018context\u2019 in data.", "Now with this basic intuition, let\u2019s go deeper into the structure of RNN. This is a simple RNN with one shallow layer. Our model is now going to take two values: the X input value at time t and the output value A from the previous cell (at time t-1).", "Please take a look at the equation \u2460. There are weights and bias as we did with simple ANN. It\u2019s just adding one more input value A0. And there are two different outputs from the cell. The output of A1 which will go to the next unit(\u2461) and the final output Y1 of the unit cell(\u2462). Don\u2019t get stressed by all these subscripts. They are just indicating to what value the weight belongs. Now let\u2019s move to the next units.", "Simple and cool, right? Now we can predict the future. If it\u2019s about the stock market, we can predict the stock price of a company. We then guess If there is a big dataset say, the history data from 10 years ago, we\u2019ll get better accuracy. So the longer the data, the better the outcome, Right? But the truth is, this model ain\u2019t as ideal as we\u2019d expect.", "The idea of remembering the past is fantastic. But there is one critical problem in back-propagation. Back-propagation is a step for going backward to update the weights of each layer. To update the weights, we get the gradient of the cost function and keep multiplying the gradients at the given layers using chain rule. The actual backward propagation in RNN is a bit complex than this diagram but let\u2019s skip them for simplicity. (For example, The real backward propagation takes not only the final output Yt but also all the other output Y used by the cost function.)", "Imagine when the gradients are bigger than 1. The updated values become so big to use it for optimizing. This is called exploding gradients. But this isn\u2019t a severe problem because we can fix the range that gradients can\u2019t go over. The real problem occurs when the gradients are smaller than 1. If we keep multiplying the values lower than 1, the result becomes smaller and smaller. After some steps, there will be no significant difference in outcome, and it can\u2019t make any update in weights. It is called vanishing gradients. It means the back-propagation effect can\u2019t go far enough to reach the early stage of layers.", "This is why people say RNN has a bad memory. If the length of the input values gets longer, we can\u2019t expect actual optimization. This is a really critical problem because the power of neural networks comes from updating weights. Would there be other ways to fix this problem?", "To be honest, it\u2019s very hard to get good memories for people with bad memories. We all know that. We can\u2019t remember too many things. The same goes for our model. Instead of dragging all the past, maybe it\u2019ll be better to remember selectively. This is like choosing only the important information and forgetting the rest. Having all those past values causes the vanishing gradients. Therefore we\u2019ll give an additional step to simple RNN, which is called Gated Recurrent Units.", "The diagram on the left shows the computation inside the unit cell of RNN. There\u2019s nothing new here. It\u2019s just showing that it takes two input values and returns two output values after the computation. On the right side, you can see one small change inside the box. The green box. What is this? This is a gate controller. This box determines whether we should remember or not.", "We\u2019ll compute the tanh activation (\u2460) as what we did with RNN, but this won\u2019t be used right ahead. It\u2019s like a candidate. Here we\u2019ll also get the new value (\u2461) for the gate. Since it takes the sigmoid function, the output value always goes between 0 to 1. So by multiplying it with the \u2460 value, we\u2019re going to decide whether we\u2019ll use it or not. When it\u2019s 0, \u201cNo Use\u201d or \u201cNo Update.\u201d (Use the previous value in this case) When it\u2019s 1, \u201cUse\u201d or \u201cUpdate.\u201d This is like opening and closing a gate.", "Now we finally arrived at the LSTM, Long Short Term Memory networks. Actually, LSTM was proposed earlier than GRUs. But I brought GRUs first because the structure of LSTM is more complex compared to that of GRUs with two more gates. Additionally, there will be a new concept here which is called \u2018cell state.\u2019 But you\u2019re already used to this concept. It\u2019s putting the value containing the memories aside from the hidden state value A. Let\u2019s have a look at what LSTM has in details. The blue box and the green box (the input gate) in the middle is the same with GRUs. There are two additional green boxes here: the forget gate and the output gate.", "The computation will also be the same, so we\u2019re going to get the output from the tanh function (\u2460) and the input gate value from the sigmoid function (\u2461). But the multiplying step is a bit different this time. Instead of taking only the result of the input gate, we also consider the forget gate value coming from the left.", "Let\u2019s see the equation in line 3 on the right. We have the forget gate, and this will be used for updating the cell state value like what you can see in line 4. Lastly there is one gate left, the output gate. And as you can see in line 6, we get the A value at time t by multiplying the output gate value and the tanh activation value of C at time t.", "With these three additional gates, LSTM can have a stronger memory ability. Those are all about controlling which parts or what amount it should remember. It\u2019s so robust and effective that it\u2019s really popular among the sequence models. Then could we say is it better than GRUs for all cases? As we say there\u2019s no silver bullet all the time, GRUs has its own strength as well. Its structure is simpler than LSTM, so it\u2019s sometimes proper to use GRUs to make a big model.", "What we\u2019ve discussed so far was the sequence model with one shallow layer. Then what it would be like when we add more layers? If we say adding more layers goes horizontal in ANN or CNN, it goes vertically in RNN. But the sequence model is already a big model with one or two layers, it may end up overfitting if we add a few more layers on top of that. Therefore applying normalization techniques such as dropout or batch normalization is required.", "This post was mainly about understanding the structure of each model from simple RNN to LSTM. The mathematical expressions we walked through can be complicated at first sight. After you get familiar with the overall structures of the models, however, it becomes easy to understand. Because maths are just a numeric expression to represent the concept in a clear and effective manner. If you can see the meaning behind all those numbers, then maths will become fun and interesting too.", "I also brought other additional resources as always. When you\u2019re ready to take a further step with the sequence models, I highly recommend you to check out these articles as well.", "Thank you for reading and I hope you found this post interesting. If there\u2019s anything need to be corrected, please share your insight with us. I\u2019m always open to talk so feel free to leave comments below and share your thoughts. I\u2019ll come back with another exciting project next time. Until then, happy machine learning!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Researcher / Data geek \ud83e\udd13 Bookworm \ud83d\udcda Travel lover \ud83c\udf0f LinkedIn: https://www.linkedin.com/in/jiwon-jeong/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F873c29da73c7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----873c29da73c7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jiwon.jeong?source=post_page-----873c29da73c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiwon.jeong?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Jiwon Jeong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1df293b5ca56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&user=Jiwon+Jeong&userId=1df293b5ca56&source=post_page-1df293b5ca56----873c29da73c7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-artificial-neural-network-6a3f2bc0eecb", "anchor_text": "The Easiest Guide for ANN"}, {"url": "https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480", "anchor_text": "The Easiest Guide for CNN"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Colah\u2019s blog"}, {"url": "https://en.wikipedia.org/wiki/DNA", "anchor_text": "DNA"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks"}, {"url": "https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/", "anchor_text": "Must-Read Tutorial to Learn Sequence Modeling"}, {"url": "https://medium.com/u/50dc5f8ee633?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Pulkit Sharma"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470", "anchor_text": "Recurrent Neural Networks by Example in Python"}, {"url": "https://medium.com/u/e2f299e30cb9?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----873c29da73c7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----873c29da73c7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----873c29da73c7---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/beginner?source=post_page-----873c29da73c7---------------beginner-----------------", "anchor_text": "Beginner"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----873c29da73c7---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&user=Jiwon+Jeong&userId=1df293b5ca56&source=-----873c29da73c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&user=Jiwon+Jeong&userId=1df293b5ca56&source=-----873c29da73c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F873c29da73c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----873c29da73c7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----873c29da73c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----873c29da73c7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----873c29da73c7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----873c29da73c7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----873c29da73c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiwon.jeong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiwon.jeong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jiwon Jeong"}, {"url": "https://medium.com/@jiwon.jeong/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "https://www.linkedin.com/in/jiwon-jeong/", "anchor_text": "https://www.linkedin.com/in/jiwon-jeong/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1df293b5ca56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&user=Jiwon+Jeong&userId=1df293b5ca56&source=post_page-1df293b5ca56--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd0a222fc96e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7&newsletterV3=1df293b5ca56&newsletterV3Id=d0a222fc96e6&user=Jiwon+Jeong&userId=1df293b5ca56&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}