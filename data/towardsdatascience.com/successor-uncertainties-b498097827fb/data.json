{"url": "https://towardsdatascience.com/successor-uncertainties-b498097827fb", "time": 1683001855.451678, "path": "towardsdatascience.com/successor-uncertainties-b498097827fb/", "webpage": {"metadata": {"title": "Successor Uncertainties. Efficient Exploration in Model-free\u2026 | by Jos\u00e9 Miguel Hern\u00e1ndez Lobato | Towards Data Science", "h1": "Successor Uncertainties", "description": "SU is the first probabilistic method for Q-functions that incorporates dependencies given by the Bellman equation. is highly scalable and as fast or faster than previous methods."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.06530", "anchor_text": "[1]", "paragraph_index": 0}, {"url": "https://github.com/DavidJanz/successor_uncertainties_tabular", "anchor_text": "[code]", "paragraph_index": 0}, {"url": "http://mlg.eng.cam.ac.uk", "anchor_text": "Machine Learning group", "paragraph_index": 1}, {"url": "https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/", "anchor_text": "Microsoft Research Cambridge", "paragraph_index": 1}, {"url": "https://openreview.net/forum?id=Bk6qQGWRb", "anchor_text": "https://openreview.net/forum?id=Bk6qQGWRb", "paragraph_index": 56}], "all_paragraphs": ["I describe here our recent NeurIPS paper [1] [code], which introduces Successor Uncertainties (SU), a state-of-the-art method for efficient exploration in model-free reinforcement learning.", "The leading authors are David Janz and Jiri Hron, two PhD students from the Cambridge Machine Learning group, with the work having originated during an internship by David Janz at Microsoft Research Cambridge.", "The main insight behind SU is to describe Q-functions using probabilistic models that directly take into account correlations between Q-function values, as given by the Bellman equation. These correlations had been ignored in previous work.", "The result is a method that outperforms competitors on tabular benchmarks and Atari games, while still being fast and highly scalable.", "We consider the general reinforcement learning (RL) setting in which an agent interacts with an environment with state s_t by taking action a_t. The environment then responds by transitioning to state s_t+1 and giving a reward value r_t+1, as shown in the figure below.", "The environment is usually stochastic and Markovian, fully determined by the unknown transition distribution P(s_t+1,r_t+1|s_t,a_t).", "The agent selects its next action as a function of the current state using a policy function \u03c0(a|s), which is a probability distribution over actions given the current state, that is, \u03c0(a|s)=P(a_t=a|s_t=s).", "The goal in RL is to find, from interactions with the environment, a policy function which maximizes the expected sum of discounted rewards", "where 0\u2264\u03b3<1 guarantees the sum to be finite.", "One of the main challenges in RL is that the effect of actions on rewards is often delayed and not noticeable until much later. This makes it difficult to identify the best action to take at any given state.", "When acting according to policy \u03c0, one way to account immediately about the effect of actions on future rewards is to use the state-action value function or Q-function.", "The Q-function returns the expected sum of discounted rewards that would be obtained when taking action \u201ca\u201d at state \u201cs\u201d and then acting according to \u03c0. In particular,", "where the expectation is over the future actions chosen by \u03c0 and the transition probabilities of the environment.", "The Q-function can be shown to satisfy the Bellman equation since the value of an action at a particular state depends on the value of other actions at the successor states. In particular,", "This equation clearly shows that Q-functions are not arbitrary functions and must satisfy specific constraints. This is going to be key in the design of SU, which is the first method to account for these constraints when constructing probabilistic models of Q-functions.", "Q-functions are very useful because we can obtain a new policy \u03c0_new which is better or equal than \u03c0 by acting greedily with respect to the Q-function for \u03c0. This process is called policy improvement. When the action space is discrete, we obtain the following greedy policy", "To implement policy improvement, we need an estimate of the Q-function for the current policy. This estimate may not be initially available. One way to obtain such an estimate is by repeatedly applying the Bellman operator on an initial random guess of the Q-function:", "which updates the Q-value for a state-action pair in terms of the Q-values of other state-action pairs. This process is called policy evaluation.", "Most model-free RL methods find optimal policies by starting with an initial random policy and then alternating policy evaluation and policy improvement steps with different degrees of granularity. This is called generalized policy iteration (GPI).", "In many GPI methods, the expectations required by the Bellman operator in the policy evaluation step are often approximated by Monte Carlo, by averaging across data collected from interactions with the environment.", "A key factor for good empirical results is then how to collect data so that convergence to an optimal policy is fast. Efficient exploration refers to the intelligent interaction with the environment that aims to make this fast convergence occur in practice.", "However, most often, model-free RL methods do not perform efficient exploration and just collect data by following the greedy policy with probability 1-\u0190 and then choosing random actions with probability \u0190.", "Thompson sampling (TS) is a strategy for efficient data collection which has shown to be highly successful across many different domains. In particular, our method, SU, uses an approximate version of TS for efficient exploration.", "In the general setting, TS works by iteratively performing the following two steps:", "The figure below shows an example iteration of TS when we want to maximize an unknown objective function.", "Plot 1 shows data obtained by evaluating the objective at three different inputs. Plot 2 shows samples from a posterior distribution over objectives given the data collected so far. Plot 3 shows a selected sample and plot 4 shows the input that maximizes the sampled function. This input is the optimal action at which we would collect data next according to TS.", "In the RL setting, TS is often called Posterior Sampling for Reinforcement Learning (PSRL) [5, 6]. When TS is applied to RL problems, \u201cH\u201d is a specific transition distribution and the posterior over \u201cH\u201d is computed given data in the form of observed tuples (r_t+1, s_t+1, s_t, a_t). Acting optimally with respect to a specific \u201cH\u201d involves then finding the optimal policy for the sampled P(s_t+1,r_t+1|s_t,a_t).", "This means that step 2 above is highly costly in practice since, once we have sampled a specific transition distribution, acting optimally requires to solve a new RL problem specified by the sampled P(s_t+1,r_t+1|s_t,a_t).", "The result is that Thompson sampling is infeasible in most RL problems.", "The large computational cost of TS in RL has motivated researchers to come up with approximations.", "A common approach is to work with a posterior distribution over Q-functions instead of with a posterior distribution over transition distributions. This approach is often called Randomized Value Function (RVF) [7]. In this case, acting optimally concerning a hypothesis sampled from the posterior distribution is as easy as computing the greedy policy for the sampled Q-function.", "RVF has proven to be successful in practice, leading to RL methods that perform more efficient exploration. However, such methods still have limitations when Q-functions are approximated by neural networks.", "First, obtaining a posterior distribution over Q-functions is not straightforward since, as we are performing off-policy RL, we do not have direct access to data in the form of inputs and corresponding output values for the Q-function. Many RVF methods using neural network approximations incorrectly assume that that is the case. Furthermore, these methods also have one of the following two problems:", "SU does not have these problems since it captures dependencies given by the Bellman equation and has a low computational cost.", "A posterior distribution over Q-functions for a given policy \u03c0 can be obtained by first, computing a posterior over transition distributions from observed tuples (r_t+1, s_t+1, s_t, a_t), and then mapping that posterior into a corresponding distribution over Q-functions.", "Surprisingly, the previous operation can be done straightforwardly when the transition distributions satisfy the following assumptions:", "The previous linear model for the reward results in a corresponding linear model for the Q-function. In particular,", "where \u03c8(s_t,a_t) are the successor features [10], that is, the discounted expected future occurrence of each \u03d5(s_t,a_t) under \u03c0.", "Under a Gaussian prior for w, we obtain a Gaussian posterior N(w|m, S) for w which, under the previous equation, results into a fully correlated Gaussian posterior for the Q-function, with mean and covariance functions given by", "Since Q-functions are only defined for a given policy \u03c0, we assume this policy to be the average greedy policy obtained by sampling Q-functions from the posterior distribution.", "This covariance function guarantees that the sampled Q-functions will satisfy the Bellman equation.", "The initial features \u03d5(s_t,a_t) and the successor features \u03c8(s_t,a_t) are computed using a multi-head neural network, with one head per possible action value and feature type (\u03d5 or \u03c8). The output of each head is multiplied by m to obtain predictions for the next reward and the Q-function value.", "The whole process is illustrated in the figure below for the Atari game setting, with the input to the heads given by a convolutional neural network followed by a fully connected layer. In tabular environments, this part of the network can be replaced by a one-hot-encoding of the state.", "SU learns the neural network parameters and m by optimizing", "in expectation over tuples (r_t+1, s_t+1, s_t, a_t) collected by interacting with the environment.", "The objective above includes three different loss terms. The first one is the temporal difference error for the successor features. This term enforces \u03c8 to converge to the discounted expected future occurrence of \u03d5. The second loss term tunes m and \u03d5 so that the error in the reward predictions is low. Finally, the third term is the temporal difference error in the estimate of the Q-function and it enforces \u03c8 to be also good for predicting the Q-function values.", "Finally, the covariance matrix S is updated online according to Bayes rule in a Gaussian linear model:", "where \u03b2 is the noise variance in the linear model for the rewards and 0\u2264\u03b6\u22641 is a decay factor that allows the model to forget, which is useful to counter the non-stationarity of \u03d5 during the learning process. In the experiments below we used \u03b2 = 0.001 (binary tree) and \u03b2 = 0.01 (Atari) and \u03b6 = 1 (binary tree) and \u03b6 = 0.99999 (Atari).", "We evaluated the performance of SU on a challenging problem that requires efficient exploration. In this problem, see the figure below, there are 2L+1 states and two possible actions that are randomly mapped to movements UP and DOWN in each state when the environment is generated. The rewards are always 0 except after reaching state s_2L which produces a reward of 1. States with odd indices and s_2L are terminal.", "This problem is challenging because an agent will always receive reward 0 except once the problem is solved, which, without intelligent exploration, will happen with an exponentially small probability as a function of L.", "The figure below shows the median number of episodes that are required to learn the optimal policy. SU is compared with other RVF methods such as Bayesian DQN (BDQN) [3], the Uncertainty Bellman Equation (UBE) [2] and Bootstrapped DQN [4]. The latter method approximates the Q-function posterior with an ensemble of Q-function estimators.", "BDQN and UBE perform the same as a policy that samples actions uniformly at random and they both struggle to find optimal policies when L is large. Bootstrapped DQN is also worse than SU, with the differences becoming smaller when Bootstrapped DQN uses 25 times more computation than SU. The reason for this is that the analytic updates used by our Bayesian linear model allow for uncertainty to get accounted for really quickly, while bootstrap DQN relies more on gradient-based updates.", "To show that it can be scaled to complex domains, we evaluated SU on the standard set of 49 Atari games [8]. Specific details of the implementation, network architecture and training procedure can be found in [1]. SU obtains a median human normalized score of 2.09 (averaged over 3 seeds) after 200M training frames under the \u2018no-ops start 30 minute emulator time\u2019 test protocol described in [9]. The table below shows that SU significantly outperforms competing methods.", "For more details, the difference in human normalized score between SU and the competing algorithms for individual games is charted in the following figure.", "Successor Uncertainties is a state-of-the-art method for efficient exploration in model-free reinforcement learning. In particular, SU", "[1] Janz* D., Hron* J., Mazur P., Hofmann K., Hern\u00e1ndez-Lobato J. M. and Tschiatschek S. Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning, In NeurIPS, 2019. * Equal contributors.", "[3] Azizzadenesheli, K., Brunskill, E., and Anandkumar, A. Efficient exploration through Bayesian deep Q-networks. In ICLR, 2018. https://openreview.net/forum?id=Bk6qQGWRb", "[5] Strens, M. A Bayesian framework for reinforcement learning. In ICML, 2000.", "[6] Osband, I., Russo, D., and Van Roy, B. (More) efficient reinforcement learning via posterior sampling. In NeurIPS, 2013.", "[7] Osband, I., Van Roy, B., and Wen, Z. Generalization and exploration via randomized value functions. In ICML, 2016.", "[9] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and Silver, D. Rainbow: combining improvements in deep reinforcement learning. In AAAI Conference on Artificial Intelligence, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "University Lecturer (US Assistant Professor) in Machine Learning at the University of Cambridge, UK."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb498097827fb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b498097827fb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jmhernandezlobato?source=post_page-----b498097827fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmhernandezlobato?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Jos\u00e9 Miguel Hern\u00e1ndez Lobato"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d06198f9726&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&user=Jos%C3%A9+Miguel+Hern%C3%A1ndez+Lobato&userId=3d06198f9726&source=post_page-3d06198f9726----b498097827fb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1810.06530", "anchor_text": "[1]"}, {"url": "https://github.com/DavidJanz/successor_uncertainties_tabular", "anchor_text": "[code]"}, {"url": "http://mlg.eng.cam.ac.uk", "anchor_text": "Machine Learning group"}, {"url": "https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/", "anchor_text": "Microsoft Research Cambridge"}, {"url": "https://openreview.net/forum?id=Bk6qQGWRb", "anchor_text": "https://openreview.net/forum?id=Bk6qQGWRb"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b498097827fb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----b498097827fb---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----b498097827fb---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&user=Jos%C3%A9+Miguel+Hern%C3%A1ndez+Lobato&userId=3d06198f9726&source=-----b498097827fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&user=Jos%C3%A9+Miguel+Hern%C3%A1ndez+Lobato&userId=3d06198f9726&source=-----b498097827fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b498097827fb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb498097827fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b498097827fb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b498097827fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b498097827fb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b498097827fb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b498097827fb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b498097827fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmhernandezlobato?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmhernandezlobato?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jos\u00e9 Miguel Hern\u00e1ndez Lobato"}, {"url": "https://medium.com/@jmhernandezlobato/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "242 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d06198f9726&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&user=Jos%C3%A9+Miguel+Hern%C3%A1ndez+Lobato&userId=3d06198f9726&source=post_page-3d06198f9726--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc16747e0fc89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessor-uncertainties-b498097827fb&newsletterV3=3d06198f9726&newsletterV3Id=c16747e0fc89&user=Jos%C3%A9+Miguel+Hern%C3%A1ndez+Lobato&userId=3d06198f9726&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}