{"url": "https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca", "time": 1682995017.7095108, "path": "towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca/", "webpage": {"metadata": {"title": "A Hitchhiker\u2019s Guide to Mixture Density Networks | by Dr. Oliver Borchers | Towards Data Science", "h1": "A Hitchhiker\u2019s Guide to Mixture Density Networks", "description": "Uncertainty is a key element in every decision we make. In business, however, managers regularly face decisions entailing a wide variety of unforeseeable consequences. A manager might deal with the\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.tensorflow.org/", "anchor_text": "www.tensorflow.org", "paragraph_index": 6}, {"url": "http://www.tensorflow.org/probability", "anchor_text": "www.tensorflow.org/probability", "paragraph_index": 6}, {"url": "https://github.com/oborchers/Medium_Repo", "anchor_text": "Github", "paragraph_index": 23}, {"url": "https://colab.research.google.com/drive/1at5lIq0jYvA58AmJ0aVgn2aUVpzIbwS3", "anchor_text": "Colab", "paragraph_index": 23}, {"url": "https://github.com/oborchers/Medium_Repo", "anchor_text": "Github", "paragraph_index": 45}, {"url": "https://colab.research.google.com/drive/1at5lIq0jYvA58AmJ0aVgn2aUVpzIbwS3", "anchor_text": "Colab", "paragraph_index": 45}, {"url": "http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf", "anchor_text": "Pattern Recognition and Machine Learning", "paragraph_index": 46}, {"url": "https://machinelearning.apple.com/2017/08/06/siri-voices.html", "anchor_text": "Deep Learning for Siri\u2019s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis", "paragraph_index": 47}, {"url": "https://arxiv.org/pdf/1308.0850.pdf", "anchor_text": "Generating Sequences With Recurrent Neural Networks", "paragraph_index": 48}, {"url": "http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html", "anchor_text": "Mixture Density Networks with Edward, Keras and TensorFlow", "paragraph_index": 49}, {"url": "https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation", "anchor_text": "Mixture Density Networks implementation for distribution and uncertainty estimation", "paragraph_index": 50}, {"url": "https://www.nature.com/articles/nature20101.pdf", "anchor_text": "Hybrid computing using a neural network with dynamic external memory", "paragraph_index": 51}], "all_paragraphs": ["Uncertainty is a key element in every decision we make. In business, however, managers regularly face decisions entailing a wide variety of unforeseeable consequences. A manager might deal with the following questions:", "Each of these questions implies a decision, which is ideally well informed and considers (or at least acknowledges) the underlying uncertainty. For example: Setting prices wrong leads to lower revenues, which in turn might cause the necessity of cost savings.", "This post is intended for a business oriented data-science audience. We are going to explore some technical questions about predicting continuous values and how the well-known linear model is limited in some situations. We are then demonstrating a variant of a neural network, called mixture density network (MDN), to circumvent these limitations. Hereby, managers might get a better grasp of the underlying uncertainty of a prediction.", "The questions posed earlier require a prediction and share three fundamental traits. The first one is, that we wish to predict a continuous quantity (price, website visits, sales, you name it). Second, predictions are made under uncertainty. Thus, we would ideally like to have a glimpse about how uncertain we are about predictions. Third, the predictions are conditioned on some input observation (product attributes, customer, advertisement design). Ideally we would not like to obtain a single answer for the question, but rather a range of answers to assess the probability of each individual answer. In short: We are looking for a probability distribution over a range of answers given the input. Thereby, we enhance our understanding of the prediction and may even make a better-informed decision as a manger or as a customer.", "Time for an example: Let us assume, that we are about to predict the price of a product, say headphones. Looking at the histogram of market prices we infer that there exist low (~$30), medium (~$60), and high (~$120) priced headphones. Reverting to a simple Gaussian distribution to model the data is doomed to fail, as is easily discernible from the curve of the empirical Gaussian distribution. Remember, that a Gaussian normal distribution (\ud835\udca9) is parametrized by two values: mean (\u03bc) and standard deviation (\u03c3). Fitting \ud835\udca9 to the data based on the sample mean (\u03bc=47.95) and std. deviation (\u03c3=27.76) assigns probability to regions of the price, which we do not observe in the fictional market (i.e. prices between $90 to $110). Additionally, negative prices are \u201creasonable\u201d according to the left side of the curve.", "The underlying data generating distribution of prices is a mixture of Gaussians. The mixture is multimodal; thus, it exhibits multiple \u201cpeaks\u201d. To accommodate the distribution of our fictional pricing data, we can use not one, not two, but three adequately mixed Gaussians components. We \u201cjust\u201d need to pick the parameters (\u03bc,\u03c3), normalize the distributions by a weighting (\u237a), and sum them (more on that later). Mixing those components neatly reflect the real data as shown in the following picture.", "We will use Tensorflow (www.tensorflow.org) and its extension Tensorflow-Probability (www.tensorflow.org/probability) in the subsequent chapters. The example was generated in Tensorflow Probability using the values shown below in the code snippet. We basically define a mixture of distributions of the same family.", "The first solution to the questions posed in the beginning of the post is, indeed, to perform a simple linear regression. We can virtually hear you yawn: Not that again.", "But wait, we will give it just a short technical shot! Given a vector x of inputs (product attributes, customer, \u2026, you name it again), we wish to predict y (price, website visits, \u2026). More precisely: We aim to obtain the probability of y given x: p(y|x). If we assume a Gaussian distribution of the real valued target data (as we usually do, when we minimize the squared error), then p(y|x) takes the well-known form of:", "In practical applications, we minimize the squared error term ( \u03bc(x, \u0398)\u2212 y )\u00b2 of the output of the linear function \u03bc given x, its parameters \u0398, and the target value y for all pairs of (x, y) in some dataset \ud835\udd3b. The learned function essentially \u201cspits\u201d out the conditional mean of the Gaussian distribution \u03bc(x, \u0398) given the data and parameters. It throws away the std. deviation and normalization constant, which do not depend on \u0398. By doing so, the model imposes multiple important assumptions, which can be very limiting in practice:", "Let us consider two situations, which graphically motivate the previously outlined technical problems:", "(LHS): The underlying function is linear. However, we observe two violations: Firstly, the std. deviation of the (noise) distribution is not constant. Secondly, the noise does depend on the input.", "(RHS): Not only is the std. deviation of the noise distribution dependent on x, but the output is additionally non-linear. Furthermore, the output distribution is multimodal. The simple mean is not a reasonable solution for some areas of the data (around \u00b1 8). Imposing the previously outlined assumptions might easily mislead, when we are about to predict outcomes which follow such complex patterns.", "To account for the outlined limitations, [2] proposed to parametrize a mixture of distributions by a DNN. Originally conceived in 1994 [1] [2], the MDN recently found a series of different applications. For example: Apple\u2019s Siri in iOS 11 uses MDN for speech generation [3]. Alex Graves used MDNs in combination with RNNs to generate artificial handwriting [4]. Additionally, there are multiple blog posts dedicated to the topic [5] [6] [7] [8]. Amazon Forecast offers the MDN as an algorithm for its customers [9], while [10] wrote a masterthesis about the topic.", "However, we wish to establish the method for a broader audience. For the simple reason that many modern neural network architectures can be extended to become MDNs (Transformer, LSTMs, CovNets, \u2026). MDNs can essentially be seen as an extension module, applicable to a broad variety of business relevant tasks.", "At its very core, the MDN concept is simple, straightforward, and appealing: Combine a deep neural network (DNN) and a mixture of distributions. The DNN provides the parameters for multiple distributions, which are then mixed by some weights. These weights are also provided by the DNN. The resulting (multimodal) conditional probability distribution helps us to model complex patterns found in real-world data. We are thus better able to asses, how likely certain values of our predictions are.", "Theoretically, a Gaussian mixture is capable of modeling arbitrary probability densities [2], if it is adequately parametrized (e.g., given enough components). Formally, the conditional probability for a mixture is defined as", "Let us elaborate on each parameter individually:", "The formulation of the conditional probability as a mixture of distributions already solves multiple problems associated with the outlined assumptions. First, the distribution can be arbitrary, as we are theoretically able to model every distribution as a mixture of Gaussians [2]. Second, using multiple distributions helps us to model multimodal signals. Consider our headphone price example, which is clearly multimodal. Third, the std. deviation is now conditioned on the input, allowing us to account for variable std. deviation. Even when we are just using a single Gaussian distribution, this advantage applies. Fourth, the problem of the linearity of the function can be circumvented by choosing a non-linear model, which conditions the distribution parameters on the input.", "To obtain the parameters for the mixture, a DNN is modified to output multiple parameter vectors. We start off with a single layer DNN and a ReLU activation. Using the hidden layer h1(x), we proceed by computing the parameters of the mixture as follows:", "The mixing coefficient must sum to unity: \u2211 \u237a(x) = 1. Therefore, we are using a softmax function to constrain the output. This step is important, as the mixture of probabilities must integrate to one. The constraints for \u03bb1 and \u03bb2 themselves depend on the distribution we are choosing for our model. The only constraint we must enforce for Gaussian is, that the std. deviation is \u03c3(x) > 0. This effect can be achieved in multiple ways. For example, we could use an exponential activation as originally propsed by Bishop [1] [2]. The exponential however, can lead to numerical instability. Alternatively, we can use a simple softplus activation, similar to the oneplus activation used in [11]. Or we employ a variant of the ELU activation with an offset. Due to the recent surge in prominence of the ELU, we are opting for the latter. Therefore, we end up with the following transformations:", "The choice of the constraints is dependent on the distributions and the data. As always: Different constraints might perform better on different datasets. One might even argue, that it is sensible from a business perspective to constrain \u03bc(x) to positive values as well. Thereby we could that negative prices are not within the realms of possibilities.", "As we now specified the parameters and the conditional probability, we have everything we need to directly minimize the average negative log-likelihood (NLL) using some form of gradient descent (SGD, Adagrad, Adadelta, Adam, RMSProp, etc.).", "The code is available on Github / Colab.", "Having established the basic theory for the MDN, we now show how to implement the model in Tensorflow/Keras. We essentially need two components: a custom layer to compute the parameters and the loss function to be minimized. For reasons of numerical stability and convenience, we will do most of the computation within Tensorflow functions. As we pronounced the flexibility of the MDN framework earlier, we are not discussing everything, but the relevant parts for you to build your own version of it. Defining the DNN is straightforward:", "Depending on the data and the application, it possibly also makes sense to impose additional activity regularization on the sigmas to prevent the std. deviation from blowing up. A simple L2 regularization would be a sensible option.", "The code example requires a \u201cnon-negative exponential linear unit\u201d activation function, which ensures that the sigmas are strictly greater than zero. Tensorflow provides a very friendly way to define the required activation function. We simply make nnelu a callable function and register it as a custom activation function in Keras.", "The remaining building block is the the implementation of the loss function. The application of Tensorflow-Probability comes in handy because we only redefine the example at the beginning of the post a little bit. The MixtureSameFamily requires a mixture distribution and component distributions. The former is a simple categorical distributions, which obtains the mixture weightings \u237a(x). The latter is a normal distribution, parametrized by mean and std. deviation. Subsequently, we just compute the log-likelihood of y and its negative average. By reverting to tensorflow probability, we avoid numerical over-/underflows (implementing this by hand can actually be quite tricky).", "Following the definition of the most important components of the MDN, only the compilation of the model remains to be done.", "Time to go back to our examples from earlier. We train a simple MDN with two layers, 200 neurons per layer, and one Gaussian component on the linear dataset. The MDN shows its strength: Due to conditioning the std. deviation of the distribution on the input, the MDN can adapt to the change of the underlying data distribution. It neatly captures the linear trend (as expected), but adapts the std. deviation according to the increase of uncertainty present in the data (I like this graph. It looks like a shooting star).", "To get a better grasp of the results, we additionally perform a comparison of the mean negative log-likelihood of several models. Namely, let\u2019s look at the null-model (sample mean and sample std. deviation), a linear model (linear conditioned mean and sample std. deviation), a DNN (non-linear conditioned mean and sample std. deviation), and the MDN (non-linear conditioned mean and non-linear conditioned std. deviation). The DNN and MDN use the same parameters and training routine. Thankfully, we can monitor the training progress of the MDN using Tensorboard. All it takes is a callback to the fit routine. Thus, we don\u2019t need to bother with storing training losses separately. And we are converging!", "All models are able to beat the null-model. The remaining models perform equal in terms of the MSE because the MSE assumes that the std. deviation of the underlying distribution is constant. We cannot adequately capture the behavior of the data! The NLL, which incorporates the std. deviation, does reflect a more nuanced picture. As the underlying function is linear, the DNN and linear model perform equal. The MDN, however, is better able to accommodate the data distribution, resulting in the lowest NLL value.", "To derive the conditional mean from the probability density of the MDN for a single datapoint, one computes:", "Looking at the formula explains the result: The mean does not incorporate the std. deviation \u03c3(x). Just looking at the mean of the MDN throws away valuable information, which we might need in real world applications. Having this distribution at our hands, we can compute more elaborate quantities. For example, the Shannon entropy can serve as an indicator of how certain we are. Or we could compute f-divergences to assess, how similar predictions are.", "Let us now turn to the second \u2014 non-linear \u2014 example. We first use the min-max scaler to transform y into a reasonable range for the DNN / MDN to speed up learning.", "Not only does the MDN capture the underlying non-linearity, it also captures the multimodality of the output and the change of the std. deviation. The data generating distribution is captured adequately. Looking at the conditional density for x = 8, we see that the MDN produces two disjoint peaks:", "The capability to model these complex distributions reflects in the NLL, where the MDN achieves the best NLL.", "We started this post with the example of predicting prices. After long technical elaborations let us return to our initial example: predicting prices.", "For ease of analysis, we are using the drosophila of datasets: Boston housing. Given some 13 independent variables, the goal is to predict the median value of owner-occupied homes in $1000\u2019s (MDEV). The example might not necessarily make full use of the MDNs capability to model multimodal distributions. Nonetheless, it shows how the MDN can model uncertainty of prices. The independent variables are transformed using the min-max scaler, while the prices are log transformed.", "Looking at the NLL, we observe a similar behavior as in the previous examples. The MDN is better able to cope with the data. Thus, while we might not have multimodality in the example, we are certainly benefitting from modeling the full conditional probability instead of just a point estimate.", "Analyzing the conditional densities for different houses helps us to better make a decision. We are fairly confident about the high price of house 18, thus, as a manager, we can set the prices accordingly.", "The prediction for house 12 is very inconclusive. It may be necessary for a human expert to directly assess the case to set the price.", "House 13 and 45 do overlap with regards to their price. It makes sense to analyze their attributes directly to see, whether they can serve as objects of interest for buyers in the same price range. Although we do not tap the full potential of the model in this simple dataset, we still benefit from the addional capabilities.", "Assessing uncertainty is a crucial aspect for modern businesses. This blog post highlighted the theoretical reasoning, the implementation details, and some tips and tricks when using MDNs. We demonstrated the capabilities of the MDN in simulated and practical applications. Due to its simplicity and modularity, we expect a broad variety of applications.", "For inquiries and questions, feel free to contact me.", "The code is available on Github / Colab. The guide was written for Tensorflow 1.12.0 and Tensorflow-Probability 0.5.0.", "[1] Christopher M. Bishop, Pattern Recognition and Machine Learning (2006)", "[3] Siri Team, Deep Learning for Siri\u2019s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis (2017)", "[4] Alex Graves, Generating Sequences With Recurrent Neural Networks (2014)", "[5] Christopher Bonnett, Mixture Density Networks with Edward, Keras and TensorFlow (2016)", "[10] Axel Brando, Mixture Density Networks implementation for distribution and uncertainty estimation (2017)", "[11] Alex Graves et al., Hybrid computing using a neural network with dynamic external memory (2016)", "Opinions expressed are solely my own and do not express the views or opinions of my employer. The author assumes no responsibility or liability for any errors or omissions in the content of this site. The information contained in this site is provided on an \u201cas is\u201d basis with no guarantees of completeness, accuracy, usefulness or timeliness.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "From data to business value: Lead Machine Learning Engineer @ Oxolo | Data Scientist| Programmer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76b435826cca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----76b435826cca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@oliverbor?source=post_page-----76b435826cca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Dr. Oliver Borchers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2fcfdd820b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=post_page-c2fcfdd820b0----76b435826cca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@adigold1?utm_source=medium&utm_medium=referral", "anchor_text": "Adi Goldstein"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@rawpixel?utm_source=medium&utm_medium=referral", "anchor_text": "rawpixel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www.tensorflow.org/", "anchor_text": "www.tensorflow.org"}, {"url": "http://www.tensorflow.org/probability", "anchor_text": "www.tensorflow.org/probability"}, {"url": "https://github.com/oborchers/Medium_Repo", "anchor_text": "Github"}, {"url": "https://colab.research.google.com/drive/1at5lIq0jYvA58AmJ0aVgn2aUVpzIbwS3", "anchor_text": "Colab"}, {"url": "https://github.com/oborchers/Medium_Repo", "anchor_text": "Github"}, {"url": "https://colab.research.google.com/drive/1at5lIq0jYvA58AmJ0aVgn2aUVpzIbwS3", "anchor_text": "Colab"}, {"url": "http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf", "anchor_text": "Pattern Recognition and Machine Learning"}, {"url": "https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf", "anchor_text": "Mixture Density Networks"}, {"url": "https://machinelearning.apple.com/2017/08/06/siri-voices.html", "anchor_text": "Deep Learning for Siri\u2019s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis"}, {"url": "https://arxiv.org/pdf/1308.0850.pdf", "anchor_text": "Generating Sequences With Recurrent Neural Networks"}, {"url": "http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html", "anchor_text": "Mixture Density Networks with Edward, Keras and TensorFlow"}, {"url": "https://ngbinghao.gitlab.io/posts/mixture-density-networks-basics/", "anchor_text": "Mixture Density Networks: Basics"}, {"url": "http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/", "anchor_text": "Mixture Density Networks with TensorFlow"}, {"url": "https://mikedusenberry.com/mixture-density-networks", "anchor_text": "Mixture Density Networks"}, {"url": "https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-mdn.html", "anchor_text": "Mixture Density Networks (MDN) Recipe"}, {"url": "https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation", "anchor_text": "Mixture Density Networks implementation for distribution and uncertainty estimation"}, {"url": "https://www.nature.com/articles/nature20101.pdf", "anchor_text": "Hybrid computing using a neural network with dynamic external memory"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----76b435826cca---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----76b435826cca---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/probability?source=post_page-----76b435826cca---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/predictions?source=post_page-----76b435826cca---------------predictions-----------------", "anchor_text": "Predictions"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----76b435826cca---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=-----76b435826cca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=-----76b435826cca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76b435826cca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F76b435826cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----76b435826cca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----76b435826cca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----76b435826cca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----76b435826cca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----76b435826cca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----76b435826cca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dr. Oliver Borchers"}, {"url": "https://medium.com/@oliverbor/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "284 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2fcfdd820b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=post_page-c2fcfdd820b0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbb76721149ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-hitchhikers-guide-to-mixture-density-networks-76b435826cca&newsletterV3=c2fcfdd820b0&newsletterV3Id=bb76721149ab&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}