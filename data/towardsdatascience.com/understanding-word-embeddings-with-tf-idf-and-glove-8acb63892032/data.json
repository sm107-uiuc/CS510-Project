{"url": "https://towardsdatascience.com/understanding-word-embeddings-with-tf-idf-and-glove-8acb63892032", "time": 1683000580.446163, "path": "towardsdatascience.com/understanding-word-embeddings-with-tf-idf-and-glove-8acb63892032/", "webpage": {"metadata": {"title": "Understanding Word Embeddings with TF-IDF and GloVe | by C\u00e9line Van den Rul | Towards Data Science", "h1": "Understanding Word Embeddings with TF-IDF and GloVe", "description": "Word embeddings are one of the most popular techniques in natural language processing. Understanding how they work and what they can be used for is therefore crucial for any data enthusiast willing\u2026"}, "outgoing_paragraph_urls": [{"url": "http://text2vec.org/glove.html", "anchor_text": "text2vec", "paragraph_index": 15}, {"url": "http://text2vec.org/glove.html", "anchor_text": "text2vec", "paragraph_index": 29}, {"url": "https://twitter.com/celine_vdr", "anchor_text": "Twitter", "paragraph_index": 38}, {"url": "https://medium.com/@celine.vdr", "anchor_text": "Medium", "paragraph_index": 38}], "all_paragraphs": ["Word embeddings are one of the most popular techniques in natural language processing. Understanding how they work and what they can be used for is therefore crucial for any data enthusiast willing to jump into the world of natural language processing.", "To show you how important they are, consider the following: we\u2019ve all used Google search to find thousands or more results appearing and matching our query. But have you every wondered how the computer understands you mean \u201cApple\u201d the company and not the fruit? Similarly, how does it return \u201cMichelle Obama\u201d if you type \u201cwife of Barack Obama\u201d?", "The answer to the above is: word embeddings. This technique allows for the representation of words in a way that captures their meanings, semantic relationships and the contexts they are used in.", "\u201cYou shall know a word by the company it keeps\u201d \u2014 Firth, J. R.", "Word embeddings is the process by which words are transformed into vectors of real numbers.", "Why do we need that? Well, most of the algorithms in machine learning cannot process strings or plain text in their raw form. Instead, they require numbers as inputs to be able to function. By transforming words into vectors, word embeddings therefore allows us to process the huge amount of text data and make them fit for machine learning algorithms.", "How does it work? One of the most basic ways to transform words into numbers is through the one-hot encoding method. Consider the sentence: \u201cI am learning how word embeddings work\u201d. The words in this sentence are \u201clearning\u201d, \u201cembeddings\u201d etc. From this, we can create a dictionary which is the list of all unique words in the sentence. In this case: [\u201cI\u201d, \u201cam\u201d, \u201clearning\u201d, \u201chow\u201d, \u201cword\u201d, \u201cembeddings\u201d, \u201cwork\u201d]. A one-hot encoded vector representation of a word can be encoded in a way that 1 stands for the position where the word exists and 0 everywhere else. For instance, the vector representation of \u201clearning\u201d is as follows: [0, 0, 1, 0, 0, 0, 0]. As a result, each word will have its unique dimension.", "In addition, they are mapped into a vector space. Why? The idea behind it is that words with similar context occupy close spatial positions. In other words, words used in a similar context are close to each other while words that aren\u2019t are far away from each other in the vector space. That\u2019s the beauty of all of this!", "In this article, I\u2019ll show two different ways of transforming words into numbers: using the TF-IDF transformation and GloVe. While TF-IDF relies on a sparse vector representation, GloVe belongs to the dense vector representations.", "TF-IDF follows a similar logic than the one-hot encoded vectors explained above. However, instead of only counting the occurence of a word in a single document it also does so in relation to the entire corpus. This allows us to detect how important a word is to a document in a corpus.", "What does this mean? Well, common words such as \u201cthe\u201d or \u201ca\u201d will appear very frequently in almost every document. However, other words might appear frequently in only one or two documents and those are the ones that are likely to be more representative of the document they are present in.", "TF-IDF aims to put this into light: weigh down the common words that occur in almost every document (e.g. \u201cthe\u201d or \u201ca\u201d) and give more importance to those that appear only in a few documents.", "In detail, TF IDF is composed of two parts: TF which is the term frequency of a word, i.e. the count of the word occurring in a document and IDF, which is the inverse document frequency, i.e. the weight component that gives higher weight to words occuring in only a few documents.", "Dense vectors fall into two categories: matrix factorization and neural embeddings. GloVe belongs to the latter category, alongside another popular neural method called Word2vec.", "In a few words, GloVe is an unsupervised learning algorithm that puts emphasis on the importance of word-word co-occurences to extract meaning rather than other techniques such as skip-gram or bag of words. The idea behind it is that a certain word generally co-occurs more often with one word than another. The word ice is more likely to occur alongside the word water for instance.", "GloVe is very easy to use in R with the text2vec package.", "Understanding word embeddings is key but grasping how to use them is as essential. The reason why word embeddings are so important goes beyond their application in data science itself but also extends to the realms of politics, communications, marketing or policy-making.", "1/ Finding the degree of similarity between two words", "Once you have transformed words into numbers, you can use similarity measures to find the degree of similarity between words.", "One useful metric is cosine similarity, which measures the cosine of the angle between two vectors. It is important to understand that it measures the orientation and not the magnitude, i.e. vectors that are similar will have a similar vector orientation. In more detail, this means that two vectors with the same orientation will have a cosine similarity of 1, two vectors oriented at 90\u00b0 relative to each other will have a similarity of 0, and two vectors diametrically opposed have a similarity of -1. And this is completely independent of their magnitude.", "Finding the similarity between words can lead to powerful insights. In marketing, this can help marketers understand to which words a specific product is associated with by consumers. As a result, this can help develop more efficient advertising campaigns with targeted search queries. In a similar way, this can also inform communication experts or politicians in the development of audience targeted campaigns.", "2/ Comparing the use of a word across different corpora", "Related to the above, one can also focus on the use of a specific word and use the measure of cosine similarity to understand how this word is used differently across different corpora. For instance, you can compare the use of the word \u201cclimate\u201d across various UN speeches or the context of the word \u201cimmigration\u201d across Republican and Democrat manifestos. From this you can extract party or even policy differences.", "Depending on the text you wish to analyse, the study possibilities given by word embeddings are infenite.", "3/ Analyse amazing things like woman + king \u2014 man =queen", "Word analogies are the true fun part of word embeddings! They allow you to make deductions in the form of: \u201ca is to b\u201d what \u201cx is to y\u201d.", "Here\u2019s an example: king \u2014 man+ woman= queen. In other words, adding the vectors king and woman while substracting man gives us the vector associated with queen. In plain English, this means: a king is to a man what a queen is to a woman.", "As humans, we know that king relates to a male figure while queen refers to a female figure. Word embeddings allow computers to understand this too by taking the difference between the vector representations of king and man. If one tries to project the woman vector through the same direction, one would thus get the word queen. The information that queen is the feminine of king has never been fed directly to the model, but the model is able to capture the relation through word embeddings.", "Another example is: paris \u2014 france +germany = berlin. In other words, Paris is to France what Berlin is to Germany!", "Here again, text2vec is an easy to use package in R to perform these word analogies from the GloVe algorithm with the measure of cosine similarity described above.", "Although word embeddings can be very useful and its application plentiful, their implementation should be considered carefully.", "Unfortunately, language is a powerful means through which race, gender discrimination and stereotypical biases are reproduced. It is therefore crucial to consider the implication that these biases could have on automated tasks which are based on processing human language.", "How and why is this a problem?", "Consider implementing machine learning algorithms based on word embeddings in environments such as job recruitment processes or translation services. In these cases, stereotypes can strike in and actually implify discrimination towards one group. A popular example given in this case is the relation between man:programmer = female, which will yield the result homemaker. The embedding model will see programmer closer with male than female because of our own social perception we have of this job which is reflected in the language we use.", "Another example can be taken from automation in cv scanning. Let\u2019s assume the company decided to train the word embeddings on a large dataset such as Wikipedia for instance. Chances to find positive adjectives like crafty, brilliant and clever in a motivation letter are high, but it was also found that these terms were closer to man than to woman in the pretrained embedding space. This gender bias will therefore be reproduced in the automated task.", "Gender inequality, race discrimination and other stereotypical biases are deeply rooted in our society and our use of the language. As such, the application of machine learning algorithms on this language runs the risk of propagating and amplifying all these biases. Algorithm are therefore never \u2018neutral\u2019 because our language itself is not neutral.", "Therefore: always be careful about the ethical implications of your model!", "Word embeddings appear everywhere in our daily interaction with computers. They can deliver insights that go beyond their mere application in data science, touching upon the realms of marketing, communications, politics and policy-making. Identifying word similarities and analogies are examples of their powerful application. Nonetheless, their application is never neutral and, as with any algorithm, always make sure to consider their ethical implications!", "I regularly write articles about Data Science and Natural Language Processing. Follow me on Twitter or Medium to check out more articles like these or simply to keep updated about the next ones!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist \u01c0 Natural Language Processing & Social Media Analytics \u01c0 Twitter: @celine_vdr"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8acb63892032&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8acb63892032--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8acb63892032--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@celine.vdr?source=post_page-----8acb63892032--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@celine.vdr?source=post_page-----8acb63892032--------------------------------", "anchor_text": "C\u00e9line Van den Rul"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c92afab8fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&user=C%C3%A9line+Van+den+Rul&userId=4c92afab8fb2&source=post_page-4c92afab8fb2----8acb63892032---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://text2vec.org/glove.html", "anchor_text": "text2vec"}, {"url": "https://medium.com/data-social/natural-language-processing-word-embeddings-e0b2edc773d2", "anchor_text": "How Natural Language Processing helps you understand party differences through wordsUsing word embeddings to analyse similarities and differences in the Democrat and Republican parties\u2019 platformsmedium.com"}, {"url": "http://text2vec.org/glove.html", "anchor_text": "text2vec"}, {"url": "https://twitter.com/celine_vdr", "anchor_text": "Twitter"}, {"url": "https://medium.com/@celine.vdr", "anchor_text": "Medium"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8acb63892032---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8acb63892032---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----8acb63892032---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8acb63892032---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----8acb63892032---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&user=C%C3%A9line+Van+den+Rul&userId=4c92afab8fb2&source=-----8acb63892032---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&user=C%C3%A9line+Van+den+Rul&userId=4c92afab8fb2&source=-----8acb63892032---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8acb63892032--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8acb63892032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8acb63892032---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8acb63892032--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8acb63892032--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8acb63892032--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8acb63892032--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8acb63892032--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8acb63892032--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8acb63892032--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8acb63892032--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@celine.vdr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@celine.vdr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "C\u00e9line Van den Rul"}, {"url": "https://medium.com/@celine.vdr/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "489 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c92afab8fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&user=C%C3%A9line+Van+den+Rul&userId=4c92afab8fb2&source=post_page-4c92afab8fb2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc1d4f8ed85d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-word-embeddings-with-tf-idf-and-glove-8acb63892032&newsletterV3=4c92afab8fb2&newsletterV3Id=c1d4f8ed85d5&user=C%C3%A9line+Van+den+Rul&userId=4c92afab8fb2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}