{"url": "https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565", "time": 1683005493.253835, "path": "towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565/", "webpage": {"metadata": {"title": "Six Spark Exercises to Rule Them All | by Andrea Ialenti | Towards Data Science", "h1": "Six Spark Exercises to Rule Them All", "description": "Spark SQL is very easy to use and difficult to master. I crafted six exercises that will resemble some typical situations that Spark developers face\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gist.github.com/aialenti/cfd4e213ebf2ef6e20b195c8fb45382c", "anchor_text": "if you can\u2019t download the data, you can find the generator script clicking here", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c", "anchor_text": "I covered Spark joins in my Medium article, \u201cThe Art of Joining in Spark\u201d, if you want to know more about it you could have a look there!", "paragraph_index": 27}], "all_paragraphs": ["Spark SQL is very easy to use, period. You might already know that it\u2019s also quite difficult to master.", "To be proficient in Spark, one must have three fundamental skills:", "I crafted the following six exercises that will resemble some typical situations that Spark developers face daily when building their pipelines: these will help to assess the skills above.", "You can find the proposed solutions at the end of the article!", "Let\u2019s describe briefly the dataset that we are going to use: it consists of three tables coming from the database of a shop, with products, sales and sellers. Data is available in Parquet files that you can download at the link below. Note that to exploit the exercises 100% you\u2019ll need to read from the files provided! (.zip, ~6GB, if you can\u2019t download the data, you can find the generator script clicking here)", "The following diagram shows how the tables can be connected:", "Each row in this table is an order and every order can contain only one product. Each row stores the following fields:", "Here\u2019s a sample of the table:", "Each row represents a distinct product. The fields are:", "This table contains the list of all the sellers:", "The best way to exploit the exercises below is to download the data and implement a working code that solves the proposed problems, ideally in a distributed environment! I would advise doing so before reading the solutions that are available at the end of the page!", "Tip: I built the dataset to allow working on a single machine: when\u00a0writing\u00a0the\u00a0code, imagine what would happen with a dataset 100 times bigger.", "Even if you\u2019d know how to solve them, my advice is not to skip the warm-up questions! (if you know Spark they\u2019ll take a few seconds).", "If you are going to do the exercise with Python, you\u2018ll need the following packages:", "Let\u2019s dive into the solutions. First, you should have noted that the warm-up questions are handy to solve the exercises:", "The solution to this exercise is quite easy. First, we simply need to count how many rows we have in every dataset:", "As you can see, we have 75,000,000 products in our dataset and 20,000,040 orders: since each order can only have a single product, some of them have never been sold. Let\u2019s find out how many products appear at least once and which is the product contained in more orders:", "The first query is counting how many distinct products we have in the sales table, while the second block is pulling the product_id that has the highest count in the sales table.", "Let\u2019s have a closer look at the second result: 19,000,000 orders out of 20 M are selling the product with product_id = 0: this is a powerful information that we should use later!", "Having some knowledge of Spark this should be straightforward: we simply need to find out \u201chow many distinct products have been sold in each date\u201d:", "Nothing much to say here, the output is the following:", "Let\u2019s work out the hard stuff! The first exercise is simply asking \u201cWhat is the average revenue of the orders?\u201d", "In theory, this is simple: we first need to calculate the revenue for each order and then get the average. Remember that revenue = price * quantity. Petty easy: the product_price is in the products table, while the amount is in the sales table.", "A first approach could be to simply join the two tables, create a new column and do the average:", "The above is correct, and it probably works quite well (especially if you are working on a local environment). But let\u2019s have a look at the execution plan DAG: at some point we will have a repartitioning (on the product_id field) and a join:", "Let\u2019s see what happens when Spark performs the join (on the Spark UI):", "Oops! One task is taking much more time than the others!", "This is a typical case of a skewed join, where one task takes a long time to execute since the join is skewed on a very small number of keys (in this case, product_id = 0). I covered Spark joins in my Medium article, \u201cThe Art of Joining in Spark\u201d, if you want to know more about it you could have a look there!", "Note that this is not a huge problem in case you are running Spark on a local system. On a distributed environment (and with more data), though, this join could take an incredible amount of time to complete (maybe never complete at all!).", "Let\u2019s fix this issue using a technique known as \u201ckey salting\u201d. I won\u2019t describe in detail since I have already covered the topic in the article linked above. As a summary, what we are going to do is the following:", "The important thing to observe here is that we are NOT salting ALL the products, but only those that drive skewness (in the example we are getting the 100 most frequent products). Salting the whole dataset would be problematic since the number of rows would grow linearly on the \u201csalting factor\u201d:", "Looking at the stages when we execute the above:", "The result of the query should be the following", "Using this technique in a local environment could lead to an increase of the execution time; in the real world, though, this trick can make the difference between completing and not completing the join.", "Question number two was: \u201cfor each seller, what is the average % contribution of an order to the sellers\u2019 daily quota?\u201d.", "This is similar to the first exercise: we can join our table with the sellers table, we calculate the percentage of the quota hit thanks to a specific order and we do the average, grouping by the seller_id.", "Again, this could generate a skewed join, since even the sellers are not evenly distributed. In this case, though, the solution is much simpler! Since the sellers table is very small, we can broadcast it, making the operations much much faster!", "\u201cBroadcasting\u201d simply means that a copy of the table is sent to every executor, allowing to \u201clocalize\u201d the task. We need to use this operator carefully: when we broadcast a table, we need to be sure that this will not become too-big-to-broadcast in the future, otherwise we\u2019ll start to have Out Of Memory errors later in time (as the broadcast dataset gets bigger).", "Question: \u201cWho are the second most selling and the least selling persons (sellers) for each product? Who are those for the product with product_id = 0\u201d.", "This sounds like window functions! Let\u2019s analyze the question: for each product, we need the second most selling and the least selling employees (sellers): we are probably going to need two rankings, one to get the second and the other one to get the last in the\u00a0sales\u00a0chart. We also need to handle some edge cases:", "The output for the second part of the question is the following:", "For this final exercise, we simply need to apply a fancy algorithm. We can do that through UDFs (User Defined Functions). A UDF is a custom function that can be invoked on datafarmes columns; as a rule of thumb, we should usually try to avoid UDFs, since Spark is not really capable to optimize them: UDF code usually runs slower than the non-UDF counterpart. Unfortunately, we cannot apply the algorithm described just using Spark SQL functions.", "The solution is something like the following:", "First, we need to define the UDF function: def algo(order_id, bill_text). The algo function receives the order_id and the bill_text as input.", "The UDF function implements the algorithm:", "Afterward, this function needs to be registered in the Spark Session through the line algo_udf = spark.udf.register(\u201calgo\u201d, algo). The first parameter is the name of the function within the Spark context while the second parameter is the actual function that will be executed.", "We apply the UDF at the following line:", "As you can see, the function takes two columns as input and it will be executed for each row (i.e. for each pair of order_id and bill_raw_text).", "In the final dataset, all the hashes should be different, so the query should return an empty dataset", "If you completed all the exercises, congratulations! Those covered some very important topics about Spark SQL development:", "Of course, the exercises above could be solved in many different ways, I\u2019m open to suggestions! I hope you enjoyed! Let me know what you think and, if you want, check out these other articles!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m a Data Scientist, I usually ride a giant unicorn with a rainbow mane. I love learning by explaining. \u201cLike a bicycle I need to move to keep my balance\u201d."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F242445b24565&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----242445b24565--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----242445b24565--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andrea.ialenti?source=post_page-----242445b24565--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.ialenti?source=post_page-----242445b24565--------------------------------", "anchor_text": "Andrea Ialenti"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4f0dc70838c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&user=Andrea+Ialenti&userId=c4f0dc70838c&source=post_page-c4f0dc70838c----242445b24565---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@xangriffin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Xan Griffin"}, {"url": "https://unsplash.com/s/photos/victory?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://gist.github.com/aialenti/cfd4e213ebf2ef6e20b195c8fb45382c", "anchor_text": "if you can\u2019t download the data, you can find the generator script clicking here"}, {"url": "https://drive.google.com/file/d/1kCXnIeoPT6p9kS_ANJ0mmpxlfDwK1yio/view?usp=sharing", "anchor_text": "DatasetToCompleteTheSixSparkExercises.zipDataset containing the three tables to do the six exercisesdrive.google.com"}, {"url": "https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c", "anchor_text": "I covered Spark joins in my Medium article, \u201cThe Art of Joining in Spark\u201d, if you want to know more about it you could have a look there!"}, {"url": "https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c", "anchor_text": "The art of joining in SparkPractical tips to speedup joins in Sparktowardsdatascience.com"}, {"url": "https://towardsdatascience.com/effortless-hyperparameters-tuning-with-apache-spark-20ff93019ef2", "anchor_text": "Effortless Hyperparameters Tuning with Apache SparkHow to run a Random Search on Spark without writing Spark code.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/clustering-pollock-1ec24c9cf447", "anchor_text": "Clustering PollockA cluster analysis on Jackson Pollock\u2019s paintings: how to use k-means to group colorstowardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----242445b24565---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----242445b24565---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/big-data?source=post_page-----242445b24565---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/spark?source=post_page-----242445b24565---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/learning?source=post_page-----242445b24565---------------learning-----------------", "anchor_text": "Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&user=Andrea+Ialenti&userId=c4f0dc70838c&source=-----242445b24565---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&user=Andrea+Ialenti&userId=c4f0dc70838c&source=-----242445b24565---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----242445b24565--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F242445b24565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----242445b24565---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----242445b24565--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----242445b24565--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----242445b24565--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----242445b24565--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----242445b24565--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----242445b24565--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----242445b24565--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----242445b24565--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.ialenti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.ialenti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrea Ialenti"}, {"url": "https://medium.com/@andrea.ialenti/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "543 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4f0dc70838c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&user=Andrea+Ialenti&userId=c4f0dc70838c&source=post_page-c4f0dc70838c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38edc5d7a9a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsix-spark-exercises-to-rule-them-all-242445b24565&newsletterV3=c4f0dc70838c&newsletterV3Id=38edc5d7a9a0&user=Andrea+Ialenti&userId=c4f0dc70838c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}