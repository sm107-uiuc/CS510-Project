{"url": "https://towardsdatascience.com/dimensionality-reduction-forensics-50014430767f", "time": 1683015630.023371, "path": "towardsdatascience.com/dimensionality-reduction-forensics-50014430767f/", "webpage": {"metadata": {"title": "Dimensionality Reduction Forensics | by Tim L\u00f6hr | Towards Data Science", "h1": "Dimensionality Reduction Forensics", "description": "In my last blog post about Gap-Statistics, I explained that most of the time data has a lot of features (hundreds or even sometimes thousands). We consider a dataset with 100 features as being 100\u2026"}, "outgoing_paragraph_urls": [{"url": "https://timloehr.me/", "anchor_text": "website", "paragraph_index": 45}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "paragraph_index": 46}], "all_paragraphs": ["In my last blog post about Gap-Statistics, I explained that most of the time data has a lot of features (hundreds or even sometimes thousands). We consider a dataset with 100 features as being 100 dimensional. So each feature represents one dimension of the dataset. That\u2019s quite a lot of dimensions for visualizing the data. We can\u2019t just cut away all features except for three, can we? Of course not! One exception could be that 98 out of 100 features are highly correlated (find out with Heatmaps), so we omit 97 and take only a single one of them. Then we would end up with a three-dimensional dataset. But this is highly unlikely!", "For that reason I will cover two important topics you need to know when working with data:", "It is intuitive but important to know, that the complexity of all pattern analysis or data exploration tasks rises with a higher dimensionality.", "We call this increase in complexity curse of dimensionality. Oftentimes some of the features are completely useless or highly correlated. Therefore a dimensionality reduction technique can be applied to get rid of as many dimensions as possible. We really want to work with data that is as low in dimensionality as possible.", "We can deduce three issues when working with high dimensional data:", "How can we plot data which has more than three dimensions?", "Yes, we can plot even in the 4D or 5D space, but it escalates quickly.", "Technically speaking there exist some methods to do that. For example, we could change the size of data points to visualize the fourth dimension, or we could additionally introduce some colors as the even fifth dimension. But you see how quickly that escalates and we as humans can not read anything informative out of this 5D plot anymore. So it is mostly useless, because one main point is that we want to gain information out of these visualizations.", "The subdivision of the feature space rises exponentially with an increase in dimensionality.", "In the right-most figure of the picture above, we see that with three dimensions we already have 27 equally sized cells. For 15 features (=15 dimensions) we would end up with already 15 million equally sized cells (3^15) in the feature space. A Random Forest needs a tremendous amount of parameters to fit this high dimensional feature space. If you know how the Random Forest works, then you know that it cuts the feature space into different classification areas, if it\u2019s the Random Forest Classifier and not the Random Forest Regressor. But other classification methods also require infeasibly more parameters to fit such a high dimensional space.", "This is maybe the most important, but also the most surprising issue:", "The higher the dimension is, the more data points lie on the very outer boundary of the feature space.", "What we can see from Figure 2 above is the formula for calculating where the data lies in the feature space. D is the number of dimensions and \u03b5 denotes the limit of the boundary shell. If you choose \u03b5 to be 0.01, it means how many data points lie on the outer 1% of the feature space. \u03b5 = 0.75 would mean how many data points lie in the outer 75% of the feature space. The green circle above represents the feature space, where \u03b5 shows that it always starts from the edge towards the circle's core.", "So, pointing back to Figure 1 we can see, that for the dimensionality of 200, we have 86% of the data points lie on the outer 1% shell of the feature space. Whereas in a 3D space, 3% of the data lies on the outer 1% shell.", "The sparsity of the feature space increases dramatically with higher dimensions. We would need a huge amount of new data samples to maintain a constant density of the feature space. But unless you use some very fancy generative models like GAN\u2019s, you can\u2019t really generate new data samples that easily.", "We will cover four dimensionality reduction techniques in this section. I give a brief overview of each of them, but just enough in order to understand how to perform forensics. Not every dimensionality technique is suitable for every data distribution. Great care needs to be taken when choosing one of the four methods, because you will be surprised that it did not work as expected if you chose the wrong technique. We need to distinguish between global and local dimensionality reduction techniques. These are the four techniques we will cover:", "There are way more methods out there, but let\u2019s focus on the ones I think are basically the major ones. The state-of-the-art t-SNE is not included, because it would break the limits of this post.", "The Principal Component Analysis (PCA) is always the first choice when you don\u2019t know which technique you need to use. It is the workhorse for many data scientists and it yields often a proper result, yet there are cases in which it fails. I mentioned that PCA is a global technique. This means that PCA is solely computed by matrix multiplication and is the data is then projected onto the new axis.", "Important: Every axis to project on is orthogonal to each other. So you can image the basic cartesian coordinate system from Figure 1 as the projection axises.", "These four steps are nothing extraordinary. It is actually just a few lines of Python code:", "You can see that global dimensionality reduction techniques have another very useful trait, which local techniques don't offer. The global methods reveal how much variance the data remains after applying dimensionality reduction. The iris dataset is a well-known dataset for trying out dimensionality reduction. It has four dimensions, but keeping only one dimension still remains 92.46% of the total variance of the data (as can be seen from the grey output box above \u201c92.46+0.00j\u201d -> this is a complex number representation). This means that 92.46% of the information is still kept within only one dimension.", "The left plot shows another example of the PCA. Notice how every red data point is projected onto the new blue axis. This would remain 96.62% of all the location information of the red dots.", "I used a dataset with two distinct clusters. After reducing the dimension from 2D to 1D, the clusters remain apart from each other, even after projecting them onto the blue arrow.", "Conclusion: We can only project data linearly on an eigenvector axis with the PCA. Keep this in mind. Can you already imagine a 2D data distribution where it could be difficult to map linearly onto and still preserve most of the information?", "The classical Multidimensional Scaling (MDS) will be even briefer, because it is very similar to the PCA. The only difference between MDS and PCA is, that MDS takes the distances between points as inputs. If the euclidian distance norm is used for MDS, it behaves 100% similar to PCA. MDS is therefore also able to measure how much variance is preserved. The formula for preservation is:", "Notice that d\u2019 (d prime) is the reduced dimension (for example 1D) and d is the initial dimension (for example 3D). d* (d star) is the optimum reduced dimension. We can sum up all of the target dimensions variances and divide it by mostly 100, since it starts with 100% variance.", "When you read my last blog post about Gap-Statistics, you notice how similar this figure above is to it. The elbow method can be used to visualize how the variance shrinks with decreasing dimensionality. The x-axis means the reduction of dimensionality. So for example at x=2, the initial dimension is reduced by two dimensions and we still preserve around 80% of the total variance. The biggest gap indicates that our best reduction result could be at reducing only two dimensions, because we lose so much variance by reducing the initial data by three dimensions.", "When using different distance metrics for the MDS", "it is very hard to know what the best metric is. When you are sure that a global dimensionality reduction technique is sufficient for your task, you could try many different distance metrics and plot the elbow method plot to see with which minimum dimension you preserve the most variance.", "Conclusion: Since MDS is the same as PCA when using the euclidian distance, you can directly choose MDS over PCA. Then you are quickly able to use different testing distance metrics.", "Let\u2019s have a look at the data distribution from Figure 7 above. It looks like the shell of a snail. No matter how we project the data onto the green principal component line (or MDS line), we will never be able to keep much of the variance. The right part of the Figure above shows how the data would be ideally projected onto the green line. This brings us not further at all.", "This same applied for a very high dimensional space. We can not really imagine how a 100D feature space would look, but the same principle applies. Some distribution can not be explained with a linear dimensionality reduction technique.", "Therefore, when we see that the reduced dimension reveals zero insights into potential clusters of the data, like above, we can be sure that PCA or MDS is not the right choice.", "Let\u2019s start with local dimensionality reduction. I will give you the algorithm and then I will explain it:", "Ah our beloved MDS. So useful! The kNN needs a fixed hyperparameter, namely the number of K. Problematic about that is, that we can\u2019t really know what the optimum value for K is. Therefore it could be possible that the local neighborhood includes points that aren\u2019t really part of it, due to the radius of the kNN growing too big, because there are too sparse data points. Applying Dijkstras\u2019s algorithm and using this distance matrix as input for the MDS is straightforward, but the devil lies in the first of those four steps.", "Let\u2019s consider the egg-like data distribution with an open bottom above on the left most of Figure 8. The best dimensionality reduction mapping would be if we could project the data along the green line. The local structure of the data distribution will be preserved. The purple circles indicate an example of two local neighborhoods.", "You can imagine it like this:", "This is the ideal case. It will totally work if the data distribution looks something like this. But I already touched on the problematic behavior of ISOMAP. If you have a close look at the right-most plot on Figur 8, you can see that there are two new blue data points. Those could be there due to noisy samples. This leads to a fatal change of the projection onto the green line.", "Conclusion: ISOMAP is very sensitive to noisy samples due to the kNN. Since kNN varies in its radius for every local neighborhood, because it wants e.g. to find 5 points in its local neighborhood, no matter how far or close the points lie. This yields problems and acts as a bridge from point A to point B in Figure 8, even though those points are supposed to be very far apart from each other.", "I will also begin with writing the basic algorithm for the LE and then I am going to explain it:", "The adjacency graph is basically just the matrix (affinity matrix) which represents the location of all neighborhood points in the graph, but within one matrix. This affinity matrix looks something like this (it is binary, but it can also be computed with other kernels than the binary heat kernel):", "W represents the binary affinity matrix. The upper W is a data distribution without noise, whereas the lower W includes some green noisy sample points. You can see that the noisy points want to create a bridge from A to B again, but the affinity matrix prevents this from happening, because the purple local neighborhood bond is too strong in the diagonal. The noise has no effect on the Laplacian Eigenmaps. The optimization aims to keep the local neighborhoods as close to together as possible (so the distance as small as possible in regards to the neighborhood).", "Therefore can have a minimization problem for the Laplacian Eigenmaps, but a maximization problem for the PCA, MDS and the ISOMAP (these three want to maximize the variance ~information of the data).", "Conclusion: The LE are intrinsically better suited to perform this kind of manifold learning (=learning a lower-dimensional embedding), because it is more robust with respect to noisy data distributions. As we know, there is often noise in the data. The LE is in all cases superior to the ISOMAP.", "Finally, let us consider some test examples:", "I hope you find this post informative! You can write me an Email if you have questions. You can find my email address on my website.", "[1] Trevor Hastie, Robert Tibshirani and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2009)", "This blog post is based from knowledge gained through the course Pattern Analysis from the Friedrich Alexander University Erlangen-N\u00fcrnberg. I used some parts of the lecture from Dr. Christian Riess to illustrate the examples for this post. So all rights go to Dr. Christian Riess."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F50014430767f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://mavengence.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Tim L\u00f6hr"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9295fa5a2499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=post_page-9295fa5a2499----50014430767f---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50014430767f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----50014430767f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50014430767f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=-----50014430767f---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://pixabay.com/de/illustrations/tatort-band-szene-kriminalit%C3%A4t-999123/", "anchor_text": "KERBSTONE"}, {"url": "https://timloehr.me/", "anchor_text": "website"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----50014430767f---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----50014430767f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/manifold-learning?source=post_page-----50014430767f---------------manifold_learning-----------------", "anchor_text": "Manifold Learning"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----50014430767f---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----50014430767f---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50014430767f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----50014430767f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50014430767f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----50014430767f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50014430767f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9295fa5a2499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=post_page-9295fa5a2499----50014430767f---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F9295fa5a2499%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----50014430767f---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Written by Tim L\u00f6hr"}, {"url": "https://mavengence.medium.com/followers?source=post_page-----50014430767f--------------------------------", "anchor_text": "26 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/tim-l%C3%B6hr-821ba8188/", "anchor_text": "https://www.linkedin.com/in/tim-l%C3%B6hr-821ba8188/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9295fa5a2499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=post_page-9295fa5a2499----50014430767f---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F9295fa5a2499%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-forensics-50014430767f&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----50014430767f---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Tim L\u00f6hr"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "K-Means Clustering and the Gap-StatisticsClosing knowledge Gaps with Gap-Statistics"}, {"url": "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "10 min read\u00b7Oct 22, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c5d414acd29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-and-the-gap-statistics-4c5d414acd29&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----4c5d414acd29----0-----------------clap_footer----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=author_recirc-----50014430767f----0---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c5d414acd29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-and-the-gap-statistics-4c5d414acd29&source=-----50014430767f----0-----------------bookmark_preview----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50014430767f----1---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----50014430767f----1-----------------bookmark_preview----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50014430767f----2---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----50014430767f----2-----------------bookmark_preview----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/biometric-signal-verification-of-handwriting-with-hmms-8808cbd05699?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Tim L\u00f6hr"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/biometric-signal-verification-of-handwriting-with-hmms-8808cbd05699?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "Biometric Signal Verification of Handwriting With HMM\u2019sComplete implementation in Python"}, {"url": "https://towardsdatascience.com/biometric-signal-verification-of-handwriting-with-hmms-8808cbd05699?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": "10 min read\u00b7Oct 14, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8808cbd05699&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbiometric-signal-verification-of-handwriting-with-hmms-8808cbd05699&user=Tim+L%C3%B6hr&userId=9295fa5a2499&source=-----8808cbd05699----3-----------------clap_footer----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/biometric-signal-verification-of-handwriting-with-hmms-8808cbd05699?source=author_recirc-----50014430767f----3---------------------8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8808cbd05699&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbiometric-signal-verification-of-handwriting-with-hmms-8808cbd05699&source=-----50014430767f----3-----------------bookmark_preview----8eeabb96_d960_453f_a43d_0b10d4d3ce5c-------", "anchor_text": ""}, {"url": "https://mavengence.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "See all from Tim L\u00f6hr"}, {"url": "https://towardsdatascience.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Carla Martins"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "How to Compare and Evaluate Unsupervised Clustering Methods?Using Python, Scikit-Learn, and Google Colab"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b720 min read\u00b7Feb 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&user=Carla+Martins&userId=a1022761a1b&source=-----84f3617e3769----0-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&source=-----50014430767f----0-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/everything-about-linear-discriminant-analysis-lda-c22adc8f5ea0?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://ithinkbot.com/?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://ithinkbot.com/?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Dr. Mandar Karhade, MD. PhD."}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/everything-about-linear-discriminant-analysis-lda-c22adc8f5ea0?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Everything about Linear Discriminant Analysis (LDA)All you need to know to conduct Linear Discriminant Analysis."}, {"url": "https://medium.com/geekculture/everything-about-linear-discriminant-analysis-lda-c22adc8f5ea0?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b76 min read\u00b7Dec 10, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Fc22adc8f5ea0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Feverything-about-linear-discriminant-analysis-lda-c22adc8f5ea0&user=Dr.+Mandar+Karhade%2C+MD.+PhD.&userId=aa38fec37f51&source=-----c22adc8f5ea0----1-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/everything-about-linear-discriminant-analysis-lda-c22adc8f5ea0?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc22adc8f5ea0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Feverything-about-linear-discriminant-analysis-lda-c22adc8f5ea0&source=-----50014430767f----1-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Dimensionality Reduction for Linearly Inseparable DataNon-linear dimensionality reduction using kernel PCA"}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b77 min read\u00b7Dec 20, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5030f0dc0f5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----5030f0dc0f5e----0-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e?source=read_next_recirc-----50014430767f----0---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5030f0dc0f5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e&source=-----50014430767f----0-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Thomas A Dorfer"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Density-Based Clustering: DBSCAN vs. HDBSCANWhich algorithm to choose for your data"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b75 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&user=Thomas+A+Dorfer&userId=7c54f9b62b90&source=-----39e02af990c7----1-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----50014430767f----1---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&source=-----50014430767f----1-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Kay Jan Wong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "7 Evaluation Metrics for Clustering AlgorithmsIn-depth explanation with Python examples of unsupervised learning evaluation metrics"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b710 min read\u00b7Dec 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&user=Kay+Jan+Wong&userId=fee8693930fb&source=-----bdc537ff54d2----2-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----50014430767f----2---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&source=-----50014430767f----2-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "3 Easy Steps to Perform Dimensionality Reduction Using Principal Component Analysis (PCA)Running the PCA algorithm twice is the most effective way of performing PCA"}, {"url": "https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": "\u00b711 min read\u00b7Jan 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F79121998b991&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2F3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----79121998b991----3-----------------clap_footer----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991?source=read_next_recirc-----50014430767f----3---------------------6fddb0b6_24bb_4476_8c08_7d2e4856e379-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79121998b991&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2F3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991&source=-----50014430767f----3-----------------bookmark_preview----6fddb0b6_24bb_4476_8c08_7d2e4856e379-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----50014430767f--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----50014430767f--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----50014430767f--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----50014430767f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----50014430767f--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----50014430767f--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----50014430767f--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}