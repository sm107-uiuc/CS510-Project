{"url": "https://towardsdatascience.com/exploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00", "time": 1683000244.1200588, "path": "towardsdatascience.com/exploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00/", "webpage": {"metadata": {"title": "Recommender Systems: Exploiting Multi-Categorical Features Using \u201cDeep Interest\u201d | by Marina Gandlin | Towards Data Science", "h1": "Recommender Systems: Exploiting Multi-Categorical Features Using \u201cDeep Interest\u201d", "description": "At Taboola, our goal is to predict whether users will click on the ads we present to them. Our models use all kinds of features, yet the most important ones tend to be related to the users\u2019 history\u2026"}, "outgoing_paragraph_urls": [{"url": "https://engineering.taboola.com/exploiting-multi-categorical-features-using-deep-interest/", "anchor_text": "Taboola Engineering Blog", "paragraph_index": 0}, {"url": "https://sigir-ecom.github.io/ecom18Papers/paper14.pdf", "anchor_text": "Alibaba research", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1706.06978", "anchor_text": "Deep Interest", "paragraph_index": 12}], "all_paragraphs": ["Originally published at Taboola Engineering Blog on September 4, 2019.", "At Taboola, our goal is to predict whether users will click on the ads we present to them. Our models use all kinds of features, yet the most important ones tend to be related to the users\u2019 history. Understanding how to use these features well can have a huge impact on the model\u2019s personalization capabilities, due to the user-specific knowledge they hold.", "User history features vary strongly between different users; for example, one popular feature is user categories \u2014 the topics a user had previously read. An example for such a list might look like this \u2014 {\u201csports\u201d, \u201cbusiness\u201d, \u201cnews\u201d}. Each value in these lists is categorical and they have multiple entries, so we name them Multi-Categorical features. Multi-Categorical lists can have any number of values per user \u2014 which means our model must handle both very long lists and completely empty lists (for new users).", "Supplying inputs of unknown length to machine learning models is an issue which needs to be addressed wisely. This post will walk you through how to integrate these varied length features in your neural network, from the most basic approach all the way to \u201cDeep Interest\u201d, the current state of the art.", "Let\u2019s continue with user categories. How can we turn a list of unknown length to a fixed-length feature which we can use? The most naive, yet very popular approach, is to simply average. Let\u2019s assume we have a separate embedding of length d for each category \u2014 so that each value of the list is mapped to a vector. We can use average pooling \u2014 average all of the user\u2019s categories and receive a vector of length d every time. More formally, if is the embedding of the jth word of overall N words, then a user\u2019s average pooling can be written as:", "But this is far from perfect. Averaging might work well, but when the list is very long the 1/N factor creates a small final result; summing different pointing vectors may cause them to cancel each other out, and division by N makes this substantially worse (If you come from a physics/EE background this should ring a bell, since this is much like what happens in non-coherent integration). This is bad since it hurts users with longer history vectors, which are the ones we have the most information about and can give better predictions for. Because of this, we (and the folks from Alibaba research) remove the 1/N factor and use sum-pooling on the user-histories, to ensure predictable customers don\u2019t have a negligible impact on the network.", "Averaging automatically assigns the same weight to all elements. Is this what we want? Let\u2019s consider a user with this list: {\u2018football\u2019, \u2018tennis\u2019, \u2018basketball\u2019, \u2018fashion\u2019}. Averaging them naively will put an emphasis on the sporting categories, and neglect \u201cfashion\u201d. For some, this would be accurate \u2014 people who are more interested in sports than in fashion; for others, it might not. What could we do to give the different categories different weights to better depict users?", "A better solution will be to use some other information, for example: the number of past views (impressions) the user had for each category. Since the number of times a user read an article correlates with his personal taste, we can use these counts as weights and do weighted average pooling. This will help us differentiate, for example, between the following two very different users:", "Can we stack even more information? Say we not only have the number of each user\u2019s views per category, but also things like the last time the user read an article of this category, or this category CTR (click-through-rate) per user (meaning, how many times the user read an article from this category out of all the times this category was presented). What would be the best way to combine these features without endless algorithm A/B testing? The answer, of course, is to harness the power of machine learning, and apply an additional layer whose job is to output a weight for each value. This is sometimes called an \u201cattention layer\u201d. This layer has all the relevant data available as input, and learns the ideal weights to average the values. Let\u2019s look at an attention subnet example for a single category value \u2014 \u201ctennis\u201d. Assuming the embedding for \u201ctennis\u201d is {5,0.3,5,1,4.2}, and the user was recommended 10 tennis articles but only clicked once, the flow can be illustrated as:", "The attention layer calculates the weight for each category embedding. In the above example, the calculated weight is 0.2. Say this user was also interested in \u201cfashion\u201d and the attention layer output the 0.5 for this category, the final calculation for this user\u2019s history embedding would be 0.2* emb(tennis) + 0.5* emb(fashion). These attention layers learn how to utilize all the information we have about this user\u2019s history for calculating weights for each category.", "Great! But can we make this even better?", "Remember, we are trying to predict whether a certain user might click on a certain ad, so why not plug-in some information about the current suggested ad? Say we have a user whose history is {\u201ctennis-equipment\u201d, \u201ckids-clothes\u201d}, and our current ad is for a tennis racket. Surely we would want more weight on the tennis-equipment and neglect the kids part for this specific case. Our previous attention layer is only designed for modeling the user\u2019s interests and does not consider interactions between the user\u2019s personal taste with the ad suggested to him during the pooling stage. We would like to intertwine this user-ad interaction before the pooling occurs in order to calculate a weight which is more specific to the current ad presented to the user.", "For this purpose, we can use the Deep Interest architecture, a work that was presented at the KDD2018 conference by a group from the Alibaba research. If we go back to our previous illustration, we can now add an embedding for the specific ad suggested to the user:", "With Deep Interest, the ad features are used as inputs for the neural network twice: once as an input for the model itself (contributing information for whether or not presenting this ad to the user will lead to a click), and a second time as an input for the user\u2019s history embedding layer (tuning the user\u2019s history attention to the parts relevant for this specific ad).", "We can also have different ways to create embeddings for the ad-features: one is to use the same embedding vector for both cases mentioned above, and another option is to generate two different embeddings for the two cases. The first option gives the embeddings twice as many gradients while training and might converge better. In the latter, we can have two different embeddings (say embedding of size 12 for the full network, and embedding of size 3 for the user history weight layer). A shorter embedding could save run-time serving but adds more parameters to the learning process and might lengthen training time. Choosing between the two options is, of course, dependent on the specific use case.", "Multi-categorical features are crucial for predictions in various common problems, and particularly when trying to predict future user behavior (like clicks, conversions) from past interactions. There are simple ways to do so \u2014 but for better predictions, we recommend the Deep Interest architecture, as it allows dynamic weighting of the feature vector according to the relevant context. This architecture is quite simple to implement and gives great results for many recommender systems. Go give it a shot!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data science team lead at Taboola. Used to design satellites, but thinks recommendation systems are way cooler."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff7058ea6ed00&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marinagandlin?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Marina Gandlin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b56ffc1598f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&user=Marina+Gandlin&userId=2b56ffc1598f&source=post_page-2b56ffc1598f----f7058ea6ed00---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@civilizen?utm_source=medium&utm_medium=referral", "anchor_text": "Irina Nalbandian"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://engineering.taboola.com/exploiting-multi-categorical-features-using-deep-interest/", "anchor_text": "Taboola Engineering Blog"}, {"url": "https://sigir-ecom.github.io/ecom18Papers/paper14.pdf", "anchor_text": "Alibaba research"}, {"url": "https://arxiv.org/abs/1706.06978", "anchor_text": "Deep Interest"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f7058ea6ed00---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f7058ea6ed00---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/personalization?source=post_page-----f7058ea6ed00---------------personalization-----------------", "anchor_text": "Personalization"}, {"url": "https://medium.com/tag/deep-interest?source=post_page-----f7058ea6ed00---------------deep_interest-----------------", "anchor_text": "Deep Interest"}, {"url": "https://medium.com/tag/recommender-systems?source=post_page-----f7058ea6ed00---------------recommender_systems-----------------", "anchor_text": "Recommender Systems"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&user=Marina+Gandlin&userId=2b56ffc1598f&source=-----f7058ea6ed00---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&user=Marina+Gandlin&userId=2b56ffc1598f&source=-----f7058ea6ed00---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff7058ea6ed00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f7058ea6ed00---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f7058ea6ed00--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marina Gandlin"}, {"url": "https://medium.com/@marinagandlin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "39 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b56ffc1598f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&user=Marina+Gandlin&userId=2b56ffc1598f&source=post_page-2b56ffc1598f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff5e5f76f2b30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploiting-multi-categorical-features-using-deep-interest-f7058ea6ed00&newsletterV3=2b56ffc1598f&newsletterV3Id=f5e5f76f2b30&user=Marina+Gandlin&userId=2b56ffc1598f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}