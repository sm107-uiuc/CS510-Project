{"url": "https://towardsdatascience.com/infinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf", "time": 1683017725.4670248, "path": "towardsdatascience.com/infinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf/", "webpage": {"metadata": {"title": "Infinitely Wide Neural-Networks | Neural Tangents Explained | by Richard Michael | Towards Data Science", "h1": "Infinitely Wide Neural-Networks | Neural Tangents Explained", "description": "One notorious problem with deep learning and deep neural networks (DNNs) is, that they can become black boxes. Lets say, that we have fitted a network with good test performance on a given\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.gaussianprocess.org/gpml/chapters/RW.pdf", "anchor_text": "Gaussian Processes", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1806.07572.pdf", "anchor_text": "4", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1806.07572.pdf", "anchor_text": "4", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Kernel_method", "anchor_text": "kernel", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space", "anchor_text": "RKHS", "paragraph_index": 4}, {"url": "https://github.com/google/jax", "anchor_text": "JAX", "paragraph_index": 13}, {"url": "https://github.com/google/neural-tangents", "anchor_text": "Neural Tangents library", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression", "anchor_text": "sklearn", "paragraph_index": 14}, {"url": "https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.relu.html", "anchor_text": "relu", "paragraph_index": 18}, {"url": "https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb", "anchor_text": "Google\u2019s Colab Cookbook.", "paragraph_index": 21}, {"url": "https://neural-tangents.readthedocs.io/en/latest/neural_tangents.predict.html", "anchor_text": "solving in closed form for gradient descent.", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "mean-squared error", "paragraph_index": 27}, {"url": "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html", "anchor_text": "jit-compilation feature", "paragraph_index": 27}, {"url": "http://M.Sc", "anchor_text": "M.Sc", "paragraph_index": 38}, {"url": "http://laplaceml.com/blog", "anchor_text": "http://laplaceml.com/blog", "paragraph_index": 38}], "all_paragraphs": ["One notorious problem with deep learning and deep neural networks (DNNs) is, that they can become black boxes. Lets say, that we have fitted a network with good test performance on a given classification problem. However we are now stuck. We cannot make sense of the final weights that have been learned or adequately visualize the problem space!Another issue arises in the real world. In practical applications of applying neural networks we often fall back to train ensembles of networks. We use the averaged output of many models. This can be more powerful than the output of one single network. When we have multiple models we can also better analyze overall performance over the given problem.Neural Tangents aims to solve these challenges elegantly.In this post we will explore the background and application of neural networks of infinite width. We take a look at Neural Tangents as a framework and high level API, which allows us to build such networks efficiently. We will use Neural tangents on a simple regression problem and train both a finite and infinite network using JAX. From this we can gain more insights from our model and can provide better analytics. We can use both gradient descent and performing inference, when building networks with Neural Tangents and we will exploit both features when looking at our model.Firstly, how do we get to infinity and back?", "Generally speaking, we build a neural network from assumptions about the underlying problem-space ; for example proximity and values of pixels, to detect edges and coherent elements in pictures. Usually a network\u2019s topology fits to solve a class of problems.We then initialize the network weights. This can be done from a Gaussian distribution. We train the model by adapting the model-parameters in order to minimize a loss function for the problem that we have previously defined. After the training is done we can test the model, with a selection of metrics on an unseen test-set of the data.Now we want to bring some light into the dark: We would like easier to explain analytical properties. One essential question is, why (deep) neural networks generalize so well even though they tend to be overparameterized [4].", "It turns out when we increase the width of a regular DNN we can gain some nicer insights and overall better understanding. Very, very, (very) wide Networks show convergence to Gaussian Processes [9]. It had been established for some time, that this is true for single layer neural networks (Ch. 4 in [8]). It has recently been shown that it can also be applicable for deeper networks [9].What do we get from transforming a Neural Network into a Gaussian Process?This allows for better analysis and uncertainties over our estimates. In addition to the previous NN metrics we can now perform Bayesian inference, assess uncertainties over prior and posterior, and analyze them.One essential cornerstone is, that a Gaussian Process is associated with a kernel for its covariance function. These kernels allow for better analytical properties. The mathematics behind kernel functions is in parts better understood and more developed than when working with regular neural networks.Figure 1 gives a graphical overview of what we are doing. We start with a regular DNN of finite width and take the width of the hidden layer to its limits.", "What connects the neural network with the fabled Gaussian Processes?One essential assumption is, that at initialization (given infinite width) a neural network is equivalent to a Gaussian Process [4]. The evolution that occurs when training the network can then be described by a kernel as has been shown by researchers at the Ecole Polytechnique Federale de Lausanne [4] .", "For the sake of a simple introduction: we say that a kernel is a function that relates two points from your data-set (X \u00d7 X) to each other. For this fact to work, you need a measurable space that has some underlying properties (- looking at you RKHS !).How your pairs of points relate to each other is dependent on a kernel function.We can use a kernel on our dataset to introduce relations between the data-points, measure similarities, and other properties.This unnecessarily short paragraph does not do the field and exciting topic of kernel methods justice and I recommend checking out more insightful resources on the subject [5][6].Now we already have a network, with its architecture, activations and also pre-activations (like convolutions, pooling \u2014 you name it) we denote that as z.", "The NNGP Kernel is simply defined as:", "In layman\u2019s terms, the last kernel describes the change in the network parameters up until z.", "Roughly speaking, there is a direct correspondence between tensor operations and kernel operations. We can therefore express relationships that come from a neural network in terms of kernel methods.That means, for each (tensor) operation in the neural network we can find an equivalent kernel operation.", "There exists a pointwise translational rule, given our two kernels, such that:", "and from this we can transform dense layers through our both kernels as:", "For an convolutional neural network that could translate into:", "And so on, all the way through the network.", "Above the curly A is a dedicated operation which describes the summation over the convolutional filter. (You can find a complete exemplary overview in Fig. 4 and Table 1 in [3].)And T is defined as:", "Now that we are familiar with some of the underlying concepts, we are going to use JAX and the Neural Tangents library. We build an exemplary network and evaluate it in detail.", "In order to get going, lets create an arbitrary regression problem from scratch, using sklearn. We create a dataset with two features x_1, x_2 and the function value f, like this:", "Sidenote: Above is only a small number of samples. You are encouraged to try out a bigger problem. Using Google Colab I ran into memory issues when using larger samples (n >750).", "From this we get data, that should look something like this:", "We split our data-set in training and testing for later processing. Be aware that we need an extra axis for our y-values for later computation and that we take a test set of 10% of our total data-set size.", "Now we create a simple hidden layer neural network, using JAX. Very simple in fact, since the hidden layer barely holds any neurons (see stax.Dense(5)) together with a relu activation function.", "Neural tangents returns three objects from this. The kernel_fn part will serve for our analysis over infinity. It is the kernel representation of the architecture that we encountered previously. The init_fn and apply_fn correspond to networks of finite width.", "The first interesting feature that neural tangents allows are insights from the kernels.We compute the kernels for our test-data as such:", "This gives us two things.:One is the performance of Bayesian inference represented through the NNGP kernel at infinite time-steps. We achieve this by looking at our NNGP kernel.Secondly, the NTK kernel corresponds to how you network would behave after having been trained for an infinite amount of time.In theory we can sample from the prior distribution. However for this specific problem it is not very informative, so we\u2019ll skip this step. You can find a nice introduction to how prior samples look in Google\u2019s Colab Cookbook.", "After this first insight, we perform inference:", "Where we solve for infinite runtime training over the network, given the kernel that we have constructed. We do this by utilizing the NTK kernel in predict_fn(x_test=X_test, get='ntk', ...This is called: solving in closed form for gradient descent.From this we can compute a mean and a covariance matrix. The variables above are ntk_mean and ntk_std . Note that for this specific problem the standard deviation is very small. Though we can still visualize how the means perform:", "Inference can be done the Bayesian way as well. We just have to substitute ntk with nngp. For the performance of NNGP see the attachment, later in this post.", "What we can do with Neural Tangents is compute the loss of the network over timesteps for both testing and training.The loss is defined as the mean squared error or in Python: 1/n * np.mean(ys**2 - 2*mean*ys + var + mean**2, axis=1)also we provide the time-steps of interest from 0 to 1000 in steps of 0.1 ( np.arange(0, 10 ** 3, 10 ** -1)):", "To have a finite comparison point we have to perform actual training of our network. As hyperparameters we set the learning rate to 0.1 and run training for 10000 steps. We use JAX\u2019s stochastic gradient descent for our optimization.:", "We also need to compute loss and gradient loss in the process of training. As a loss function we use the mean-squared error. Note that we also use the JAX specific jit-compilation feature to make loss and grad-loss more performant in the process.", "From the last figure, we can see that the finite network performs better after 10 steps. Though please keep in mind that this is a very simple exercise. For more complex tasks [3] shows that Neural Tangents performs at least as well as finite networks.", "Working with covariances and kernels in a large problem domain can be very expensive. Some of the key challenges are: (1) Computing covariances when (a) inverting matrices, (b) constructing them, ( c) updates to them and (2) computing kernels (a) for multiple kernels (b) across all the data.", "Neural Tangents offers solutions to make these problems more feasible:", "This all combined makes computational intense tasks more feasible on standard hardware.", "We have seen what benefits can come from using infinitely wide neural nets:", "Taking NNs to the their limits, ties them well together with Gaussian Processes; a classical machine learning approach. This allows for performing inference, observe how the data-set correlates through the given kernel(s). We have also seen, how Neural Tangents makes this accessible and can give insights and improvements to conventional network structures when building artificial neural networks.Neural Tangents is an exciting module to allow better insights into deep neural network structures and for more analytical work on the outputs.Neural Tangents is a high level API that takes a way a lot of the heavy lifting and problems that arise when one takes his network to the limits \u2014 with Python and JAX it is even more accessible.", "The makers of Neural Tangents have already announced that they are looking into more network structures for future work to come.", "For the math and computer science enthusiast I highly recommend checking out the reference publications.", "The mentioned code takes a lot of elements from [2].", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Data Scientist and M.Sc. student in Bioinformatics at the University of Copenhagen. You can find more content on my weekly blog http://laplaceml.com/blog"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd6c6d896fcbf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rcml.medium.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": ""}, {"url": "https://rcml.medium.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Richard Michael"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7229c4f1db59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&user=Richard+Michael&userId=7229c4f1db59&source=post_page-7229c4f1db59----d6c6d896fcbf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.nasa.gov/image-feature/goddard/2020/hubble-views-edge-of-stellar-blast", "anchor_text": "ESA/Hubble & NASA,"}, {"url": "http://www.gaussianprocess.org/gpml/chapters/RW.pdf", "anchor_text": "Gaussian Processes"}, {"url": "https://arxiv.org/pdf/1806.07572.pdf", "anchor_text": "4"}, {"url": "https://arxiv.org/pdf/1806.07572.pdf", "anchor_text": "4"}, {"url": "https://en.wikipedia.org/wiki/Kernel_method", "anchor_text": "kernel"}, {"url": "https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space", "anchor_text": "RKHS"}, {"url": "https://github.com/google/jax", "anchor_text": "JAX"}, {"url": "https://github.com/google/neural-tangents", "anchor_text": "Neural Tangents library"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression", "anchor_text": "sklearn"}, {"url": "https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.relu.html", "anchor_text": "relu"}, {"url": "https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb", "anchor_text": "Google\u2019s Colab Cookbook."}, {"url": "https://neural-tangents.readthedocs.io/en/latest/neural_tangents.predict.html", "anchor_text": "solving in closed form for gradient descent."}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "mean-squared error"}, {"url": "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html", "anchor_text": "jit-compilation feature"}, {"url": "https://colab.research.google.com/drive/1s2QdQyS9YndXpUoG-0-EtDQdXbFYyxT9?usp=sharing", "anchor_text": "NeuralTangentscolab.research.google.com"}, {"url": "http://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html", "anchor_text": "Fast and Easy Infinitely Wide Networks with Neural Tangents"}, {"url": "https://ai.googleblog.com/", "anchor_text": "Google AI Blog"}, {"url": "https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb#scrollTo=gUwuP34vPexj", "anchor_text": "Neural Tangents Cookbook in Colab"}, {"url": "https://arxiv.org/pdf/1912.02803.pdf", "anchor_text": "NEURAL TANGENTS FAST AND EASY INFINITE NEURAL NETWORKS IN PYTHON"}, {"url": "https://arxiv.org/pdf/1806.07572.pdf", "anchor_text": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"url": "https://arxiv.org/pdf/math/0701907.pdf", "anchor_text": "Kernel Methods in Machine Learning"}, {"url": "https://www.cs.toronto.edu/~duvenaud/cookbook/", "anchor_text": "Kernel Cookbook"}, {"url": "https://www.cs.toronto.edu/~duvenaud/", "anchor_text": "David Duvenaud"}, {"url": "https://arxiv.org/abs/1605.07146", "anchor_text": "Wide Residual Networks"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zagoruyko%2C+S", "anchor_text": "Sergey Zagoruyko"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Komodakis%2C+N", "anchor_text": "Nikos Komodakis"}, {"url": "http://www.gaussianprocess.org/gpml/", "anchor_text": "Gaussian Processes for Machine Learning"}, {"url": "https://arxiv.org/pdf/1711.00165.pdf", "anchor_text": "Deep Neural Networks as Gaussian Processes"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d6c6d896fcbf---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d6c6d896fcbf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deep-neural-networks?source=post_page-----d6c6d896fcbf---------------deep_neural_networks-----------------", "anchor_text": "Deep Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d6c6d896fcbf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d6c6d896fcbf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&user=Richard+Michael&userId=7229c4f1db59&source=-----d6c6d896fcbf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&user=Richard+Michael&userId=7229c4f1db59&source=-----d6c6d896fcbf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd6c6d896fcbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d6c6d896fcbf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d6c6d896fcbf--------------------------------", "anchor_text": ""}, {"url": "https://rcml.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rcml.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Richard Michael"}, {"url": "https://rcml.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "72 Followers"}, {"url": "http://M.Sc", "anchor_text": "M.Sc"}, {"url": "http://laplaceml.com/blog", "anchor_text": "http://laplaceml.com/blog"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7229c4f1db59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&user=Richard+Michael&userId=7229c4f1db59&source=post_page-7229c4f1db59--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7c745d7317a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf&newsletterV3=7229c4f1db59&newsletterV3Id=7c745d7317a0&user=Richard+Michael&userId=7229c4f1db59&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}