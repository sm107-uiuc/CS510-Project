{"url": "https://towardsdatascience.com/what-to-expect-from-reinforcement-learning-a22e8c16f40c", "time": 1682995340.449143, "path": "towardsdatascience.com/what-to-expect-from-reinforcement-learning-a22e8c16f40c/", "webpage": {"metadata": {"title": "What to expect from Reinforcement Learning? | by Moritz Kirschte | Towards Data Science", "h1": "What to expect from Reinforcement Learning?", "description": "This story provides you with the basic intuition about Reinforcement Learning by anchoring its objective i.a. through sketching Q-learning and model-based RL."}, "outgoing_paragraph_urls": [{"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "DeepRL Course at UC Berkeley", "paragraph_index": 45}, {"url": "https://www.meetup.com/de-DE/meetupai-Hamburg/", "anchor_text": "meetup.ai Hamburg", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Tay_(bot)", "anchor_text": "Tay", "paragraph_index": 47}, {"url": "https://youtu.be/HT-UZkiOLv8", "anchor_text": "new tactics in GO", "paragraph_index": 48}, {"url": "http://linkedin.com/in/kirschte", "anchor_text": "http://linkedin.com/in/kirschte", "paragraph_index": 50}], "all_paragraphs": ["Geoffrey Hinton once gave the example of \u201cThe trophy would not fit in the suitcase because it was too [big/small]\u201d, where it clearly depends on our own prior world knowledge to what \u201cit\u201d is referring to. We\u2019re considerably confident that a suitcase is regularly bigger than a trophy so that either a \u201ctoo big trophy\u201d or a \u201ctoo small suitcase\u201d is meant.", "That said, through naturally optimizing each sentence or even each document separately to its desired translation does not only suffer from its lacking real-world context furthermore it\u2019s rather stochastic due to language as a fuzzy system. In this case, there are multiple valid translations although a perfect translation might be impossible to determine.", "Nonetheless, there ought to be the most optimal translation for a given situation as expressed by metrics such as readability of the target group, persuasiveness or information density. By optimizing to those metrics though e.g. Reinforcement Learning instead of simply imitating human behavior as being practiced in supervised learning, we\u2019re following a more end-to-end approach increasing the capabilities of our system considerably.", "In this story, we\u2019ll first cover a little bit of a context and first attempts of leveraging an AI-automation of complex tasks like self-driving cars, intelligent sorting robots, etc. through Imitation Learning. Then we\u2019ll stick deeper into why Reinforcement Learning might be a more capable choice, when it\u2019s worth to be considered and how it actually works in principle but also by sketching two popular algorithms abstractly. Summing up with encouragement on my behalf about seeking practical use cases of this powerful technique, does help to spread RL more over right into the industry where those algorithms support finding more beneficial solutions. Or a nerd would prefer saying:", "Back in 2016, NVIDIA achieved ground truthing innovation: Instead of an explicit decomposition of the self-driving car objective (e.g. lane marking detection, path planning, etc.) in a hand-crafted internal representation, their system abstracts and optimizes all necessary processing steps including the internal representation automatically. In NVIDIA\u2019s approach, the car\u2019s reaction given certain feedback of its sensors (e.g. steering angle, speed, front image, etc.), which could, of course, preprocessed by advanced deep learning techniques, wasn\u2019t explicitly programmed, alternatively they followed the concept of Imitation Learning.", "Imagine you\u2019re driving a car now, would you break down your driving decision process into small criteria each by itself combined and finally evaluated with the help of a huge manifesto?", "Never ever! We as humans learned how to drive once by an unknown learning function, which couldn\u2019t be extracted. Nevertheless, the results of the learned driving function could be recorded (i.e. steering angle, speed, etc.), so that a neural network can learn how to map from a front-facing image sequence to exactly those desired action. This behavior cloning is done by NVIDIA\u2019s DAVE-2 system and is called Imitation Learning, which by definition imitates the behavior of an already functional system by end-to-end training for the final action itself instead of fractions of what would be the final action.", "Nonetheless, Imitation Learning suffers various issues, which could be in practice actually mitigated by workarounds in order to become functional, yet, it still remains under its possible capabilities:", "Is Reinforcement Learning be able to alleviate those three issues?", "Yes, all of them besides the Markov property which couldn\u2019t really be unfulfilled in practice.", "Reinforcement Learning\u2019s idea is essentially that of directly optimizing for the higher objective a human would also use instead of just imitating its fruits. With this approach we don\u2019t dictate the system what we think is relevant, alternately it figures it out on its own.", "Did you remember the three main issues with imitation learning?The first one can actually be solved by providing your system the opportunity to decide on its own, which action it will need in order to make progress. With that, the second issue also disappears just because it doesn\u2019t imitate anymore and learns on its own evaluated by a higher reward objective (the third foresight issue).", "To emphasize that: It learns like a human originally learned the task, but by starting at the very first beginning. The first few training hours would literally mean just data collection by taking random actions and hopefully, we\u2019ll do something great (determined by the reward function) so that we can learn to reinforce that behavior, which should become more likely in future iterations.", "This could be compared to the try-and-error learning behavior of a recent-born baby, which doesn\u2019t really know its own capabilities. His way of handling that situation is to explore its environment by crawling and crying, which is the current best choice for him.", "Note, that as we\u2019ll become better, we\u2019ll have to continue to not only take those actions which were promising in past iterations but to also consider new uncommon actions. This is called exploitation-exploration tradeoff, where with algorithms like \u03b5-greedy a small percentage is allocated to random actions. This is a point where most humans are stopping by claiming \u201cThat works well so why changing something?\u201d. There is always a better solution, it just takes time, costs and effort to explore it\u2026", "I don\u2019t wanna confuse you, so I\u2019ll quickly go through the most basic technical terms here and clarify shortly the whole loop process in Reinforcement Learning.", "There are basically just two parties involved: The environment and the agent. The environment provides the agent with a suitable observation of the current state (e.g. an image, video, sensory data, etc.) which gets processed by the agent through a policy (e.g. a convolutional neural network) outputting the most likely action in that current state which could then executed by the agent in its environment. The Environment now responses with a reward signal evaluating the quality of that step. It could be a positive reward signal to reinforce certain behavior or a negative one to punish bad decisions. Of course, the whole process is repeated until either the episode terminates by reaching the goal or we reach an upper limit. Some algorithms depend on data collected through this whole episode like Policy Gradient, others just need a batch of {state, action, reward, next state} to be learned.", "It\u2019s simply maximizing the sum of all expected future rewards in a given episode.", "Here, this data batch is graphically visualized. It\u2019s important to note, that we\u2019re now needing a continuous flow of data. So a one-time labeling data process like being done in Imagenet isn\u2019t valuable anymore. That\u2019s because we\u2019ll need to collect even in those regions data, the algorithm doesn\u2019t even know in the beginning, that such a region might exist. Like a baby, who doesn\u2019t know how fun playing with a toy could be until it eventually founds one. Or a car which doesn\u2019t know that it should be aware of a slithering risk while snowing until it comes into such a situation.", "That\u2019s why the environment is an essential part of the algorithm\u2019s design and couldn\u2019t be precomputed like in Supervised Learning.", "Q-Learning is one of the most famous concrete algorithms in Reinforcement Learning. It was named after the Q-function which estimates the sum over all future reward by taking a certain action in a given state. Note, that it does not just enable the opportunity to learn off-policy meaning any {state, action, reward, next state}-batch is sufficient instead of one whole episode containing multiple of these batches, furthermore, it also reduces variance: Through the Q-function calculation, the algorithm doesn\u2019t just rely on information of one episode. Instead, it prefers those steps which turned in similar situations over multiple episodes out to be good. One episode for itself is always by some means uncommon and flawed but summed together their basic common pattern is emphasized.", "And how do you actually train this Q-function?", "The Q-function outputting all future rewards could be represented by either a huge tabular with the states as rows and the actions as columns or by a neural network.", "Abstractly spoken, training could be as simple as expressed in above\u2019s picture: Add to the current reward of your data batch the Q-Value of the next state (assuming you\u2019ll always take the best action according to the Q-Value) and this is per definition the current Q-Value. Now try to maximize this reward.", "The discount factor \u03b3 on his behalf ensures that the algorithm prefers achieving a certain reward now over the same reward in two or more timesteps so it does not reach the goal certainly in some point of the infinite horizon but instead as soon as possible. Multiplying the discount factor to every calculated Q-Value tend to let future reward less rewarding.", "In the example of FrozenLake, an agent interacts with his environment by receiving a state (1 to 16: current plate) and sending the desired action (calculated with the Q-function) resulting in a certain reward.", "Removal of the discount factor would, in this case, result in a tabular containing either 0.00 for holes or 1.00 for the remaining lakes: It does learn how to solve the task without making a mistake, but won\u2019t solve it as soon as possible as it would be logical to us humans.", "Moreover, in a more realistic scenario, one can\u2019t just use a tabular even when considering images as input states or even continuous states. This is where neural networks come into place. Sadly, in this setting, it is not guaranteed that it will converge however, in practice it is often the case.", "Rather useful than our toy example is the game called Breakout:", "Four Decades ago, Steve Jobs and Woz programmed Breakout at Atari in four nights by just using 42 TTL-chips. Did they even imagine back then that\u2019s now possible with Reinforcement Learning to learn an advanced policy capable of observing on its own how to play best? An AI not specifically programmed to one game and clearly having no access to the internal state representation? It\u2019s more than just getting familiar with available actions and keeping the ball alive, it\u2019s about the strategy of playing: Avoiding both penalties at hitting the orange blocks (increased speed) and the top of the game (shortened the paddle) seems to work out quite well. At least in later iterations\u2026", "Last but not least the supreme discipline, the cr\u00e8me de la cr\u00e8me is still missing: Meta-learning. The learn-how-to-learn algorithm par excellence! Oh no, not that cr\u00e8me. For now, we\u2019ll stick with model-based RL being as visionary as Meta-learning yet simpler in its usage.", "Rather than learning a reward function as in Q-learning in model-based RL an even more end-to-end approach is considered: Dreaming about what the environment might be and then acting according to your dream as best as possible.", "Essentially you start off with the same kind of data but continue with learning a neural network called dynamics model dreaming about how the next state might be given the current state and a to-be-executed action in that state. With that, it is actually possible to predict in the algorithm\u2019s mind multiple steps into the future without executing that action following into a more deliberate behavior.", "In the next step, planning through a spanned tree by considering every reachable state and its corresponding reward within a predefined horizon (e.g. 15 timesteps) with Monte-Carlo Tree Search turns out to be appropriate.", "And how many actions do we execute?", "Just the first one! Why? It\u2019s because executing the first action of that spanned tree being on the path of a high-reward trajectory is pretty accurate in comparison to the reality whereas the whole path would be especially error-prone since the learned dynamic model is not 100% accurate. In this case, errors would add up in each timestep as well as our model would be unable to predict unexpectable environmental changes.", "Consider e.g. the case of a navigation system in an autonomous car: Sure it can predict what particular action sequence to take to get to the driver\u2019s destination, however, the whole driving process depends furthermore on other cars and pedestrians the traffic light circuit, yet even the wind, etc. Once encountered such an unexpected behavior it should be able to take corrective action to its optimal navigation. Hence just one action is executed before replanning again with the real new state received from the environment.", "Sure. That\u2019s about the intuition of model-based RL. But why is it in some sense visionary?", "It\u2019s because of its ability to dream. Suppose e.g. the Visual Foresight technique in the images below. As one might think, those images are real images, but the contradictory is true! They\u2019re predicted ones from the dynamics model.", "This system figures out on its own that moving the robot arm beside a toy would most likely result in a moving behavior of that toy, too. And as expected is the toy disappearing of its original position with an ongoing movement of the robot arm as additionally visualized in the lower row with a likelihood map of an appearance with respect to the green toy.", "Fair enough. But are there are any practical usages especially in comparison to Q-Learning?", "Of course! In fact, it takes huge advantages with respect to the sample-efficiency (i.e. episodes needed) through the planning process as seen below.", "However, exactly this planing strength is also its main downside: It is very vulnerable to overfitting of the dynamics model often mitigated by shallower neural networks in practice which in essence can\u2019t model the objective as effective as in the Q-learning case.", "I\u2019m glad that you made it so far and encourage you as a final remark to never say", "Self-learning algorithms is something for the future", "again because now and especially after completing the great DeepRL Course at UC Berkeley you can make it a topic of today! That\u2019s also why I gave a talk last month at meetup.ai Hamburg about exactly the same content.", "Get out there and find use cases where not only a clear reward function (margin, CTR, accidents per mile, Jenga-tower stability, etc.) is available but also a safe-environment could be provided so that mistakes don\u2019t hurt.", "Self-learning doesn\u2019t mean that it\u2019ll continue to learn in the wild as Microsoft\u2019s Tay bot on Twitter in 2016 did. You wouldn\u2019t just get a massive shitstorm, you\u2019re also unable to prove in any sense that your software is production-ready because the final software isn\u2019t there yet.", "RL could as a starting point also be operated in a smaller sense:Self-learned in a safe environment, one could e.g. take the observed and hopefully unseen results (item order in an online shop, user experience through a website, new tactics in GO) and then statically implemented these in the real environments leading into a dramatic increase the system\u2019s capability.", "Any other thoughts? Feel free to leave a note. \ud83d\ude09", "AI Researcher @novomind | Host & Public Speaker @meetup.ai | AGI / RL Enthusiast | Ambassador of freedom, awardee @FNF | http://linkedin.com/in/kirschte"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa22e8c16f40c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/inside-ai/home", "anchor_text": "Inside AI"}, {"url": "https://medium.com/@kirschte?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Moritz Kirschte"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe9624bd27418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=post_page-e9624bd27418----a22e8c16f40c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa22e8c16f40c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=-----a22e8c16f40c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa22e8c16f40c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=-----a22e8c16f40c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://gym.openai.com/envs/LunarLanderContinuous-v2/", "anchor_text": "Lunar Lander"}, {"url": "https://devblogs.nvidia.com/deep-learning-self-driving-cars/", "anchor_text": "End to End Learning for Self-Driving Cars"}, {"url": "https://homes.cs.washington.edu/~todorov/papers/LiThesis.pdf", "anchor_text": "Li & Todorov"}, {"url": "https://en.wikipedia.org/wiki/Markov_property", "anchor_text": "Markov property"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "DeepRL Course"}, {"url": "https://people.eecs.berkeley.edu/~svlevine/", "anchor_text": "Sergey Levine"}, {"url": "https://devblogs.nvidia.com/deep-learning-self-driving-cars/", "anchor_text": "NVIDIA"}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "FrozenLake"}, {"url": "https://gym.openai.com/envs/Breakout-v0/", "anchor_text": "Breakout"}, {"url": "https://sites.google.com/view/sna-visual-mpc", "anchor_text": "Visual Planning"}, {"url": "https://arxiv.org/abs/1708.02596", "anchor_text": "here"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "DeepRL Course at UC Berkeley"}, {"url": "https://www.meetup.com/de-DE/meetupai-Hamburg/", "anchor_text": "meetup.ai Hamburg"}, {"url": "https://en.wikipedia.org/wiki/Tay_(bot)", "anchor_text": "Tay"}, {"url": "https://youtu.be/HT-UZkiOLv8", "anchor_text": "new tactics in GO"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a22e8c16f40c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a22e8c16f40c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----a22e8c16f40c---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----a22e8c16f40c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----a22e8c16f40c---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa22e8c16f40c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=-----a22e8c16f40c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa22e8c16f40c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=-----a22e8c16f40c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa22e8c16f40c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe9624bd27418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=post_page-e9624bd27418----a22e8c16f40c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe9624bd27418%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=-----a22e8c16f40c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Written by Moritz Kirschte"}, {"url": "https://medium.com/@kirschte/followers?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "69 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://linkedin.com/in/kirschte", "anchor_text": "http://linkedin.com/in/kirschte"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe9624bd27418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=post_page-e9624bd27418----a22e8c16f40c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe9624bd27418%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-to-expect-from-reinforcement-learning-a22e8c16f40c&user=Moritz+Kirschte&userId=e9624bd27418&source=-----a22e8c16f40c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Moritz Kirschte"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Building a Turing Machine with Reinforcement Learningbeyond manual algorithmic design"}, {"url": "https://towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "11 min read\u00b7Apr 19, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=-----9d06a4f0ce6----0-----------------clap_footer----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6?source=author_recirc-----a22e8c16f40c----0---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=-----a22e8c16f40c----0-----------------bookmark_preview----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a22e8c16f40c----1---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----a22e8c16f40c----1-----------------bookmark_preview----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a22e8c16f40c----2---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a22e8c16f40c----2-----------------bookmark_preview----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----3-----------------clap_footer----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a22e8c16f40c----3---------------------b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----a22e8c16f40c----3-----------------bookmark_preview----b67761ff_6a8c_424e_a2ae_82ba1a47cbcd-------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "See all from Moritz Kirschte"}, {"url": "https://towardsdatascience.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----a22e8c16f40c----0-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a22e8c16f40c----1-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----a22e8c16f40c----0---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----a22e8c16f40c----0-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----a22e8c16f40c----1---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----a22e8c16f40c----1-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----2-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----a22e8c16f40c----2---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----a22e8c16f40c----2-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "Mark Riedl"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "A Very Gentle Introduction to Large Language Models without the Hype[This is a work in progress]"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": "38 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&user=Mark+Riedl&userId=7247bdeb9655&source=-----5f67941fa59e----3-----------------clap_footer----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----a22e8c16f40c----3---------------------6fb6fb17_ba42_4b54_b485_209aad9108f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "53"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&source=-----a22e8c16f40c----3-----------------bookmark_preview----6fb6fb17_ba42_4b54_b485_209aad9108f5-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----a22e8c16f40c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}