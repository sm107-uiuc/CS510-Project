{"url": "https://towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af", "time": 1682994421.2360268, "path": "towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af/", "webpage": {"metadata": {"title": "Know your enemy. Why adversarial examples are more\u2026 | by Oscar Knagg | Towards Data Science", "h1": "Know your enemy", "description": "Adversarial examples are inputs to ML models that are specially crafted to cause the model to make a mistake \u2014 optical illusions for computers. As shown below, adversarial examples are typically\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST", "paragraph_index": 1}, {"url": "http://mathworld.wolfram.com/Norm.html", "anchor_text": "norm", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1607.02533.pdf", "anchor_text": "Kurakin et al", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/1707.07397.pdf", "anchor_text": "Athalye et al", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "Brown et al", "paragraph_index": 22}, {"url": "https://arxiv.org/pdf/1611.02770.pdf", "anchor_text": "Liu et al", "paragraph_index": 25}, {"url": "https://clarifai.com/", "anchor_text": "Clarifai.com", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1605.07277.pdf", "anchor_text": "Papernot et al", "paragraph_index": 27}, {"url": "https://arxiv.org/pdf/1801.00634.pdf", "anchor_text": "recent", "paragraph_index": 28}, {"url": "https://arxiv.org/pdf/1801.02774.pdf", "anchor_text": "results", "paragraph_index": 28}, {"url": "https://arxiv.org/pdf/1802.08195.pdf", "anchor_text": "this brilliant paper", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Saccade", "anchor_text": "saccades", "paragraph_index": 33}], "all_paragraphs": ["Adversarial examples are inputs to ML models that are specially crafted to cause the model to make a mistake \u2014 optical illusions for computers. As shown below, adversarial examples are typically created by adding a small amount of carefully calculated noise to a natural image. The fact that this kind of attack can fool state of the art image recognition models while often being imperceptible to humans demonstrates a fundamental difference in the way humans and deep convolutional networks process visual information. This not only raises interesting theoretical issues but also brings into question the readiness of neural networks for security-critical applications.", "Adversarial examples are a particularly fascinating machine learning phenomenon because there are so many open questions surrounding them. Why do they fool deep networks and not humans? Is it possible to completely secure neural networks against these attacks? To this date no-one has managed to produce an MNIST digit classification model that is completely robust to adversarial examples even though MNIST is considered a toy problem by most machine learning practitioners.", "This post will cover the following:", "Creating adversarial samples is remarkably simple and is closely related to the way neural networks are trained i.e. gradient descent. The principle of gradient descent is simple. To find the minimum of an unknown function, first start at a random point then take a small step in the direction of the gradient of that function. For a sufficiently small step size and a non-pathological function this will always converge (at least to a local minimum).", "To apply this to network training we first define a loss function of the training data and network parameters (such as the mean squared error, shown below) that represents how wrong the output of a model was for a particular sample. Hence finding the minimum of this loss function is equivalent to finding a good set of network parameters.", "Secondly we differentiate this loss with respect to the parameters, theta, and update the parameters such that the loss on that sample will decrease. In practice there are many more details such as learning rate, batch size, momentum etc. but the principle is just gradient descent.", "Creating an adversarial example is done by turning this process upside down. If we held the parameters of the model constant and differentiated the loss with respect to the input data, x, we could then update x in a such a way that the expected loss of the model would increase on that sample.", "In this section I talk about the perturbations on image pixels just to be clear it is not only image classification models that are vulnerable to adversarial attack.", "This brings us to the Fast Gradient-Sign Method (FGSM), the simplest method of creating an adversarial example. This is exactly the same as a single step of gradient ascent except we fix the perturbation on each pixel to be of fixed size, epsilon, hence guaranteeing that no pixel in the adversarial example is more than epsilon from the original image. This method produced the example at the top of this post.", "In practice this simple very simple method causes the accuracy of state of the art ImageNet models to drop to almost zero while the images are imperceptibly different to humans. This is quite an unfortunate blow to the image that neural networks have superhuman image recognition performance (often claimed on ImageNet) and exposes some fundamental blind spots in the classification functions learned by deep neural networks. Would you trust the visual processing in your self-driving car to a network that can be fooled by a few shifted pixels?", "Even though this is possibly the most interesting question surrounding adversarial examples I can\u2019t give you a satisfying answer \u2014 this is a completely open question! Many proposals have been put forward including", "Before I go on to showcase some of the fascinating results that have come out of adversarial examples I will formalise them and categorise the kinds of attacks that can be performed.", "An adversarial example is typically defined as the epsilon bounded example x\u2019 which satisfies", "i.e. the decision of the classifier with parameters, theta, is not the true label, y . A bounded adversarial example also satisfies", "i.e. the distance between the adversarial and original example is less than some small epsilon under a particular norm (AKA way of measuring distance).", "The choice of the norm, p, determines the limitations placed on the adversarial examples. All of these norms aim to quantify how imperceptible to humans an adversarial example is.", "However some recent work has taken a different approach and taken a different approach and uses an unbounded definition. An adversarial perturbation is any modification to an image that retains the semantics of the original image but fools a machine learning model, even though the attack may be perceptible to humans. This is a much less well defined class of perturbations but a useful one as demonstrated by the examples below.", "The types of algorithms aka attacks that can be performed on machine learning models can be categorised also.", "You might raise a concern that although a white-box attack like the FGSM method above is very effective, how likely is it that attacker will have access to the full weights of the network that powers the vision component of your self-driving car? Well it turns out that black-box attacks can be just as effective as white-box attacks. This fact as well as a few of the results I will share with you in the next few paragraphs make the security implications of adversarial attacks much more real.", "After reading this far you might be thinking that adversarial examples are just an academic curiosity, or at least limited to digital environments such as evasion of spam filters. It\u2019s easy to believe that the imperceptible distortions that you\u2019ve seen so far would never be effective in the real world because they would be destroyed by variations in camera quality, viewpoint, lighting etc\u2026 Well Kurakin et al would like to have a word with you.", "In their brilliant paper Kurakin et al demonstrate a black-box attack on the TensorFlow Camera Demo app which uses the powerful Inception classifier. They were able to show that print-outs of adversarial examples created with the FGSM method were correctly classified at half the rate of print-outs of regular examples. This is is a much less successful attack than in the digital domain but still a blow against neural networks.", "Athalye et al went one step further to generate truly robust real-world adversarial examples by inventing the expectation-over-transformation (EOT) attack. The premise of the EOT attack is as follows: we expect that some adversarial perturbations will be rendered ineffective by naturally occurring image transformations (lighting, viewpoint etc\u2026) so we should optimise our adversarial example to account for this. They achieve this by using an iterative attack process of alternating gradient ascent steps and random transformations.", "In a similar approach Brown et al were able to use the EOT attack to generate an adversarial sticker that can be printed out and applied to any image. These adversarial examples are designed to be robust over a large range of locations, scales, rotations and background images and hence could be applied to anything. These attacks stretch the definition of an adversarial example as they are clearly visible to humans however they also produced some disguised versions. An adversarial patch taking up 10% of an image was able to fool a network 90% of the time. In contrast a sticker of an actual toaster the same size caused a toaster classification 10% of the time!", "Maybe it\u2019s just me but I find the adversarial sticker to be artistically pleasing, in fact it even contains some toaster-like visual features! Generally the more robust an adversarial example is the more it aligns with human perception. This intriguing phenomenon is an empirical hint that there is some connection between human perception and convolutional neural networks \u2014 this is something that I\u2019ll discuss more in a later section.", "When I first discovered adversarial examples I thought an obvious and bulletproof attack would be to use an ensemble of models. An adversarial example can clearly trick one model by exploiting the peculiarity of its particular gradients but it\u2019s never going to fool multiple models at the same time right? Well it turns out I was wrong.", "Liu et al thoroughly examined the transferability of adversarial examples on multiple state of the art classifiers. Each cell in the table above shows the accuracy of the column model on adversarial examples generated to attack the row model e.g. the 4th cell from the left on the first rows indicates that attacks against ResNet-152 cause the accuracy of VGG016 to drop to 20% (from 70%). Hence we can see that an ensemble of top models would still likely be fooled by an attack aimed at a single model!", "The authors went further and fully demonstrated the transferability of adversarial examples by launching black box attacks against Clarifai.com using adversarial examples crafted to fool an ensemble of classifiers. They reasoned that since we don\u2019t know the dataset, architecture or training procedure of the Clarifai model then they could improve their chances by aiming to fool an ensemble. Impressively they were able to cause the Clarifai.com model to generate irrelevant labels 76% of the time with this approach!", "It would still be reasonable to believe that it\u2019s only neural networks that are vulnerable to these kinds of attacks but Papernot et al show that adversarial examples transfer between a wide variety of model types including classical machine learning algorithms such as SVMs and decision trees.", "I believe that the results discussed in this section provide evidence that adversarial examples aren\u2019t the flaws of any particular model but a possibly inherent problem in performing classification in high-dimensional spaces. Some recent results support this but clearly humans are able to avoid this somehow.", "I\u2019ve shown you in previous sections that robust adversarial examples align more with human perception and that adversarial examples can transfer between classifiers. In this brilliant paper Elsayed et al show that adversarial examples can transfer to the best performing classifier of all \u2014 the human visual system.", "The this carefully designed study human participants performed variants of the following trial repeatedly:", "You may have noticed that the participants are only shown images for a very short time interval. In fact this study was designed to maximise the chances of transferring adversarial examples to humans with the following choices", "Nonetheless adversarial attacks lowered the accuracy of humans on these trials was reduced by approximately 10% and the pool of participants was large enough to make this a statistically significant result!", "In my eyes this paper shows a definite connection between deep convolutional networks and the initial rapid visual response of human vision. However human vision is much more complicated than just that first response and involves many eye movements known as saccades between different regions of interest. It\u2019s possible that this or some other feature of the human visual system is the \u201csecret sauce\u201d that renders human vision invulnerable to adversarial examples, although even human vision suffers from optical illusions (though these are very different from adversarial examples). Or perhaps we will one day discover a method to produce adversarial examples for time-unlimited humans \u2014 a truly disturbing thought!", "I can\u2019t claim that this article is a comprehensive review of adversarial examples as I\u2019ve touched on only a fraction of the brilliant work that is being done in this area but I hope that after reading this post you\u2019ll agree with me that adversarial examples are a fascinating phenomenon. I\u2019m confident that one day the research community will discover image recognition techniques that are invulnerable to attack and that we\u2019ll learn a lot about the human visual system in the process.", "Stay tuned for a follow up post in which I will guide you through a specific adversarial attack with code. I\u2019ll also provide an adversarial training method from the recent literature and show that it provides some unexpected benefits!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I like to build novel things"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5936bccb24af&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5936bccb24af--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@oknagg?source=post_page-----5936bccb24af--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Oscar Knagg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc510ccc9027c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&user=Oscar+Knagg&userId=c510ccc9027c&source=post_page-c510ccc9027c----5936bccb24af---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/1QbHek3ZnGk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "naomi tamar"}, {"url": "https://unsplash.com/search/photos/deep-sea?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1412.6572.pdf", "anchor_text": "Goodfellow et al"}, {"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "ref"}, {"url": "https://arxiv.org/pdf/1805.12152v3.pdf", "anchor_text": "ref"}, {"url": "https://arxiv.org/pdf/1704.08847.pdf", "anchor_text": "ref"}, {"url": "https://arxiv.org/pdf/1801.00634.pdf", "anchor_text": "ref"}, {"url": "http://mathworld.wolfram.com/Norm.html", "anchor_text": "norm"}, {"url": "https://arxiv.org/pdf/1607.02533.pdf", "anchor_text": "Kurakin et al"}, {"url": "https://arxiv.org/pdf/1607.02533.pdf", "anchor_text": "https://arxiv.org/pdf/1607.02533.pdf"}, {"url": "https://arxiv.org/pdf/1707.07397.pdf", "anchor_text": "Athalye et al"}, {"url": "https://arxiv.org/pdf/1707.07397.pdf", "anchor_text": "Athalye et al"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "Brown et al"}, {"url": "https://www.youtube.com/watch?v=i1sp4X57TL4", "anchor_text": "this"}, {"url": "https://arxiv.org/pdf/1611.02770.pdf", "anchor_text": "Liu et al"}, {"url": "https://arxiv.org/pdf/1611.02770.pdf", "anchor_text": "Liu et al"}, {"url": "https://clarifai.com/", "anchor_text": "Clarifai.com"}, {"url": "https://arxiv.org/pdf/1605.07277.pdf", "anchor_text": "Papernot et al"}, {"url": "https://arxiv.org/pdf/1801.00634.pdf", "anchor_text": "recent"}, {"url": "https://arxiv.org/pdf/1801.02774.pdf", "anchor_text": "results"}, {"url": "https://arxiv.org/pdf/1802.08195.pdf", "anchor_text": "this brilliant paper"}, {"url": "https://www.biorxiv.org/content/biorxiv/early/2018/02/12/240614.full.pdf", "anchor_text": "believed"}, {"url": "https://arxiv.org/pdf/1802.08195.pdf", "anchor_text": "Elsayed et al."}, {"url": "https://en.wikipedia.org/wiki/Saccade", "anchor_text": "saccades"}, {"url": "https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3", "anchor_text": "here it is"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5936bccb24af---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----5936bccb24af---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/adversarial-examples?source=post_page-----5936bccb24af---------------adversarial_examples-----------------", "anchor_text": "Adversarial Examples"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5936bccb24af---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5936bccb24af---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&user=Oscar+Knagg&userId=c510ccc9027c&source=-----5936bccb24af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&user=Oscar+Knagg&userId=c510ccc9027c&source=-----5936bccb24af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5936bccb24af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5936bccb24af---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5936bccb24af--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5936bccb24af--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5936bccb24af--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5936bccb24af--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5936bccb24af--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Oscar Knagg"}, {"url": "https://medium.com/@oknagg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "654 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc510ccc9027c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&user=Oscar+Knagg&userId=c510ccc9027c&source=post_page-c510ccc9027c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3bb863a32894&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af&newsletterV3=c510ccc9027c&newsletterV3Id=3bb863a32894&user=Oscar+Knagg&userId=c510ccc9027c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}