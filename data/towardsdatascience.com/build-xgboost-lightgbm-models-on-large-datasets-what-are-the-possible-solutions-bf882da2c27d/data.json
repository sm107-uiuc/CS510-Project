{"url": "https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d", "time": 1682995627.621557, "path": "towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d/", "webpage": {"metadata": {"title": "Build XGBoost / LightGBM models on large datasets \u2014 what are the possible solutions? | by Shiu-Tang Li | Towards Data Science", "h1": "Build XGBoost / LightGBM models on large datasets \u2014 what are the possible solutions?", "description": "XGBoost and LightGBM have been dominating all recent kaggle competitions for tabular data. Simply go to any competition page (tabular data) and check out the kernels and you\u2019ll see. In these\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65", "anchor_text": "https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65", "paragraph_index": 1}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html", "paragraph_index": 9}, {"url": "https://github.com/dotnet/machinelearning/issues/1651", "anchor_text": "https://github.com/dotnet/machinelearning/issues/1651", "paragraph_index": 9}, {"url": "https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html", "paragraph_index": 14}, {"url": "https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava", "anchor_text": "https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava", "paragraph_index": 17}, {"url": "https://github.com/slundberg/shap/issues/464", "anchor_text": "https://github.com/slundberg/shap/issues/464", "paragraph_index": 20}, {"url": "https://github.com/dmlc/xgboost/issues/4276", "anchor_text": "https://github.com/dmlc/xgboost/issues/4276", "paragraph_index": 20}, {"url": "https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning", "anchor_text": "https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning", "paragraph_index": 21}], "all_paragraphs": ["XGBoost and LightGBM have been dominating all recent kaggle competitions for tabular data. Simply go to any competition page (tabular data) and check out the kernels and you\u2019ll see. In these competitions, the data is not \u2018huge\u2019 \u2014 well, don\u2019t tell me the data you\u2019re handling is huge if it can be trained on your laptop. For these cases, Jupyter notebook is enough for XGBoost and LightGBM model constructions.", "When the data become larger, but not super large, while you still want to stick with Jupyter notebooks, say, to build the models \u2014one way is to use some memory reduction tricks (For example, ArjanGroen\u2019s code: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65); or use cloud services, say rent an EC2 on AWS. For example, the r5.24xlarge instances have 768 GiB Memory costing $6 / hour, which I think it can already handle a lot of data that your boss think they\u2019re really \u201cbig\u201d.", "But what if the data is even bigger?", "We need distributed Machine Learning tools. As far as I know, if we\u2019d like to use scalable XGBoost or LightGBM, we have these options available:", "1 XGBoost4j on Scala-Spark2 LightGBM on Spark (PySpark / Scala / R)3 XGBoost with H2O.ai4 XGBoost on Amazon SageMaker", "I would like to point out some of the issues of each tool based on my personal experience, and provide some resources if you\u2019d like to use them. I\u2019m also happy to learn from you if you also had the similar issues / how you get them solved! Please comment below.", "Before I dive into these tools, there\u2019re a few things good to know beforehand.", "XGBoost is a very fast and accurate ML algorithm, but it\u2019s now challenged by LightGBM \u2014 which runs even faster (for some datasets, it\u2019s 10X faster based on their benchmark), with comparable model accuracy, and more hyperparameters for users to tune. The key difference in speed is because XGBoost split the tree nodes one level at a time, and LightGBM does that one node at a time.", "So XGBoost developers later improved their algorithms to catch up with LightGBM, allowing users to also run XGBoost in split-by-leaf mode (grow_policy = \u2018lossguide\u2019). Now XGBoost is much faster with this improvement, but LightGBM is still about 1.3X \u2014 1.5X the speed of XGB based on my tests on a few datasets. (Welcome to share your test outcomes!)", "The readers can go with either option with their own preference. One more thing to add here: XGBoost has a feature that LightGBM lacks \u2014 \u201cmonotonic constraint\u201d. It will sacrifice some model accuracy and increase training time, but may improve model interpretability. (Reference: https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html and https://github.com/dotnet/machinelearning/issues/1651)", "For random forest algorithm, the more trees built, the less variance the model is. But up to some point, you can\u2019t really improve the model further by adding in more trees.", "XGBoost and LightGBM do not work this way. the model accuracy keeps improving when number of trees increases, but after certain point the performance begins to drop \u2014 a sign of overfitting; and the performance gets worse with more trees built.", "In order to find the \u2018sweet spot\u2019, you can do cross validations or simply do training-validation set splitting, and then use early stopping time to find where it should stop training; or, you can build a few models with different number of trees (say 50, 100, 200), and then pick the best one among them.", "If you don\u2019t care about extreme performance, you can set a higher learning rate, build only 10\u201350 trees (say). It may under-fit a bit but you still have a pretty accurate model, and this way you can save time finding the optimal number of trees. Another benefit with this approach is the model is simpler (fewer trees built).", "If the reader plans to go with this option, https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html is a good starting point. I\u2019d like point out a few issues here (as of this article is posted):", "The major issues based on my personal experience:", "Some example codes (not including vector assembler):", "This is my personal favorite solution. The model can be built using H2O.ai, integrated in a Pysparkling Water (H2O.ai + PySpark) pipeline:https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava", "It\u2019s easy to build a model with optimized number of rounds with cross validations -", "And the XGBoost model can be saved and used in Python with cv_xgb.save_mojo() . Use h2o.save_model() if you\u2019d like to save the model in h2o format instead.", "My only complaint about it is that the saved model (the one saved with save.mojo) can\u2019t be used with SHAP package to generate SHAP feature importance (But XGBoost feature importance, .get_fscore() , works fine). Seems like there\u2019re some issues with the original XGBoost package.https://github.com/slundberg/shap/issues/464https://github.com/dmlc/xgboost/issues/4276", "This is a pretty new solution by AWS. The two main features are automatic hyperparameter tuning with Bayesian optimization, and the model can be deployed as an endpoint. A few examples can be found on their Github: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning. Here are some of my concerns with it:", "But other than these issues, we can still leverage its endpoint feature. You can train your XGB model anywhere, put it in XGBoost image from Amazon ECR (Elastic Container Registry), and then deploy it as an endpoint.", "XGBoost / LightGBM are rather new ML tools, and they both have the potentials to become stronger. The developers already did a fantastic job creating these tools to make people\u2019s life easier. I point out some of my observations and share my experience here, with the hope that they can become even better and more easy-to-use tools.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Mathematician \u2192 Data scientist \u2192 Software engineer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbf882da2c27d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shiutangli?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shiutangli?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Shiu-Tang Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e77e054ef75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&user=Shiu-Tang+Li&userId=1e77e054ef75&source=post_page-1e77e054ef75----bf882da2c27d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/KMn4VEeEPR8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Sean O."}, {"url": "https://unsplash.com/search/photos/beach?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65", "anchor_text": "https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html"}, {"url": "https://github.com/dotnet/machinelearning/issues/1651", "anchor_text": "https://github.com/dotnet/machinelearning/issues/1651"}, {"url": "https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html"}, {"url": "https://github.com/dmlc/xgboost/issues/3724", "anchor_text": "https://github.com/dmlc/xgboost/issues/3724"}, {"url": "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm-faq/missing_values.html", "anchor_text": "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm-faq/missing_values.html"}, {"url": "https://github.com/dmlc/xgboost/releases", "anchor_text": "https://github.com/dmlc/xgboost/releases"}, {"url": "https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md", "anchor_text": "https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md"}, {"url": "https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava", "anchor_text": "https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava"}, {"url": "https://github.com/slundberg/shap/issues/464", "anchor_text": "https://github.com/slundberg/shap/issues/464"}, {"url": "https://github.com/dmlc/xgboost/issues/4276", "anchor_text": "https://github.com/dmlc/xgboost/issues/4276"}, {"url": "https://github.com/h2oai/h2o-3/blob/373ca6b1bc7d194c6c70e1070f2f6f416f56b3d0/Changes.md", "anchor_text": "https://github.com/h2oai/h2o-3/blob/373ca6b1bc7d194c6c70e1070f2f6f416f56b3d0/Changes.md"}, {"url": "https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning", "anchor_text": "https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning"}, {"url": "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb", "anchor_text": "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb"}, {"url": "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb", "anchor_text": "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb"}, {"url": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html", "anchor_text": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bf882da2c27d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----bf882da2c27d---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/spark?source=post_page-----bf882da2c27d---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bf882da2c27d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/big-data?source=post_page-----bf882da2c27d---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&user=Shiu-Tang+Li&userId=1e77e054ef75&source=-----bf882da2c27d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&user=Shiu-Tang+Li&userId=1e77e054ef75&source=-----bf882da2c27d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbf882da2c27d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bf882da2c27d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bf882da2c27d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bf882da2c27d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bf882da2c27d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shiutangli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shiutangli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shiu-Tang Li"}, {"url": "https://medium.com/@shiutangli/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e77e054ef75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&user=Shiu-Tang+Li&userId=1e77e054ef75&source=post_page-1e77e054ef75--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fceccfcc5a7a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&newsletterV3=1e77e054ef75&newsletterV3Id=ceccfcc5a7a4&user=Shiu-Tang+Li&userId=1e77e054ef75&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}