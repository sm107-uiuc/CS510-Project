{"url": "https://towardsdatascience.com/3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b", "time": 1683017314.1833038, "path": "towardsdatascience.com/3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b/", "webpage": {"metadata": {"title": "3 Pre-Trained Model Series to Use for NLP with Transfer Learning | by Orhan G. Yal\u00e7\u0131n | Towards Data Science", "h1": "3 Pre-Trained Model Series to Use for NLP with Transfer Learning", "description": "Using State-of-the-Art Pre-trained Neural Network Models (OpenAI's GPTs, BERTs, ELMos) to Tackle Natural Language Processing Problems with Transfer Learning"}, "outgoing_paragraph_urls": [{"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin", "paragraph_index": 0}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/using-recurrent-neural-networks-to-predict-bitcoin-btc-prices-c4ff70f9f3e4", "anchor_text": "Recurrent Neural Networks", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d", "anchor_text": "Convolutional Neural Networks", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b", "anchor_text": "GANs", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/image-noise-reduction-in-10-minutes-with-convolutional-autoencoders-d16219d2956a", "anchor_text": "Autoencoders", "paragraph_index": 3}, {"url": "https://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf", "anchor_text": "Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned.", "paragraph_index": 6}, {"url": "http://papers.neurips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf", "anchor_text": "Discriminability-Based Transfer between Neural Networks", "paragraph_index": 9}, {"url": "https://www.springer.com/journal/10994", "anchor_text": "Machine Learning", "paragraph_index": 9}, {"url": "https://link.springer.com/journal/10994/28/1/page/1", "anchor_text": "special issue", "paragraph_index": 9}, {"url": "https://www.springer.com/gp/book/9780792380474", "anchor_text": "Learning to Learn", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Andrew_Ng", "anchor_text": "Andrew Ng", "paragraph_index": 10}, {"url": "https://www.youtube.com/watch?v=wjqaz6m42wU", "anchor_text": "Transfer learning will be the next driver of machine learning\u2019s commercial success after supervised learning.", "paragraph_index": 11}, {"url": "http://image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 13}, {"url": "https://dumps.wikimedia.org/", "anchor_text": "the Wikipedia Corpus", "paragraph_index": 13}, {"url": "https://keras.io/api/applications/", "anchor_text": "143,667,240 parameters", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "image classification", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Bidirectional Encoder Representations from Transformers", "paragraph_index": 28}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s Transformers library", "paragraph_index": 29}, {"url": "http://towardsdatascience.com", "anchor_text": "TDS", "paragraph_index": 29}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "Embeddings from Language Models", "paragraph_index": 31}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 34}, {"url": "https://medium.us4.list-manage.com/subscribe?u=bf0e9524ea3d765ba10131675&id=52221072de", "anchor_text": "subscribe to the mailing list", "paragraph_index": 37}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai", "paragraph_index": 40}], "all_paragraphs": ["Before we start, if you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let\u2019s connect via Linkedin! Please do not hesitate to send a contact request! Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "If you have been trying to build machine learning models with high accuracy; but never tried Transfer Learning, this article will change your life. At least, it did mine!", "Note that this post is also a follow-up post of a post on Transfer Learning for Computer vision tasks. It has started to gain popularity, and now I wanted to share the NLP version of that with you. But, just in case, check it out:", "Most of us have already tried several machine learning tutorials to grasp the basics of neural networks. These tutorials helped us understand the basics of artificial neural networks such as Recurrent Neural Networks, Convolutional Neural Networks, GANs, and Autoencoders. But, their main functionality was to prepare you for real-world implementations.", "Now, if you are planning to build an AI system that utilizes deep learning, you have to either", "Transfer learning is a subfield of machine learning and artificial intelligence, which aims to apply the knowledge gained from one task (source task) to a different but similar task (target task). In other words:", "Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned.", "For example, the knowledge gained while learning to classify Wikipedia texts can help tackle legal text classification problems. Another example would be using the knowledge gained while learning to classify cars to recognize the birds in the sky. As you can see, there is a relation between these examples. We are not using a text classification model on bird detection.", "In summary, transfer learning saves us from reinventing the wheel, meaning we don\u2019t waste time doing the things that have already been done by a major company. Thanks to transfer learning, we can build AI applications in a very short amount of time.", "The history of Transfer Learning dates back to 1993. With her paper, Discriminability-Based Transfer between Neural Networks, Lorien Pratt opened the pandora\u2019s box and introduced the world to the potential of transfer learning. In July 1997, the journal Machine Learning published a special issue for transfer learning papers. As the field advanced, adjacent topics such as multi-task learning were also included under the field of transfer learning. Learning to Learn is one of the pioneer books in this field. Today, transfer learning is a powerful source for tech entrepreneurs to build new AI solutions and researchers to push machine learning frontiers.", "To show the power of transfer learning, we can quote from Andrew Ng:", "Transfer learning will be the next driver of machine learning\u2019s commercial success after supervised learning.", "There are three requirements to achieve transfer learning:", "A pre-trained model is a model created and trained by someone else to solve a similar problem. In practice, someone is almost always a tech giant or a group of star researchers. They usually choose a very large dataset as their base datasets, such as ImageNet or the Wikipedia Corpus. Then, they create a large neural network (e.g., VGG19 has 143,667,240 parameters) to solve a particular problem (e.g., this problem is image classification for VGG19). Of course, this pre-trained model must be made public so that we can take it and repurpose it.", "After getting our hands on these pre-trained models, we repurpose the learned knowledge, which includes the layers, features, weights, and biases. There are several ways to load a pre-trained model into our environment. In the end, it is just a file/folder which contains the relevant information. Deep learning libraries already host many of these pre-trained models, which makes them more accessible and convenient:", "You can use one of the sources above to load a trained model. It will usually come with all the layers and weights, and you can edit the network as you wish. Additionally, some research labs maintain their own repos, as you will see for ELMo later in this post.", "Well, while the current model may work for our problem. It is often better to fine-tune the pre-trained model for two reasons:", "Generally speaking, in a neural network, while the bottom and mid-level layers usually represent general features, the top layers represent the problem-specific features. Since our new problem is different than the original problem, we tend to drop the top layers. By adding layers specific to our problems, we can achieve higher accuracy.", "After dropping the top layers, we need to place our own layers so that we can get the output we want. For example, a model trained with English Wikipedia such as BERT can be customized by adding additional layers and further trained with the IMDB Reviews dataset to predict movie reviews sentiments.", "After adding our custom layers to the pre-trained model, we can configure it with special loss functions and optimizers and fine-tune it with extra training.", "For a quick Transfer Learning tutorial, you may visit the post below:", "Here are the three pre-trained network series you can use for natural language processing tasks ranging from text classification, sentiment analysis, text generation, word embedding, machine translation, and so on:", "While BERT and OpenAI GPT are based on transformers network, ELMo takes advantage of bidirectional LSTM network.", "Ok, let\u2019s dive into them one-by-one.", "There are three generations of GPT models created by OpenAI. GPT, which stands for Generative Pre-trained Transformers, is an autoregressive language model that uses deep learning to produce human-like text. Currently, the most advanced GPT available is GPT-3; and the most complex version of GPT-3 has over 175 billion parameters. Before the release of GPT-3 in May 2020, the most complex pre-trained NLP model was Microsoft\u2019s Turing NLG.", "GPT-3 can create very realistic text, which is sometimes difficult to distinguish from the human-generated text. That\u2019s why the engineers warned of the GPT-3\u2019s potential dangers and called for risk mitigation research. Here is a video about 14 cool apps built on GPT-3:", "As opposed to most other pre-trained NLP models, OpenAI chose not to share the GPT-3's source code. Instead, they allowed invitation-based API access, and you can apply for a license by visiting their website. Check it out:", "On September 22, 2020, Microsoft announced it had licensed \u201cexclusive\u201d use of GPT-3. Therefore, while others have to rely on the API to receive output, Microsoft has control of the source code. Here is brief info about its size and performance:", "BERT stands for Bidirectional Encoder Representations from Transformers, and it is a state-of-the-art machine learning model used for NLP tasks. Jacob Devlin and his colleagues developed BERT at Google in 2018. Devlin and his colleagues trained the BERT on English Wikipedia (2.5B words) and BooksCorpus (0.8B words) and achieved the best accuracies for some of the NLP tasks in 2018. There are two pre-trained general BERT variations: The base model is a 12-layer, 768-hidden, 12-heads, 110M parameter neural network architecture, whereas the large model is a 24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture. Figure 2 shows the visualization of the BERT network created by Devlin et al.", "Even though BERT seems more inferior to GPT-3, the availability of source code to the public makes the model much more popular among developers. You can easily load a BERT variation for your NLP task using the Hugging Face\u2019s Transformers library. Besides, there are several BERT variations, such as original BERT, RoBERTa (by Facebook), DistilBERT, and XLNet. Here is a helpful TDS post on their comparison:", "Here is brief info about BERT\u2019s size and performance:", "ELMo, short for Embeddings from Language Models, is a word embedding system for representing words and phrases as vectors. ELMo models the syntax and semantic of words as well as their linguistic context, and it was developed by the Allen Institute for Brain Science. There several variations of ELMo, and the most complex ELMo model (ELMo 5.5B) was trained on a dataset of 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008\u20132012 (3.6B). While both BERT and GPT models are based on transformation networks, ELMo models are based on bi-directional LSTM networks.", "Here is brief info about ELMo\u2019s size and performance:", "Just like BERT models, we also have access to ELMo source code. You can download the different variations of ELMos from Allen NLP\u2019s Website:", "Although there are several other pre-trained NLP models available in the market (e.g., GloVe), GPT, BERT, and ELMo are currently the best pre-trained models out there. Since this post aims to introduce these models, we will not have a code-along tutorial. But, I will share several tutorials where we exploit these very advanced pre-trained NLP models.", "In a world where we have easy access to state-of-the-art neural network models, trying to build your own model with limited resources is like trying to reinvent the wheel. It is pointless.", "Instead, try to work with these train models, add a couple of new layers on top considering your particular natural language processing task, and train. The results will be much more successful than a model you build from scratch.", "If you would like to have access to full code on Google Colab, and have access to my latest content, subscribe to the mailing list:\u2709\ufe0f", "If you are interested in deep learning, also check out the guide to my content on artificial intelligence:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write about AI and data apps here building them at Vizio.ai with my team. Feel free to get in touch!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb2e45c1c275b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a----b2e45c1c275b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "\u2190\u2190\u2190 Part 1"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e", "anchor_text": "\u2190\u2190 Part 2"}, {"url": "https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671", "anchor_text": "\u2190 Part 3"}, {"url": "https://unsplash.com/@codestorm?utm_source=medium&utm_medium=referral", "anchor_text": "Safar Safarov"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin"}, {"url": "https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc", "anchor_text": "4 Pre-Trained CNN Models to Use for Computer Vision with Transfer LearningUsing State-of-the-Art Pre-trained Neural Network Models to Tackle Computer Vision Problems with Transfer Learningtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/using-recurrent-neural-networks-to-predict-bitcoin-btc-prices-c4ff70f9f3e4", "anchor_text": "Recurrent Neural Networks"}, {"url": "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d", "anchor_text": "Convolutional Neural Networks"}, {"url": "https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b", "anchor_text": "GANs"}, {"url": "https://towardsdatascience.com/image-noise-reduction-in-10-minutes-with-convolutional-autoencoders-d16219d2956a", "anchor_text": "Autoencoders"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning"}, {"url": "https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/#:~:text=In%20the%20past%20few%20years,cost%20at%20least%20%244.6%20million.", "anchor_text": "According to BD Tech Talks"}, {"url": "https://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf", "anchor_text": "Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned."}, {"url": "http://papers.neurips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf", "anchor_text": "Discriminability-Based Transfer between Neural Networks"}, {"url": "https://www.springer.com/journal/10994", "anchor_text": "Machine Learning"}, {"url": "https://link.springer.com/journal/10994/28/1/page/1", "anchor_text": "special issue"}, {"url": "https://www.springer.com/gp/book/9780792380474", "anchor_text": "Learning to Learn"}, {"url": "https://en.wikipedia.org/wiki/Andrew_Ng", "anchor_text": "Andrew Ng"}, {"url": "https://www.youtube.com/watch?v=wjqaz6m42wU", "anchor_text": "Transfer learning will be the next driver of machine learning\u2019s commercial success after supervised learning."}, {"url": "http://image-net.org/", "anchor_text": "ImageNet"}, {"url": "https://dumps.wikimedia.org/", "anchor_text": "the Wikipedia Corpus"}, {"url": "https://keras.io/api/applications/", "anchor_text": "143,667,240 parameters"}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "image classification"}, {"url": "https://tfhub.dev/", "anchor_text": "TensorFlow Hub"}, {"url": "https://pytorch.org/hub/", "anchor_text": "PyTorch Hub"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671", "anchor_text": "Sentiment Analysis in 10 Minutes with BERT and Hugging FaceLearn the basics of the pre-trained NLP model, BERT, and build a sentiment classifier using the IMDB movie reviews\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "Open AI GPT Series"}, {"url": "https://medium.com/r?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1810.04805.pdf", "anchor_text": "BERT Variations"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT paper"}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "OpenAI APIWe're releasing an API for accessing new AI models developed by OpenAI. Unlike most AI systems which are designed for\u2026openai.com"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Bidirectional Encoder Representations from Transformers"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT paper"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s Transformers library"}, {"url": "http://towardsdatascience.com", "anchor_text": "TDS"}, {"url": "https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8", "anchor_text": "BERT, RoBERTa, DistilBERT, XLNet \u2014 which one to use?Google\u2019s BERT and recent transformer-based methods have taken the NLP landscape by a storm, outperforming the\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "Embeddings from Language Models"}, {"url": "https://allennlp.org/elmo", "anchor_text": "AllenNLPELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g\u2026allennlp.org"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://medium.us4.list-manage.com/subscribe?u=bf0e9524ea3d765ba10131675&id=52221072de", "anchor_text": "subscribe to the mailing list"}, {"url": "https://medium.us4.list-manage.com/subscribe?u=bf0e9524ea3d765ba10131675&id=52221072de", "anchor_text": "Subsribe Now"}, {"url": "https://oyalcin.medium.com/a-guide-to-my-content-on-artificial-intelligence-c70c9b4a3b17", "anchor_text": "A Guide to My Content on Artificial IntelligenceThe guide to help you navigate around my content with ease.oyalcin.medium.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b2e45c1c275b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b2e45c1c275b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b2e45c1c275b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----b2e45c1c275b---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----b2e45c1c275b---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----b2e45c1c275b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----b2e45c1c275b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb2e45c1c275b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b2e45c1c275b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b2e45c1c275b--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://blog.orhangaziyalcin.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6340e0deb03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-pre-trained-model-series-to-use-for-nlp-with-transfer-learning-b2e45c1c275b&newsletterV3=ff47ab81282a&newsletterV3Id=6340e0deb03&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}