{"url": "https://towardsdatascience.com/model-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec", "time": 1682993920.1995, "path": "towardsdatascience.com/model-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec/", "webpage": {"metadata": {"title": "Introduction to PyTorch Model Compression Through Teacher-Student Knowledge Distillation | by Moussa Taifi PhD | Towards Data Science", "h1": "Introduction to PyTorch Model Compression Through Teacher-Student Knowledge Distillation", "description": "Serving ML models in resource constrained mobile and real-time systems can be a real problem. This post is an example of Teacher-Student Knowledge Distillation on a recommendation task using PyTorch."}, "outgoing_paragraph_urls": [{"url": "https://www.re-work.co/events/deep-learning-summit-toronto-canada-2018/schedule#", "anchor_text": "Deep Learning Summit 2018", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Knowledge Distillation", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1809.07428", "anchor_text": "Ranking Distillation", "paragraph_index": 3}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "Movielens 100K dataset", "paragraph_index": 4}, {"url": "https://github.com/maciejkula/spotlight", "anchor_text": "Spotlight from Maciej Kula", "paragraph_index": 6}, {"url": "http://www.moussataifi.com/books", "anchor_text": "www.moussataifi.com/books", "paragraph_index": 21}], "all_paragraphs": ["Serving ML models in resource constrained mobile and real-time systems can be a real problem. The ML community has been developing solutions to compress the size of the models generated by larger clusters of servers. Model compression promises savings on the inference time, power efficiency and model size. All of that can let that flying rescue drone cover more land surface on a single battery charge, as well as not draining the batteries of your mobile app users.", "Model Knowledge distillation is a method used to reduce the size of a model without loosing too much of its predictive powers.", "Geoffrey Hinton\u2019s talk at the Deep Learning Summit 2018 about using Knowledge Distillation (KD) led me to look up the current state of the art for another class of problems: Recommender systems (RecSys).", "This led me to the excellent work published in KDD 2018 by Jiaxi Tang on Ranking Distillation (RD), where he discusses his method related to applying knowledge distillation to ranking tasks.", "In this blog I replicated a small part of this Ranking Distillation work on the Movielens 100K dataset. Working on that was a bit of a realization. Even if KD is a solid conceptual framework for distilling knowledge from one model to a small model, applying it on Ranking tasks for recommender systems is not a trivial task.", "The first challenge is that we are working at a lower level of abstraction than the usual fit/predict API that exists in higher level libraries such as Scikit-learn and Keras. This is because the change needed to implement this KD is at the loss function formulation itself. To tackle that, I followed on the footsteps of the RD paper and used the elegant PyTorch API for building this KD in RecSys.", "The second challenge is that even if PyTorch is an elegant library we need a higher level framework that specializes on RecSys with PyTorch. The framework of choice these days seems to be Spotlight from Maciej Kula. I highly recommend it, the API design is easy to use and it lets the user customize most aspects that we are going to need for this experiment.", "The goal is to generate 3 models: Student model, Student model with distillation, and Teacher Model from the Movielens 100K dataset and compare their MAP@K metric as well as their physical disk size.", "We need to explain the strategy we are going to use to teach some of that Dark Knowledge from the Teacher model to the Student model with distillation. Here is an attempt to explain what is going to happen during the training:", "In the above figure we show the training flow:", "First we need some training data, which we use to build a pre-trained Teacher model. We are using the movielens 100K dataset and only using the movie/user interaction. We will try to predict which top 5 movies a users is most probable to rate.", "For this we will use the ImplicitFactorizationModel that the Spotlight Library provides. This model uses an embeddings-based model structure:", "For the loss we use a method similar to the following negative logarithmic of the likelihood function. We sample both positive and negative pairs, and we ask the optimizer to improve the ranking items from the positive pairs (d+) and decrease items from the negative pairs (d-):", "Training a \u201clarge\u201d teach model with 200 as the size for each embedding layer on the movielens dataset give us the following metrics:", "Let\u2019s try the same with a much smaller model with 2 as the size of each embedding layer:", "This is what we try next. We train a third model which is the student model with a boost from the pre-trained Teacher model.", "To do that we need to mix the two losses we get from both model in the loss function. This is where PyTorch shines. All we have to do is define a modified loss function that sums up the student and teacher losses and let gradient descent do its magic. At its core if you are a bit familiar with the positive vs negative loss from using a log sigmoid loss, we pass the current batch of data through the teacher network and get candidate predictions, and use them to generate the teacher loss values. The final loss we use for our optimization is the sum of the three losses pos/neg/teacher. Here is a snippet of the combined loss function:", "Here is a table with all these values for comparison", "I invite you to dig deeper in the KDD2018 paper if you are interested in this type of cross model interactions. We did not cover how to improve this setup with weighting the Teacher\u2019s model loss, or by only considering the top-k recommendations of the teacher\u2019s model. Probably a future post.", "Tang Jiaxi, and Ke Wang. \u201cRanking Distillation: Learning Compact Ranking Models With High Performance for Recommender System.\u201d Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Data Science Platform Engineer \u2014 CS PhD\u2014 Cloudamize-Appnexus-Xandr-AT&T-Microsoft \u2014 Books: www.moussataifi.com/books"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5d81c0f2c0ec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://farmi.medium.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": ""}, {"url": "https://farmi.medium.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Moussa Taifi PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F215bedf6878d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=post_page-215bedf6878d----5d81c0f2c0ec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.re-work.co/events/deep-learning-summit-toronto-canada-2018/schedule#", "anchor_text": "Deep Learning Summit 2018"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Knowledge Distillation"}, {"url": "https://arxiv.org/abs/1809.07428", "anchor_text": "Ranking Distillation"}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "Movielens 100K dataset"}, {"url": "https://github.com/maciejkula/spotlight", "anchor_text": "Spotlight from Maciej Kula"}, {"url": "https://github.com/maciejkula/spotlight", "anchor_text": "https://github.com/maciejkula/spotlight"}, {"url": "https://pytorch.org/", "anchor_text": "https://pytorch.org/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5d81c0f2c0ec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/recommendation-system?source=post_page-----5d81c0f2c0ec---------------recommendation_system-----------------", "anchor_text": "Recommendation System"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----5d81c0f2c0ec---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----5d81c0f2c0ec---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=-----5d81c0f2c0ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=-----5d81c0f2c0ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5d81c0f2c0ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5d81c0f2c0ec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5d81c0f2c0ec--------------------------------", "anchor_text": ""}, {"url": "https://farmi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://farmi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Moussa Taifi PhD"}, {"url": "https://farmi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "412 Followers"}, {"url": "http://www.moussataifi.com/books", "anchor_text": "www.moussataifi.com/books"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F215bedf6878d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=post_page-215bedf6878d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F524d0004e06d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec&newsletterV3=215bedf6878d&newsletterV3Id=524d0004e06d&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}