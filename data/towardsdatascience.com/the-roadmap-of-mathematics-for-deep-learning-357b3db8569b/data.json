{"url": "https://towardsdatascience.com/the-roadmap-of-mathematics-for-deep-learning-357b3db8569b", "time": 1683015558.483244, "path": "towardsdatascience.com/the-roadmap-of-mathematics-for-deep-learning-357b3db8569b/", "webpage": {"metadata": {"title": "The Roadmap of Mathematics for Machine Learning | by Tivadar Danka | Towards Data Science", "h1": "The Roadmap of Mathematics for Machine Learning", "description": "Knowing the mathematics behind machine learning algorithms is a superpower. If you have ever built a model for a real-life problem, you probably experienced that being familiar with the details can\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/playlist?list=PL590CCC2BC5AF3BC1", "anchor_text": "Single Variable Calculus course from MIT", "paragraph_index": 17}, {"url": "https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/", "anchor_text": "Calculus book by Gilbert Strang", "paragraph_index": 17}, {"url": "http://linear.axler.net/", "anchor_text": "Linear Algebra Done Right", "paragraph_index": 61}, {"url": "https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D", "anchor_text": "Linear Algebra course from MIT OpenCourseWare,", "paragraph_index": 61}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "paragraph_index": 119}], "all_paragraphs": ["Knowing the mathematics behind machine learning algorithms is a superpower. If you have ever built a model for a real-life problem, you probably experienced that being familiar with the details can go a long way if you want to move beyond baseline performance. This is especially true when you want to push the boundaries of state of the art.", "However, most of this knowledge is hidden behind layers of advanced mathematics. Understanding methods like stochastic gradient descent might seem difficult since it is built on top of multivariable calculus and probability theory.", "With proper foundations, though, most ideas can be seen as quite natural. If you are a beginner and don\u2019t necessarily have formal education in higher mathematics, creating a curriculum for yourself is hard. In this post, my goal is to present a roadmap, taking you from absolute zero to a deep understanding of how neural networks work.", "To keep things simple, the aim is not to cover everything. Instead, we will focus on getting our directions. This way, you will be able to study other topics without difficulties, if need be.", "Instead of reading through in one sitting, I recommend using this article as a reference point through your studies. Go deep into a concept that is introduced, then check the roadmap and move on. I firmly believe that this is the best way to study: I will show you the road, but you must walk it.", "Most of machine learning is built upon three pillars: linear algebra, calculus, and probability theory. Since the last one builds on the first two, we should start with them. Calculus and linear algebra can be studied independently, as is usually the case in a standard curriculum.", "Calculus is the study of differentiation and integration of functions. Essentially, a neural network is a differentiable function, so calculus will be a fundamental tool to train neural networks, as we will see.", "To familiarize yourself with the concepts, you should make things simple and study functions of a single variable for the first time. By definition, the derivative of a function is defined by", "where the ratio for a given h is the slope of the line between the points (x, f(x)) and (x+h, f(x+h)).", "In the limit, this is essentially the slope of the tangent line at the point x. The figure below illustrates the concept.", "Differentiation can be used to optimize functions: the derivative is zero at local maxima or minima. (However, this is not true in the other direction; see f(x) = x\u00b3 at 0.) Points where the derivative is zero, are called critical points. Whether a critical point is minima or maxima can be decided by looking at the second derivative:", "There are several essential rules regarding differentiation, but probably the most important is the so-called chain rule:", "that tells us how to calculate the derivative of composed functions.", "Integration is often called the inverse of differentiation. This is true because", "which holds for any integrable function f(x). The integral of a function can also be thought of as the signed area under the curve. For instance,", "because when the function is negative, the area there also has a negative sign.", "Integration itself plays a role in understanding the concept of expected value. For instance, quantities like entropy and Kullback-Leibler divergence are defined in terms of integrals.", "I would recommend the Single Variable Calculus course from MIT. (In general, online courses from MIT are always excellent learning resources.) If you are more of a book person, there are many textbooks available. The Calculus book by Gilbert Strang accompanying the previously mentioned course is again a great resource, completely free of charge.", "As I mentioned, neural networks are essentially functions, which are trained using the tools of calculus. However, they are described with linear algebraic concepts like matrix multiplication.", "Linear algebra is a vast subject with many essential aspects of machine learning, so this will be a significant segment.", "To have a good understanding of linear algebra, I would suggest starting with vector spaces. It is better if we talk about a special case first. You can think of each point in the plane as a tuple", "These are essentially vectors pointing from zero to (x\u2081, x\u2082). You can add these vectors together and multiply them with scalars:", "This is the prototypical model of a vector space. In general, a set of vectors V is a vector space over the real numbers if you can add vectors together and multiply a vector with a real number, such that the following properties hold:", "Don\u2019t panic! I know that this looks very scary (at least it looked that to me when I was a freshman student with a math major), but it really isn\u2019t. These just guarantee that vectors can be added together and scaled just as you would expect. When thinking about vector spaces, it helps if you mentally model them as", "If you feel like you have a good understanding of vector spaces, the next step would be to understand how to measure a vector\u2019s magnitude. By default, a vector space in itself gives no tools for this. How would you do this on the plane? You have probably learned that there, we have", "This is a special case of a norm. In general, a vector space V is normed if there is a function", "Again, this might be scary, but this is a simple and essential concept. There are a bunch of norms out there, but the most important is the p-norm family", "(with p = 2 we obtain the special case mentioned above) and the supremum norm", "Sometimes, like for p = 2, the norm comes from a so-called inner product, which is a bilinear function", "A vector space with an inner product is called inner product space. An example is the classical Euclidean product", "Every inner product can be turned into a norm by", "When the inner product for two vectors is zero, we say that the vectors are orthogonal to each other. (Try to come up with some concrete examples on the plane to understand the concept deeper.)", "Although vector spaces are infinite (in our case), you can find a finite set of vectors that can be used to express all vectors in the space. For example, on the plane, we have", "This is a special case of basis and orthonormal basis.", "In general, basis is a minimal set of vectors", "such that their linear combinations span the vector space:", "A basis always exists for any vector space. (It may not be a finite set, but that shouldn\u2019t concern us now.) Without a doubt, a basis simplifies things greatly when talking about linear spaces.", "When the vectors in a basis are orthogonal to each other, we call it an orthogonal basis. If each basis vector\u2019s norm is 1 for an orthogonal basis, we say it is orthonormal.", "One of the key objects related to vector spaces are linear transformations. If you have seen a neural network before, you know that one of the fundamental building blocks are layers of the form", "where A is a matrix, b and x are vectors, and \u03c3 is the sigmoid function. (Or any activation function, really.) Well, the part Ax is a linear transformation. In general, the function", "is a linear transformation between vector spaces V and W if", "holds for all x, y in V, and all a real number.", "To give a concrete example, rotations around the origin in the plane are linear transformations.", "Undoubtedly, the most crucial fact about linear transformations is that they can be represented with matrices, as you\u2019ll see next in your studies.", "If linear transformations are clear, you can turn to the study of matrices. (Linear algebra courses often start with matrices, but I would recommend it this way for reasons to be explained later.)", "The most important operation for matrices is the matrix product. In general, if A and B are matrices defined by", "then their product can be obtained by", "This might seem difficult to comprehend, but it is actually quite straightforward. Take a look at the figure below, demonstrating how to calculate the element in the 2nd row, 1 st column of the product.", "The reason why matrix multiplication is defined the way it is because matrices represent linear transformations between vector spaces. Matrix multiplication is the composition of linear transformations.", "If you would like to read more about this, there is a great article right here at Towards Data Science, explaining things in detail.", "In my opinion, determinants are hands down one of the most challenging concepts to grasp in linear algebra. Depending on your learning resource, it is usually defined by either a recursive definition or a sum that iterates through all permutations. None of them are tractable without significant experience in mathematics.", "To understand this concept, watch this video below. Trust me, it is magic.", "To summarize, the determinant of a matrix describes how the volume of an object scales under the corresponding linear transformation. If the transformation changes orientations, the sign of the determinant is negative.", "You will eventually need to understand how to calculate the determinant, but I wouldn\u2019t worry about it now.", "A standard first linear algebra course usually ends with eigenvalues/eigenvectors and some special matrix decompositions like the Singular Value Decomposition.", "Let\u2019s suppose that we have a matrix A. The number \u03bb is an eigenvalue of A if there is a vector x (called eigenvector) such that", "holds. In other words, the linear transformation represented by A is a scaling by \u03bb for the vector x. This concept plays an essential role in linear algebra. (And practically in every field which uses linear algebra extensively.)", "At this point, you are ready to familiarize yourself with a few matrix decompositions. If you think about it for a second, what type of matrices are the best from a computational perspective? Diagonal matrices! If a linear transformation has a diagonal matrix, it is trivial to compute its value on an arbitrary vector.", "Most special forms aim to decompose a matrix A to a product of matrices, where preferably at least one is diagonal. Singular Value Decomposition, or SVD in short, the most famous one, states that there are special matrices U, V, and a diagonal matrix \u03a3 such that", "holds. (U and V are so-called unitary matrices, which I don\u2019t define here, suffice to know that it is a special family of matrices.)", "SVD is also used to perform Principal Component Analysis, one of the simplest and most well-known dimensionality reduction methods.", "Linear algebra can be taught in many ways. The path I outlined here was inspired by the textbook Linear Algebra Done Right by Sheldon Axler. For an online lecture, I would recommend the Linear Algebra course from MIT OpenCourseWare, an excellent resource.", "If a course might be too much, there are great articles out there, for instance, the following.", "This is the part where linear algebra and calculus come together, laying the foundations for the primary tool to train neural networks: gradient descent. Mathematically speaking, a neural network is simply a function of multiple variables. (Although, the number of variables can be in the millions.)", "Similar to univariable calculus, the two main topics here are differentiation and integration. Let\u2019s suppose that we have a function", "mapping vectors to real numbers. In two dimensions (that is, for n = 2), you can imagine its plot as a surface. (Since humans don\u2019t see higher than three dimensions, it is hard to visualize functions with more than two real variables.)", "In a single variable, the derivative was the slope of the tangent line. How would you define tangents here? A point on the surface has several tangents, not just one. However, there are two special tangents: one is parallel to the x-z plane, while the other is to the y-z plane. Their slope is determined by the partial derivatives, defined by", "That is, you take the derivative of the functions obtained by fixing all but one variable. (The formal definition is identical for \u2265 3 variables, just more complicated notation.)", "Tangents in these special directions span the tangent plane.", "There is another special direction: the gradient, which is the vector defined by", "The gradient always points to the direction of the largest increase! So, if you would take a tiny step in this direction, your elevation would be the maximal among all the other directions you could have chosen. This is the basic idea of gradient descent, which is an algorithm to maximize functions. Its steps are the following.", "Of course, there are several flaws in this basic algorithm, which was improved several times over the years. Modern gradient descent based optimizers employ many tricks like adaptive step size, momentum, and other methods, which we are not going to detail here.", "Calculating the gradient in practice is difficult. Functions are often described by the composition of other functions, for instance, the familiar linear layer", "where A is a matrix, b and x are vectors, and \u03c3 is the sigmoid function. (Of course, there can be are other activations, but we shall stick to this for simplicity.) How would you calculate this gradient? At this point, it is not even clear how to define the gradient for vector-vector functions such as this, so let\u2019s discuss! A function", "can always be written in terms of vector-scalar functions like", "The gradient of g is defined by the matrix whose k-th row is the k-th component\u2019s gradient. That is,", "This matrix is called the total derivative of g.", "things become a bit more complicated because it is the composition of two functions:", "defined by applying the univariate sigmoid componentwise. The function l can be decomposed further to m functions mapping from the n-dimensional vector space to the space of real numbers:", "If you calculate the total derivative, you\u2019ll see that", "This is the chain rule for multivariate functions in its full generality. Without it, there would be no easy way to calculate the gradient of a neural network, which is ultimately a composition of many functions.", "Similarly to the univariate case, the gradient and the derivatives play a role in determining whether a given point in space is local minima or maxima. (Or neither.) To provide a concrete example, training a neural network is equivalent to minimizing the loss function on the parameters\u2019 training data. It is all about finding the optimal parameter configuration w for which the minimum is attained:", "are the neural network and the loss function, respectively.", "For a general differentiable vector-scalar function of say n variables, there are n\u00b2 second derivatives, forming the Hessian matrix", "In multiple variables, the determinant of the Hessian takes the role of the second derivative. Similarly, it can be used to tell whether a critical point (that is, where all of the derivatives are zero) is minima, maxima, or just a saddle point.", "There are a lot of awesome online courses on multivariable calculus. I have two specific recommendations:", "Now we are ready to take on the final subject: probability theory!", "Probability theory is the mathematically rigorous study of chance, fundamental to all fields of science.", "Setting exact definitions aside, let\u2019s ponder a bit about what probability represents. Let\u2019s say I toss a coin, with a 50% chance (or 0.5 probability) of it being heads. After repeating the experiment 10 times, how many heads did I get?", "If you have answered 5, you are wrong. Heads being 0.5 probability doesn\u2019t guarantee that every second throw is heads. Instead, what it means that if you repeat the experiment n times where n is a really large number, the number of heads will be very close to n/2.", "To get a good grip on probability, I would recommend my post below, which I wrote a while ago, to offer a concise yet mathematically correct presentation of the concept.", "Besides the basics, there are some advanced things you need to understand, first and foremost, expected value and entropy.", "Supposed that you play a game with your friend. You toss a classical six-sided dice, and if the outcome is 1 or 2, you win 300 bucks. Otherwise, you lose 200. What are your average earnings per round if you play this game long enough? Should you be even playing this game?", "Well, you win 100 bucks with probability 1/3, and you lose 200 with probability 2/3. That is if X is the random variable encoding the result of the dice throw, then", "This is the expected value, that is, the average amount of money you will receive per round in the long run. Since this is negative, you will lose money, so you should never play this game.", "Generally speaking, the expected value is defined by", "In machine learning, loss functions for training neural networks are expected values in one way or another.", "People often falsely attribute certain phenomena to the law of large numbers. For instance, gamblers who are on a losing streak believe that they should soon win because of the law of large numbers. This is totally wrong. Let\u2019s see what this is really!", "are random variables representing the independent repetitions of the same experiment. (Say, rolling a dice or tossing a coin.)", "Essentially, the law of large numbers states that", "That is the average of the outcomes in the long run equal to the expected value.", "An interpretation is that if a random event is repeated enough times, individual results might not matter. So, if you are playing in a casino with a game that has negative expected value (as they all do), it doesn\u2019t matter that you win occasionally. The law of large numbers implies that you will lose money.", "To get a little bit ahead, LLN is going to be essential for stochastic gradient descent.", "Let\u2019s play a game. I have thought of a number between 1 and 1024, and you have to guess it. You can ask questions, but your goal is to use as few questions as possible. How much do you need?", "If you play it smart, you will perform a binary search with your questions. First you may ask: is the number between 1 and 512? With this, you have cut the search space in half. Using this strategy, you can figure out the answer in", "But what if I didn\u2019t use the uniform distribution when picking the number? For instance, I could have used a Poisson distribution.", "Here, you would probably need fewer questions because you know that the distribution tends to concentrate around specific points. (Which depends on the parameter.)", "In the extreme case, when the distribution is concentrated on a single number, you need zero questions to guess it correctly. Generally, the number of questions depends on the information carried by the distribution. The uniform distribution contains the least amount of information, while singular ones are pure information.", "Entropy is a way to quantify this. It is defined by", "for continuous, real-valued ones. (The base of the logarithm is usually 2, e, or 10, but it doesn\u2019t really matter.)", "If you have ever worked with classification models before, you probably encountered the cross-entropy loss, defined by", "where P is the ground truth (a distribution concentrated to a single class), while the hatted version represents the class predictions. This measures how much \u201cinformation\u201d the predictions have compared to the ground truth. When the predictions match, the cross-entropy loss is zero.", "Another frequently used quantity is the Kullback-Leibler divergence, defined by", "where P and Q are two probability distributions. This is essentially cross-entropy minus the entropy, which can be thought of as quantifying how different the two distributions are. This is useful, for instance, when training generative adversarial networks. Minimizing the Kullback-Leibler divergence guarantees that the two distributions are similar.", "Here, I would recommend two books for you:", "These are the two fundamental textbooks, and they teach you much more than probability theory. Both of them go way beyond the basics, but the corresponding chapters provide an excellent introduction.", "With this, we reviewed the necessary mathematics for understanding neural networks. Now, you are ready for the fun part: machine learning!", "To really understand how neural networks work, you still have to learn some optimization and mathematical statistics. These subjects build upon the foundations we set. I won\u2019t go into details, since this is beyond the scope of the article, but I have prepared a study roadmap to guide you.", "If you would like to read more about some of these topics, check out some of my articles below!", "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I want to democratize machine learning. Math PhD with an INTJ personality. Chaotic good."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F357b3db8569b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----357b3db8569b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----357b3db8569b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2----357b3db8569b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/playlist?list=PL590CCC2BC5AF3BC1", "anchor_text": "Single Variable Calculus course from MIT"}, {"url": "https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/", "anchor_text": "Calculus book by Gilbert Strang"}, {"url": "https://towardsdatascience.com/why-is-linear-algebra-taught-so-badly-5c215710ca2c", "anchor_text": "Why is Linear Algebra Taught So Badly?Linear algebra is one of the cornerstones of machine learning. It\u2019s more intuitive than you might thinktowardsdatascience.com"}, {"url": "http://linear.axler.net/", "anchor_text": "Linear Algebra Done Right"}, {"url": "https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D", "anchor_text": "Linear Algebra course from MIT OpenCourseWare,"}, {"url": "https://towardsdatascience.com/mathematics-for-ai-linear-algebra-and-how-to-understand-it-better-63b430999069", "anchor_text": "Mathematics for AI: Linear Algebra and How to Understand It BetterHow it works, where it is used and how to learn it faster.towardsdatascience.com"}, {"url": "https://www.youtube.com/playlist?list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7", "anchor_text": "Khan Academy on multivariable calculus"}, {"url": "https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38", "anchor_text": "MIT multivariable calculus"}, {"url": "https://towardsdatascience.com/the-mathematical-foundations-of-probability-beb8d8426651", "anchor_text": "The mathematical foundations of probabilityA measure-theoretic introductiontowardsdatascience.com"}, {"url": "https://en.wikipedia.org/wiki/Poisson_distribution", "anchor_text": "Wikipedia"}, {"url": "https://www.springer.com/gp/book/9780387310732", "anchor_text": "Pattern Recognition and Machine Learning by Christopher Bishop"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman"}, {"url": "https://towardsdatascience.com/the-statistical-foundations-of-machine-learning-973c356a95f", "anchor_text": "The statistical foundations of machine learningA look beyond function fittingtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-to-build-a-diy-deep-learning-framework-in-numpy-59b5b618f9b7", "anchor_text": "How to build a DIY deep learning framework in NumPyUnderstanding the fine details of neural nets by building one from scratchtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/the-mathematics-of-optimization-for-deep-learning-11af2b1fda30", "anchor_text": "The mathematics of optimization for deep learningA brief guide about how to minimize a function with millions of variablestowardsdatascience.com"}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----357b3db8569b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----357b3db8569b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----357b3db8569b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----357b3db8569b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----357b3db8569b---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&user=Tivadar+Danka&userId=26fd873de5f2&source=-----357b3db8569b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&user=Tivadar+Danka&userId=26fd873de5f2&source=-----357b3db8569b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F357b3db8569b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----357b3db8569b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----357b3db8569b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----357b3db8569b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----357b3db8569b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----357b3db8569b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----357b3db8569b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/@tivadar.danka/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F471a6c88bdce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-roadmap-of-mathematics-for-deep-learning-357b3db8569b&newsletterV3=26fd873de5f2&newsletterV3Id=471a6c88bdce&user=Tivadar+Danka&userId=26fd873de5f2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}