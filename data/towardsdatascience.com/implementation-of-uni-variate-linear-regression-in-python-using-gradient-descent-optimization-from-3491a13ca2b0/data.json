{"url": "https://towardsdatascience.com/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0", "time": 1682994298.170374, "path": "towardsdatascience.com/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0/", "webpage": {"metadata": {"title": "Implementation of Uni-Variate Polynomial Regression in Python using Gradient Descent Optimization from scratch | by Navoneel Chakrabarty | Towards Data Science", "h1": "Implementation of Uni-Variate Polynomial Regression in Python using Gradient Descent Optimization from scratch", "description": "Regression is an approach of Continuous Classification of Data or data-points in feature-space. Francis Galton invented the usage of Regression Line in 1886 [1]. It is a particular case of Polynomial\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Regression is an approach of Continuous Classification of Data or data-points in feature-space. Francis Galton invented the usage of Regression Line in 1886 [1].", "It is a particular case of Polynomial Regression, where the degree of the polynomial in the hypothesis is 1. General Polynomial Regression is discussed in the later half of the article. As the name suggests, \u201cLinear\u201d, this means that the hypothesis regarding the Machine Learning Algorithm is linear in nature or simply a linear equation. Yeah!! it\u2019s a linear equation indeed. In Uni-Variate Linear Regression, there is a single feature or variable on which the Target Variable depends upon.", "The hypothesis for Uni-Variate Linear Regression is given below:", "The above hypothesis can also be written in Matrix Multiplication format or in terms of Vector Algebra as:", "There is a cost function associated with the hypothesis dependent upon parameters, theta_0 and theta_1.", "The cost function for Linear Regression in general is given below:", "Now these 2 parameters, theta_0 and theta_1 has to assume such values that the value of this cost function (i.e., the cost) assumes a minimum value possible. So, the basic objective now is to find the values of theta_0 and theta_1 for which the cost is minimum or to simply find the minima of the Cost Function.", "Gradient Descent is one of the most prominent Convex Optimization Technique using which minima of the functions can be found. The Gradient Descent Algorithm is given below:", "There are 2 approaches of Gradient Descent:", "Implementation of Linear Regression using Stochastic Gradient Descent:", "In Stochastic Gradient Descent, the Gradient Descent Algorithm is run, taking a single instance from the data-set at a time.", "The implementation is done by creating 3 modules having different operations:", "=>hypothesis(): It is the function that calculates and outputs the hypothesis value of the Target Variable, given theta (theta_0 and theta_1) and Feature, X as input. The implementation of hypothesis() is given below:", "=>SGD(): It is the function that performs the Stochastic Gradient Descent Algorithm taking current values of theta_0 and theta_1, alpha, number of iterations (num_iters), hypothesis value(h), feature set (X) and Target Variable set (y) as input and outputs the optimized theta (theta_0 and theta_1) at each iteration characterized by an instance. The implementation of SGD() is given below:", "=>sgd_linear_regression(): It is the principal function that takes the feature set (X), Target Variable set (y), learning rate and number of iterations (num_iters) as input and outputs the final optimized theta i.e., the values of theta_0 and theta_1 for which the cost function almost achieves minima following Stochastic Gradient Descent.", "Now, moving on to a practical practice dataset containing information about how Profit of a Company depends on the Population of a City. The Data-set is available on GitHub link,", "Problem Statement: \u201cGiven the population of a city, analyze and predict the profit of a company using Linear Regression\u201d", "Data Reading into Numpy Arrays :", "Data Visualization: The dataset can be visualised using a Scatter Plot:", "The Scatter Plot Data Visualization looks like -", "The theta output comes out to be:", "Visualization of theta on Scatter Plot:", "The Regression Line Visualization of the obtained theta can be done on Scatter Plot:", "The Regression Line Visualization comes out to be:", "Implementation of Linear Regression using Batch Gradient Descent:", "In Batch Gradient Descent, the Gradient Descent Algorithm is run, taking all the instances from the data-set at once.", "The implementation is done by creating 3 modules having different operations:", "=>hypothesis(): It is the function that calculates and outputs the hypothesis value of the Target Variable, given theta (theta_0 and theta_1) and Feature, X as input. The implementation of hypothesis() remains the same.", "=>BGD(): It is the function that performs the Batch Gradient Descent Algorithm taking current values of theta_0 and theta_1, alpha, number of iterations (num_iters), list of hypothesis values of all samples(h), feature set (X) and Target Variable set (y) as input and outputs the optimized theta (theta_0 and theta_1), theta_0 history (theta_0) and theta_1 history (theta_1) i.e., the value of theta_0 and theta_1 at each iteration and finally the cost history which contains the value of the cost function over all the iterations. The implementation of Gradient_Descent() is given below:", "=>linear_regression(): It is the principal function that takes the feature set (X), Target Variable set (y), learning rate and number of iterations (num_iters) as input and outputs the final optimized theta i.e., the values of theta_0 and theta_1 for which the cost function almost achieves minima following Batch Gradient Descent and cost which stores the value of cost for every iteration.", "Using the 3-module-Linear Regression-BGD on the same Profit Estimation Dataset:", "The theta output comes out to be:", "Visualization of theta on Scatter Plot:", "The Regression Line Visualization of the obtained theta can be done on Scatter Plot:", "The Regression Line Visualization comes out to be:", "Also, the cost has been reduced in the course of Batch Gradient Descent iteration-by-iteration. The reduction in the cost is shown with the help of Line Curve and Surface Plot.", "Line Curve for representing reduction in cost in 300 iterations:", "The line curve comes out to be:", "Surface Plot for representing reduction in cost:", "The Model Performance Analysis is done on the following metrics:", "=>Mean Absolute Error: Average of the mod(differences) between the predictions and actual observations over a sample of instances.", "=>Mean Square Error: Average of the squared differences between the predictions and actual observations over sample of instances.", "=>Root Mean Square Error: Square root of the average of the squared differences between the predictions and actual observations over a sample of instances.", "=>R-Square Score or Coefficient of Determination:", "So, Batch Gradient Descent is a Clear Winner over Stochastic Gradient Descent in all respects !!", "That\u2019s all about the Implementation of Uni-Variate Linear Regression in Python using Gradient Descent from Scratch.", "In Predictive Analytics Problems involving Regression on a Single Feature or Variable (known as Uni-Variate Regression), Polynomial Regression is an important variant of Regression Analysis that serves as a performance booster mainly over Linear Regression. In this article, I will be going through Polynomial Regression, its Python Implementation from Scratch and Application on a Practical Problem and Performance Analysis.", "As the prefix, \u201cPolynomial\u201d suggests, the corresponding hypothesis of the Machine Learning Algorithm is a Polynomial or a Polynomial Equation. So, this can be of any degree, like if the hypothesis is a 1st degree Polynomial, then it is a Linear Equation, and hence called Linear Regression, if the hypothesis is a 2nd degree Polynomial, then it is a Quadratic Equation, similarly if 3rd degree Polynomial, then it is a Cubic Equation and so on. So, it can be stated that:", "\u201cLinear Regression is a proper subset or special-case approach of Polynomial Regression, so Polynomial Regression is also called Generalized Regression\u201d", "The hypothesis for Polynomial Regression is given below:", "where theta_0, theta_1, theta_2, theta_3,\u2026., theta_n are the parameters and x is the single feature or variable", "The above hypothesis can also be written in Matrix Multiplication format or in terms of Vector Algebra as:", "Here, also there is a cost function associated with the hypothesis dependent upon parameters, theta_0, theta_1, theta_2, theta_3,\u2026., theta_n.", "The cost function for Generalized Regression in general is given below:", "So, these parameters, theta_0, theta_1, theta_2, \u2026, theta_n have to assume such values for which the cost function (or simply cost) reaches to its minimum value possible. In other words, the minima of the Cost Function needs to be found out.", "Batch Gradient Descent can be used as the Optimization Function.", "Implementation of Polynomial Regression using Batch Gradient Descent:", "The implementation is done by creating 3 modules performing different operations.", "=>hypothesis(): It is the function that calculates and outputs the hypothesis value of the Target Variable, given theta (theta_0, theta_1, theta_2, theta_3, \u2026., theta_n), Feature X and degree of Polynomial in Polynomial Regression (n) as input. The implementation of hypothesis() is given below:", "=>BGD(): It is the function that performs the Batch Gradient Descent Algorithm taking current value of theta (theta_0, theta_1,\u2026, theta_n), learning rate (alpha), number of iterations (num_iters), list of hypothesis values of all samples (h), feature set (X), Target Variable set (y) and Degree of Polynomial in Polynomial Regression (n) as input and outputs the optimized theta (theta_0, theta_1, theta_2, theta_3, \u2026, theta_n), theta_history (list containing value of theta for every iteration) and finally the cost history (cost) which contains the value of the cost function over all the iterations. The implementation of BGD() is given below:", "=>poly_regression(): It is the principal function that takes the feature set (X), Target Variable set (y), learning rate (alpha), degree of the polynomial in polynomial regression (n) and number of iterations (num_iters) as input and outputs the final optimized theta i.e., the values of [theta_0, theta_1, theta_2, theta_3,\u2026.,theta_n] for which the cost function almost achieves minima following Batch Gradient Descent, theta_history which stores the values of theta for every iteration and cost which stores the value of cost for every iteration.", "Now, moving on to a practical practice dataset containing information about how Profit of a Company depends on the Population of a City.", "Using the 3-module Polynomial Regression on the Profit Estimation Dataset available at,", "Problem Statement: \u201cGiven the population of a city, analyze and predict the profit of a company using Polynomial Regression\u201d", "Visualization of theta on Scatter Plot:", "The Regression Line Visualization of the obtained theta can be done on Scatter Plot:", "The Regression Line Visualization comes out to be:", "Also, the cost has been reduced in the course of Batch Gradient Descent iteration-by-iteration. The reduction in the cost is shown with the help of Line Curve.", "The line curve comes out to be:", "Now, Model Performance Analysis along with Comparison of Polynomial Regression with Linear Regression (with same number of iterations) have to be done.", "So, Linear Regression is done using Batch Gradient Descent with 3,00,000 iterations and 0.0001 learning rate (alpha) using the implementation in my previous article:", "Visualization of theta on Scatter Plot:", "The Regression Line Visualization of the obtained theta can be done on Scatter Plot:", "Also, the cost has been reduced in the course of Batch Gradient Descent iteration-by-iteration. The reduction in the cost is shown with the help of Line Curve.", "The line curve comes out to be:", "Performance Analysis (Linear Regression Vs Quadratic Regression using BGD Optimization):", "But here, using Batch Gradient Descent Optimization, we end up with Linear Regression outperforming Polynomial(Quadratic) Regression in all respects. But, in practice, always Polynomial (higher degree or generalized) Regression performs better than Linear Regression. Although using BGD, we cannot obtain experimental results that can match the proposition because of some drawbacks of BGD Optimization itself. There is another method of optimization or finding the minima of the Cost Function in Polynomial (also Linear) Regression known as OLS (Ordinary Least Squares) or Normal Equation Method.", "I will be discussing these in my later articles, mentioning reference to this article. Using OLS, it can be clearly proved that the Polynomial (Quadratic as of in this experiment) performs better than Linear Regression. Apart from Uni-Variate Problems, Polynomial Regression can also be used in Multi-Variate Problems (multiple features) using appropriate Feature Engineering Techniques.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Mining | Data Analytics | Machine Learning | Financial Data Science | Natural Language Processing | Deep Learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3491a13ca2b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nc2012.medium.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": ""}, {"url": "https://nc2012.medium.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Navoneel Chakrabarty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7384b8693848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&user=Navoneel+Chakrabarty&userId=7384b8693848&source=post_page-7384b8693848----3491a13ca2b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/navoneel1092283/univariate_regression", "anchor_text": "navoneel1092283/univariate_regressionContribute to navoneel1092283/univariate_regression development by creating an account on GitHub.github.com"}, {"url": "https://github.com/navoneel1092283/univariate_regression", "anchor_text": "navoneel1092283/univariate_regressionContribute to navoneel1092283/univariate_regression development by creating an account on GitHub.github.com"}, {"url": "https://towardsdatascience.com/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0", "anchor_text": "Implementation of Uni-Variate Linear Regression in Python using Gradient Descent Optimization from\u2026Learn, Code and Tune\u2026.towardsdatascience.com"}, {"url": "https://en.wikipedia.org/wiki/Johns_Hopkins_University_Press", "anchor_text": "Johns Hopkins University Press"}, {"url": "https://en.wikipedia.org/wiki/International_Standard_Book_Number", "anchor_text": "ISBN"}, {"url": "https://en.wikipedia.org/wiki/Special:BookSources/0-8018-7403-3", "anchor_text": "0\u20138018\u20137403\u20133"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3491a13ca2b0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/calculus?source=post_page-----3491a13ca2b0---------------calculus-----------------", "anchor_text": "Calculus"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----3491a13ca2b0---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/python3?source=post_page-----3491a13ca2b0---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3491a13ca2b0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&user=Navoneel+Chakrabarty&userId=7384b8693848&source=-----3491a13ca2b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&user=Navoneel+Chakrabarty&userId=7384b8693848&source=-----3491a13ca2b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3491a13ca2b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3491a13ca2b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3491a13ca2b0--------------------------------", "anchor_text": ""}, {"url": "https://nc2012.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nc2012.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Navoneel Chakrabarty"}, {"url": "https://nc2012.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "255 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7384b8693848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&user=Navoneel+Chakrabarty&userId=7384b8693848&source=post_page-7384b8693848--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6e8a2f5e5fc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0&newsletterV3=7384b8693848&newsletterV3Id=6e8a2f5e5fc0&user=Navoneel+Chakrabarty&userId=7384b8693848&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}