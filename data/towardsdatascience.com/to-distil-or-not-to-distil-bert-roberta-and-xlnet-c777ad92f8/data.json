{"url": "https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8", "time": 1683003705.773351, "path": "towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8/", "webpage": {"metadata": {"title": "To Distil or Not To Distil: BERT, RoBERTa, and XLNet | by Thilina Rajapakse | Towards Data Science", "h1": "To Distil or Not To Distil: BERT, RoBERTa, and XLNet", "description": "This is becoming a bit of a cliche, but Transformer models have transformed Natural Language Processing. BERT (and friends) have swept away previously set benchmarks in nearly every common NLP task\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 2}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face Transformers", "paragraph_index": 2}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html", "anchor_text": "Matthews correlation coefficient", "paragraph_index": 22}, {"url": "https://app.wandb.ai/thilina/ag-news-transformers-comparison?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 32}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 39}], "all_paragraphs": ["This is becoming a bit of a cliche, but Transformer models have transformed Natural Language Processing. BERT (and friends) have swept away previously set benchmarks in nearly every common NLP task. This success, and the ensuing popularity, has seen the development of a whole host of new models ( aforementioned friends) based on the BERT architecture and training techniques. Indeed we have been blessed with so many new models that there is some confusion about which model should be picked for which task. I\u2019ve been asked this question many times but in most cases, the answer is a definite \u201cit depends\u201d.", "With that in mind, I will not be attempting to choose the best model but rather will aim to provide a comparison between several of the most common ones, particularly in terms of final accuracy and training time. As such, I will be using the same hyperparameters to train all the models, meaning that this is not necessarily the best-case scenario for each (or any) model. However, I hope that this comparison will give you a feel for how each model typically performs under the same circumstances.", "The experiment is performed using the Simple Transformers library, which is aimed at making Transformer models easy and straightforward to use. This library is built on top of the popular Hugging Face Transformers library.", "If you wish to follow along with the experiment, you can get the environment ready in a few short steps;", "Note that I will be using both CUDA and FP16 training (with Apex).", "The AG News dataset is used to train and evaluate the models. To get the dataset ready;", "The training time of any machine learning model will depend heavily on the hardware used so your mileage may vary! I\u2019ve attached the hardware I used for comparison.", "Of the two GPUs listed above, all experiments were run only on the NVIDIA Titan RTX GPU. Also, the only task performed by this GPU during the experiments was the experiment itself (everything including OS GUI stuff was handled by the RTX 2080) in order to ensure that all models had access to the same hardware resources at all times.", "We\u2019ll be trying out the models given below.", "A vital part of successfully training a good model is to get the hyperparameters right. In my experience, most pre-trained Transformer models will converge and give good results as long as the hyperparameter values are reasonable. Therefore, the values given below can be considered to be reasonable defaults for all the models, rather than optimized for any particular model.", "A quick explanation of the chosen values;", "Transformer models are constrained to a maximum number of tokens per input example. A token is a \u201cword\u201d (not necessarily a proper English word) present in the vocabulary of the model. Any input text is split until the entire input is represented by tokens that are present in the model\u2019s vocabulary. If the resulting representation contains more than the maximum number of tokens allowed, it will be truncated to the maximum length. If shorter, it will be padded up to the maximum length.", "Since all but a handful of examples in the AG News dataset are less than 256 tokens in length, I decided to go with this value. Note that increasing the maximum length increases resource consumption.", "Fairly obvious, each model will be trained for a maximum of 5 epochs where an epoch is a single pass over all the training data.", "The number of examples that will be processed in parallel during training. Larger batch sizes tend to reduce training time (up to a certain point until the compute cores of the GPU are fully utilized) but consumes more GPU memory. In general, it is safe to use the largest batch size that your GPU can handle.", "Controls how large a step is taken when updating model weights during training.", "Early stopping is a technique used to prevent machine learning models from overfitting to training data. The general idea is to terminate training once the model stops improving its performance on the validation/test data. The patience is how many steps to wait before termination. With a patience of 3, we will terminate training if the evaluation loss does not improve for 3 consecutive evaluations.", "We are setting a manual seed value of 4 to ensure that the results can be reproduced.", "This script can be used to train each of the models on the AG News datasets. This will also send the training progress to Weights & Biases for easy visualization. You can disable this by removing the lines wandb_project and wandb_kwargs in the train_args. The training progress can then be obtained from the files x-y-training_progress_scores.csv where x and y are model_type andmodel_name respectively. Visualization is also supported with Tensorboard (the runs are saved to runs/).", "The script expects two arguments, model_type and model_name. Here, model_type is the Transformer architecture and the model_name is a specific pre-trained model of that architecture. The bash script below will run the training for each of the models used in this experiment.", "We will train each of the models on the AG News dataset and compare their performance.", "The hyperparameters specified earlier will be used for each model. Importantly, we will be using early stopping to ensure the models don\u2019t overfit to the training data and this gives us some additional insight into how quickly each model tends to converge as well.", "The metric used for evaluation is the Matthews correlation coefficient (MCC). The MCC score of the model on the evaluation dataset will be calculated every 1000 training steps and at the end of every training epoch.", "A word to the wise: Results may vary depending on the datasets used and the chosen hyperparameters. This is not, and is not intended as, a conclusive benchmark of the models.", "Now that I\u2019ve (hopefully) given enough caveats to protect myself from accusations of blasphemy from ardent academics, let\u2019s get to it!", "First, a look at the runtimes for each model. Note that this takes into account both the training time and the time taken for the evaluations performed during training.", "Also, the models that reach peak performance faster will have shorter runtimes because of early stopping.", "In order to see the whole picture, it is important to take the effect of early stopping into consideration.", "The number of steps for convergence exhibits the same trend.", "Plotting the total number of steps:", "All of these models perform quite well on the AG News dataset.", "As stated earlier, all the models demonstrate good performance on the test data which is a testament to the ability of Transformer models in NLP tasks.", "These visualizations (and more) can be found here.", "While there are noticeable differences in terms of performance and training time across the various Transformer models, the best one to use often comes down to the specific dataset, task, and the requirements.", "That said, we can draw some general rules of thumb (thumbs?).", "Based on these observations (and my personal experience), I would generally recommend starting out with the distilled models and moving onto to the base models and/or XLNet if the performance is not satisfactory.", "The \u201clarge\u201d versions of these models are another option but I find that they are usually harder to train properly and certainly take a lot longer to train due to their size.", "Thoughts? Experiences? Insights? Please do share if you have any!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc777ad92f8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----c777ad92f8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@victoriano?utm_source=medium&utm_medium=referral", "anchor_text": "Victoriano Izquierdo"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face Transformers"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "here"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz", "anchor_text": "Download"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html", "anchor_text": "Matthews correlation coefficient"}, {"url": "https://app.wandb.ai/thilina/ag-news-transformers-comparison?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c777ad92f8---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c777ad92f8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c777ad92f8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c777ad92f8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----c777ad92f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----c777ad92f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc777ad92f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c777ad92f8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c777ad92f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c777ad92f8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c777ad92f8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c777ad92f8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c777ad92f8--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}