{"url": "https://towardsdatascience.com/lightgbm-800340f21415", "time": 1683004093.518166, "path": "towardsdatascience.com/lightgbm-800340f21415/", "webpage": {"metadata": {"title": "LightGBM. The one who said the King was naked | by Manu Joseph | Towards Data Science", "h1": "LightGBM", "description": "XGBoost reigned king for a while, both in accuracy and performance, until a contender rose to the challenge. LightGBM came out from Microsoft Research as a more efficient GBM which was the need of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/", "anchor_text": "XGBoost", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/NP-hardness", "anchor_text": "NP-Hard", "paragraph_index": 11}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "The Gradient Boosters", "paragraph_index": 27}], "all_paragraphs": ["XGBoost reigned king for a while, both in accuracy and performance, until a contender rose to the challenge. LightGBM came out from Microsoft Research as a more efficient GBM which was the need of the hour as datasets kept growing in size. LightGBM was faster than XGBoost and in some cases gave higher accuracy as well. Although XGBoost made some changes and implemented the innovations LightGBM brought forward and caught up, LightGBM had already made it\u2019s splash. It became a staple component in the winning ensembles in many Kaggle Competitions.", "The starting point for the LightGBM was XGBoost. So essentially, they took XGBoost and optimized it, and therefore, it has all the innovations XGBoost had (more or less), and some additional ones. Let\u2019s take a look at the incremental improvements that LightGBM made:", "One of the main changes from all the other GBMs, like XGBoost, is the way tree is constructed. In LightGBM, a leaf-wise tree growth strategy is adopted.", "All the other popular GBM implementations follow somehting called a Level-wise tree growth, where you find the best possible node to split and you split that one level down. This strategy will result in symmetric trees, where every node in a level will have child nodes resulting in an additional layer of depth.", "In LightGBM, the leaf-wise tree growth finds the leaves which will reduce the loss the maximum, and split only that leaf and not bother with the rest of the leaves in the same level. This results in an asymmetrical tree where subsequent splitting can very well happen only on one side of the tree.", "Leaf-wise tree growth strategy tend to achieve lower loss as compared to the level-wise growth strategy, but it also tends to overfit, especially small datasets. So small datasets, the level-wise growth acts like a regularization to restrict the complexity of the tree, where as the leaf-wise growth tends to be greedy.", "Subsampling or Downsampling is one of the ways with which we introduce variety and speed up the training process in an ensemble. It is also a form of regularization as it restricts from fitting to the complete training data. Usually, this subsampling is done by taking a random sample from the training dataset and building a tree on that subset. But what LightGBM introduced was an intelligent way of doing this downsampling.", "The core of the idea is that the gradients of different samples is an indicator to how big of a role does it play in the tree building process. The instances with larger gradients (under-trained), contribute a lot more to the tree building process than instances with small gradients. So, when we downsample, we should strive to keep the instances with large gradients so that the tree building is the most efficient.", "The most straightforward idea is to discard the instances with low gradients and build the tree on just the large gradient instances. But this would change the distribution of the data which in turn would hurt the accuracy of the model. And hence, the GOSS method.", "If we select a instances with large gradients and randomly samples b instances with small gradients, we amplify the sampled data by (1-a)/b", "The motivation behind EFB is a common theme between LightGBM and XGBoost. In many real world problems, although there are a lot of features, most of them are really sparse, like on-hot encoded categorical variables. The way LightGBM tackles this problem is slightly different.", "The crux of the idea lies in the fact that many of these sparse features are exclusive, i.e. they do not take non-zero values simultaneously. And we can efficiently bundle these features and treat them as one. But finding the optimal feature bundles is an NP-Hard problem.", "To this end, the paper proposes a Greedy Approximation to the problem, which is the Exclusive Feature Bundling algorithm. The algorithm is also slightly fuzzy in nature, as it will allow bundling features which are not 100% mutually exclusive, but it tries to maintain the balance between accuracy and efficiency when selecting the bundles.", "The algorithm, on a high level, is:", "The amount of time it takes to build a tree is proportional to the number of splits that have to be evaluated. And when you have continuous or categorical features with high cardinality, this time increases drastically. But most of the splits that can be made for a feature only offer miniscule changes in performance. And this concept is why a histogram based method is applied to tree building.", "The core idea is to group features into set of bins and perform splits based on these bins. This reduces the time complexity from O(#data) to O(#bins).", "In another innovation, similar to XGBoost, LightGBM ignores the zero feature values while creating the histograms. And this reduces the cost of building the histogram from O(#data) to O(#non-zero data).", "In many real world datasets, Categorical features make a big presence and thereby it becomes essential to deal with them appropriately. The most common approach is to represent a categorical feature as it\u2019s one-hot representation, but this is sub-optimal for tree learners. If you have high-cardinality categorical features, your tree needs to be very deep to achieve accuracy.", "LightGBM takes in a list of categorical features as an input so that it can deal with it more optimally. It takes inspiration from \u201cOn Grouping for Maximum Homogeneity\u201d by Fisher, Walter D. and uses the following methodology for finding the best split for categorical features.", "There are a few hyperparameters which help you tune the way the categorical features are dealt with[4]:", "The majority of the incremental performance improvements were made through GOSS and EFB.", "xgb_exa is the original XGBoost, xgb_his is the histogram based version(which came out later), lgb_baseline is the LightGBM without EFB and GOSS, and LightGBM is with EFB and GOSS. It is quite evident that the improvement in GOSS and EFB is phenomenal as compared to lgb_baseline.", "The rest of the improvements in performance is derived from the ability to parallelize the learning. There are two main ways of parallelizing the learning process:", "Feature Parallel tries to parallelize the \u201cFind the best split\u201d part in a distributed manner. Evaluating different splits are done in parallel across multiple workers, and then they communicate with each other to decide among themselves who has the best split.", "Data Parallel tries to parallelize the whole decision learning. In this, we typically split the data and send different parts of the data to different workers who calculate the histograms based on the section of the data they receive. Then they communicate to merge the histogram at a global level and this global level histogram is what is used in the tree learning process.", "Voting Parallel is a special case of Data Parallel, where the communication cost in Data Parallel is capped to a constant.", "LightGBM is one of those algorithms which has a lot, and I mean a lot, of hyperparameters. It is so flexible that it is intimidating for the beginner. But there is a way to use the algorithm and still not tune like 80% of those parameters. Let\u2019s look at a few parameters that you can start tuning and then build up confidence and start tweaking the rest.", "Other Articles in The Gradient Boosters", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F800340f21415&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----800340f21415--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----800340f21415--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@manujosephv?source=post_page-----800340f21415--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=post_page-----800340f21415--------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5----800340f21415---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "THE GRADIENT BOOSTERS"}, {"url": "https://unsplash.com/photos/-IZ2sgQKIhM", "anchor_text": "Unsplash"}, {"url": "https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/", "anchor_text": "XGBoost"}, {"url": "https://en.wikipedia.org/wiki/NP-hardness", "anchor_text": "NP-Hard"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#objective", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#boosting", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#learning_rate", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#num_leaves", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#max_depth", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#min_data_in_leaf", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#min_sum_hessian_in_leaf", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#lambda_l1", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#lambda_l2", "anchor_text": "\ud83d\udd17\ufe0e"}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "The Gradient Boosters"}, {"url": "https://towardsdatascience.com/the-good-old-gradient-boosting-f4614b0e62b0", "anchor_text": "The Good Old Gradient Boosting"}, {"url": "https://towardsdatascience.com/regularized-greedy-forest-a17cd0c85f06", "anchor_text": "Regularized Greedy Forest"}, {"url": "https://towardsdatascience.com/xgboost-b736c2ab10ce", "anchor_text": "XGBoost"}, {"url": "https://towardsdatascience.com/lightgbm-800340f21415", "anchor_text": "LightGBM"}, {"url": "http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf", "anchor_text": "On Grouping for Maximum Homogeneity"}, {"url": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#core-parameters", "anchor_text": "https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#core-parameters"}, {"url": "https://deep-and-shallow.com/2020/02/21/the-gradient-boosters-iii-lightgbm/", "anchor_text": "http://deep-and-shallow.com"}, {"url": "https://medium.com/tag/the-gradient-boosters?source=post_page-----800340f21415---------------the_gradient_boosters-----------------", "anchor_text": "The Gradient Boosters"}, {"url": "https://medium.com/tag/data-science?source=post_page-----800340f21415---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----800340f21415---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----800340f21415---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/lightgbm?source=post_page-----800340f21415---------------lightgbm-----------------", "anchor_text": "Lightgbm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----800340f21415---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----800340f21415---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----800340f21415--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F800340f21415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----800340f21415---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----800340f21415--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----800340f21415--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----800340f21415--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----800340f21415--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----800340f21415--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----800340f21415--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----800340f21415--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----800340f21415--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/@manujosephv/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "183 Followers"}, {"url": "https://www.linkedin.com/in/manujosephv/", "anchor_text": "https://www.linkedin.com/in/manujosephv/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7cea8b947fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flightgbm-800340f21415&newsletterV3=c8dcc7fb5ce5&newsletterV3Id=7cea8b947fdd&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}