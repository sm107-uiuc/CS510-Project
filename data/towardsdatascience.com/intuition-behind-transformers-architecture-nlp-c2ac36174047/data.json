{"url": "https://towardsdatascience.com/intuition-behind-transformers-architecture-nlp-c2ac36174047", "time": 1683017927.7021608, "path": "towardsdatascience.com/intuition-behind-transformers-architecture-nlp-c2ac36174047/", "webpage": {"metadata": {"title": "Intuition Behind Transformers Architecture in NLP. | by Oleg Borisov | Towards Data Science", "h1": "Intuition Behind Transformers Architecture in NLP.", "description": "One of the most impactful breakthroughs in NLP happened just couple of years ago, when Ashish Vaswani and his team introduced Transformers architecture in 2017. Simplicity and efficiency of the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/huggingface/transformers", "anchor_text": "Huggingface", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba", "anchor_text": "previous articles", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba", "anchor_text": "embeddings article", "paragraph_index": 44}, {"url": "http://oleg.borisov.tilda.ws/", "anchor_text": "http://oleg.borisov.tilda.ws/", "paragraph_index": 62}], "all_paragraphs": ["One of the most impactful breakthroughs in NLP happened just couple of years ago, when Ashish Vaswani and his team introduced Transformers architecture in 2017. Simplicity and efficiency of the presented architecture, allowed other researchers to create very big and very impressive Language Models like BERT and GPT.", "Surely, there are a lot of sources that try to explain how transformers work, unfortunately, some intuitive aspects of the explanation are often missed and not presented. Thus, it is unclear: how people came up with this idea? What was the inspiration? What are the Key, Query and Values matrices in the context of transformers? How can we interpret all components?", "Of course, implementation of Transformers in some libraries like Huggingface does not require us to know how the model works and it could be treated like a blackbox, but as with many things it could be very useful to understand all underlying mechanisms in order to be able to either: create something new, improve previous solution or understand which task is suitable for which operation.", "In this article, I would like to explain transformers architecture on a very basic and intuitive level, so that even a reader who has never heard of transformers will be able to follow though and understand every single detail. I also believe that people who are familiar with this architecture will gain a new perspective on transformers by reading this article. So without further ado, let\u2019s begin.", "As we all know, language has a sequential nature, and order of the words matters. Similarly to the language, time series also have this importance of the order in which data points occur. So, let\u2019s take a look at one time-series analysis example and check if we could apply similar ideas to the NLP task. Say we have some time-series data as shown below:", "In this example, we have some stock price and its value changes over the time, we would like to analyse the trend, unfortunately, our data is noisy, and hence it is a bit hard to extract the trend.", "In statistics, it is known that we could apply convolution operation using some Kernel in order to reduce the noise. It could be done as shown below for some point x_i.", "After the convolution we will get a denoised representation y_i of our initial data point x_i. If we apply convolution to our example graph, using the Kernel [0.25, 0.5, 0.25] we will get.", "As you can clearly see, the overall trend is that the stock price is raising.", "Ok, sounds great, shall we apply some similar idea in NLP domain? In fact, yes, we can do it, and Convolutional Neural Networks have been using this approach while learning the values for the kernel matrix, but there is a huge drawback!", "In the time-series example we have considered the neighbouring points are correlated with each-other thus we could smoothen the graph, unfortunately , the same thing does not apply to the language (at least not always). Consider example below:", "When Ben and I were playing football, John called me and asked to help him on the weekend to assemble his new furniture.", "In this case, if we do not use a large window size, there is no possibility for the system to understand to whom the words: \u201chim/his\u201d, are referring to. In some languages like german, it could happen that you would have a very large sentence, and the verb is going to be in the end, so sometimes you would only guess what is happening in the sentence until you read the verb in the end.", "Therefore, while, the idea of moving to the representation space is very good, using convolution/window/kernel approach to do that is not applicable for the NLP tasks. So let\u2019s think if there exists some other approach that we could use, and let\u2019s focus on idea of so called self-attention.", "In self-attention, the main goal is to understand, how the words in a sentence are interconnected with each other. In the example sentence we considered earlier, if we were to ask question \u201cDoing what?\u201d then word Ben should be connected (using terminology attend) to the phrase playing football. similarly words him/his are connected to the word John, as he was the one to ask for help and his furniture needs to be assembled. As we have seen window approach is not applicable here, so let us take a look at some other approach.", "In one of my previous articles, I have talked about a very cool and useful way of representing the words called Embeddings. One of the nice properties of word embeddings is that if two words are similar than their cosine similarity is going to be equal to 1.", "For simplicity, consider that we have a 4 word sentence (hence we work with 4 tokens). Let us, first convert our tokens to the vector representation using some embedding so that we get tokens x_i.", "Now, for the sake of simplicity of an example, let\u2019s convert vector x_2 to the representation space y_2, using all our word-vectors x_1, x_2, x_3, x_4.", "Similarly as in the time-series example we are going to do as shown below:", "Where we know what the x_i values are, and need to figure out how to compute weights w_{ij} as we do not want to use any hardcoded values and want to make sure that weights can reflect similarities of words given the context of the sentence. One of the possible ways to do that could be dot product between two vectors (if two vectors are independent dot product would be zero, if they are similar dot product will be a large positive number). So, let\u2019s do it this way and define weight w_{ij} as:", "And, since in this example we are working with vector x_2, I will update our schematic diagram as shown below", "Now, some weights are going turn out to be super large, some of them will turn out to be relatively small, so to make our model consistent, we can normalise the weights to make sure that they sum to 1.", "And now, after the normalisation, exactly these weights w_{i2} are going to be used to compute representation vector y_2.", "And thus the final diagram would look like:", "Now we have found y_2, by querying x_2 and we can apply similar procedure to find representation vectors for other tokens: y_1, y_3, y_4. And to align out graph with Transformers terminology in this case: Query, Key Values are marked as in figure below.", "As you could have noticed, this is a very robust and easy to perform method, which did not even include any Neural Network and learning parameters at all (Even though I used word weights to make analogy with time-series example, they are not connected with NN weights).", "But\u2026 aren\u2019t we a little bit naive by just using this simple architecture to reflect self-attention? Probably yes, as in this case it is still might be complicated for us to say wether word \u201chim\u201d refers to Bob or to John. Therefore, in this architecture, we only rely on the quality of our Embedding, and if it is bad, then so are our results going to be.", "So, what could be done in this case? How about we would introduce some simple Feedforward Neural Network at the points of our Key, Query and Value parts. Thus our original architecture could be slightly updated as follows:", "And this is what actually happens behind the Transformer\u2019s so called \u201cScaled Dot-Product Attention\u201d. Which in paper is shown like below.", "Where above, Q is matrix that contains queries, K \u2014 keys, and V \u2014 values. Which we get by passing words embeddings from our sentence through the respective Feedforward NN.", "The only things that were not discussed so far are: the scaling part and the mask. The scaling part is needed just to stabilise the gradients. Mask is used in the decoding part during the training process, to prevent us from attending the words that the is not supposed to see yet (If you still do not understand it don\u2019t worry, for now it is not that important).", "And what is the beauty of it, is that the whole computation could be completely done in a parallel manner, as we can supply the entire sentence to the model which is going to perform all the computation simultaneously!", "And in fact this complicated diagram can be easily simplified and shown as formula:", "There is another question that one could ask: is having one attention enough for us? Unfortunately, no, that is why we need multiple attention mechanisms.", "Today my friend John bought a car.", "From this sentence, let\u2019s consider what words could be related/connected to the word \u201cbought\u201d:", "As you can see we in the language we usually have different words related to the query word depending on the \u201cquestion\u201d that we might be asking. Having only one self-attention is going to oversaturate our mechanism and it might become too complicated for us to find what word matters the most.", "To make analogy with Convolutional Neural Networks, we have multiple kernels in each layer, where each of the kernels learns their own unique features.", "So, instead of using one self-attention, let\u2019s use multiple of those, and this combined block is called called Multi-Headed Attention.", "Let\u2019s now take a moment and see how that works, as it also has some important details that one needs to understand.", "As you can see from the image above, we have sentence X (which is a matrix that contains stacked word embeddings x_1, x_2, \u2026). Which we pass through the Linear layer and get our Key, Query and Value matrices, which we then supply to the Scaled Dot-Product Attention as described. The only difference now, is that we need to perform all those operations h times, where h corresponds to the number of heads that we define.", "Now, there is one important part that I would like to focus our attention on.", "Each Head does not process the whole embedding vector, it processes just a part of the vector.", "Assume that our embedding is of size d, and that we have h heads, that means that the first head is going to process the first d/h dimensions of the vector, the second head is going to process the next d/h dimensions, and we continue in the same pattern", "Why is this done? As we know from the embeddings article each dimension of the embedding space could represent some special information (if a word represented has wings or is a type of bird) or some abstract information information like:", "So it is possible also that the embedding could also learn to reflect different type of information that is going to help us to distinguish the connection between different words in a sentence. And using this approach, for example one of the Heads will be able to answer the question \u201cwho?\u201d, the other Head will answer question \u201cdid what?\u201d, some other Head will learn connection to the question \u201cwhen?\u201d etc.", "After the split of embeddings is done and all the words are passed through the Scaled Dot-Product Attention layer, we will concatenate the output vectors from all our heads, to make sure that for each word in our sentence we can answer a particular question (that head has managed to learn).", "In the end we make another pass through the Linear FF layer to make sure that our output has some particular dimension.", "This is what the Multi-Head attention is about, which is actually the main, the hardest and the most confusing part of transformer, which turned out to be not too complicated!", "Now, let\u2019s look at the global picture, focus on the encoder architecture of the transformer (In this article I am not going to touch on decoder, as it has a similar structure, with some little differences which could be omitted for this article).", "As the first step, our original text is passed through the Input Embedding which is straight forward word-vector representation part. Then we need to sum it up with the Positional Encoding, which is crucial for transformer as we process all tokens all together at the same time, so this allows the transformer to know the order of the tokens inside of the sentence", "We can either train it from scratch or use a deterministic approach by using some special formula", "After that, we just our vectors to the Transformer block which on the figure shown as the grey box. This Transformer block is repeated N times to allow us to have deeper architecture, so output of block 1 is used as input for the block 2.", "As you can see when the input enters the transformer block, it passes through the Multi-Headed Attention mechanism (which we now understand very well), but you notice that there is a residual connection, that also passes our input vector around the Multi-Headed Attention block. Why is it there?", "Think about the training process for a second. Imagine that we are now in a process of updating the weights of the NN and a gradient signal comes in", "The main problem here is that since we have N transformer blocks, h Multi-Headed Attention blocks in each transformer blocks, then we risk having a vanishing gradient problem, and which might result in a problem of not training our NN at all.", "So we can consider those residual connections to be very important optimisation approach as well as the trick that helps us to avoid vanishing gradients problem.", "So as we have seen in this article is that Transformer architecture is a very neat and beautiful solution that allowed to make some incredible progress in NLP. I really like to think about Transformers as of the undirectional architecture as we can directly supply to it the whole sentence, and the NN will be able to use its Self-Attention mechanism to find the dependencies between the words in a sentence no matter how far apart those words are located in the sentence.", "LSTMs and Bi-LSTMs might have hard time to catch on those word relationships in the sentence, because of the sequential nature of training them.", "Transformers are much easier and much faster to train as the batch-wise operations are very natural using this architecture. While with RNNs to do some similar operations would require one to implement some special tricks which might be quite complex and sometimes counterintuitive.", "I hope after reading this article you have managed to understand that Transformer is a quite simple architecture once you understand the Self-Attention part, and that there is nothing to be afraid of!", "Stay tuned for more NLP and Machine Learning articles, stay safe and enjoy your holidays!", "Natural Language Processing Expert. Artificial Intelligence Developer. Visit my Personal Web-Page for more details: http://oleg.borisov.tilda.ws/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc2ac36174047&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://oleg-borisov.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Oleg Borisov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F458e87b651da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=post_page-458e87b651da----c2ac36174047---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2ac36174047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=-----c2ac36174047---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc2ac36174047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=-----c2ac36174047---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@waldemarbrandt67w?utm_source=medium&utm_medium=referral", "anchor_text": "Waldemar Brandt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Huggingface"}, {"url": "http://www.invasivecode.com/weblog/convolutional-neural-networks-ios-10-macos-sierra/", "anchor_text": "http://www.invasivecode.com/weblog/convolutional-neural-networks-ios-10-macos-sierra/"}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba", "anchor_text": "previous articles"}, {"url": "https://oleg-borisov.medium.com/", "anchor_text": "Oleg Borisov"}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba", "anchor_text": "embeddings article"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c2ac36174047---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c2ac36174047---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----c2ac36174047---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----c2ac36174047---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----c2ac36174047---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2ac36174047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=-----c2ac36174047---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2ac36174047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=-----c2ac36174047---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc2ac36174047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F458e87b651da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=post_page-458e87b651da----c2ac36174047---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc4641e194723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&newsletterV3=458e87b651da&newsletterV3Id=c4641e194723&user=Oleg+Borisov&userId=458e87b651da&source=-----c2ac36174047---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Written by Oleg Borisov"}, {"url": "https://oleg-borisov.medium.com/followers?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "88 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://oleg.borisov.tilda.ws/", "anchor_text": "http://oleg.borisov.tilda.ws/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F458e87b651da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&user=Oleg+Borisov&userId=458e87b651da&source=post_page-458e87b651da----c2ac36174047---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc4641e194723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-transformers-architecture-nlp-c2ac36174047&newsletterV3=458e87b651da&newsletterV3Id=c4641e194723&user=Oleg+Borisov&userId=458e87b651da&source=-----c2ac36174047---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Oleg Borisov"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Text Generation Using N-Gram ModelLet\u2019s build a simple Language Model"}, {"url": "https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "7 min read\u00b7Oct 27, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d12d9802aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-using-n-gram-model-8d12d9802aa0&user=Oleg+Borisov&userId=458e87b651da&source=-----8d12d9802aa0----0-----------------clap_footer----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=author_recirc-----c2ac36174047----0---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d12d9802aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-using-n-gram-model-8d12d9802aa0&source=-----c2ac36174047----0-----------------bookmark_preview----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2ac36174047----1---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----c2ac36174047----1-----------------bookmark_preview----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c2ac36174047----2---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----c2ac36174047----2-----------------bookmark_preview----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Oleg Borisov"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "Word Embeddings: Intuition behind the vector representation of the wordsIn this article I would like to talk about how the words are commonly represented in Natural Language Processing (NLP), and what are the\u2026"}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": "9 min read\u00b7Dec 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e4eb2410bba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba&user=Oleg+Borisov&userId=458e87b651da&source=-----7e4eb2410bba----3-----------------clap_footer----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba?source=author_recirc-----c2ac36174047----3---------------------37b9649d_5d30_4d8a_9fa7_ac2fca287800-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e4eb2410bba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba&source=-----c2ac36174047----3-----------------bookmark_preview----37b9649d_5d30_4d8a_9fa7_ac2fca287800-------", "anchor_text": ""}, {"url": "https://oleg-borisov.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "See all from Oleg Borisov"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----c2ac36174047----0-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----1-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----c2ac36174047----1-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Benjamin Marie"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Translate with GPT-3Machine translation but without a machine translation system"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b718 min read\u00b7Nov 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&user=Benjamin+Marie&userId=ad2a414578b3&source=-----9903c4a6f385----0-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----c2ac36174047----0---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&source=-----c2ac36174047----0-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----1-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----c2ac36174047----1---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----c2ac36174047----1-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----2-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----c2ac36174047----2---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----c2ac36174047----2-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/predict/gpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://e2analyst.medium.com/?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://e2analyst.medium.com/?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "E2Analyst"}, {"url": "https://medium.com/predict?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "Predict"}, {"url": "https://medium.com/predict/gpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "GPT-4: Everything you want to know about OpenAI\u2019s new AI modelFrom Text to Images, GPT-4 is Set to Revolutionize the Way We Interact with AI"}, {"url": "https://medium.com/predict/gpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": "\u00b78 min read\u00b7Mar 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpredict%2Fa5977b42e495&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpredict%2Fgpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495&user=E2Analyst&userId=98e22e9a969b&source=-----a5977b42e495----3-----------------clap_footer----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/predict/gpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495?source=read_next_recirc-----c2ac36174047----3---------------------82d40651_3541_4322_a6c3_81c693c9cfa6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5977b42e495&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpredict%2Fgpt-4-everything-you-want-to-know-about-openais-new-ai-model-a5977b42e495&source=-----c2ac36174047----3-----------------bookmark_preview----82d40651_3541_4322_a6c3_81c693c9cfa6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c2ac36174047--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----c2ac36174047--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}