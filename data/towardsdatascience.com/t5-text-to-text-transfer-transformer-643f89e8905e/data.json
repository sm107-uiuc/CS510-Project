{"url": "https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e", "time": 1683011789.39039, "path": "towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e/", "webpage": {"metadata": {"title": "T5: Text-To-Text Transfer Transformer | by Rohan Jagtap | Towards Data Science", "h1": "T5: Text-To-Text Transfer Transformer", "description": "With the burgeoning of Transfer Learning, Deep Learning has achieved many wonders. More specifically, in NLP, with the rise of the Transformer (Vaswani et. al.), various approaches for \u2018Language\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "Transfer Learning", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al.", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al.", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al.", "paragraph_index": 14}, {"url": "https://nlp.cs.washington.edu/triviaqa/", "anchor_text": "TriviaQA", "paragraph_index": 21}, {"url": "https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a", "anchor_text": "WebQuestions", "paragraph_index": 21}, {"url": "https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html", "anchor_text": "Natural Questions", "paragraph_index": 21}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "official blog", "paragraph_index": 24}], "all_paragraphs": ["With the burgeoning of Transfer Learning, Deep Learning has achieved many wonders. More specifically, in NLP, with the rise of the Transformer (Vaswani et. al.), various approaches for \u2018Language Modeling\u2019 have arisen wherein we leverage transfer learning by pre-training the model for a very generic task and then fine-tuning it on specific downstream problems.", "In this article, we\u2019ll discuss Google\u2019s state of the art, T5 \u2014 Text-to-Text Transfer Transformer Model which was proposed earlier this year in the paper, \u201cExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u201d. This paper is essentially a survey of modern transfer learning techniques used in language understanding and hence proposes a unified framework that attempts to combine all language problems into a text-to-text format. We will discuss this approach in greater detail in the coming sections. Moreover, the authors have also open-sourced a new dataset (for facilitating their work) called C4 \u2014 Colossal Clean Crawled Corpus.", "As mentioned earlier, T5 attempts to combine all the downstream tasks into a text-to-text format.", "Consider the example of a BERT-style architecture that is pre-trained on a Masked LM and Next Sentence Prediction objective and then, fine-tuned on downstream tasks (for example predicting a class label in classification or the span of the input in QnA). Here, we separately fine-tune different instances of the pre-trained model on different downstream tasks.", "The text-to-text framework on the contrary, suggests using the same model, same loss function, and the same hyperparameters on all the NLP tasks. In this approach, the inputs are modeled in such a way that the model shall recognize a task, and the output is simply the \u201ctext\u201d version of the expected outcome. Refer to the above animation to get a clearer view of this.", "Fun fact: We can even apply T5 to regression tasks by training it to output the string representation of the expected output.", "It is a stereotype to pre-train language models on huge unlabeled datasets. Common Crawl is one of such datasets. It is obtained by scraping web pages and ignoring the markup from the HTML. It produces around 20TB of scraped data each month. However, Common Crawl contains large amounts of gibberish text like menus or error messages, or duplicate text. Moreover, there is also an appreciable amount of useless text with respect to our tasks like offensive words, placeholder text, or source codes.", "For C4, the authors took Common Crawl scrape from April 2019 and applied some cleansing filters on it:", "This resulted in a 750GB dataset which is not just reasonably larger than the most pre-training datasets but also contains a relatively very clean text.", "This is one of the major concerns of T5 as this is what makes the unified text-to-text approach possible. To avail the same model for all the downstream tasks, a task-specific text prefix is added to the original input that is fed to the model. This text prefix is also considered as a hyperparameter.", "As an example,to ask the model to translate the sentence \u201cThat is good.\u201d from English to German, the model would be fed the sequence \u201ctranslate English to German: That is good.\u201d and would be trained to output \u201cDas ist gut.\u201d", "Similarly, for classification tasks, the model predicts a single word corresponding to the target label.", "For example, on the MNLI benchmark the goal is to predict whether a premise implies (\u201centailment\u201d), contradicts (\u201ccontradiction\u201d), or neither (\u201cneutral\u201d) a hypothesis. With our preprocessing, the input sequence becomes \u201cmnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\u201d with the corresponding target word \u201centailment\u201d.", "Here\u2019s an issue with this. What if the predicted word is something else i.e. not \u201centailment\u201d, \u201ccontradiction\u201d or \u201cneutral\u201d. Well, in that case, the model is trained to consider all the other words as wrong.", "The proposed model is essentially a Encoder-Decoder Transformer (Vaswani et. al.) with some architectural changes (like applying Layer Normalization before a sub block and then adding the initial input to the sub-block output; also known as pre-norm). Moreover, the model configuration is similar to BERT base (Devlin et. al.).", "We\u2019ll skip these architectures as they\u2019re out of scope for this article. If you\u2019re interested in knowing the specifications of these models in particular, I have already covered them in the following articles:", "At an architectural level, there are several options in selecting the training approach:The paper is an exhaustive survey on many modern approaches for language understanding. Hence, many architectural specifications have been explored and compared.", "With experimentation, the best results were obtained with the Encoder-Decoder approach.", "With respect to the pre-training objective too, the authors have explored some of the approaches in practice:", "After exploration, the denoising objective had the most promising results.", "First things first, T5 has achieved the state of the art in many GLUE, SuperGLUE tasks along with translation and summarization benchmarks.", "T5 is surprisingly good at this task. The full 11-billion parameter model produces the exact text of the answer 50.1%, 37.4%, and 34.5% of the time on TriviaQA, WebQuestions, and Natural Questions, respectively.", "To generate realistic text, T5 relies on a fill-in-the-blanks type task with which it is familiar due to the pre-training. So, the authors have created a new downstream task called sized fill-in-the-blank. For example, given the sentence, \u201cI like to eat peanut butter and _4_ sandwiches,\u201d, the model will be trained to predict approximately 4 words for the blank.", "Fun fact: The model also adjusts its predictions based on the requested size of the missing text.", "For the demonstration of the above, refer to the official blog.", "In this article, we dived deep into Google\u2019s T5 model which is one of the state of the art models in language understanding. We saw the new dataset: C4. The main takeaway from this article would be the empirical results obtained by the T5 authors regarding the training approaches, model architectures and the datasets. Moreover, it can be also observed that DL is approaching more and more towards achieving human quality understanding\u2014 in this context, generalizing to just one model for many NLP tasks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F643f89e8905e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----643f89e8905e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Google AI Blog"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "Transfer Learning"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al."}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Google AI Blog"}, {"url": "https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words", "anchor_text": "List of Dirty, Naughty, Obscene or Otherwise Bad Words"}, {"url": "https://pypi.org/project/langdetect/", "anchor_text": "langdetect"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "T5 Paper"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "T5 Paper"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al."}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al."}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7"}, {"url": "https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "the Paper"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "the Paper"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "the Paper"}, {"url": "https://nlp.cs.washington.edu/triviaqa/", "anchor_text": "TriviaQA"}, {"url": "https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a", "anchor_text": "WebQuestions"}, {"url": "https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html", "anchor_text": "Natural Questions"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Google AI Blog"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "official blog"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Google AI Blog"}, {"url": "https://github.com/google-research/text-to-text-transfer-transformer", "anchor_text": "https://github.com/google-research/text-to-text-transfer-transformer"}, {"url": "https://huggingface.co/transformers/model_doc/t5.html", "anchor_text": "https://huggingface.co/transformers/model_doc/t5.html"}, {"url": "https://www.tensorflow.org/datasets/catalog/c4", "anchor_text": "https://www.tensorflow.org/datasets/catalog/c4"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "https://arxiv.org/abs/1910.10683"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Exploring Transfer Learning with T5: the Text-To-Text Transfer TransformerOver the past few years, transfer learning has led to a new wave of state-of-the-art results in natural language\u2026ai.googleblog.com"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers ExplainedAn exhaustive explanation of Google\u2019s Transformer model; from theory to implementationtowardsdatascience.com"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT: Pre-Training of Transformers for Language UnderstandingUnderstanding Transformer-Based Self-Supervised Architecturesmedium.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----643f89e8905e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----643f89e8905e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----643f89e8905e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----643f89e8905e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----643f89e8905e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&user=Rohan+Jagtap&userId=39646f947a4c&source=-----643f89e8905e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&user=Rohan+Jagtap&userId=39646f947a4c&source=-----643f89e8905e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F643f89e8905e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----643f89e8905e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----643f89e8905e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----643f89e8905e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----643f89e8905e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----643f89e8905e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----643f89e8905e--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transfer-transformer-643f89e8905e&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}