{"url": "https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801", "time": 1682997708.153451, "path": "towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801/", "webpage": {"metadata": {"title": "Spectral Graph Convolution Explained and Implemented Step By Step | by Boris Knyazev | Towards Data Science", "h1": "Spectral Graph Convolution Explained and Implemented Step By Step", "description": "First, let\u2019s recall what is a graph. A graph G is a set of nodes (vertices) connected by directed/undirected edges. In this post, I will assume an undirected graph G with N nodes. Each node in this\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.cs.yale.edu/homes/spielman/561/", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al., 2014, ICLR 2014", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Discrete_Fourier_transform", "anchor_text": "Fourier Transform", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/DFT_matrix", "anchor_text": "DFT matrix", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Laplacian_matrix", "anchor_text": "graph Laplacian", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "convolution theorem", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal component analysis (PCA)", "paragraph_index": 12}, {"url": "http://outobox.cs.umn.edu/PCA_on_a_Graph.pdf", "anchor_text": "this paper", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Eigenface", "anchor_text": "Eigenfaces", "paragraph_index": 12}, {"url": "http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/", "anchor_text": "eigenvalues tell us a lot about graph connectivity", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al.", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "spectral convolution of signals on regular grids", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al.", "paragraph_index": 18}, {"url": "https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py", "anchor_text": "this code", "paragraph_index": 19}, {"url": "https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49", "anchor_text": "my another post", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al.", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling, ICLR, 2017", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1901.01484", "anchor_text": "Liao et al.", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1904.07785", "anchor_text": "Xu et al.", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1705.07664", "anchor_text": "Levie et al., 2018", "paragraph_index": 26}, {"url": "https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49", "anchor_text": "Tutorial on Graph Neural Networks for Computer Vision and Beyond", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1606.09375", "anchor_text": "Defferrard et al.", "paragraph_index": 27}, {"url": "https://medium.com/u/6cf41cb2c546?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Mohamed Amer", "paragraph_index": 28}, {"url": "https://mohamedramer.com/", "anchor_text": "homepage", "paragraph_index": 28}, {"url": "https://www.gwtaylor.ca/", "anchor_text": "homepage", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/carolynaugusta/", "anchor_text": "Carolyn Augusta", "paragraph_index": 28}, {"url": "https://github.com/bknyaz/", "anchor_text": "Github", "paragraph_index": 29}, {"url": "https://www.linkedin.com/in/boris-knyazev-39690948/", "anchor_text": "LinkedIn", "paragraph_index": 29}, {"url": "https://twitter.com/BorisAKnyazev", "anchor_text": "Twitter", "paragraph_index": 29}, {"url": "https://bknyaz.github.io/", "anchor_text": "My homepage", "paragraph_index": 29}, {"url": "http://twitter.com/misc", "anchor_text": "@misc", "paragraph_index": 30}, {"url": "http://bknyaz.github.io/", "anchor_text": "http://bknyaz.github.io/", "paragraph_index": 32}], "all_paragraphs": ["First, let\u2019s recall what is a graph. A graph G is a set of nodes (vertices) connected by directed/undirected edges. In this post, I will assume an undirected graph G with N nodes. Each node in this graph has a C-dimensional feature vector, and features of all nodes are represented as an N\u00d7C dimensional matrix X\u207d\u02e1\u207e. Edges of a graph are represented as an N\u00d7N matrix A, where the entry A\u1d62\u2c7c indicates if node i is connected (adjacent) to node j. This matrix is called an adjacency matrix.", "Spectral analysis of graphs (see lecture notes here and earlier work here) has been useful for graph clustering, community discovery and other mainly unsupervised learning tasks. In this post, I basically describe the work of Bruna et al., 2014, ICLR 2014 who combined spectral analysis with convolutional neural networks (ConvNets) giving rise to spectral graph convolutional networks that can be trained in a supervised way, for example for the graph classification task.", "Despite that spectral graph convolution is currently less commonly used compared to spatial graph convolution methods, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other methods. Plus, in the conclusion I refer to some recent exciting works making spectral graph convolution more competitive.", "While \u201cspectral\u201d may sound complicated, for our purpose it\u2019s enough to understand that it simply means decomposing a signal/audio/image/graph into a combination (usually, a sum) of simple elements (wavelets, graphlets). To have some nice properties of such a decomposition, these simple elements are usually orthogonal, i.e. mutually linearly independent, and therefore form a basis.", "When we talk about \u201cspectral\u201d in signal/image processing, we imply the Fourier Transform, which offers us a particular basis (DFT matrix, e.g. scipy.linalg.dft in Python) of elementary sine and cosine waves of different frequencies, so that we can represent our signal/image as a sum of these waves. But when we talk about graphs and graph neural networks (GNNs), \u201cspectral\u201d implies eigen-decomposition of the graph Laplacian L. You can think of the the graph Laplacian L as an adjacency matrix A normalized in a special way, whereas eigen-decomposition is a way to find those elementary orthogonal components that make up our graph.", "Intuitively, the graph Laplacian shows in what directions and how smoothly the \u201cenergy\u201d will diffuse over a graph if we put some \u201cpotential\u201d in node i. A typical use-case of Laplacian in mathematics and physics is to solve how a signal (wave) propagates in a dynamic system. Diffusion is smooth when there is no sudden changes of values between neighbors as in the animation below.", "In the rest of the post, I\u2019m going to assume \u201csymmetric normalized Laplacian\u201d, which is often used in graph neural networks, because it is normalized so that when you stack many graph layers, the node features propagate in a more smooth way without explosion or vanishing of feature values or gradients. It is computed based only on an adjacency matrix A of a graph, which can be done in a few lines of Python code as follows:", "Here, we assume that A is symmetric, i.e. A = A\u1d40 and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. An interesting property of an adjacency matrix A is that A\u207f (matrix product taken n times) exposes n-hop connections between nodes (see here for more details).", "Let\u2019s generate three graphs and visualize their adjacency matrices and Laplacians as well as their powers.", "For example, imagine that the star graph above in the middle is made from metal, so that it transfers heat well. Then, if we start to heat up node 0 (dark blue), this heat will propagate to other nodes in a way defined by the Laplacian. In the particular case of a star graph with all edges equal, heat will spread uniformly to all other nodes, which is not true for other graphs due to their structure.", "In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers. Similarly to the first part of my tutorial, to understand spectral graph convolution from the computer vision perspective, I\u2019m going to use the MNIST dataset, which defines images on a 28\u00d728 regular grid graph.", "In signal processing, it can be shown that convolution in the spatial domain is multiplication in the frequency domain (a.k.a. convolution theorem). The same theorem can be applied to graphs. In signal processing, to transform a signal to the frequency domain, we use the Discrete Fourier Transform, which is basically matrix multiplication of a signal with a special matrix (basis, DFT matrix). This basis assumes a regular grid, so we cannot use it for irregular graphs, which is a typical case. Instead, we use a more general basis, which is eigenvectors V of the graph Laplacian L, which can be found by eigen-decomposition: L=V\u039bV\u1d40, where \u039b are eigenvalues of L.", "PCA vs eigen-decomposition of the graph Laplacian. To compute spectral graph convolution in practice, it\u2019s enough to use a few eigenvectors corresponding to the smallest eigenvalues. At first glance, it seems to be an opposite strategy compared to frequently used in computer vision Principal component analysis (PCA), where we are more interested in the eigenvectors corresponding to the largest eigenvalues. However, this difference is simply due to the negation used to compute the Laplacian above, therefore eigenvalues computed using PCA are inversely proportional to eigenvalues of the graph Laplcacian (see this paper for a formal analysis). Note also that PCA is applied to the covariance matrix of a dataset for the purpose to extract the largest factors of variation, i.e. the dimensions along which data vary the most, like in Eigenfaces. This variation is measured by eigenvalues, so that the smallest eigenvalues essentially correspond to noisy or \u201cspurious\u201d features, which are assumed to be useless or even harmful in practice.", "Eigen-decomposition of the graph Laplacian is applied to a single graph for the purpose to extract subgraphs or clusters (communities) of nodes, and eigenvalues tell us a lot about graph connectivity. I will use eigenvectors corresponding to the 20 smallest eigenvalues in our examples below, assuming that 20 is much smaller than the number of nodes N (N=784 in case of MNIST). To find eigenvalues and eigenvectors below on the left, I use a 28\u00d728 regular graph, whereas on the right I follow the experiment of Bruna et al. and construct an irregular graph by sampling 400 random locations on a 28\u00d728 regular grid (see their paper for more details about this experiment).", "So, given graph Laplacian L, node features X and filters W_spectral, in Python spectral convolution on graphs looks very simple:", "where we assume that our node features X\u207d\u02e1\u207e are 1-dimensional, e.g. MNIST pixels, but it can be extended to a C-dimensional case: we will just need to repeat this convolution for each channel and then sum over C as in signal/image convolution.", "Formula (3) is essentially the same as spectral convolution of signals on regular grids using the Fourier Transform, and so creates a few problems for machine learning:", "These issues prevent scaling to datasets with large graphs of variable structure. Further efforts, summarized below, were focused on resolving these and other issues.", "Bruna et al. were one of the first to apply spectral graph analysis to learn convolutional filters for the graph classification problem. The filters learned using formula (3) above act on the entire graph, i.e. they have global support. In the computer vision context, this would be the same as training convolutional filters of size 28\u00d728 pixels on MNIST, i.e. filters have the same size as the input (note that we would still slide a filter, but over a zero-padded image). While for MNIST we can actually train such filters, the common wisdom suggests to avoid that, as it makes training much harder due to the potential explosion of the number of parameters and difficulty of training large filters that can capture useful features shared across different images.", "I actually successfully trained such a model using PyTorch and this code from my GitHub. You should run it using mnist_fc.py --model conv. After training for 100 epochs, the filters look like mixtures of digits:", "To reiterate, we generally want to make filters smaller and more local (which is not exactly the same as I\u2019ll note below).", "To enforce that implicitly, they proposed to smooth filters in the spectral domain, which makes them more local in the spatial domain according to the spectral theory. The idea is that you can represent our filter W_spectral from formula (3) as a sum of \ud835\udc3e predefined functions, such as splines, and instead of learning N values of W, we learn K coefficients \u03b1 of this sum:", "While the dimensionality of fk does depend on the number of nodes N, these functions are fixed, so we don\u2019t learn them. The only thing we learn are coefficients \u03b1, and so W_spectral is no longer dependent on N. Neat, right?", "To make our approximation in formula (4) reasonable, we want K<<N to reduce the number of trainable parameters from N to K and, more importantly, make it independent of N, so that our GNN can digest graphs of any size. We can use different bases to perform this \u201cexpansion\u201d, depending on which properties we need. For instance, cubic splines shown above are known as very smooth functions (i.e. you cannot see knots, i.e. where the pieces of the piecewise spline polynomial meet). The Chebyshev polynomial, which I discuss in my another post, has the minimum \ud835\udc59\u221e distance between the approximating function. The Fourier basis is the one that preserves most of the signal energy after transformation. Most bases are orthogonal, because it would be redundant to have terms that can be expressed by each other.", "Note that filters W_spectral are still as large as the input, but their effective width is small. In case of MNIST images, we would have 28\u00d728 filters, in which only a small fraction of values would have an absolute magnitude larger than 0 and all of them should be located close to each other, i.e. the filter would be local and effectively small, something like the one below (second from the left):", "To summarize, smoothing in the spectral domain allowed Bruna et al. to learn more local filters. The model with such filters can achieve similar results as the model without smoothing (i.e. using our formula (3)), but with much fewer trainable parameters, because the filter size is independent of the input graph size, which is important to scale the model to datasets with larger graphs. However, learned filters W_spectral still depend on eigenvectors V, which makes it challenging to apply this model to datasets with variable graph structures.", "Despite the drawbacks of the original spectral graph convolution method, it has been developed a lot and has remained a quite competitive method in some applications, because spectral filters can better capture global complex patterns in graphs, which local methods like GCN (Kipf & Welling, ICLR, 2017) cannot unless stacked in a deep network. For example, two ICLR 2019 papers, of Liao et al. on \u201cLanczosNet\u201d and Xu et al. on \u201cGraph Wavelet Neural Network\u201d, address some shortcomings of spectral graph convolution and show great results in predicting molecule properties and node classification. Another interesting work of Levie et al., 2018 on \u201cCayleyNets\u201d showed strong performance in node classification, matrix completion (recommender systems) and community detection. So, depending on your application and infrastructure, spectral graph convolution can be a good choice.", "In another part of my Tutorial on Graph Neural Networks for Computer Vision and Beyond I explain Chebyshev spectral graph convolution introduced by Defferrard et al. in 2016, which is still a very strong baseline that has some nice properties and is easy to implement as I demonstrate using PyTorch.", "Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of Mohamed Amer (homepage) and my PhD advisor Graham Taylor (homepage). I also thank Carolyn Augusta for useful feedback.", "Find me on Github, LinkedIn and Twitter. My homepage.", "If you want to cite this blog post in your paper, please use:@misc{knyazev2019tutorial, title={Tutorial on Graph Neural Networks for Computer Vision and Beyond}, author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R}, year={2019}}", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student at University of Guelph, Machine Learning Research Group http://bknyaz.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2e495b57f801&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e495b57f801--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@BorisAKnyazev?source=post_page-----2e495b57f801--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Boris Knyazev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3bd901bbd24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&user=Boris+Knyazev&userId=3bd901bbd24&source=post_page-3bd901bbd24----2e495b57f801---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.cs.yale.edu/homes/spielman/561/", "anchor_text": "here"}, {"url": "https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al., 2014, ICLR 2014"}, {"url": "https://en.wikipedia.org/wiki/Discrete_Fourier_transform", "anchor_text": "Fourier Transform"}, {"url": "https://en.wikipedia.org/wiki/DFT_matrix", "anchor_text": "DFT matrix"}, {"url": "https://en.wikipedia.org/wiki/Laplacian_matrix", "anchor_text": "graph Laplacian"}, {"url": "https://en.wikipedia.org/wiki/Laplacian_matrix", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers", "anchor_text": "here"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "convolution theorem"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal component analysis (PCA)"}, {"url": "http://outobox.cs.umn.edu/PCA_on_a_Graph.pdf", "anchor_text": "this paper"}, {"url": "https://en.wikipedia.org/wiki/Eigenface", "anchor_text": "Eigenfaces"}, {"url": "http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/", "anchor_text": "eigenvalues tell us a lot about graph connectivity"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al."}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al., 2014, ICLR 2014"}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "spectral convolution of signals on regular grids"}, {"url": "https://joyfoodsunshine.com/strawberry-banana-smoothie/", "anchor_text": "joyfoodsunshine.com"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al."}, {"url": "https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py", "anchor_text": "this code"}, {"url": "https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49", "anchor_text": "my another post"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al."}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling, ICLR, 2017"}, {"url": "https://arxiv.org/abs/1901.01484", "anchor_text": "Liao et al."}, {"url": "https://arxiv.org/abs/1904.07785", "anchor_text": "Xu et al."}, {"url": "https://arxiv.org/abs/1705.07664", "anchor_text": "Levie et al., 2018"}, {"url": "https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49", "anchor_text": "Tutorial on Graph Neural Networks for Computer Vision and Beyond"}, {"url": "https://arxiv.org/abs/1606.09375", "anchor_text": "Defferrard et al."}, {"url": "https://medium.com/u/6cf41cb2c546?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Mohamed Amer"}, {"url": "https://mohamedramer.com/", "anchor_text": "homepage"}, {"url": "https://www.gwtaylor.ca/", "anchor_text": "homepage"}, {"url": "https://www.linkedin.com/in/carolynaugusta/", "anchor_text": "Carolyn Augusta"}, {"url": "https://github.com/bknyaz/", "anchor_text": "Github"}, {"url": "https://www.linkedin.com/in/boris-knyazev-39690948/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/BorisAKnyazev", "anchor_text": "Twitter"}, {"url": "https://bknyaz.github.io/", "anchor_text": "My homepage"}, {"url": "http://twitter.com/misc", "anchor_text": "@misc"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2e495b57f801---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/graph-neural-networks?source=post_page-----2e495b57f801---------------graph_neural_networks-----------------", "anchor_text": "Graph Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2e495b57f801---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----2e495b57f801---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----2e495b57f801---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&user=Boris+Knyazev&userId=3bd901bbd24&source=-----2e495b57f801---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&user=Boris+Knyazev&userId=3bd901bbd24&source=-----2e495b57f801---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2e495b57f801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2e495b57f801---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2e495b57f801--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2e495b57f801--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2e495b57f801--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2e495b57f801--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2e495b57f801--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Boris Knyazev"}, {"url": "https://medium.com/@BorisAKnyazev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "664 Followers"}, {"url": "http://bknyaz.github.io/", "anchor_text": "http://bknyaz.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3bd901bbd24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&user=Boris+Knyazev&userId=3bd901bbd24&source=post_page-3bd901bbd24--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8482c870bfd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&newsletterV3=3bd901bbd24&newsletterV3Id=8482c870bfd3&user=Boris+Knyazev&userId=3bd901bbd24&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}