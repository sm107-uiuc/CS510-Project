{"url": "https://towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b", "time": 1683012645.345949, "path": "towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b/", "webpage": {"metadata": {"title": "A Minimal Working Example for Continuous Policy Gradients in TensorFlow 2.0 | by Wouter van Heeswijk, PhD | Towards Data Science", "h1": "A Minimal Working Example for Continuous Policy Gradients in TensorFlow 2.0", "description": "At the root of all the sophisticated actor-critic algorithms that are designed and applied these days is the vanilla policy gradient algorithm, which essentially is an actor-only algorithm. Nowadays\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.researchgate.net/publication/343714359_Implementing_Gaussian_Actor_Networks_for_Continuous_Control_in_TensorFlow_20", "anchor_text": "Implementing Gaussian Actor Networks for Continuous Control in TensorFlow 2.0", "paragraph_index": 19}, {"url": "http://www.github.com/woutervanheeswijk/example_continuous_control", "anchor_text": "GitHub repository", "paragraph_index": 20}, {"url": "https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/", "anchor_text": "https://www.tensorflow.org/api_docs/python/tf/GradientTape", "paragraph_index": 22}], "all_paragraphs": ["At the root of all the sophisticated actor-critic algorithms that are designed and applied these days is the vanilla policy gradient algorithm, which essentially is an actor-only algorithm. Nowadays, the actor that learns the decision-making policy is often represented by a neural network. In continuous control problems, this network outputs the relevant distribution parameters to sample appropriate actions.", "With so many deep reinforcement learning algorithms in circulation, you\u2019d expect it to be easy to find abundant plug-and-play TensorFlow implementations for a basic actor network in continuous control, but this is hardly the case. Various reasons may exist for this. First, TensorFlow 2.0 was released only in September 2019, differing quite substantially from its predecessor. Second, most implementations focus on discrete action spaces rather than continuous ones. Third, there are many different implementations in circulation, yet some are tailored such that they only work in specific problem settings. It can be a tad frustrating to plow through several hundred lines of code riddled with placeholders and class members, only to find out the approach is not suitable to your problem after all. This article \u2014 based on our ResearchGate note [1] \u2014 provides a minimal working example that functions in TensorFlow 2.0. We will show that the real magic happens in only three lines of code!", "In this article, we present a simple and generic implementation for an actor network in the context of the vanilla policy gradient algorithm REINFORCE [2]. In the continuous variant, we usually draw actions from a Gaussian distribution; the goal is to learn an appropriate mean \u03bc and a standard deviation \u03c3. The actor network learns and outputs these parameters.", "Let\u2019s formalize this actor network a bit more. Here, the input is the state s or a feature array \u03d5(s), followed by one or more hidden layers that transform the input, with the output being \u03bc and \u03c3. Once obtaining this output, an action a is randomly drawn from the corresponding Gaussian distribution. Thus, we have a=\u03bc(s)+\u03c3(s)\u03be , where \u03be \u223c \ud835\udca9(0,1).", "After taking our action a, we observe a corresponding reward signal v. Together with some learning rate \u03b1, we may update the weights into a direction that improves the expected reward of our policy. The corresponding update rule [2] \u2014 based on gradient ascent \u2014 is given by:", "If we use a linear approximation scheme \u03bc_\u03b8(s)=\u03b8^\u22a4 \u03d5(s), we may directly apply these update rules on each feature weight. For neural networks, it may not be as straightforward how we should perform this update though.", "Neural networks are trained by minimizing a loss function. We often compute the loss by computing the mean-squared error (squaring the difference between the predicted- and observed value). For instance, in a critic network the loss could be defined as (r\u209c + Q\u209c\u208a\u2081 - Q\u209c)\u00b2, with Q\u209c being the predicted value and r\u209c + Q\u209c\u208a\u2081 the observed value. After computing the loss, we backpropagate it through the network, computing the partial losses and gradients required to update the network weights.", "At first glance, the update equations have little in common with such a loss function. We simply try to improve our policy by moving into a certain direction, but do not have an explicit \u2018target\u2019 or \u2018true value\u2019 in mind. Indeed, we will need to define a \u2018pseudo loss function\u2019 that helps us update the network [3]. The link between the traditional update rules and this loss function become more clear when expressing the update rule into its generic form:", "Transformation into a loss function is fairly straightforward. As the loss is only the input for the backpropagation procedure, we first drop the learning rate \u03b1 and gradient \u2207_\u03b8. Furthermore, neural networks are updated using gradient descent instead of gradient ascent, so we must add a minus sign. These steps yield the following loss function:", "Quite similar to the update rule, right? To provide some intuition: remind that the log transformation yields a negative number for all values smaller than 1. If we have an action with a low probability and a high reward, we\u2019d want to observe a large loss, i.e., a strong signal to update our policy into the direction of that high reward. The loss function does precisely that.", "To apply the update for a Gaussian policy, we can simply substitute \u03c0_\u03b8 with the Gaussian probability density function (pdf) \u2014 note that in the continuous domain we work with pdf values rather than actual probabilities \u2014 to obtain the so-called weighted Gaussian log likelihood loss function:", "Enough mathematics for now, it\u2019s time for the implementation.", "We just defined the loss function, but unfortunately we cannot directly apply it in Tensorflow 2.0. When training a neural network, you may be used to something like model.compile(loss='mse',optimizer=opt), followed by model.fitormodel.train_on_batch, but this doesn\u2019t work. First of all, the Gaussian log likelihood loss function is not a default one in TensorFlow 2.0 \u2014 it is in the Theano library for example[4] \u2014 meaning we have to create a custom loss function. More restrictive though: TensorFlow 2.0 requires a loss function to have exactly two arguments, y_true and y_predicted. As we just saw, we have three arguments due to multiplying with the reward. Let\u2019s worry about that later though and first present our custom Guassian loss function:", "So we have the correct loss function now, but we cannot apply it!? Of course we can \u2014 otherwise all of this would have been fairly pointless \u2014 it\u2019s just slightly different than you might be used to.", "This is where the GradientTapefunctionality comes in, which is a novel addition to TensorFlow 2.0 [5]. It essentially records your forward steps on a \u2018tape\u2019 such that it can apply automatic differentiation. The updating approach consists of three steps [6]. First, in our custom loss function we make a forward pass through the actor network \u2014 which is memorized \u2014 and calculate the loss. Second, with the function .trainable_variables, we recall the weights found during our forward pass. Subsequently, tape.gradient calculates all the gradients for you by simply plugging in the loss value and the trainable variables. Third, with optimizer.apply_gradients we update the network weights, where the optimizer is one of your choosing (e.g., SGD, Adam, RMSprop). In Python, the update steps look as follows:", "So in the end, we only need a few lines of codes to perform the update!", "We present a minimal working example for a continuous control problem, the full code can be found on my GitHub. We consider an extremely simple problem, namely a one-shot game with only one state and a trivial optimal policy. The closer we are to the (fixed but unknown) target, the higher our reward. The reward function is formally denoted as R =\u03b6 \u03b2 / max(\u03b6,|\u03c4 - a|), with \u03b2 as the maximum reward, \u03c4 as the target and \u03b6 as the target range.", "To represent the actor we define a dense neural network (using Keras) that takes the fixed state (a tensor with value 1) as input, performs transformations in two hidden layers with ReLUs as activation functions (five per layer) and returns \u03bc and \u03c3 as output. We initialize bias weights such that we start with \u03bc=0 and \u03c3=1. For our optimizer, we use Adam with its default learning rate of 0.001.", "Some sample runs are shown in the figure below. Note that the convergence pattern is in line with our expectations. At first the losses are relatively high, causing \u03bc to move into the direction of higher rewards and \u03c3 to increase and allow for more exploration. Once hitting the target the observed losses decrease, resulting in \u03bc to stabilize and \u03c3 to drop to nearly 0.", "This article is partially based on my ResearchGate paper: \u2018Implementing Gaussian Actor Networks for Continuous Control in TensorFlow 2.0\u2019 , available at ResearchGate.", "The GitHub code (implemented using Python 3.8 and TensorFlow 2.3) can be found at my GitHub repository .", "Looking to implement the discrete variant or deep Q-learning? Check out:", "[5] Rosebrock, A. (2020) Using TensorFlow and GradientTape to train a Keras model. https://www.tensorflow.org/api_docs/python/tf/GradientTape", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Assistant professor in Financial Engineering and Operations Research. Writing about reinforcement learning, optimization problems, and data science."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd3413ec38c6b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----d3413ec38c6b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lenin33?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lenin Estrada"}, {"url": "https://unsplash.com/@lenin33?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.researchgate.net/publication/343714359_Implementing_Gaussian_Actor_Networks_for_Continuous_Control_in_TensorFlow_20", "anchor_text": "Implementing Gaussian Actor Networks for Continuous Control in TensorFlow 2.0"}, {"url": "http://www.github.com/woutervanheeswijk/example_continuous_control", "anchor_text": "GitHub repository"}, {"url": "https://towardsdatascience.com/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7", "anchor_text": "A Minimal Working Example for Discrete Policy Gradients in TensorFlow 2.0A multi-armed bandit example for training discrete actor networks. With the aid of the GradientTape functionality, the\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e", "anchor_text": "A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0A multi-armed bandit example to train a Q-network. The update procedure takes just a few lines of code using TensorFlowtowardsdatascience.com"}, {"url": "https://www.researchgate.net/publication/343714359_Implementing_Gaussian_Actor_Networks_for_Continuous_Control_in_TensorFlow_20", "anchor_text": "https://www.researchgate.net/publication/343714359_Implementing_Gaussian_Actor_Networks_for_Continuous_Control_in_TensorFlow_20"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf"}, {"url": "https://theanets.readthedocs.io/en/stable/api/generated/theanets.losses.GaussianLogLikelihood.html#theanets.losses.GaussianLogLikelihood", "anchor_text": "https://theanets.readthedocs.io/en/stable/api/generated/theanets.losses.GaussianLogLikelihood.html#theanets.losses.GaussianLogLikelihood"}, {"url": "https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/", "anchor_text": "https://www.tensorflow.org/api_docs/python/tf/GradientTape"}, {"url": "https://keras.io/examples/rl/actor_critic_cartpole/", "anchor_text": "https://keras.io/examples/rl/actor_critic_cartpole/"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d3413ec38c6b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/actor-network?source=post_page-----d3413ec38c6b---------------actor_network-----------------", "anchor_text": "Actor Network"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----d3413ec38c6b---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/continuous-control?source=post_page-----d3413ec38c6b---------------continuous_control-----------------", "anchor_text": "Continuous Control"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----d3413ec38c6b---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----d3413ec38c6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----d3413ec38c6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd3413ec38c6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d3413ec38c6b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d3413ec38c6b--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://wvheeswijk.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd27145dcd242&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b&newsletterV3=33f45c9ab481&newsletterV3Id=d27145dcd242&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}