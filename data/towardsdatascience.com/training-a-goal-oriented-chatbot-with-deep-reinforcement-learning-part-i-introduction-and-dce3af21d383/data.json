{"url": "https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383", "time": 1682994035.3290958, "path": "towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383/", "webpage": {"metadata": {"title": "Training a Goal-Oriented Chatbot with Deep Reinforcement Learning \u2014 Part I | by Max Brenner | Towards Data Science", "h1": "Training a Goal-Oriented Chatbot with Deep Reinforcement Learning \u2014 Part I", "description": "In this series we are going to be learning about goal-oriented chatbots and training one with deep reinforcement learning in python!"}, "outgoing_paragraph_urls": [{"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364", "anchor_text": "Part IV: User Simulator and Error Model Controller", "paragraph_index": 2}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d", "anchor_text": "Part V: Running the Agent and Future Research", "paragraph_index": 3}, {"url": "https://github.com/MiuLab/TC-Bot", "anchor_text": "TC-Bot", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1703.01008.pdf", "anchor_text": "paper", "paragraph_index": 7}, {"url": "https://github.com/keras-team/keras", "anchor_text": "Keras", "paragraph_index": 10}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/train.py", "anchor_text": "train.py", "paragraph_index": 11}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_db.txt", "anchor_text": "Database", "paragraph_index": 12}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_dict.txt", "anchor_text": "Database Dictionary", "paragraph_index": 14}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_user_goals.txt", "anchor_text": "User Goal List", "paragraph_index": 15}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py", "anchor_text": "dialogue_config.py", "paragraph_index": 18}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/train.py", "anchor_text": "train.py", "paragraph_index": 29}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c", "anchor_text": "Next", "paragraph_index": 30}], "all_paragraphs": ["In this series we are going to be learning about goal-oriented chatbots and training one with deep reinforcement learning in python! All from scratch! The code for this tutorial series can be found here.", "Part I: Introduction and Training Loop", "Part IV: User Simulator and Error Model Controller", "Part V: Running the Agent and Future Research", "A goal-oriented (GO) chatbot attempts to solve a specific problem for a user. These chatbots can help people book a ticket, find a reservation, etc. There are two main ways to train a GO chatbot: Supervised learning with an encoder-decoder that directly maps user dialogue to responses and reinforcement learning which trains a chatbot through trial-and-error conversations with either real users or a rule-based user simulator. GO chatbots trained through deep reinforcement learning is a very exciting and a ripe research field with numerous practical applications!", "The dialogue system for a GO chatbot using reinforcement learning is split into 3 main parts: The Dialogue Manager (DM), Natural Language Understanding (NLU) unit and Natural Language Generator (NLG) unit. Furthermore, the DM is split into the Dialogue State Tracker (DST) or just State Tracker (ST) and the policy for the agent itself, which is represented by a neural network in many cases. In addition the system loop contains a user with a user goal. A user goal represents what the user wishes to get out of the conversation which in the case of the diagram below is a restaurant reservation.", "In this loop, the user utters something which is processed by the NLU component into what\u2019s known as a semantic frame which is a lower-level representation of a natural language utterance that can be processed by the agent. The DST processes the user dialogue act (semantic frame) and the history of the current conversation into a state representation that can be used by the agent\u2019s policy. This state is then fed as input into the policy or neural network of the agent and outcomes an action in the form of a semantic frame. A database (DB) can also be queried to add information to the agent action such as restaurant info or movie ticket info. The agent (also known as \u201csystem\u201d in this diagram) action is then processed by the NLG component which converts it to natural language for the user to read.", "This tutorial and accompanying code is based off a dialogue system by MiuLab called TC-Bot. The main contribution of their paper is that it shows how to simulate a user using basic rules so that the agent can be trained with reinforcement learning very quickly, compared to training an agent with real people. Other papers have done this as well but this paper stands out as a good example (with code!) of how to make a successful training system.", "We will go into more detail on the user sim later in the series but for now understand that it is a deterministic rule-based simulator that attempts to model a real user. In this case it is based on user agenda modeling which means it uses an internal state that represents the constraints and needs of the user sim. This internal state keeps track of the current conversation as well as what it still needs to do to complete its current goal. The goal is picked randomly from a list of available user goals where a goal consists of a set of constraints and other information that guides the actions of the user sim as it attempts to fulfill its current goal. The error model controller (EMC) is used to add error to the user sim\u2019s action at the level of the semantic frame which was shown to improve the results of training.", "Here is a list of things that you should know that will not be covered in this series but are important to be able to understand the code:", "We will be coding in Python \u2265 3.5, Keras (any recent version) and of course numpy.", "Now let\u2019s move on to the data we will be using! We will be running over train.py in this part.", "Database: The database is of movie tickets with different attributes or slots. Here are the a few of the items (in no particular order):", "It is organized as a dictionary with the keys being the index of the ticket as a long (like integer) and the values also as dictionaries which contain the movie info that ticket represents. As you can see not all of the tickets have the same attributes present and obviously not always the same values!", "Database Dictionary: Another file contains a dictionary where the keys are the different slots that can be in a ticket and the values are lists of the possible values for each slot. Here are a few of the different items (truncated value lists):", "User Goal List: Finally, we have the user goals as a list of dictionaries that contain request and inform slots of each goal. We will go into more detail on what this means later. Examples:", "The goal with this database is for the agent to find a ticket that fits the user\u2019s constraints which is given by that episode\u2019s user goal. This is no easy task as each ticket is unique and most have different slots!", "It is very important to understand the anatomy of an action in this system. Both the user sim and the agent take as input and output actions in the form of semantic frames, if we ignore the natural language for a second. An action contains an intent, and inform and request slots. Slot in this tutorial series means key, value pair usually referring to a single inform or request. For example, in the dict {\u2018starttime\u2019: \u2019tonight\u2019, \u2018theater\u2019: \u2019regal 16\u2019}, \u2018starttime\u2019: \u2019tonight\u2019 and \u2018theater\u2019: \u2019regal 16\u2019 are both slots. Example actions:", "The intent represents the type of action it is, listed below. The rest of the action is split into inform slots which contain constraints and request slots which contain information that needs to be filled out. The list of possible keys are listed in dialogue_config.py and their values in the database dict mentioned above. A inform slot is information that the sender wishes the receiver to know. It is made of a key from the list of keys, and a value from that key\u2019s list of values. A request slot contains a key the sender wants to find a value for from the receiver. So it is a key from the list of keys and \u2018UNK\u2019 as the value which means \u201cunknown\u201d as the sender does not know what value works for this slot yet.", "A state is created by the ST as input for the agent to select an appropriate action. It is a numpy array of useful information from the history of the current conversation. We will go more in depth on this in part II.", "Now that the bookkeeping is done, lets get to the training loop and some code!", "This diagram represents the flow of a single round, one complete loop, in training. The 4 main parts of this system are the agent dqn_agent, dialogue state tracker state_tracker, user (or user simulator) user and EMC emc. Let\u2019s walk through the stages of one round:", "An important note is that as with any DQN agent the memory buffer is filled to some extent in a \u201cwarm-up\u201d stage. Unlike in many uses for DQNs in games the agent does not take random actions in this stage. Instead, during warm-up it uses a very simple rule-based algorithm, which will be explained in part II.", "As you can see we are not using any natural language (NL) components, so actions will always be semantic frames. In this series we are training the DM which does not require NL. NLG and NLU are pretrained separately from the agent and are not necessary to understand how to train an agent with DRL. Don\u2019t fret! You will still learn plenty and honestly learning how to train the agent with DRL is much more interesting then knowing how to train NL components with supervised learning. Take a look at part V of this series to see where you can learn about adding NL components!", "Before we get to the warm-up and training loops here is the episode reset function which is called before every episode. Also take note that a conversation is the same thing as an episode, and I use them interchangeably.", "In short, episode reset refreshes the objects and gets the initial user action of the episode.", "First, we define the outer loop to run only until the agent\u2019s memory has been filled to WARMUP_MEM or its memory buffer is full altogether. Next we have to reset the episode each loop and get the initial state. The inner loop runs run_round(state, warmup=True) until done == true meaning the episode is over.", "Ignoring some of the additional variables, the loop is very much the same as warm-up. The main difference so far is that this method ends its outer loop when the number of episodes has reached NUM_EP_TRAIN.", "After every period of a certain number of episodes (TRAIN_FREQ) the agent is trained with its memory of experiences.", "Take a look at the complete code in train.py to get the whole picture!", "That is it for the main training loop of a GO chatbot trained with DRL. It\u2019s important to understand the diagram. Next we will learn about the agent, the kinds of actions it can take, the warm-up policy and the internal weight optimization process.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interested in all things machine learning, procedural and generative"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdce3af21d383&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dce3af21d383--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maxbrenner110?source=post_page-----dce3af21d383--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Max Brenner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe83c3988e008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&user=Max+Brenner&userId=e83c3988e008&source=post_page-e83c3988e008----dce3af21d383---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "here"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c", "anchor_text": "Part II: DQN Agent"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iii-dialogue-state-d29c2828ce2a", "anchor_text": "Part III: Dialogue State Tracker"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364", "anchor_text": "Part IV: User Simulator and Error Model Controller"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d", "anchor_text": "Part V: Running the Agent and Future Research"}, {"url": "https://ilievskiv.github.io/2017/10/15/go-chatbots-intro/", "anchor_text": ""}, {"url": "http://aclweb.org/anthology/I17-1074", "anchor_text": ""}, {"url": "https://github.com/MiuLab/TC-Bot", "anchor_text": "TC-Bot"}, {"url": "https://arxiv.org/pdf/1703.01008.pdf", "anchor_text": "paper"}, {"url": "https://realpython.com/python-dicts/", "anchor_text": "dictionaries in python"}, {"url": "https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/", "anchor_text": "DQN"}, {"url": "https://github.com/keras-team/keras", "anchor_text": "Keras"}, {"url": "https://github.com/keras-team/keras", "anchor_text": "Keras"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/train.py", "anchor_text": "train.py"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_db.txt", "anchor_text": "Database"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_dict.txt", "anchor_text": "Database Dictionary"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_user_goals.txt", "anchor_text": "User Goal List"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py", "anchor_text": "dialogue_config.py"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/train.py", "anchor_text": "train.py"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c", "anchor_text": "Next"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dce3af21d383---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dce3af21d383---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----dce3af21d383---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----dce3af21d383---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&user=Max+Brenner&userId=e83c3988e008&source=-----dce3af21d383---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&user=Max+Brenner&userId=e83c3988e008&source=-----dce3af21d383---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdce3af21d383&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dce3af21d383---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dce3af21d383--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dce3af21d383--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dce3af21d383--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dce3af21d383--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dce3af21d383--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Brenner"}, {"url": "https://medium.com/@maxbrenner110/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "238 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe83c3988e008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&user=Max+Brenner&userId=e83c3988e008&source=post_page-e83c3988e008--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffb2cbe1972e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383&newsletterV3=e83c3988e008&newsletterV3Id=fb2cbe1972e0&user=Max+Brenner&userId=e83c3988e008&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}