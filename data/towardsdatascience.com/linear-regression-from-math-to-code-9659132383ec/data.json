{"url": "https://towardsdatascience.com/linear-regression-from-math-to-code-9659132383ec", "time": 1683013402.482641, "path": "towardsdatascience.com/linear-regression-from-math-to-code-9659132383ec/", "webpage": {"metadata": {"title": "Linear Regression: From Math to Code | by hollan haule | Towards Data Science", "h1": "Linear Regression: From Math to Code", "description": "If you have a background in statistics, perhaps even at high school level, you are probably familiar with Linear Regression (LR). Briefly, it is one of the core techniques used in statistics and\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["If you have a background in statistics, perhaps even at high school level, you are probably familiar with Linear Regression (LR). Briefly, it is one of the core techniques used in statistics and machine learning (ML) to define a model that best describes a set observed data, D, generated by an unknown process. While you can find plenty of examples online that explains how to do LR along with example code, my post attempts to provide a stepwise breakdown of the mathematics behind Bayesian LR and further attempts to explain how to implement it using Python code. Please note that my understanding and explanation of this concept including notation, is entirely based on the chapter 3 from the book [1]. The references are provided at the end for you to have a look. For those who are familiar with the algorithm, this post may act as a revision of the concepts behind it. For those who are not familiar, I hope this post acts a guide as you dive through the concepts provided by other authors. It is essential to have some knowledge of linear algebra, probability theory and calculus to get enough out of this post. Okay let\u2019s begin!", "Say you have a set, D, of observed data, (t\u1d62,x\u1d62), obtained from an unknown process where x\u1d62 and t\u1d62 are inputs and outputs respectively. Bold x\u1d62 means it\u2019s a vector. The relationship between an output of this unknown process, t\u1d62 and its input x\u1d62, can be described by a function t\u1d62 = f(x\u1d62,w). Geometrically, From a linear algebra point of view, once we have the right set of basis vectors, we can represent any vector/point in the vector space. The idea here is that our input lives in a D-Dimensional space, \u211d\u1d30, and our output lives in an M-dimensional space,\u211d\u1d39, which i like to call the output space. Our modeling problem now becomes finding a suitable value of M i.e. suitable number of basis and parameter values, w, that combines these basis. These basis are again, functions of x, which I believe is for tuning purposes!. These functions are referred to as basis functions. There are many functions that can be used as basis functions, such as polynomials, gaussians, sigmoids e.t.c. In the code section of this post we are going to use polynomials.", "This function describes a pattern in this data, and if there is indeed a pattern, next time we have an input x\u1d62, we expect to measure the same output t\u1d62, which can also be predicted by our function. However, practically this will not be entirely accurate as there is always some noise in our measurements. Therefore, our function should account for this noise. Our observations will tend to center around our function prediction. Because of the central limit theorem, the distribution of these predictions approximates to a Gaussian distribution.", "The joint probability of all of observations, assuming that they are independent and identically distributed (i.i.d) is a product of the probability distributions of the individual observations, expressed below.", "This expression is called likelihood, and if we apply log on both sides so that we can work with additions instead of products we obtain the expression below.", "The greek \u03a6 symbol is called the design matrix, we will see one way of building this in the code section. t is a column vector of output observations t\u1d62 and X is a matrix where each row is an observation x\u1d62. Assuming we fix a value for M, the values of the parameters of our model that maximizes the likelihood has a closed form solution, referred to as the Maximum likelihood estimate.", "To get the intuition for maximum likelihood, let\u2019s do a simple thought experiment. Say we flip a coin once, maybe we get a Tail. Okay, let\u2019s flip for a second time, aand we get a Tail again. Okay, maybe this coin is flawed. Let\u2019s flip one more time, and again this time we still get a tail. Based on these observations we might as well conclude that the chances of getting a Tail are 100%. But we know that our coin is fair i.e. prior knowledge. Using Baye\u2019s theorem, we use this prior knowledge to come up with a better expression for computing our parameter w.", "Applying log to both sides of the equation and solving for the optimum parameter w , as it was done before, we obtain the expression below and the parameter estimate below it. This is called Maximum a Posteriori estimate (MAP).", "Both both maximum likelihood and maximum a posteriori only gives us an exact estimate of model parameters which we can later use to make predictions. However, when making predictions it\u2019s important to have a measure of uncertainty so that we make proper decisions from the predictions. Clearly if we end here it\u2019s not sufficient for building a robust model.", "The Bayesian approach (BLR), takes maximum a posteriori one step further to form a predictive distribution. From this distribution, each future input will have corresponding target mean and variance/covariance. Below is a high level view of the math.", "Now the predictive distribution can be obtained using the equation below", "Both the posterior parameter distribution and predictive distribution can be obtained using the conditional and marginal distribution properties of gaussian distribution. See chapter 2 of [1] for more details.", "In all of the methods above, we have assumed that the variance of our data and parameters (i.e w), \u03b2\u207b\u00b9 and \u03b1\u207b\u00b9, respectively are known. In practice these values are unknown and can be estimated from the dataset. A full Bayesian treatment, which i\u2019m not going to cover in this post, offers a technique called evidence approximation that can be used to approximate these values. However, there are a couple of things worth pointing out and I will just mention them here. If \u03b1\u207b\u00b9 is very high, w will approximate to the maximum likelihood estimate. If the dataset is very small, w will approximate to the prior. As the dataset size increases towards infinity, the variance of our predictions become smaller approaching the limit \u03b2\u207b\u00b9.", "Using Python code, let\u2019s begin by constructing the design matrix. As stated earlier, we are going to use polynomial basis functions in this case.", "Lets first create a class for doing regression using maximum likelihood estimate (ML):", "Now let\u2019s create a class for doing prediction using Maximum a Posteriori (MAP):", "Finally, let\u2019s create a class for doing Bayesian Linear Regression. Note that this is not fully Bayesian, as the \u03b1\u207b\u00b9 and \u03b2\u207b\u00b9 are assumed to be known and should be passed in as standard deviations during class instantiation.", "Now let\u2019s test our models. We are going to use synthetic data, so first we create a simple function to generate synthetic data and then set some parameters and prepare training and testing data.", "Now create models, train, predict and plot i.e. for ML, MAP and BLR:", "In this post I have tried to cover the core mathematics behind linear regression and attempted to implement it using python code. However, as you can see, the model is not the best fit for the observed data. The primary reason being that the model is not complex enough to fit the data (i.e. underfitting). Model complexity is the number of parameters, M, of the model which for this example, I have used a random value of 7. The Bayesian approach also offers an approach that can approximate the optimal value for model complexity which I may cover in another post.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student | Coding | Math"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9659132383ec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9659132383ec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9659132383ec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hhollan?source=post_page-----9659132383ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhollan?source=post_page-----9659132383ec--------------------------------", "anchor_text": "hollan haule"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2de4078be2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&user=hollan+haule&userId=b2de4078be2f&source=post_page-b2de4078be2f----9659132383ec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9659132383ec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----9659132383ec---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----9659132383ec---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/math?source=post_page-----9659132383ec---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&user=hollan+haule&userId=b2de4078be2f&source=-----9659132383ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&user=hollan+haule&userId=b2de4078be2f&source=-----9659132383ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9659132383ec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9659132383ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9659132383ec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9659132383ec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9659132383ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9659132383ec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9659132383ec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9659132383ec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9659132383ec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9659132383ec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9659132383ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhollan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhollan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "hollan haule"}, {"url": "https://medium.com/@hhollan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "18 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2de4078be2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&user=hollan+haule&userId=b2de4078be2f&source=post_page-b2de4078be2f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa514e2903369&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-math-to-code-9659132383ec&newsletterV3=b2de4078be2f&newsletterV3Id=a514e2903369&user=hollan+haule&userId=b2de4078be2f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}