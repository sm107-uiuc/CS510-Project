{"url": "https://towardsdatascience.com/training-bert-at-a-university-eedcf940c754", "time": 1683017560.6993952, "path": "towardsdatascience.com/training-bert-at-a-university-eedcf940c754/", "webpage": {"metadata": {"title": "Training BERT at a University. Here\u2019s how we train enormous machine\u2026 | by Yifan Ding | Towards Data Science", "h1": "Training BERT at a University", "description": "If you\u2019re reading this post, you\u2019ve probably heard about the remarkable performance of new machine learning models like BERT, GPT-2/3, and other deep learning models for language, image, audio, and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "recently estimated", "paragraph_index": 1}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "HetSeq", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/2009.14783", "anchor_text": "available on ArXiV", "paragraph_index": 6}, {"url": "https://drive.google.com/file/d/1Vq_UO-T9345uYs8a7zloukGfhDXSDd2A/view?usp=sharing", "anchor_text": "this link", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/2009.14783", "anchor_text": "https://arxiv.org/pdf/2009.14783", "paragraph_index": 41}, {"url": "https://hetseq.readthedocs.io/en/master/task.html", "anchor_text": "Task", "paragraph_index": 44}, {"url": "https://hetseq.readthedocs.io/en/master/model.html", "anchor_text": "Model", "paragraph_index": 44}, {"url": "https://hetseq.readthedocs.io/en/master/dataset.html", "anchor_text": "Dataset", "paragraph_index": 44}, {"url": "https://hetseq.readthedocs.io/en/master/optimizer.html", "anchor_text": "Optimizer", "paragraph_index": 44}, {"url": "https://hetseq.readthedocs.io/en/master/lr_scheduler.html", "anchor_text": "Learning Rate Scheduler", "paragraph_index": 44}, {"url": "https://hetseq.readthedocs.io/en/master/extending.html", "anchor_text": "documentation", "paragraph_index": 44}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "HetSeq", "paragraph_index": 45}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "https://github.com/yifding/hetseq", "paragraph_index": 46}, {"url": "https://hetseq.readthedocs.io/", "anchor_text": "hetseq.readthedocs.io", "paragraph_index": 46}, {"url": "https://arxiv.org/pdf/2009.14783.pdf", "anchor_text": "HetSeq: Distributed GPU Training on Heterogeneous Infrastructure", "paragraph_index": 47}], "all_paragraphs": ["If you\u2019re reading this post, you\u2019ve probably heard about the remarkable performance of new machine learning models like BERT, GPT-2/3, and other deep learning models for language, image, audio, and video data.", "You may ask: Why do these semi-magical machine learning models perform so well? The short answer is that these models are enormously complex and are trained on an enormous amount of data. In fact, Lambda Labs recently estimated that it would require $4.6 million to train the GPT-3 on a single GPU \u2014 if such a thing were possible.", "Instead, platforms like PyTorch and Tensorflow are able to train these enormous models because they distribute the workload over hundreds (or thousands) of GPUs at the same time. Unfortunately, these platforms require that each individual GPU system be identical (i.e., they have the same memory capacity and compute performance).", "Unfortunately, most organizations not named Google or Microsoft do not have a thousand identical GPUs. Instead, small and medium organizations have a piecemeal approach to purchasing computer systems resulting in a heterogeneous infrastructure, which cannot be easily adapted to compute large models. Under these circumstances training even moderately-sized models could take weeks or even months to complete.", "If not addressed, universities and other small organizations risk losing relevance in the race to develop newer and better machine learning models.", "To help remedy this situation we recently released a software package called HetSeq, which is adapted from the popular PyTorch package and provides the capability to train large neural network models on heterogeneous infrastructure.", "Experiments, details of which can be found in an article (available on ArXiV) published at the 2021 AAAI/IAAI Conference, show that base-BERT can be trained in about a day over 8 different GPU systems, most of which we had to \u201cborrow\u201d from idle labs from across Notre Dame.", "Before we introduce HetSeq, we first need a little background:", "This code shows the training step of a basic supervised learning model in a neural network framework. Given some architecture, this task optimizes the model parameters via SGD on the loss function between predicted instances and ground truth.", "The actual training process is made of four individual steps: (1) data loading, (2) the forward pass, (3) the backward process, (4) update.", "On a single GPU the first thing that happens is that the existing model parameters (which are initially random) and the data are transferred to the GPU. Typically, the dataset includes a large number of training instances which will not all fit on a single GPU. In this common case we split the dataset into multiple batches and load them one at a time.", "The next step is to compute the loss function. To do this, the data batch is passed over the model (hence \u201cforward pass\u201d) and compared against the ground truth training labels. In the block, the forward pass has two steps: generate predicted label (output) and measure the difference (loss) between output and target.", "The loss computed in the previous step determines how much to change the model parameters; this is called the gradient, which is applied backward over the neural network architecture (hence backward pass or backpropagation).", "Remember that the goal of this whole process is to optimize the model parameters so that when the data is passed forward over them, then they will minimize the loss. So it is important that the model parameters are updated according to the values of the gradient.", "All together, one iteration of data loading, a forward pass of a single data instance, followed by a backward pass, and then the parameter update is called one step. Once all the data batches in the entire dataset are processed, we say that one epoch has been completed. Finally, it has been shown that the learning rate needs to change as the number of epochs increases.", "Since data batches are independent from one another, it is rather straightforward to parallelize this process by sending different data batches to different GPUs. Then, if we can somehow combine the computed loss and synchronize the updated model parameters, then we can make training much faster.", "This isn\u2019t a new idea. In PyTorch, we use the torch.nn.parallel.DistributedDataParallel (DDP)module instead of the torch.nn.Modulemodule for the model. Each GPU is an individual process, and the communication between GPUs occurs with standard IPC.", "But that\u2019s not the end of it. The four steps need some tweaking:", "With DDP we split each data batch over many different GPUs \u2014 as many as we have available. In this case it is critical that each GPU has the same model parameters.", "This is the core idea of distributed data parallel (DDP): each GPU has identical model parameters yet process different data batches simultaneously.", "Once different data batches are loaded into different GPUs, the next step is to perform a forward pass and compute the loss functions. Unlike in the single GPU case, we now need to compute the total loss of the all data batches, which is the sum of all the losses across all the GPUs. Because our goal is to compute the average loss for the backward step. It is important to output the number of instances (# of ins.) to the final output. We sum up the losses as well as number of instances.", "We use average loss to obtain gradients of the model parameters by backward pass. Before this begins the average loss needs to be communicated to the different GPUs so the model parameters can stay synchronized. Once the same parameters across different GPU obtain their gradients, gradient synchronization is executed to give them the same gradients.", "Once the gradients are synchronized, we can update model parameters in parallel using the individual optimizers on each GPU.", "The next training step can usually begin immediately. However, because nearly all the parameters are in floatand because computing error can occur in some GPUs, especially when many training steps are performed, we occasionally synchronize parameters at the beginning or end of a step.", "These changes are reflected in the following pseudocode. Notably, the function takes device id (i.e., the GPU id), the Model has to perform parameter synchronization before each forward pass, the loss function has to averaged before the backward pass, and finally the gradients need to be averaged before the model parameters are updated.", "So far, we\u2019ve talked about how to utilize multiple GPUs on the single node. That\u2019s great, but it\u2019ll only get us so far. If we want to really scale up, we need to distribute the workload across multiple nodes, each having multiple GPUs.", "Fortunately, the same mechanism used to address GPUs on a single node can be extended to multiple nodes. You can simply set the node index, i.e., the rankparameter in theinit_process_groupfunction, globally so that each GPU has a unique ID across all nodes.", "When you have multiple nodes with multiple GPUs, communication needs to occur between GPUs on the same node and across different nodes in order to share calculated gradients and parameter updates during the training procedure.", "Of course, inter-node communication is muchslower than intra-node communication. And sharing gradients and parameter updates become a complete mess when the nodes and GPUs are not identical \u2014 as is the case when you don\u2019t have a billion dollars to spend on a data center with custom compute hardware.", "At most university-compute centers, various research labs share their computing resources. There are different models for how this is done, but usually the IT administrators take a fair amount of control over the systems and prevent the scientists for installing (or upgrading or downgrading) needed software.", "This means that, if a large model needs to be trained, then some poor graduate students need to make the training algorithm fit the infrastructure.", "And this is difficult for a few reasons:", "Because of these problems we created a general system that wraps up all the complicated parts of DDP, data splitting, compatibility, and customizability and deployed this system at Notre Dame.", "We call this system HetSeq. It was adapted from the popular PyTorch package and provides the capability to train large neural network models on heterogeneous infrastructure. It can be lightly set up over shared file system without extra extra packages and administrative privileges.", "Here\u2019s how to train BERT with HetSeq.", "Let\u2019s get started with Anaconda. We\u2019ll Create a virtual environment and install python.", "Then we\u2019ll install packages and HetSeq bindings: we\u2019ll download HetSeq from GitHub, install packages in requirements.txt, as well as HetSeq and bindings from setup.py.", "The last step before we train is to download the BERT data files including training corpus, model configuration, and BPE dictionary from this link. Download DATA.zip, unzip it and place it into the preprocessing/directory.", "The cool thing about HetSeq is that it abstracts-away all the details about distributed processing. So the training code for 100 GPUs the almost the same same as 1 GPU! Let\u2019s try it out!", "In this case let\u2019s assume we have two compute nodes.", "The two blocks of code are run on two different nodes. The TCP/IP address needs be set to one of the node\u2019s IP addresses. Once these get started up, you\u2019ll be able to watch the code run across 8 GPUs on 2 different nodes!", "We have done some experiments (see https://arxiv.org/pdf/2009.14783 for details) over various homogeneous (hom) and heterogeneous (het) settings.", "In total we were able to commandeer 32 GPUs across 8 heterogeneous nodes to reduce the training time for the BERT language model from seven days to about one day. \ud83e\udd29\ud83e\udd29\ud83e\udd29", "The HetSeq package contains three major modules illustrated on the left figure: train.py, task.py, and controller.py to coordinate the main components illustrated on the right. The train.py module initializes the distributed system and its various components.The task.py module defines the model, dataset, data loader, and optimizer functions; it also executes the forward pass and backpropagation functions. The controller.py module acts as the main training controller. It executes the actual model, optimizer, and learning rate scheduler; loads and saves the checkpoint; communicates the loss; and updates the parameters.", "No problem. You can extend HetSeq to any other model. But you need to define a new Task with the corresponding Model, Dataset, Optimizer and Learning Rate Scheduler. An MNISTexample is given with all the extended classes. Pre-defined optimizers, Learning Rate Scheduler, datasetsand modelscan be reused in other applications. Check out our documentation for more details!", "In this post, we introduce the background and actual steps to train BERT from scratch at university by using our released package \u2014 HetSeq. It allows us to set up easily on heterogeneous systems with multiple nodes multiple GPUs.", "For more information, please check out HetSeq package (https://github.com/yifding/hetseq) and documentation (hetseq.readthedocs.io). Cheers!", "[1] Yifan Ding, Nicholas Botzer and Tim Weninger. HetSeq: Distributed GPU Training on Heterogeneous Infrastructure. Proc. of Association for the Advancement of Artificial Intelligence (AAAI) Innovative Application of Artificial Intelligence, 2021.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Yifan Ding is currently a Ph.D. student at University of Notre Dame, working with professor Tim Weninger on text mining and machine learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feedcf940c754&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dyf0125.medium.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": ""}, {"url": "https://dyf0125.medium.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Yifan Ding"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf1e0d54c326&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&user=Yifan+Ding&userId=cf1e0d54c326&source=post_page-cf1e0d54c326----eedcf940c754---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "https://github.com/yifding/hetseq"}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "recently estimated"}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "HetSeq"}, {"url": "https://arxiv.org/abs/2009.14783", "anchor_text": "available on ArXiV"}, {"url": "https://drive.google.com/file/d/1Vq_UO-T9345uYs8a7zloukGfhDXSDd2A/view?usp=sharing", "anchor_text": "this link"}, {"url": "https://arxiv.org/pdf/2009.14783", "anchor_text": "https://arxiv.org/pdf/2009.14783"}, {"url": "https://hetseq.readthedocs.io/en/master/task.html", "anchor_text": "Task"}, {"url": "https://hetseq.readthedocs.io/en/master/model.html", "anchor_text": "Model"}, {"url": "https://hetseq.readthedocs.io/en/master/dataset.html", "anchor_text": "Dataset"}, {"url": "https://hetseq.readthedocs.io/en/master/optimizer.html", "anchor_text": "Optimizer"}, {"url": "https://hetseq.readthedocs.io/en/master/lr_scheduler.html", "anchor_text": "Learning Rate Scheduler"}, {"url": "https://hetseq.readthedocs.io/en/master/extending.html", "anchor_text": "documentation"}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "HetSeq"}, {"url": "https://github.com/yifding/hetseq", "anchor_text": "https://github.com/yifding/hetseq"}, {"url": "https://hetseq.readthedocs.io/", "anchor_text": "hetseq.readthedocs.io"}, {"url": "https://arxiv.org/pdf/2009.14783.pdf", "anchor_text": "HetSeq: Distributed GPU Training on Heterogeneous Infrastructure"}, {"url": "https://medium.com/tag/distributed-systems?source=post_page-----eedcf940c754---------------distributed_systems-----------------", "anchor_text": "Distributed Systems"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----eedcf940c754---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/bert?source=post_page-----eedcf940c754---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/distributed-training?source=post_page-----eedcf940c754---------------distributed_training-----------------", "anchor_text": "Distributed Training"}, {"url": "https://medium.com/tag/open-source?source=post_page-----eedcf940c754---------------open_source-----------------", "anchor_text": "Open Source"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&user=Yifan+Ding&userId=cf1e0d54c326&source=-----eedcf940c754---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&user=Yifan+Ding&userId=cf1e0d54c326&source=-----eedcf940c754---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feedcf940c754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eedcf940c754---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eedcf940c754--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eedcf940c754--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eedcf940c754--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eedcf940c754--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eedcf940c754--------------------------------", "anchor_text": ""}, {"url": "https://dyf0125.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dyf0125.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yifan Ding"}, {"url": "https://dyf0125.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "14 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf1e0d54c326&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&user=Yifan+Ding&userId=cf1e0d54c326&source=post_page-cf1e0d54c326--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fcf1e0d54c326%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-bert-at-a-university-eedcf940c754&user=Yifan+Ding&userId=cf1e0d54c326&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}