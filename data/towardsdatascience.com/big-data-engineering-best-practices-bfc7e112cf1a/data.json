{"url": "https://towardsdatascience.com/big-data-engineering-best-practices-bfc7e112cf1a", "time": 1683013775.162716, "path": "towardsdatascience.com/big-data-engineering-best-practices-bfc7e112cf1a/", "webpage": {"metadata": {"title": "Big Data Engineering \u2014 Best Practices | by Kaya Kupferschmidt | Towards Data Science", "h1": "Big Data Engineering \u2014 Best Practices", "description": "This is part 1 of a series on data engineering in a big data environment. It will reflect my personal journey of lessons learnt and culminate in the open source tool Flowman I created to take the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://flowman.readthedocs.io", "anchor_text": "Flowman", "paragraph_index": 0}, {"url": "https://flowman.readthedocs.io", "anchor_text": "Flowman", "paragraph_index": 1}], "all_paragraphs": ["This is part 1 of a series on data engineering in a big data environment. It will reflect my personal journey of lessons learnt and culminate in the open source tool Flowman I created to take the burden of reimplementing all the boiler plate code over and over again in a couple of projects.", "This series is about building data pipelines with Apache Spark for batch processing. But some aspects are also valid for other frameworks or for stream processing. Eventually I will introduce Flowman, an Apache Spark based application that simplifies the implementation of data pipelines for batch processing.", "An every growing number of companies and projects build their data processing pipelines using Apache Spark as the central data processing framework. And there are very good reasons to do that (more on those in another part of this series).", "Apache Spark as being a framework per se does not provide much guidance to follow best practices when designing data pipelines nor does it take care of many details which are not directly part of the data transformation itself but which are important from a broader point of view like schema management and the ability to reprocess data in case of failures or logical errors.", "Before discussing various aspects to keep in mind while developing data processing pipelines (with whatever technology \u2014 although I prefer Apache Spark), let us first have a look what a typical data pipelines actually does.", "I would say that most data pipelines essentially contain three steps:", "Apache Spark well supports all these steps and you can implement a simple data pipeline with a couple of lines of code. Once you have it in production, some questions might arise over time leading to non-functional requirements and best practices. These are not directly implemented by Apache Spark, you have to take care of these yourself. This series is about supporting you with building rock solid data pipelines which also take care of many of those non-functional requirements which are nevertheless really important in production.", "This first part of the series is about best practices. This refers to specific ways of doing things in order to enable stable operations. The techniques to implement many of them is not revolutionary, but they are not discussed about very often (at least this is my impression).", "The first requirement is to provide some form of logging. This is not very exciting, and many developers already provide that. But often the question is how verbose the logging should be: Should every detail be logged to the console or only problems?", "This question is hard to answer, but I prefer to log all important decisions my application makes and to log all important variables and state information that has some influence on the application. You might want to ask yourself: What kind of information would be helpful for most incidents where your application does not work as expected?", "Typical examples of what I am logging:", "As mentioned in the last item, alerting is also something that you should think of. Most of the time it is desirable to store all logs in a central logging aggregator like Graylog where you can easily search for specific issues and setup alerts.", "Related to logging there is also the topic metrics. With this term I do not want to refer only to the internal technical metrics of Apache Spark, but also to some metrics with more business relevance. For example it might be interesting to provide metrics about the number of records read and written \u2014 both metrics are not directly available in Apache Spark, at least not per data source and data sink.", "A very important feature that you should plan in from the very beginning are so called reruns. In many batch processing applications, input data is provided in time slices (daily or hourly) and you only want to process the new data.", "But what happens if something goes wrong? For example what should be done if the input data is incomplete or corrupted? Or if your application contains some logic error and produces wrong results? The very simple answer to this situation is reruns. This term refers to the capability of your application to reprocess old data in case of any issue. But this capability doesn\u2019t come automatically, you have to carefully think about your data management strategy how to organize both your input and our output data to support reruns.", "There are a couple of aspects required for reruns, which are discussed separately in the next topics.", "In order to support reruns, you have to think about the data organization. In case of any error, you ideally can simply remove the output of a specific batch run and replace it with the results of a new batch run.", "This is very easily possible, if you use some simple partitioning mechanism. With partitioning I refer to use subfolders (for file based outputs) or Hive partitions (when you are using Hive) to organize your data which logically belongs to the same output (like a single Hive table). The basic idea is that every batch run should write into a separate partition.", "For example if you application processes new data coming in every hour, simply create partitions using the data and hour as its identifier. In a file based workflow, the partition is a directory which might look as follows for some imaginary data warehouse containing customer and transaction data", "Inside the directory, all files are stored from the specific job run for 2020\u201309\u201312 08:00 . In case of any error in that batch run, you can simply remove the whole directory and restart the job.", "If you are using Hive (and I strongly recommend to do so if you are using Spark on top of some shared file system or object store), partitions are a core feature of Hive. Unfortunately Spark doesn\u2019t support writing into a specific partition very well (but that limitation can be worked around).", "Schema management is a very important topic. This term refers to all project and development tasks about the data input and output format.", "When you read in the data of some source system, you expect a specific format of the data. This includes both the technical file format (like CSV, JSON or Parquet) and the set of columns and data types used to store the data. I highly recommend two things:", "The first advise improves the robustness of your application since silent schema changes are detected earlier, because the application should report an error if the input data does not match your expected schema any more (some relaxations are allowed and even advisable, more on that below). Many companies even have some lightweight organizational process for negotiating schemas \u2014 rightfully in my opinion, since a schema of a data export that is picked up by another application is a technical contract. And both parties (the delivering and the consuming side) should be aware of that by explicitly using the schema for writing and reading.", "The second advise is the more difficult one, especially combined with a rerun capability. Changing data processing to reflect a new version of the input schema can break the ability to reprocess old data, since it is probably stored using an older version of the schema. This means that you need to think about how your data pipeline can possibly work with different schema versions. A simple solution could be to use an older version of your data pipeline to process older data \u2014 but too often this is not a good option, because the older version of the application is missing some important features which should also be applied to older data.", "So my advise for reruns with different schema versions is to create some sort of super schema that is compatible with all versions of the input data \u2014 at least as an internal intermediate representation in your application before any business logic is applied. I also advise to try to negotiate with the source system to only allow compatible changes for new schema versions. Unfortunately the term compatible highly depends on the technology being used, for example Spring has other restrictions for changing types in JSON as Spark.", "Finally as an even broader concept, some sort of \u201cdata ownership\u201d should be available in every project. This term actually refers to two possibly different aspects, where responsibilities should be clear in case of any question or incident that needs some care taking:", "This was part 1 of a series about building robust data pipelines with Apache Spark. You might feel a little bit betrayed, because it didn\u2019t contain any actual code. Nevertheless I think it is important to discuss some concepts first. The next part will focus more on Apache Spark.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance Big Data and Machine Learning expert at dimajix."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbfc7e112cf1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@kupferk", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kupferk.medium.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Kaya Kupferschmidt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1b1c406b9d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=post_page-a1b1c406b9d0----bfc7e112cf1a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "https://unsplash.com/@realaxer?utm_source=medium&utm_medium=referral", "anchor_text": "tian kuan"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://flowman.readthedocs.io", "anchor_text": "Flowman"}, {"url": "https://medium.com/@kupferk/big-data-engineering-apache-spark-d67be2d9b76f", "anchor_text": "Part 2: Big Data Engineering \u2014 Apache Spark"}, {"url": "https://towardsdatascience.com/big-data-engineering-declarative-data-flows-3a63d1802846", "anchor_text": "Part 3: Big Data Engineering \u2014 Declarative Data Flows"}, {"url": "https://towardsdatascience.com/big-data-engineering-flowman-up-and-running-cd234ac6c98e", "anchor_text": "Part 4: Big Data Engineering \u2014 Flowman up and running"}, {"url": "https://flowman.readthedocs.io", "anchor_text": "Flowman"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Tyson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@nananadolgo?utm_source=medium&utm_medium=referral", "anchor_text": "Nana Smirnova"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@officestock?utm_source=medium&utm_medium=referral", "anchor_text": "Sebastian Herrmann"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/spark?source=post_page-----bfc7e112cf1a---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----bfc7e112cf1a---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/big-data?source=post_page-----bfc7e112cf1a---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----bfc7e112cf1a---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=-----bfc7e112cf1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=-----bfc7e112cf1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbfc7e112cf1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bfc7e112cf1a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bfc7e112cf1a--------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kaya Kupferschmidt"}, {"url": "https://kupferk.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "223 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1b1c406b9d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=post_page-a1b1c406b9d0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F874fa4e516b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-engineering-best-practices-bfc7e112cf1a&newsletterV3=a1b1c406b9d0&newsletterV3Id=874fa4e516b6&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}