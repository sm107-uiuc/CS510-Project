{"url": "https://towardsdatascience.com/demystifying-logistic-regression-ee24c1104d45", "time": 1682994739.3110201, "path": "towardsdatascience.com/demystifying-logistic-regression-ee24c1104d45/", "webpage": {"metadata": {"title": "Demystifying Logistic Regression. A geometrical study of Logistic\u2026 | by Dhairya Kumar | Towards Data Science", "h1": "Demystifying Logistic Regression", "description": "Logistic Regression is one of the most popular classification techniques. In most of the tutorials and articles, people usually explain the probabilistic interpretation of Logistic Regression. So in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else", "anchor_text": "this", "paragraph_index": 16}, {"url": "https://www.linkedin.com/in/dhairya-kumar/", "anchor_text": "LinkedIn", "paragraph_index": 52}, {"url": "https://twitter.com/DhairyaKumar16", "anchor_text": "Twitter", "paragraph_index": 52}, {"url": "https://github.com/Dhairya10", "anchor_text": "Github", "paragraph_index": 52}, {"url": "https://alpha-dev.in/", "anchor_text": "website", "paragraph_index": 52}], "all_paragraphs": ["Logistic Regression is one of the most popular classification techniques. In most of the tutorials and articles, people usually explain the probabilistic interpretation of Logistic Regression. So in this article, I will try to give the geometric intuition of Logistic Regression. The topics that I will cover in this article \u2014", "From the above image we can simply think of Logistic Regression as the process of finding a plane that best separates our classes and as mentioned above Logistic Regression assumes that the classes are linearly separable.", "So now what we need is a classifier that can separate the two classes. From figure1 we can observe that W^T * Xi > 0 represents the positive class as the positive class points lie in the direction of W and W^T * Xi < 0 represents the negative class .", "In the above paragraph, I mentioned that the objective of Logistic Regression is to find the plane that best separates the two classes and if you are an inquisitive person then you must be wondering how will we determine the best separation. So let\u2019s try to understand this.", "In order to measure anything we need a value and in this case, we will get that value by defining an optimisation function and the result of that function will be used to determine which plane is the best. That is just as vague and abstract as it can get, but I would like to explain it properly using few cases and their corresponding examples. Hold tight !!", "If you look at these cases carefully then you will observe that Yi * W^T*Xi > 0 means that we have correctly classified the points and Yi * W^T * Xi < 0 means that we have incorrectly classified the points.", "Looks like we have found our much-awaited optimisation function.", "So the plane with the maximum value for this function will act as the decision surface (the plane that best separates our points).", "Before you start celebrating the fact that we have got our optimisation function, let\u2019s analyse this function and make sure that this function works properly regardless of the dataset.", "As you might have guessed by now, our optimisation function is not robust enough to handle any outliers. Intuitively if you look at the above figure you will realise that \u312b1 is a better plane than \u312b2 as \u312b1 has correctly classified 14 data points and \u312b2 only correctly classified a single data point but according to our optimisation function, \u312b2 is better.", "There are various methods to remove outliers from the dataset but there is no such method that can remove 100% outliers and as we have seen above, even a single outlier can heavily impact our search for the best plane.", "So how can we handle this problem of outliers? Enter Sigmoid Function.", "The basic idea behind the Sigmoid function is squishing. Squishing can be explained as follows.", "The Sigmoid function squishes the larger values and all the values will lie between 0 and 1.", "Now you must be wondering that there are various other functions that can do the same job of limiting the values of our function within a certain range, then what is so special about sigmoid function.", "There are various reasons to choose sigmoid function over others \u2014", "If you are still not convinced then you can check this link to know more about sigmoid function.", "So our new optimisation function is \u2014", "We can further modify it by taking the log of this function to simplify the math.Since log is a monotonically increasing function therefore it won\u2019t affect our model. If you are not aware of what a monotonically increasing function is, then here is a brief overview \u2014", "There are still few transformations left before we get to the best version of our optimisation function.", "exp(-Zi) will always be positive. We are looking to minimise our optimisation function and the smallest value for exp(-Zi) is 0 .", "The minimum value for our optimisation function is 0, which occurs when exp(-Zi) is 0 as log(1+0) = 0.", "So the overall minimum value for our optimisation function will occur when", "Let\u2019s take a closer look at the term Zi .", "Since it is a supervised learning algorithm therefore we are given the values of X and Y.", "X \u2014 Features on the basis of which we predict the correct class label", "Y \u2014 The correct class label", "So we can\u2019t change Xi or Yi hence the only term left to manipulate is \u201cW\u201d. You can get a slight intuition that if we pick a really large value of W then only our Z will move closer to infinity.", "In order to move the value of Zi to infinity we will pick a very large value (either + or -) for W.", "So as you can see if we pick a large value for W then we can accomplish our goal of making Zi -> +\u221e", "By using the above strategy everything looks fine as our goal was to make -", "Zi-> \u221e and make log(1 + exp(- Zi)) -> 0 and if we use this strategy then we can successfully do that.", "The only problem with this strategy is that we can successfully minimise our optimisation function for all values of \u2018 i\u2019. Sounds a bit ironic as our goal was to minimise the function for all values of i, and all of a sudden it became a problem. If you are frustrated then it\u2019s a really good sign and it means that you have understood each and every detail till now. So let\u2019s dig deeper into this problem.", "The main problem here is that we are overfitting our model. If you are not familiar with the term overfitting then here is a brief overview -", "That is not a technically correct definition and I just wanted to give you an intuition of overfitting.", "Here the red dots represent negative data points and green dots represent positive data points.", "As you can see in the case of overfitting that our decision surface perfectly classifies each and every point and we will get a 100% accurate result on our training data in this case. But consider this scenario \u2014", "Here the blue point is our test data point and we want to predict whether it belongs to positive or negative class and as you can see, according to our decision surface it is a negative class point but we can see that it is most probably a positive class point since it is closer to positive class points than negative class points. This is called Overfitting.", "This is exactly how our model will look like if we follow the above strategy of always choosing a large value of W and making Zi -> +\u221e", "Now that you have FINALLY understood what the actual problem is, we can move towards finding a solution and that solution is Regularisation.", "A lot of you might have a vague idea about this and you must have heard that it is used to prevent overfitting and underfitting but very few people actually know how can we prevent underfitting and overfitting by using regularisation. So BRACE YOURSELF you are about to join that elite group.", "There are two major types of regularisation \u2014", "In L2 regularisation we introduce an additional term called the regularisation term in order to prevent overfitting.", "Here \u2018\u03bb\u2019 is a hyperparameter that will play an important role in our classification model but first let\u2019s focus on the effect of this regularisation term.", "If you remember that our goal was to make Zi -> +\u221e and since Xi and Yi are fixed therefore we could only tweak the value of W and here you can see that we are multiplying W^TW with \u03bb.", "So earlier we were increasing the value of W to make it +\u221e or -\u221e but now if we try to do that then although the value of our loss term will move towards 0 the value of our regularisation term will be very very large. So there is essentially a trade-off between loss term and regularisation term.", "The regularisation term essentially penalises our model for choosing very large values of W, hence avoiding overfitting.", "\u03bb plays a key role in optimising our function.", "The purpose of L1 regularisation is the same as that of L2 i.e. avoiding overfitting in this case.", "The main difference between L1 and L2 Regularisation is that L1 Regularisation creates sparse vectors.", "And with that, we have come to the end of this article. Thanks a ton for reading it.", "You can clap if you want. IT\u2019S FREE.", "My LinkedIn, Twitter and GithubYou can check out my website to know more about me and my work.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Applying ML to solve important problems at scale"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fee24c1104d45&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dhairyakumar10.medium.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": ""}, {"url": "https://dhairyakumar10.medium.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Dhairya Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39186e93c835&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&user=Dhairya+Kumar&userId=39186e93c835&source=post_page-39186e93c835----ee24c1104d45---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hjrc33?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "H\u00e9ctor J. Rivas"}, {"url": "https://unsplash.com/collections/1413080/geometry?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else", "anchor_text": "this"}, {"url": "https://www.linkedin.com/in/dhairya-kumar/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/DhairyaKumar16", "anchor_text": "Twitter"}, {"url": "https://github.com/Dhairya10", "anchor_text": "Github"}, {"url": "https://alpha-dev.in/", "anchor_text": "website"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ee24c1104d45---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----ee24c1104d45---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&user=Dhairya+Kumar&userId=39186e93c835&source=-----ee24c1104d45---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&user=Dhairya+Kumar&userId=39186e93c835&source=-----ee24c1104d45---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fee24c1104d45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ee24c1104d45---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ee24c1104d45--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ee24c1104d45--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ee24c1104d45--------------------------------", "anchor_text": ""}, {"url": "https://dhairyakumar10.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dhairyakumar10.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dhairya Kumar"}, {"url": "https://dhairyakumar10.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "271 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39186e93c835&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&user=Dhairya+Kumar&userId=39186e93c835&source=post_page-39186e93c835--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6336ccd93ed3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-logistic-regression-ee24c1104d45&newsletterV3=39186e93c835&newsletterV3Id=6336ccd93ed3&user=Dhairya+Kumar&userId=39186e93c835&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}