{"url": "https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a", "time": 1682994777.593626, "path": "towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a/", "webpage": {"metadata": {"title": "Neural Networks from scratch with Numpy \u2014 Part 2: Linear Regression | by Suraj Donthi | Towards Data Science", "h1": "Neural Networks from scratch with Numpy \u2014 Part 2: Linear Regression", "description": "In this tutorial, you will dig deep into implementing a Linear Perceptron (Linear Regression) from which you\u2019ll be able to predict the outcome of a problem! This tutorial will apparently include a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction", "anchor_text": "Khan Academy video", "paragraph_index": 62}], "all_paragraphs": ["In the previous tutorial, you got a very brief overview of a perceptron.", "In this tutorial, you will dig deep into implementing a Linear Perceptron (Linear Regression) from which you\u2019ll be able to predict the outcome of a problem!", "This tutorial will apparently include a bit more of math as it is inevitable, but there\u2019s no need to worry as I will explain them ground up. Regardless of this, it must be realized that all machine learning algorithms are basically mathematical formulations that are finally implemented in the form of code.", "Before we start off, remember that we had used the threshold activation function to mimic the function of AND and NOR Gates?!", "Here we will use another extremely simple activation function called linear activation function (equivalent to not having any activation!).", "Let us find out the wonders that this activation function can do!", "Let\u2019s assume that there is only one input and bias to the perceptron as shown below:", "The resulting linear output (i.e., the sum) will be", ". This is the equation of a straight line, as shown in the below figure.", "It must be noted here that when no activation function is used, we can say that the activation function is linear.", "This is a multivariate(multiple variables) linear equation.", "Let us see how this is utilized for predicting the actual output of y in the next section i.e., Linear Regression.", "Fitting a linear equation on a given set of data in n-dimensional space is called Linear Regression. The below GIF image shows an example of Linear Regression.", "In simple words, you try to find the best values of m and b that best fits the set of points as shown in the above figure. When we have obtained the best possible fit, we can predict the y values given x.", "A very popular example is the housing price prediction problem. In this problem, you are given a set of values like the area of the house and the number of rooms, etc. as features and you must predict the price of the house given these values.", "So, the big question is\u2026 How does the prediction algorithm work? How does it learn to predict?", "Let\u2019s learn this on the go!", "Let\u2019s start by importing the required packages.", "You\u2019ll use the sklearn dataset generator for creating the dataset. You will also use the package for splitting the data into training and test data. If you are not aware of sklearn, it is a rich package with many machine learning algorithms. Although you get pre-built functions for performing linear regression, you are going to build it from scratch in this tutorial.", "For creating the dataset, you must first set a list of hyperparameters \u2014 while m and b are parameters, the number of samples, the number of input features, the number of neurons, the learning rate, the number of iterations/epochs for training, etc. are called hyperparameters. You shall learn about these hyperparameters as you implement the algorithm.", "For now, you shall set the number of training samples, the number of input features, the learning rate, and epochs. You shall understand the learning rate and epochs in a short while.", "Your first task would be to import or generate data. In this tutorial, you\u2019ll generate the dataset using sklearn's make_regression function.", "For the purpose of learning, we shall keep the number of features minimal so that it is easy to visualize. Hence, you must choose only one feature.", "Now, it\u2019s time to visualize what the data generator has cooked up!", "Let\u2019s check the shape of the vectors for consistency.", "We need reset the size of y to (200, 1) so that we do not get errors during vector multiplications.", "Next, you will have to split the dataset into train and test sets, so that you can test the accuracy of the regression model using a part of the dataset once you have trained the model.", "Now let\u2019s split the data into train set and test set.", "In our case, the training set is 80% and the test set is 20%.", "Let\u2019s check the shape of the Train and Test datasets created.", "So, what have we achieved till now?", "We have done the initial data preprocessing and also explored the data through visualizing it. This is typically the first step while modeling any machine learning algorithm. We have also split the data for testing the accuracy of the model once it is trained.", "Clearly as shown in the above Linear Regression GIF image, we need to consider a random line at first and then fit it on the data through training.", "Therefore, the next step is to randomly generate a line with a random slope and an intercept(bias). The goal is to achieve the best fit for the line.", "Now, given m & b, we can plot the line generated.", "Let\u2019s update the function plot_graph to show the predicted line too.", "Since the line is now generated, you\u2019ll need to predict the values it is producing for a given value of x. From this value, all there is to do is to calculate their mean squared error. Why?", "How could we find the difference between the actual output and the predicted output?", "The simplest way would be to just subtract these two differences. We have a random line that gives an output y_pred for every x that is given, but it\u2019s surely not the actual output. Luckily, we have the actual output of all x too! So what we do is instead of taking the difference directly (which is technically called absolute distance or L1 distance), we square it (called the Euclidean distance or L2 distance) and take the mean for all the given points & this is called Mean Squared Error.", "Let us now predict the values of y_pred from the parameters m & b given the datapoints X_train by defining a function forward_prop.", "As mentioned earlier, now that you have both the corresponding values for X_train and the predicted values for y_pred you\u2019ll calculate the Cost/Error/Loss Function.", "Summing over all M examples, we obtain the Loss fn. as below:", "Our goal is to obviously minimize the Loss so the regression line predicts more accurately.", "You will also save each value of Loss that will be computed to graphically visualize how it changes during training.", "Let\u2019s modify the above plot_graph function defined above to plot the Loss too.", "You\u2019ll visualize the line created from the parameters m and b.", "Now that you have computed the loss, let\u2019s minimize it.", "Since Loss is the dependent variable and m & b are the independent variables, we\u2019ll have to update m & b so as to find the minimum Loss.", "So, the immediate question would be\u2026", "How can I update the parameters m and b?", "Let us for instance consider just a single parameter p as shown below and let t(target) be the value that has to be predicted. We see that as cost converges to the minima, the parameter p reaches a specific value called the optimal value. Let\u2019s say the optimum value of p is a.", "You can make a few observations from this graph.", "It is clear from the graph, that as p moves towards a (minima), the Cost decreases, and as it moves away from it, the cost increases.", "Now, how can we make p move towards a, regardless of whether it is on the left or to the right of a as shown in the figure?", "Let us consider the p of the curve. From calculus, we know that the slope of a curve at a point is given by dy/dx (here it is dL/dp where L \u2192 Loss). From the fig., when p is to the left of a, the slope is obviously \u2012ve and when it\u2019s to the right, the slope would be +ve. But we see that if p is to the left of a, some value must be added to p. Likewise, some value must be subtracted when p is to the right of a.", "This means that when the slope is \u2012ve implies p = p + (some val.) and when the slope is +ve implies p = p \u2012 (some val.) to move towards a.", "\u2234 We subtract the slope from p. This way, the slope is negated and it ensures that it always moves towards a. The resulting equation would be,", "It must also be observed that if the cost is too high, the slope will be too high. Hence, while subtracting the slope from p, p value might overshoot a. Hence, it is necessary to decrease the value of the slope so that p does not overshoot a. Therefore, we introduce a dampening factor called the Learning Rate (\u03b1) to the slope. You\u2019ll see later that by varying \u03b1 the rate of decrease in error varies.", "What we finally obtain would be,", "As shown in the figure, the trajectory taken by p against cost is that of a Bell curve.", "This method is called Gradient Descent.", "In our case, we use two parameters m and b. Therefore, the Bell curve would be 3-dimensional as shown in the below figure.", "As mentioned, you\u2019ll compute the partial derivative of the loss function w.r.t to the parameters m & b. [Note: It is usually expected that you know the basic concepts of partial derivatives. However if you do not, you can refer this wondeful Khan Academy video]", "Now we subtract the slope of the parameters m and b from their respective derivatives along with the dampening factor \u03b1(alpha).", "From decreasing the values of m and b, they are incrementally moving towards the minima. So updating the parameters this way has to be done for many iterations, which is called epochs.", "Let us define a function grad_desc, which calls both gradient and update_params.", "We have now defined everything that we need, so let\u2019s compile all the functions into one and see how our algorithm works. So, before you can actually run the code, you\u2019ll have to set the hyperparameters.", "Since you have trained the parameters for 60 epochs and the regression line looks to be fitting the data, you can move forward to the last phase, i.e., prediction on our test data and checking the accuracy.", "For checking the accuracy, you can take the mean of percentage error for all the test data points.", "The accuracy is 80% which is \u201cok\u201d considering the variance in the data as is seen in the above graphs.", "I was hoping to introduce something really interesting in the article and as a bonus, I have also added an intro to Neural Networks. But this surely comes with a catch!", "The Neural Network is shown below.", "From the image, we observe that there are two inputs each to the two neurons in the first layer and an output neuron in the second layer.", "We will be using matrices for representing our above equations. We can represent them in vector (single column matrix) form as:", "While doing matrix computations, we\u2019ll need to take care of the dimensions and multiply. Hence, we rearrange a bit to arrive at the required output.", "The expansion of the equation is not required and hence let\u2019s stick to", "Now the output from the 2\u207f\u1d48 layer will be:", "From the above set of equations, we see that a neural network with a linear activation function reduces to a linear equation.", "The whole purpose of neural networks was to create a very complex function that can fit on any sort of data and as it can be clearly seen, a neural network with linear activation functions fails the purpose. Hence, it should be strictly noted that a linear function cannot be used as an activation function for the neural network, although it can be used only in the last layer for regression problems.", "Then I guess you\u2019ll have to hold your horses until the next tutorial to implement one!", "Go ahead clone it and start running the cells on your Colab to see the miracles of Gradient Descent!!", "In the next tutorial, you\u2019ll learn about Sigmoid Activation Function and perform Logistic Regression which is the most important key to implement neural networks.", "You can read through the next article on Logistic Regression here:", "Are you working on any cool Deep Learning project?", "You can connect with me on Linkedin:", "OR message me on Twitter for any queries:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe53c0c7dea3a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@surajdonthi95?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@surajdonthi95?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Suraj Donthi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25979339f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&user=Suraj+Donthi&userId=c25979339f86&source=post_page-c25979339f86----e53c0c7dea3a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2", "anchor_text": "Neural Networks from scratch with Numpy: IntroductionIn this tutorial, you will get a brief understanding of what Neural Networks are and how they have been developed. In\u2026towardsdatascience.com"}, {"url": "https://camo.githubusercontent.com/310f1cd8eb881d776474abb62acf1d17a911c2e4/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a656549766c776b4d4e473177536d6a334652364d32672e676966", "anchor_text": "Source Link"}, {"url": "https://camo.githubusercontent.com/a401a48f5503c52004369148a784e779aa7e3411/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a70775049472d475748796150564d564747354f6841512e676966", "anchor_text": "Source Link"}, {"url": "https://imgflip.com/i/2dz47q", "anchor_text": "https://imgflip.com/i/2dz47q"}, {"url": "https://camo.githubusercontent.com/b9d3586045436ffb04c29c82923bec26d494eb1c/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f4f3972635a566d5263454771492f67697068792e676966", "anchor_text": "Source"}, {"url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction", "anchor_text": "Khan Academy video"}, {"url": "https://i.chzbgr.com/full/2689942016/h8B269F68/", "anchor_text": "https://i.chzbgr.com/full/2689942016/h8B269F68/"}, {"url": "https://github.com/SurajDonthi/Article-Tutorials/blob/master/NN%20with%20Numpy%202/Neural_Networks_for_Absolute_Beginners_Part_2_Linear_Regression.ipynb", "anchor_text": "SurajDonthi/Article-TutorialsArticles related to Neural Networks will be posted here. - SurajDonthi/Article-Tutorialsgithub.com"}, {"url": "https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-3-logistic-regression-18b474096a4e", "anchor_text": "Neural Networks from scratch with Numpy \u2014 Part 3: Logistic RegressionThe sigmoid activation function is the most elemental concept in Neural Networks. In this tutorial, you will learn to\u2026towardsdatascience.com"}, {"url": "https://www.linkedin.com/in/suraj-donthi/", "anchor_text": "Suraj Donthi | LinkedInSuraj Donthi is a Computer Vision Consultant | Machine Learning & Deep Learning Practitioner and Trainer. Connect with him...www.linkedin.com"}, {"url": "https://twitter.com/suraj_donthi", "anchor_text": "Suraj Donthi (@suraj_donthi) | Twittertwitter.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e53c0c7dea3a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e53c0c7dea3a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----e53c0c7dea3a---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/perceptron?source=post_page-----e53c0c7dea3a---------------perceptron-----------------", "anchor_text": "Perceptron"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e53c0c7dea3a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&user=Suraj+Donthi&userId=c25979339f86&source=-----e53c0c7dea3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&user=Suraj+Donthi&userId=c25979339f86&source=-----e53c0c7dea3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe53c0c7dea3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e53c0c7dea3a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e53c0c7dea3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@surajdonthi95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@surajdonthi95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Suraj Donthi"}, {"url": "https://medium.com/@surajdonthi95/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "75 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25979339f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&user=Suraj+Donthi&userId=c25979339f86&source=post_page-c25979339f86--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fadf447e480a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a&newsletterV3=c25979339f86&newsletterV3Id=adf447e480a4&user=Suraj+Donthi&userId=c25979339f86&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}