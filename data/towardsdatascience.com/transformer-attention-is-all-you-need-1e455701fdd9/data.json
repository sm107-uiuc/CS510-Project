{"url": "https://towardsdatascience.com/transformer-attention-is-all-you-need-1e455701fdd9", "time": 1683001010.4898748, "path": "towardsdatascience.com/transformer-attention-is-all-you-need-1e455701fdd9/", "webpage": {"metadata": {"title": "Transformer \u2014 Attention is all you need | by Pranay Dugar | Towards Data Science", "h1": "Transformer \u2014 Attention is all you need", "description": "In the previous post, we discussed attention-based seq2seq models and the logic behind their inception. The plan was to create a PyTorch implementation story about the same but turns out, PyTorch\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@catchpranay/day-1-2-attention-seq2seq-models-65df3f49e263", "anchor_text": "post", "paragraph_index": 0}, {"url": "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "[1]", "paragraph_index": 17}], "all_paragraphs": ["In the previous post, we discussed attention-based seq2seq models and the logic behind their inception. The plan was to create a PyTorch implementation story about the same but turns out, PyTorch documentation provides an excellent procedure here. So here, I move onto the next item in my plan \u2014 the transformer \u2014 which works on the principle of Self Attention.", "Let's do a two-line recap of the attention-based model. Its primary ideology was that it took an input sequence and all the hidden states associated with it and at every instance of the output, it decided which part of the input was useful, and subsequently decided the output based on that. The sequential nature was captured by using either of RNNs or LSTMs in both the encoder and the decoder.", "In the Attention is all you need paper, the authors have shown that this sequential nature can be captured by using only the attention mechanism \u2014 without any use of LSTMs or RNNs.", "In this post, we will follow a similar structure as in the previous post, starting off with the black box, and slowly understanding each of the components one by one thus increasing the clarity of the whole architecture. At the end of this, you will have a good grasp of each component of the transformer network and how each of them helps in getting the desired result.", "Pre-requisites: Basic knowledge about Attention networks and other encoder-decoder-based models. Neural networks and Normalization techniques.", "Let us first understand the basic similarities and differences between the attention and the transformer models. Both aim to achieve the same result using an encoder-decoder approach. The encoder converts the original input sequence into its latent representation in the form of hidden state vectors. The decoder tries to predict the output sequence using this latent representation. But the RNN based approach has an inherent flaw. Due to the fundamental constraint of sequential computation, it is not possible to parallelize the network, which makes it hard to train on long sequences. This, in turn, puts a constraint on the batch size that can be used while training. This has been alleviated by the transformer and we\u2019ll soon learn how. So let\u2019s just dive right into it.", "(I\u2019d like to call out that this post has a little bit of extra reading and lesser images but I\u2019ll ensure that the reading is as simple as possible.)", "The transformer architecture continues with the Encoder-Decoder framework that was a part of the original Attention networks \u2014 given an input sequence, create an encoding of it based on the context and decode that context-based encoding to the output sequence.", "Except for the issue of not being able to parallelize, another important reason for working on improvement was that the attention-based model would inadvertently give a higher weight-age to the elements in the sequence closer to a position. Though this might make sense in the sense of understanding the grammatical formation of various parts of the sentence, it is hard to find relations between words far apart in the sentence.", "Like the last post, we will incrementally arrive at the self-attention model, starting off with the basic architecture and slowly getting to the complete transformer.", "As seen, it follows the encoder-decoder design, meanwhile replacing the LSTMs with the Self Attention layer and the sequential nature being identified using the Positional Encodings. One important point to remember is that all these components are only made of fully connected (FC) layers. Since the whole architecture is FC layers, it\u2019s easy to parallelize it.", "So we are now left with the following doubts:", "As we clarify each of these questions, the model will become clearer.", "Given a sequence of tokens x1, x2, x3, \u2026, the input sequence corresponds to an embedding of each of these tokens. This embedding could be something as simple as one-hot encodings. For eg. in the case of a sentence, x1, x2, x3, \u2026would correspond to the words in the sentence. The input seq could then be the one-hot enc of each of these words.", "As there is no component in this new encoder-decoder architecture that explains the sequential nature of the data, we need to inject some information about the relative or absolute position of the tokens in the sequence. This is the task of the positional encoding module.", "Ignoring the mathematical formulation, given an embedding for token x at position i, a positional encoding for the i\u2019th position is added to that embedding. This injection of position is done such that each positional encoding is distinct from any other. Every dimension of the positional enc corresponds to a sinusoid wavelength, with the final enc being the value of each of these sinusoid waves at the i\u2019th point.", "As described by the authors of \u201cAttention is All You Need\u201d,", "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.[1]", "This layer aims to encode a word based on all other words in the sequence. It measures the encoding of the word against the encoding of another word and gives a new encoding. The method in which this is done is a little complex and I\u2019ll try to break it down as far as possible.", "Given an embedding x, it learns three separate smaller embeddings from it \u2014 query, key, and value. They have the same number of dimensions. What I mean by learns is that during the training phase, the Wq, Wk, and Wv matrices are learned based on the loss which has back-propagated. x is multiplied with these three matrices to get the query, key, and value embeddings.", "For the purpose of understanding these terms, let\u2019s treat these words as living entities. Each word wants to know how much every other word values it and then create a better version of itself that represents this valuation.", "Say x1 wants to know its value with respect to x2. So it\u2019ll \u2018query\u2019 x2. x2 will provide the answer in the form of its own \u2018key\u2019, which can then be used to get a score representing how much it values x1 by taking a dot product with the query. Since both have the same size, this will be a single number. This step will be performed with every word.", "Now, x1 will take all these scores and perform softmax to ensure that the score is bounded while also ensuring that the relative difference between the scores is maintained. (There is also the step of dividing the score before softmax by the square root of the d_model \u2014 embedding dimension \u2014 to ensure stable gradients in case the score is too large in cases where d_model is a large number.)", "This scoring and softmax task is performed by every word against all other words. The above diagram paints a picture of this whole explanation and will be more easily understood now.", "x1 will now use this score and the \u2018value\u2019 of the corresponding word to get a new value of itself with respect to that word. If the word is not relevant to x1 then the score will be small and the corresponding value will be reduced as a factor of that score and similarly the significant words will get their values bolstered by the score.", "Finally, the word x1 will create a new \u2018value\u2019 for itself by summing up the values received. This will be the new embedding of the word.", "But the Self Attention layer is a little more complex than just this. The transformer architecture has added Multi-head attention to this layer. Once this concept is clear we will move closer to the final transformer design.", "Multiple such sets of query, key, and value are learned using the same original x1, x2, etc. embedding. The self-attention steps are performed on each set separately and new embeddings v\u20191, v\u20192, etc. are created for each set. These are then concatenated and multiplied with another learned matrix Z (which is trained jointly) which reduces these multiple embeddings to a single embedding for x\u20191, x\u20192, etc. This is why this is referred to as Multi-head attention, each v\u2019 representing a head of the self-attention model.", "The intuitive reason for this design is that each of the heads looks at the original embedding in a different context as each of the Q, K, V matrices are initialized randomly at the beginning and then modified based on the loss backpropagated during training. So the final embedding is learned taking into consideration various contexts at the same time.", "So at the end of this whole section on self-attention, we see that the self-attention layer takes as input a position injected na\u00efve form of embeddings and outputs more context-aware embeddings. With this, we\u2019ve cleared up the most difficult of the above-specified questions.", "The key feature of layer normalization is that it normalizes the inputs across the features, unlike batch normalization which normalizes each feature across a batch. Batch norm has the flaw that it imposes a lower bound on the batch size. In layer norm, the statistics are computed across each feature and are independent of other examples. It has been seen to perform better experimentally.", "In the transformers, layer normalization is done with residuals, allowing it to retain some form of information from the previous layer.", "Each encoder and decoder block contains a feed-forward neural net. It consists of two Linear layers with a relu activation between them. It is applied to each position separately and identically. Hence the input to it is a set of embeddings x\u20191, x\u20192, etc. and the output is another set of embeddings x\u2019\u20191, x\u2019\u20192, etc. of the same dimensions mapped to another latent space which is common to the whole language.", "With this, the encoder side of the network should be clear. Now we are only left with two of the questions we started out with \u2014 Masked self-attention and Enc-Dec attention.", "At any position, a word may depend on both the words before it as well as the ones after it. For eg. in \u201cI saw the ______ chasing a mouse.\u201d we would intuitively fill cat as that is the most probable one. So while encoding a word, it needs to know everything that comes in the whole sentence. This is why in the self-attention layer, the query was performed with all words against all words. But at the time of decoding, when trying to predict the next word in the sentence (which is why we have a shifted input sequence for decoder input), logically, it should not know what are the words that are present after the word we are trying to predict. Hence, the embeddings for all these are masked by multiplying with 0, rendering any value from them to become 0 and only predicting based on the embeddings created using the words which came before it.", "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.", "If you observe the network above carefully, the input from the encoder module is coming in from the encoder in this layer. Before this, the decoder has used the information from whatever information can be gained from its previous predictions and learned new embeddings for those words. At this layer, it uses the encoder to get a better understanding of the context and the complete sentence as a whole in the original sequence. How does it do that?", "The Decoder queries all the existing words with the encoded embeddings of the words in the original sequence, which carry both the positional information as well as the contextual information. The new embeddings are now injected with this information and the output sequence is now predicted based on this.", "With this, we have now cleared up the transformer architecture, but there is one part which I still haven\u2019t specified.", "This is the architecture of the transformer we have until now. What we need to note is that the output of the encoder is an improved version of the original embeddings. So we should be able to improve it further by adding more. This is the point that is leveraged in the final design of the transformer network.", "The original architecture has a stack of 4 such encoders and decoders. Only the output of the final encoder is taken for the decoder input.", "We started off with the basic design of a transformer with the various layers in a single encoder and decoder set. We then understood the details of how each layer functions. Within this, we also covered the multi-head architecture of the self-attention layer. Once both the encoder and the decoder were clear, we moved on to the final piece in the architecture, the encoder-decoder stack.", "This completes the detailed design of the Transformer architecture. I hope this had all the details that are needed to understand the complete picture. One of the most prevalent examples of a transformer-based network is the BERT model for the task of Natural Language Processing tasks.", "I haven\u2019t covered the training and prediction details of the network as that would take another complete post and this was easily the more interesting and complicated part for me. In the next post, I'll cover something about how we can use transformers for images and the task of unsupervised learning.", "I hope you found this useful and easily understandable. In case of any corrections or any kind of feedback, I\u2019d love to hear from you. Please comment here and let me know or email me @ catchpranay@gmail.com", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e455701fdd9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@catchpranay?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@catchpranay?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Pranay Dugar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F65033b05ecc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&user=Pranay+Dugar&userId=65033b05ecc1&source=post_page-65033b05ecc1----1e455701fdd9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@catchpranay/day-1-2-attention-seq2seq-models-65df3f49e263", "anchor_text": "post"}, {"url": "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html", "anchor_text": "here"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "[1]"}, {"url": "https://arxiv.org/abs/1803.08494", "anchor_text": "[2]"}, {"url": "https://medium.com/@catchpranay/day-1-2-attention-seq2seq-models-65df3f49e263", "anchor_text": "Day 1 & 2: Attention \u2014 Seq2Seq ModelsSequence-to-sequence (abrv. Seq2Seq) models are deep learning models that have achieved a lot of success in tasks like\u2026medium.com"}, {"url": "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html", "anchor_text": "Translation with a Sequence to Sequence Network and Attention - PyTorch Tutorials 1.1.0\u2026Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a\u2026pytorch.org"}, {"url": "https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04", "anchor_text": "What is a Transformer?An Introduction to Transformers and Sequence-to-Sequence Learning for Machine Learningmedium.com"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated TransformerDiscussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations\u2026jalammar.github.io"}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "An Overview of Normalization Methods in Deep LearningNormalization in deep learning has always been a hot topic. Getting normalization right can be a crucial factor in\u2026mlexplained.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1e455701fdd9---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/self-attention?source=post_page-----1e455701fdd9---------------self_attention-----------------", "anchor_text": "Self Attention"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1e455701fdd9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1e455701fdd9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1e455701fdd9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&user=Pranay+Dugar&userId=65033b05ecc1&source=-----1e455701fdd9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&user=Pranay+Dugar&userId=65033b05ecc1&source=-----1e455701fdd9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1e455701fdd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1e455701fdd9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e455701fdd9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1e455701fdd9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1e455701fdd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@catchpranay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@catchpranay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pranay Dugar"}, {"url": "https://medium.com/@catchpranay/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "123 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F65033b05ecc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&user=Pranay+Dugar&userId=65033b05ecc1&source=post_page-65033b05ecc1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd9e0af989054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-attention-is-all-you-need-1e455701fdd9&newsletterV3=65033b05ecc1&newsletterV3Id=d9e0af989054&user=Pranay+Dugar&userId=65033b05ecc1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}