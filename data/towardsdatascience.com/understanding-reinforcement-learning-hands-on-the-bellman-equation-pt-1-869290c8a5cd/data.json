{"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd", "time": 1683015414.584952, "path": "towardsdatascience.com/understanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd/", "webpage": {"metadata": {"title": "Understanding Reinforcement Learning Hands-On: The Bellman Equation Part 1 | by Alejandro Aristizabal | Towards Data Science", "h1": "Understanding Reinforcement Learning Hands-On: The Bellman Equation Part 1", "description": "Welcome to the fifth entry on a series on Reinforcement Learning. In the previous article, we presented the MDP Framework for describing complex environments. This allowed us to create a more robust\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/infinite-geometric-series#:~:text=An%20infinite%20geometric%20series%20is,of%20all%20finite%20geometric%20series.", "anchor_text": "geometric series", "paragraph_index": 26}], "all_paragraphs": ["Welcome to the fifth entry on a series on Reinforcement Learning. In the previous article, we presented the MDP Framework for describing complex environments. This allowed us to create a more robust and diverse scenario for the basic Multi-Armed Bandits problem, which we called the Casinos Environment. We then implemented this scenario using OpenAI\u2019s gym, and made a simple agent that acted randomly to showcase how an interaction is realized under the MDP Framework.", "Today, we\u2019re going to focus back on the agents, and show a way in which we can describe an agent\u2019s behavior in complex scenarios, where past actions determine future rewards. This will be achieved by presenting the Bellman Equation, which encapsulates all that is needed to understand how an agent behaves on MDPs. The objective of this article is to offer the first steps towards deriving the Bellman equation, which can be considered to be the cornerstone of this branch of Machine Learning. We will continue developing the intuition behind this equation in the next articles. This article will be heavily focused on theory, but has been toned down so that everything has intuitive sense and meaning.", "We are now confronted with a more complex environment. One which contains multiple states, and in which taking certain actions can lead us to different outcomes. Our decisions no longer have no impact in the world around us, and being short-sighted will no longer necessarily lead to the best solution. Our agents are no longer dealing with the simple Multi-Armed Bandits situation, and we must adapt them if we want them to succeed.", "Previously, we developed strategies that where based on determining the value of the agent\u2019s actions. Knowing how valuable each action was allowed the agent to optimize its behavior by taking the highest-valued one. This was possible due to the fact that every action taken was independent from each other, and that the agent was always presented with the same situation no matter what action it chose. This time, however, merely choosing the best action presented to the agent could be detrimental in the future. Let\u2019s see an example.", "We will explore two different agents living inside two exactly equal environments. The environment is unknown to us, so the only way we can learn from it is by looking at the interactions done by these agents. We\u2019re going to follow them step by step, and try to determine which one behaved better.", "Here, both agents start at an arbitrary state we called S0. The first agent took the action Up, while the second Right. So far it seems that the second agent behaved better, since it obtained a reward of +5, contrast to the +2 the first agent received. Let\u2019s continue the trajectories.", "Now things aren\u2019t looking that great for the second Agent. Even though the Right action seemed like the best choice at the first step, it lead to a really bad state, where we end up losing some of the previously obtained reward. On the other hand, the first agent didn\u2019t choose the immediate optimal action, but was rewarded on the long run. Clearly, basing our behaviors solely on the value of actions isn\u2019t an optimal strategy. How can we do better?", "It\u2019s time to reveal the environment behind the interactions above. Here\u2019s an MDP describing all the states, actions and rewards.", "Notice that there is really only one choice behind this MDP, which is taken at S0. Every other state points you to a predetermined path. Also note that S4 has only one action, which doesn\u2019t give any reward nor transitions to another state. This kind of states are called Terminal States, as they represent a place where no further interaction changes the situation for the agent.", "Assume now that you can specify where an agent begins interacting. Looking at the diagram of the environment, which would be the best state to place our agent? Which would be the worst? It should be easy to conclude that the worst state to be is S1, since in the long run we would end up losing as much as we earn. On the other hand, S2 seems a great starting point, as we would en up earning +4. S0 isn\u2019t as straightforward, and we\u2019re going to skip it for now.", "What we just did was look at each state and accumulate the rewards we would obtain if we started at that state. We just gave each position on our MDP an intrinsic Value. We could make this clearer by drawing all the paths and displaying the accumulated rewards. We\u2019re going to define the value of a state s as v(s).", "Did you notice that we started from the top? If you look at each path, you may notice a lot of repetitive calculations. Value of S1 depends on the value of S3, which depends on the value of S2 which depends on the value of S4. We can simplify this by stating that the value of each state depends on the value of the next one, as well as the immediate reward received. With that, we can define the value of each state as:", "This process can be generalized into a basic formula:", "Where s is the current state, s\u2019 is the next state and r is the reward obtained by transitioning to s\u2019. So far we haven\u2019t done anything out of the ordinary. We just assigned each state a value, that indicates how much reward we expect to receive if an agent started its interaction from there. With this, an agent will be able to evaluate its behavior considering future rewards.", "Things are looking fine with our definition of value, but there are still some details that must be covered to make it more robust. For that, we will briefly touch on other MDPs that will point at some problems with our current definition. Don\u2019t worry, we will get back to the previous example later on.", "Here we have a new MDP, in which all transitions give no reward except for the last one, where we get +5. This is an MDP with delayed reward, since the agent receives no feedback for its actions until the last moment. Looking at the above image, which state would be best for our agent to be? Our intuition would suggest that S2 is the best state, as it is right next to the reward, but what does the value of each state says?", "According to our current definition of value, every state other than S3 is equally valuable, and that is not wrong\u2026 but why does it feel that way? It appears that our agent has no sense of urgency, it doesn\u2019t care if it receives a reward now or later. Let\u2019s look at another MDP:", "Here we have an MDP where our agent would start at state S1. The agent would then have a choice between going to a state where he is rewarded +2 indefinitely or +1 indefinitely. Which path would you take? It would seem obvious that taking the path with higher reward per step is better. Choosing S0 would give me double the reward than S2. This seems like a no-brainer, but then again what does the value function say?", "Both states are valued equally, as they both would reach infinity sooner or later. Again, we see that our agent has no sense of urgency, since it assumes that its interactions can extend indefinitely. How can we solve this?", "We could modify the environment or the interaction to enforce some sense of urgency. In the case of the delayed reward MDP we could make every transition have a small negative reward, which would therefore make the agent attempt to take as little steps as possible to reach the last state. In the case of the infinite interaction, we could simply remove infinity out of the equation by enforcing a maximum number of steps the agent can take. If the agent can only run 10 interactions, then it is clear which choice is better. But, there is a simple mathematical trick that can solve both problems without having to change the environment or limit its behavior.", "So far we\u2019ve seen two extreme ways in which an agent can evaluate an environment. The first one was looking at the immediate rewards and disregarding any future outcomes. The second one was taking into consideration all the rewards that could be obtained up to infinity. In real life, agents have a balance between prioritizing rewards now and in the long run. We usually look ahead up to a certain point, and start caring less and less about future outcomes the farther we look. What changes do we need to do to obtain the same behavior with our agents? Let\u2019s look at our generalized value function again", "Here we are stating that the value of a state is defined by the immediate reward r and future rewards, which are captured by v(s\u2019). What if we tell our agent to just care a little less about future rewards? This could be done by simply attenuating this term like so:", "This small change seems trivial, but it has cascading effects. Let\u2019s see how adding this value changes the way our agent sees both problematic examples.", "Looks promising! The farther we are from the immediate reward, the less valued that state is. Since we\u2019re asking the agent to prioritize future rewards less, it shows a preference for immediate rewards, while still considering future events. What about the infinite MDP?", "There\u2019s no need to understand the formulas, but realize that by attenuating the future rewards from the value function, even infinite interactions have a defined value, and that our intuition holds true. Now, S0 is valued more (20) because it yields double the rewards of S2 (10).", "This number we added to the formula is called the Discount Factor (\ud835\udefe), and can have any value between 0 and 1. This is how the generalized value function looks with the discount factor added.", "The reason this modification works is because adding the discount factor transforms the value function into a geometric series, which will always converge under the conditions established in the previous paragraph.", "You can think of this number as a measure of how short-sighted vs far-sighted the agent is when taking a decision. A discount factor of 0 means the agent only considers immediate rewards, while a value of 1 means projecting its decisions all the way to infinity. The closer this value is from 0, the less it looks ahead. We can get a general idea of how far away an agent plans ahead using a plot comparing multiple discount factors.", "Here we\u2019re plotting how much an agent cares for each reward observed in the future, depending on the chosen discount factor. For example, a discount factor of 0.25 (red) means that the agent will look ahead about 5 steps into the future. With a discount factor of 0.75 (orange) the agent will plan up to around 20 steps ahead.", "In today's article, we started analyzing how an agent can evaluate a complex environment using the value function. This function allowed the agent to add an intrinsic value to each state, which represents how much reward would the agent expect to see in the long run. We also balanced the agent\u2019s capacity of planning ahead by adding a discount factor into the value function. This discount factor solves some problems that could be encountered on different MDPs, such as delayed rewards and infinite reward accumulation.", "So far we\u2019ve only explored simple MDPs, where the agent basically has no choice but to follow a sequence of states, and where each action has a deterministic outcome. Next article we\u2019ll take these complications into account, which will lead us to derive the complete Bellman equation. We will also get back to the first example MDP presented here, so we can finish solving it. See you then!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Musician, Sound Engineer and Programmer. Interested in Sound Design and Machine Learning for the arts."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F869290c8a5cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alejandro.aristizabal24?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Alejandro Aristizabal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd49e13de55bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=post_page-d49e13de55bb----869290c8a5cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral", "anchor_text": "Javier Allegue Barros"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-1-introduction-44e3b011cf6", "anchor_text": "Introduction"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "Multi-Armed Bandits"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Multi-Armed%20Bandits.ipynb", "anchor_text": "Notebook"}, {"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55", "anchor_text": "Non-Stationary"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Non-Stationarity.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782", "anchor_text": "Markov Decision Processes"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Markov%20Decision%20Processes.ipynb", "anchor_text": "Notebook"}, {"url": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/infinite-geometric-series#:~:text=An%20infinite%20geometric%20series%20is,of%20all%20finite%20geometric%20series.", "anchor_text": "geometric series"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-1-introduction-44e3b011cf6", "anchor_text": "Introduction"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "Multi-Armed Bandits"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Multi-Armed%20Bandits.ipynb", "anchor_text": "Notebook"}, {"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55", "anchor_text": "Non-Stationary"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Non-Stationarity.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782", "anchor_text": "Markov Decision Processes"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Markov%20Decision%20Processes.ipynb", "anchor_text": "Notebook"}, {"url": "https://www.coursera.org/specializations/reinforcement-learning", "anchor_text": "Reinforcement Learning Specialization"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "Reinforcement learning: An introduction"}, {"url": "https://medium.com/tag/bellman-equation?source=post_page-----869290c8a5cd---------------bellman_equation-----------------", "anchor_text": "Bellman Equation"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----869290c8a5cd---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----869290c8a5cd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/game-theory?source=post_page-----869290c8a5cd---------------game_theory-----------------", "anchor_text": "Game Theory"}, {"url": "https://medium.com/tag/markov-decision-process?source=post_page-----869290c8a5cd---------------markov_decision_process-----------------", "anchor_text": "Markov Decision Process"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=-----869290c8a5cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=-----869290c8a5cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F869290c8a5cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----869290c8a5cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----869290c8a5cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----869290c8a5cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----869290c8a5cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alejandro Aristizabal"}, {"url": "https://medium.com/@alejandro.aristizabal24/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "50 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd49e13de55bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=post_page-d49e13de55bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F97b8debaf8c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-the-bellman-equation-pt-1-869290c8a5cd&newsletterV3=d49e13de55bb&newsletterV3Id=97b8debaf8c7&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}