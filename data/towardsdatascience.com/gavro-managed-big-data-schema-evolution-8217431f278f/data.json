{"url": "https://towardsdatascience.com/gavro-managed-big-data-schema-evolution-8217431f278f", "time": 1683007172.751652, "path": "towardsdatascience.com/gavro-managed-big-data-schema-evolution-8217431f278f/", "webpage": {"metadata": {"title": "GAVRO \u2014 Managed Big Data Schema Evolution | by Gary Strange | Towards Data Science", "h1": "GAVRO \u2014 Managed Big Data Schema Evolution", "description": "Managing schema changes has always proved troublesome for architects and software engineers. Building a big-data platform is no different and managing schema evolution is still a challenge that needs\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/GaryStrange/GAVRO", "anchor_text": "repo", "paragraph_index": 2}, {"url": "https://github.com/GaryStrange/GAVRO/blob/master/README.md", "anchor_text": "GitHub repo", "paragraph_index": 4}, {"url": "https://medium.com/@gary.strange/evolving-into-a-big-data-driven-business-in-the-azure-cloud-data-ingestion-65bc8b659570?source=your_stories_page---------------------------", "anchor_text": "Evolving into a Big-Data Driven Business in the Azure Cloud: Data Ingestion", "paragraph_index": 16}, {"url": "https://avro.apache.org/docs/current/spec.html", "anchor_text": "AVRO specification", "paragraph_index": 21}], "all_paragraphs": ["Managing schema changes has always proved troublesome for architects and software engineers. Building a big-data platform is no different and managing schema evolution is still a challenge that needs solving. NoSQL, Hadoop and the schema-on-read mantra have gone some way towards alleviating the trappings of strict schema enforcement. However, integration developers, analysts and data scientists are still hindered by the amount of data wrangling they need to perform when extracting accurate insights from big-data.", "Successful business\u2019 grow and evolve at pace accelerating and amplifying the volatility of known data schemas. Datasets are not static and constantly evolving, so knowing what business-fact data represents in the current and historical periods of the business is crucial to making confident information insights.", "Through this article and accompanying GitHub repo, I\u2019ll demonstrate how you can manage schema evolution in a big-data platform using Microsoft Azure technologies.", "This is an area that tends to be overlooked in practice until you run into your first production issues. Without thinking through data management and schema evolution carefully, people often pay a much higher cost later on.", "As a writer, it's difficult to decide how to tell your story. Do I jump straight into the technical solution to satisfy the engineers looking for succinct examples or do I start with the why\u2019s and motivations? So I\u2019ll leave it up to the reader. If you want to jump straight into the technical example head to the GitHub repo. If you want the finer details, read on\u2026", "Serializing data to be stored in files to be analysed later, is fairly straight forward if consumers understand the schema that was used to write the data and the schema never changes.", "But what happens if the schema evolves over time? It becomes a little more complicated.", "When the write-schema evolves due to a new business requirement, consumers (readers) must understand when the new schema was introduced and the definition of the new schema to successfully de-serialize the data. Failure to comprehend the schema-change event will impact data processing pipelines and services will error as they fail to de-serialize the data.", "There are a few solutions to this problem\u2026 (this is by no means an exhaustive list).", "The writer and the reader coordinate their backlogs and software releases. This may work well when the writer and the reader applications are developed and maintained by the same engineering team. However, it\u2019s often the case that the writer and reader are working to different objectives and priorities across the enterprise. Temporally coupling independent team backlogs through strict interface dependencies is to be avoided as it inhibits agility and delivery velocity.", "Versioning write-schemas enables forward and backwards compatibility management. Providing forward and backward compatibility de-couples backlogs and priorities, allowing engineering teams independent progression of their goals.", "Versioning is generally discussed in the context of two distinct sub-topics.", "Major \u2014 A major version change typically breaks interfaces and contracts between systems. A major schema change would typically inhibit readers from reading the data written by the new schema version. Forward and backward compatibility is difficult or impossible.", "Minor \u2014 A minor version change is typically considered to be a low impact change. Forward and backward compatibility is often possible. Readers typically continue to operate as they previously did, successfully de-serialising data without progressing to the newest version of the schema.", "Wouldn\u2019t it be nice to build a data ingestion architecture that had some resilience to change? More specifically, resilience to schema evolution.", "Below is the Azure architecture I\u2019ll use to describe how schema evolution can be managed successfully.", "Kafka\u2019s Schema Registry provides a great example of managing schema evolution over streaming architecture. Azure Event Hubs, Microsoft\u2019s Kafka like product, doesn\u2019t currently have a schema registry feature. Events published to Event Hubs are serialised into a binary blob nested in the body of Event Hubs Avro schema (Fig.1). We will get into the details shortly, but essentially the published event data is schema-less, any down-stream readers need to de-serialise the binary blob by asserting a schema at read time. There are some clever-work-arounds\u00b9 that utilise Confluent\u2019s schema-registry alongside Event Hubs. I will build on these suggestions and provide an alternative approach to schema evolution resilience. In my previous story (Evolving into a Big-Data Driven Business in the Azure Cloud: Data Ingestion), I described a Data Lake ingestion architecture that utilises Event Hubs and Event Hub Capture to form a batch layer for big data analytics. I\u2019ll use this architecture as reference for handling schema evolution.", "How many? How many Event Hubs should I have? Or to put it another way, should I have one big pipe for all my data or many smaller pipes for each message type? The same question has been asked regarding Kafka topics and there is no definitive answer\u00b2. One thing is highly probably, different use cases will favour different approaches. If your concern is just to get messages from A to B or you\u2019re integrating with architecture outside of your control, messages might flow through one Event Hub, one big pipe. If some of your data is highly sensitive and you only want certain subscribers to read and process that data or you may need specific partition strategies which would lead to the adoption of many event hubs within a namespace, many smaller pipes.", "At first glance, these issues may seem to be unrelated. However, they are manifestations of the same core problem. How to manage the de-serialisation of data.", "All messages on Event Hubs are anonymous blobs of binary. One option would be for consumers to infer the schema. However, this approach is non-deterministic and based on sampling, so the inferred schema can only be an approximation. Another approach might be to assert the schema on consumption. However, this means that engineering teams consuming messages are temporarily coupled to the evolution of the schema, even for minor changes. Event Hub Capture offers us an opportunity to break the temporal coupling and allow consumers to consume data from t0** at their own pace. However, if a consumer wants to read and make use of all the AVRO files, produced by the Event Hub Capture process, they will also need to know which write schemas were used to write the binary messages over the period that the events were captured. This could be many months or even years of data. As a consumer, I would need to know the schema evolution time-line or I will struggle to make use of the data.", "** well at least from the begging of the Event Hub capture configuration.", "Early impressions of Event Hub Capture might lead you to believe that AVRO was being used to help address the concerns detailed above. However, after reading the AVRO specification it would seem that only minor version changes are possible. So breaking changes cannot be managed and AVRO files with multiple message types would be impossible.", "Event Hubs allow us to add additional metadata when we publish messages. This metadata is the key to managing schema evolution.", "1) The write-schema is stored with each message in the Event Hub client properties dictionary. This would severely inflate the storage costs.", "2) A message type identifier is stored in the Event Hub client properties dictionary. The identifier is then used to lookup the schema from a central store.", "With both of these solutions, the schema is always directly or indirectly stored with the data. The files produced by Event Hub Capture will always have a means of identifying the write schema. Moreover, each file can contain x number of message types and y number of message versions.", "Let\u2019s take a look at an Azure Function that publishes messages to Event Hub using the client SDK.", "The schema identifier is always stored alongside the data (line 17).", "In the example above, the function uses a timer trigger to execute new instances of the function every 5 seconds. The function trigger is irrelevant, and it could easily be a CosmosDB Change Feed Processing binding or any other bindings that produce data to be processed. Moreover, using a function app is also irrelevant, what matters is what you publish to the Event Hub. The function app lends itself to a succinct example.", "It's important to note the schema version of the message is being persisted alongside the message by adding a reference to eventData.Properties. This metadata attribution is critical when it comes to reading the data at a later date. When events are published to Event Hub the schema identifier is always stored alongside the data.", "I configure Event Hub Capture to produce a new AVRO file every minute or every 500mb, whichever comes first. So, we now have the schema identifier and data captured in neatly partitioned AVRO files, but how do we process it in our big data pipelines. In my previous story, I covered the subject of maintaining a schema repository to capture a truthful account of all the enterprise's schemas. This repo is used to create an artefact that will be consumed in the data processing pipeline. The artefact is a simple key-value store connecting versioned schema identifiers with the write schema used. For the purpose of this document, I\u2019ll use a simple Databrick Python notebook to process the AVRO data.", "I won\u2019t go into a full description of the complete notebook but focus on the most important cells (the complete notebook is in the GitHub repo).", "The first is the reading of the Event Hub Data Capture AVRO. Spark\u2019s AVRO dataframeReader is used to read AVRO files from storage and de-serialise them into a data-frame. We can allow Spark to infer the schema at this point as we know it to be non-volatile (i.e. the Azure Event Hub schema). The properties attribute holds the information about the schema version that was used to write the data in the binary field \u2018Body\u2019. A simple projection is run over the data to process a refined data-frame with three columns. The schema version is extracted from the properties object (the stored value from the serialised properties dictionary is stored in the child attribute member2). The \u2018Body\u2019 attribute is cast to a string as we want to use spark\u2019s JSON de-serialiser on it later in the notebook.", "The second is the schema lookup object. For the purpose of simplifying the example, I\u2019m manually creating some schemas that will be used to deserialise the AVRO data. However, in practice, these schema\u2019s will be generated from a schema repository and be stored as runtime artefacts. Note to self, need to write this up as a follow-up article.", "The schemas, stored in a one-dimensional array, represent an entity that has evolved. In this theoretical example, the business has grown and started trading overseas in new currencies. Transactions now need currency identifiers, so a new attribute \u2018Currency\u2019 was added to the sales-order data schema. As readers, we need to be able to de-serialise the new data successfully.", "The third cell I\u2019d like to focus on is the cell that actually reads and de-serialises the data. The Event Hub Data Capture output that was read into a data-frame previously is used to determine a distinct list of schema versions present in the data. For each schema version, a new temporary SparkSQL table will be created to access the de-serialised data. The original AVRO data-frame is filtered on each iteration of the \u2018for\u2019 loop, grouping records by distinct schema-version to produce subsets of data. Each subset is then de-serialised using the corresponding schema in the salesOrderSchemaDictionary. A number of new temporary tables will be created and the output of this cell will display a list of created objects.", "Finally, SparkSQL can be used to explore the successful deserialised data in the temporary tables.", "I should have started by clearing up what GAVRO is. Sorry to disappoint, but it\u2019s not some new Apache incubator project that you wasn\u2019t aware of. It\u2019s an endearing name that my colleagues gave to the method I described in this article. I believe it\u2019s a combination of my first initial and AVRO, at first I found their nickname for the method to be a product of the team's camaraderie, but then it stuck.", "I don\u2019t believe in designing and prescribing methods that are completely exact and should be unconditionally applied to every enterprise because every enterprise is different. So if you take anything away from reading this then I hope it\u2019s the motivation to think about the connotations of badly managed schema evolution within your big data pipe-lines. We hear time and time again about the struggles organisation\u2019s have with extracting information and actionable insight from big-data and how expensive data-scientists are wasting 80% of their time wrestling with data preparation. Schema management is a weapon when applied properly, that can be used to accelerate data understanding and reduce time to insight. So take the time to invest in it and you will reap healthy returns.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Gary is a Big Data Architect at ASOS, a leading online fashion destination for 20-somethings. He advises 11 teams across three domains."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8217431f278f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8217431f278f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gary.strange?source=post_page-----8217431f278f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gary.strange?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Gary Strange"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3e25bbe2c359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&user=Gary+Strange&userId=3e25bbe2c359&source=post_page-3e25bbe2c359----8217431f278f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://Pixabay.com", "anchor_text": "Pixabay.com"}, {"url": "https://github.com/GaryStrange/GAVRO", "anchor_text": "repo"}, {"url": "https://docs.confluent.io/current/schema-registry/avro.html#schema-evolution-and-compatibility", "anchor_text": "https://docs.confluent.io/current/schema-registry/avro.html#schema-evolution-and-compatibility"}, {"url": "https://github.com/GaryStrange/GAVRO/blob/master/README.md", "anchor_text": "GitHub repo"}, {"url": "https://medium.com/@gary.strange/evolving-into-a-big-data-driven-business-in-the-azure-cloud-data-ingestion-65bc8b659570?source=your_stories_page---------------------------", "anchor_text": "Evolving into a Big-Data Driven Business in the Azure Cloud: Data Ingestion"}, {"url": "https://avro.apache.org/docs/current/spec.html", "anchor_text": "AVRO specification"}, {"url": "https://azure.microsoft.com/en-gb/blog/schema-validation-with-event-hubs/", "anchor_text": "https://azure.microsoft.com/en-gb/blog/schema-validation-with-event-hubs/"}, {"url": "https://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html", "anchor_text": "https://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html"}, {"url": "https://pulsar.apache.org/docs/en/schema-evolution-compatibility/", "anchor_text": "https://pulsar.apache.org/docs/en/schema-evolution-compatibility/"}, {"url": "https://docs.confluent.io/current/schema-registry/index.html", "anchor_text": "https://docs.confluent.io/current/schema-registry/index.html"}, {"url": "https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying", "anchor_text": "https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"}, {"url": "https://medium.com/tag/azure-event-hub?source=post_page-----8217431f278f---------------azure_event_hub-----------------", "anchor_text": "Azure Event Hub"}, {"url": "https://medium.com/tag/avro?source=post_page-----8217431f278f---------------avro-----------------", "anchor_text": "Avro"}, {"url": "https://medium.com/tag/schema?source=post_page-----8217431f278f---------------schema-----------------", "anchor_text": "Schema"}, {"url": "https://medium.com/tag/big-data?source=post_page-----8217431f278f---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/azure-databricks?source=post_page-----8217431f278f---------------azure_databricks-----------------", "anchor_text": "Azure Databricks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&user=Gary+Strange&userId=3e25bbe2c359&source=-----8217431f278f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&user=Gary+Strange&userId=3e25bbe2c359&source=-----8217431f278f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8217431f278f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8217431f278f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8217431f278f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8217431f278f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8217431f278f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8217431f278f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8217431f278f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8217431f278f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gary.strange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gary.strange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gary Strange"}, {"url": "https://medium.com/@gary.strange/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "89 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3e25bbe2c359&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&user=Gary+Strange&userId=3e25bbe2c359&source=post_page-3e25bbe2c359--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F57642ddee56e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgavro-managed-big-data-schema-evolution-8217431f278f&newsletterV3=3e25bbe2c359&newsletterV3Id=57642ddee56e&user=Gary+Strange&userId=3e25bbe2c359&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}