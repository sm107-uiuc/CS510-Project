{"url": "https://towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764", "time": 1683017676.8404129, "path": "towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764/", "webpage": {"metadata": {"title": "Huggingface\ud83e\udd17Transformers: Retraining roberta-base using the RoBERTa MLM Procedure | by Tanmay Garg | Medium | Towards Data Science", "h1": "Transformers: Retraining roberta-base using the RoBERTa pre-training procedure", "description": "In this tutorial we will be retraining the \"roberta-base\" LM using the pre-trained \"roberta-base\" tokenizer that comes bundled along with the LM."}, "outgoing_paragraph_urls": [{"url": "https://twitter.com/lmthang/status/1050543868041555969", "anchor_text": "multitude", "paragraph_index": 0}, {"url": "https://huggingface.co/blog/how-to-train", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://huggingface.co/roberta-base", "anchor_text": "this", "paragraph_index": 10}, {"url": "https://github.com/huggingface/datasets", "anchor_text": "here", "paragraph_index": 16}, {"url": "http://Skit.ai", "anchor_text": "Skit.ai", "paragraph_index": 28}], "all_paragraphs": ["Since BERT (Devlin et al., 2019) came out, the NLP community has been booming with the Transformer (Vaswani et al., 2017) encoder based Language Models enjoying state of the art (SOTA) results on a multitude of downstream tasks.", "The RoBERTa model (Liu et al., 2019) introduces some key modifications above the BERT MLM (masked-language modeling) training procedure. The authors highlight \u201cthe importance of exploring previously unexplored design choices of BERT\u201d. Details of these design choices can be found in the paper\u2019s Experimental Setup section.", "The \ud83e\udd17Transformers library comes bundled with classes & utilities to apply various tasks on the RoBERTa model.", "For example, it is extremely easy to get predictions for a sequence containing a mask token!", "This signifies what the \u201croberta-base\u201d model predicts to be the best alternatives for the <mask> token.", "It is oftentimes desirable to re-train the LM to better capture the language characteristics of a downstream task.", "For example, RoBERTa is trained on BookCorpus (Zhu et al., 2015), amongst other datasets.", "A recently published work BerTweet (Nguyen et al., 2020) provides a pre-trained BERT model (using the RoBERTa procedure) on vast Twitter corpora in English. They argue that BerTweet better models the characteristic of language used on the Twitter subspace, outperforming previous SOTA models on Tweet NLP tasks.", "Hence, it is a good indicator that the performance on downstream tasks is greatly influenced by what our LM captures!", "This post does not delve into training the LM and tokenizer from scratch. Training from scratch is quite sufficiently covered in an official \ud83e\udd17 post here.", "Please note that these pre-trained models have been trained using a huge amount of data and computing resources. For example, according to this description, \u201croberta-base\u201d was trained on 1024 V100 GPUs for 500K steps. Yes, you read that right. 1-0-2-4 GPUs \ud83e\udd2f. Such magnitudes of resources are often not available to us.", "However, what we can do is retrain these available models for a few more epochs on a smaller dataset!", "We will be using transformers v3.5.1, however, this tutorial should work fine with the recently released v4.0.0 as well.", "We will be using the Hate Speech Detection dataset (Basile et al., 2019) made available through TweetEval (Barbieri et al., 2020). Of course, you can use a dataset that better suits your downstream task! \ud83e\udd29", "Since our data is already present in a single file, we can go ahead and use the LineByLineTextDataset class.", "The block_size argument gives the largest token length supported by the LM to be trained. \u201croberta-base\u201d supports sequences of length 512 (including special tokens like <s> (start of sequence) and </s> (end of sequence).", "For a finer control over the dataset, you can explore \ud83e\udd17Datasets here.", "The data collator object helps us to form input data batches in a form on which the LM can be trained. For example, it pads all examples of a batch to bring them to the same length.", "As the names are quite self-explanatory, the TrainingArguments object holds some fields that help define the training process. The Trainer finally brings all of the objects that we have created till now together to facilitate the train process.", "seed=1: seeds the RNG for the Trainer so that the results can be replicated when needed.", "It took ~100mins for train to finish on Google Colab.", "trainer.save_model(output_dir): helps us save the model to the output_dir so that we can load it using from_pretrained (or as done below).", "Go ahead and verify your LM! You\u2019re in luck if your python environment has TensorBoard available, because the Trainer object logs the training in \u2018./runs\u2019 directory.", "As before, we can get predictions for a sequence containing a mask token!", "Oof, that\u2019s hurtful! \ud83d\ude49 I had to censor a word out for our PG-13 audiences \ud83d\udc76", "Jokes aside, it also tells that our model has trained correctly and has started capturing the hateful language of the dataset.", "Go ahead, tweak the hyper-parameters through the TrainerArguments object to see which setting gives the best results for your downstream task!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ Skit.ai | IIIT Delhi | NLP | ML"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7422160d5764&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tanmay17061.medium.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Tanmay Garg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73cb2d1309f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&user=Tanmay+Garg&userId=73cb2d1309f8&source=post_page-73cb2d1309f8----7422160d5764---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@agk42?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Alex Knight"}, {"url": "https://unsplash.com/s/photos/robot?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://twitter.com/lmthang/status/1050543868041555969", "anchor_text": "multitude"}, {"url": "https://huggingface.co/blog/how-to-train", "anchor_text": "here"}, {"url": "https://huggingface.co/roberta-base", "anchor_text": "this"}, {"url": "https://github.com/huggingface/datasets", "anchor_text": "here"}, {"url": "https://medium.com/tag/nlp?source=post_page-----7422160d5764---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----7422160d5764---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/language-model?source=post_page-----7422160d5764---------------language_model-----------------", "anchor_text": "Language Model"}, {"url": "https://medium.com/tag/transformers?source=post_page-----7422160d5764---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----7422160d5764---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&user=Tanmay+Garg&userId=73cb2d1309f8&source=-----7422160d5764---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&user=Tanmay+Garg&userId=73cb2d1309f8&source=-----7422160d5764---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7422160d5764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7422160d5764---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7422160d5764--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7422160d5764--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7422160d5764--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7422160d5764--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7422160d5764--------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tanmay Garg"}, {"url": "https://tanmay17061.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "46 Followers"}, {"url": "http://Skit.ai", "anchor_text": "Skit.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73cb2d1309f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&user=Tanmay+Garg&userId=73cb2d1309f8&source=post_page-73cb2d1309f8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f621934819d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764&newsletterV3=73cb2d1309f8&newsletterV3Id=6f621934819d&user=Tanmay+Garg&userId=73cb2d1309f8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}