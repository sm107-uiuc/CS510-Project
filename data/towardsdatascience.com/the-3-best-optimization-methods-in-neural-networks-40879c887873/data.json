{"url": "https://towardsdatascience.com/the-3-best-optimization-methods-in-neural-networks-40879c887873", "time": 1682995390.6432152, "path": "towardsdatascience.com/the-3-best-optimization-methods-in-neural-networks-40879c887873/", "webpage": {"metadata": {"title": "The 3 Best Optimization Methods in Neural Networks | by Marco Peixeiro | Towards Data Science", "h1": "The 3 Best Optimization Methods in Neural Networks", "description": "Deep learning is an iterative process. With so many parameters to tune or methods to try, it is important to be able to train models fast, in order to quickly complete the iterative cycle. This is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/channel/UC-0lpiwlftqwC7znCcF83qg?view_as=subscriber", "anchor_text": "YouTube channel", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3", "anchor_text": "Previously", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/almost-everything-you-need-to-know-about-time-series-860241bdc578", "anchor_text": "this", "paragraph_index": 16}, {"url": "https://github.com/marcopeix/Deep_Learning_AI/blob/master/2.Improving%20Deep%20Neural%20Networks/2.Algorithm%20Optimization/Optimization%20Methods.ipynb", "anchor_text": "notebook", "paragraph_index": 28}], "all_paragraphs": ["Deep learning is an iterative process. With so many parameters to tune or methods to try, it is important to be able to train models fast, in order to quickly complete the iterative cycle. This is key to increasing the speed and efficiency of a machine learning team.", "Hence the importance of optimization algorithms such as stochastic gradient descent, min-batch gradient descent, gradient descent with momentum and the Adam optimizer.", "These methods make it possible for our neural network to learn. However, some methods perform better than others in terms of speed.", "Here, you will learn about the best alternatives to stochastic gradient descent and we will implement each method to see how fast a neural network can learn using each method.", "For hands-on video tutorials on machine learning, deep learning, and artificial intelligence, checkout my YouTube channel.", "Traditional gradient descent needs to process all of the training examples before making the first update to the parameters. From now, updating the parameters will be referred to as taking a step.", "Now, we know that deep learning works best with large amounts of data. Therefore, gradient descent will need to train on millions of training points before taking a single step. This is obviously inefficient.", "Instead, consider breaking up the test set into smaller sets. Each small set is called a mini-batch. Say each mini-batch has 64 training points. Then, we could train the algorithm on a mini-batch at a time and take a step once training is done for each mini-batch!", "Hence the name: mini-batch gradient descent.", "Previously, we have seen cost plots where it smoothly goes down after each iteration, as shown below.", "In the case where min-batch gradient descent is used, the plot will oscillate much more, with a general downward trend. We will see an example later when we code this method. The oscillations make sense, because a new set of data is used to optimize the cost function, which means that it might increase sometimes before going back down.", "Let\u2019s consider both extremes. On one hand, you could set your mini-batch size to the size of all your training set. This would simply result in a traditional gradient descent method (also called batch gradient descent).", "On the other hand, you could set your mini-batch size to 1. This means that each step is taken after training on only 1 data point. This is called stochastic gradient descent. However, this is method is not very good, because it often takes steps in the wrong direction and it will not converge to the global minimum; it will instead oscillate around the global minimum.", "Thus, your mini-batch size should be between those two extremes. In general, the following guidelines can be followed:", "Again, the mini-batch size can be chosen iteratively. You will sometimes need to test different sizes to see which makes training the fastest.", "Gradient descent with momentum involves applying exponential smoothing to the computed gradient. This will speed up training, because the algorithm will oscillate less towards the minimum and it will take more steps towards the minimum.", "If exponential smoothing is unknown to you, you might want to read this article.", "Usually, simple exponential smoothing is used, meaning that there are two more hyperparameters to tune: the learning rate alpha and the smoothing parameter beta.", "Usually, this method almost always works better than traditional gradient descent, and it can be coupled with mini-batch gradient descent.", "Adam stands for: adaptive moment estimation. Briefly, this method combines momentum and RMSprop (root mean squared prop).", "RMSprop has smooths the gradient, just like momentum, but it uses a different approach that is best explained mathematically.", "First, the gradient is calculated as such:", "Then, the weights and bias matrix is updated like this:", "Notice that beta2 is a new hyperparameter (not to be confused with beta from momentum). Also, epsilon is a very small value to prevent dividing by 0.", "Therefore, combining momentum and RMSprop, Adam introduces four hyperparameters:", "As mentioned above, you usually do not need to tune beta, beta2, and epsilon as the values listed above will generally work well. Only the learning rate is left to tune in order to accelerate training.", "Now that you learned about each optimization method, let\u2019s implement them in Python and see how they compare to one another.", "We will run each method on sample dataset to see how a neural network will perform. To test the training speed, we will keep the number of epochs constant to 10 000.", "Of course, the full code is available in a notebook.", "Let\u2019s start off by coding the stochastic gradient descent method:", "This is fairly straight forward, since we use a single data point to take a step in gradient descent.", "Mini-batch gradient descent is a bit harder to implement, because the training size is likely not divisible by the mini-batch size. Therefore, we need to address the last batch to accommodate that:", "To implement momentum, we first need to initialize the velocity:", "And then, we can update our parameters:", "Now, since Adam combines both momentum and RMSprop, we need to initialize two parameters:", "Then, we can update the parameters as such:", "Now, we will train a neural network with different optimization methods to see how fast it can learn.", "Then, after training with mini-batch gradient descent on 10 000 epochs we get:", "As you can see, we only achieved 80% test accuracy after 10 000 epochs and the decision boundary is not very good.", "Now, after training with momentum, we get:", "Interestingly, momentum did not really help.", "Now, we use Adam and we get:", "Amazing! Using Adam, we achieved a much better accuracy with 10 000 epochs.", "Note here that a better method is simply a faster one. The other methods could have given much better accuracy score given more epochs. Still, the objective of this exercise was to evaluate the speed of each method, and Adam is a clear winner.", "That\u2019s it! You learned the different optimization methods for neural networks, implemented them, and found out that Adam performs really well.", "Typically, AI practitioners use mini-batch gradient descent or Adam, as they perform well most of the time.", "Luckily, deep learning frameworks have built-in functions for optimization methods. In the next post, we will introduce TensorFlow and see how easy it ease to code bigger, more complex neural networks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior data scientist | Author | Instructor. I write hands-on articles with a focus on practical skills."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F40879c887873&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----40879c887873--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40879c887873--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marcopeixeiro?source=post_page-----40879c887873--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcopeixeiro?source=post_page-----40879c887873--------------------------------", "anchor_text": "Marco Peixeiro"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F741c1c8fcfbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=post_page-741c1c8fcfbd----40879c887873---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rawpixel?utm_source=medium&utm_medium=referral", "anchor_text": "rawpixel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/channel/UC-0lpiwlftqwC7znCcF83qg?view_as=subscriber", "anchor_text": "YouTube channel"}, {"url": "https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3", "anchor_text": "Previously"}, {"url": "https://towardsdatascience.com/almost-everything-you-need-to-know-about-time-series-860241bdc578", "anchor_text": "this"}, {"url": "https://github.com/marcopeix/Deep_Learning_AI/blob/master/2.Improving%20Deep%20Neural%20Networks/2.Algorithm%20Optimization/Optimization%20Methods.ipynb", "anchor_text": "notebook"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----40879c887873---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----40879c887873---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----40879c887873---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/python?source=post_page-----40879c887873---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----40879c887873---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=-----40879c887873---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=-----40879c887873---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40879c887873--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F40879c887873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----40879c887873---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----40879c887873--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----40879c887873--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----40879c887873--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----40879c887873--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----40879c887873--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----40879c887873--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----40879c887873--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----40879c887873--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcopeixeiro?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcopeixeiro?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marco Peixeiro"}, {"url": "https://medium.com/@marcopeixeiro/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F741c1c8fcfbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=post_page-741c1c8fcfbd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9835bccb3d51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-3-best-optimization-methods-in-neural-networks-40879c887873&newsletterV3=741c1c8fcfbd&newsletterV3Id=9835bccb3d51&user=Marco+Peixeiro&userId=741c1c8fcfbd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.manning.com/books/time-series-forecasting-in-python-book", "anchor_text": "Time Series Forecasting in Python2022"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}