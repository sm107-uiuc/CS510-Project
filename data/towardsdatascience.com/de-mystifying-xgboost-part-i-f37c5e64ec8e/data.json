{"url": "https://towardsdatascience.com/de-mystifying-xgboost-part-i-f37c5e64ec8e", "time": 1683004496.991982, "path": "towardsdatascience.com/de-mystifying-xgboost-part-i-f37c5e64ec8e/", "webpage": {"metadata": {"title": "Understanding XGBoost | Towards Data Science", "h1": "De-Mystifying XGBoost Part I", "description": "Step by step understanding of the maths behind the algorithm and hyper parameter tuning of XGBoost"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@b.debanjan/de-mystifying-xgboost-part-ii-175252dcdbc5", "anchor_text": "Part II", "paragraph_index": 3}, {"url": "https://medium.com/@b.debanjan/container-for-almost-all-ml-use-cases-de-mystifying-xgboost-iii-434310b782e8?source=friends_link&sk=c19c4e76d35a1706ed1fb9ed74f33404", "anchor_text": "Part III", "paragraph_index": 4}], "all_paragraphs": ["There are already so many de-mystifying, fantastic blogs on the inner workings of XGBoost. Why another one? Well, I always thought there should be one which stitches them together. One that brings together the theory with real examples. One that really de-mystifies. One that I can refer to myself. So, I thought of making a series. Like below.", "I have thought of breaking the series into 5 parts.", "Part I will introduce the math behind XgBoost. More visually, to imprint it into our neural network \ud83d\ude00.", "In Part II, I will dive into the intuition behind various parameters when using XGBoost \u2014 stitching it with the math and real examples. Answering why and when kind of questions. But only the most important parameters. This one will be the most interesting without any doubt.", "In Part III, I will show how to create a container, super fast, (on GCP) using which we can work on almost all ML problems and specifically XgBoost using both python and R in Jupyter notebook with python kernel!!.", "Part IV, we will use that container and see how all the concepts and intuition we discussed in previous parts, apply to a publicly available data set.", "Part V, I will discuss the intuition behind the rest of the parameters which we did not cover in Part II.", "All we need to know for this section is a bit of high school differential calculus. It\u2019s OK if you left high school long back and only have a faint idea of what differential calculus is, in that case just believe me :)", "If possible, just read up a bit on Sigmoid function and Logistic Loss. That\u2019s it. I have some good links in the Reference section. It\u2019s OK if you do it later.", "Let's start straight away with Taylor\u2019s expansion of a function l(Z) at Z=z. Don\u2019t worry if it does not look inviting. Will make it easy..", "This means, given we know how to calculate l(Z) at l(Z=a), its first, second, third etc order derivatives w.r.t Z at l(a), and we know delta Z; we can approximate the value of l(Z=z) where z = a + delta Z, some val close to a.", "For e.g, lets say l(Z) = Z\u00b2. The first and second order derivatives of Z\u00b2 is 2Z and 2 respectively. Now lets say a = 2 and we want to find l(Z=2.1). Delta Z= 0.1. Lets use Taylor\u2019s expansion.", "Note, I only used the first and second order derivatives from the expression above. First, obviously, because there is no third order derivative of Z\u00b2 (it's zero \u2014 right ?) and hence all terms after the third term will be 0.", "Second, Taylor\u2019s series is used as an approximation. Even if there might be more valid derivatives, we can stop at the third term (i.e till second derivative). The farther we go, the denominator and numerator increase and decrease exponentially, respectively, so anyways, they do not contribute a lot.", "Now, l(Z) = Z\u00b2 was not so enlightening an example, was it? I know, like come on! We do not need a Taylor series to approximate the value of Z\u00b2 for any value of Z=a. Duh!. In fact, if l(Z) has a fixed form for every Z, and we have a calculator at hand, we will never need Taylor\u2019s series for an \u201capproximation\u201d.", "But let's say l(Z) is \ud835\udc52^Z (that is e to the power of Z b.t.w) and you want to find l(0.35), without a calculator. Aha! got you! \ud83e\udd13. But you can approximate that value easily using Taylor\u2019s expansion. You know l(0)=1, delta Z = 0.35 and the first order and second order derivatives of e^Z is also e^Z.", "That\u2019s just the tip of the iceberg though. Taylor series is used widely for representing functions in few terms, which have applications in finding limits, integrals and minima. It is also widely used to approximate the effects of natural forces on functions of physical laws in nature etc. Different topic altogether. Also, there are tons of blogs on Taylor series. Do read them. But not now! After you are finished with my blog \ud83d\ude00.", "Now, let\u2019s up the challenge a bit more. Let\u2019s say, we want to approximate a function l(Z=z), where z = a + delta Z, but delta Z is not fixed. Huh? OK. Hang on. It means, we know a but z is not fixed as delta Z can vary. The task is to derive the right delta Z and hence get the next Z s.t, l(Z=z) is minimal, given certain conditions imposed on what values delta Z can assume.", "Now, to do this, and approximate l(Z=z), we can again use the Taylor series trick. Given, we can somehow calculate l(Z=a) and its first and second order derivatives, we can find the best delta Z if we solve like below.", "Wow! Easy right? So if delta Z is chosen as -(G/H), we will get the next best Z for minimal l(Z=z) and the value of l(Z=z) will be l(a) -1/2(G\u00b2/H).", "That\u2019s pretty much XgBoost (or rather a Boosting algorithm) on a board. Don\u2019t worry we are going to break it up. I will suggest, print out the picture (from GitHub) and keep it handy while you read the remaining of the blog. Do gloss over it for a while and see what you make of it.", "Let\u2019s say, we have a binary classification task at hand. Initially, we have a set of samples, their known supervised labels y and a default model score value y hat = 0 (which means the default probability value is 0.5, for all samples; same as default base_score in XGBoost). See left of Fig 3 for understanding why y hat = 0 score gives a probability of 0.5 using the Sigmoid function.", "Below is a sigmoid function just for our ready reference.", "Now given y hat = 0, a set of samples with features like c1,c2 .. c5, denoted as X, and known labels y, we have to find some way to modify the y hats s.t for samples with label 0, they move towards -5 and the reverse for those samples with label 1. Let's assume we have a function which will let us do that. A tree function, which will give us the next y hats, with the purpose of minimizing a loss function l (also known as the objective function in XGBoost context)? We will soon learn how to derive that tree function. For now, let's just say that it gives us our delta y hats.", "So wherever you see f(x) in Fig 3 above, that f, is a tree function, or simply put a tree. f is defined by conditions like depth of tree (max_depth), min sample weight (or sum of hessian \u2014 we will soon know) in a leaf (min_child_weight), choice of features available while constructing the split conditions like c1<v1 (colsample_bytree, colsample_bylevel, colsample_bynode), and so on. These are called the Tree booster parameters.", "Lets assume, we have only 5 samples. In Fig 3, we can see that the first tree f1 has 4 leaf nodes. I have used some illustrative conditions like c1<v1 for building the trees. A path without a condition label is respectively the not condition of the corresponding labeled path. Each leaf node can have one or more samples in it. Each sample has a value on the leaf node. This is called the leaf score. These leaf scores are the output of the tree function f on X.", "All samples in a leaf will have the same weight, or output value or score. (This is the same as leaf scores in CART trees just in case you are thinking of it ) The samples belonging to a leaf being decided by how their feature values interact with the tree function f. Like x5 in the tree function f1, is on the fourth leaf, because for x5, c1\u2265v1 and c3\u2265v3.", "Now, let's start stitching the picture with what we already know.", "If you notice, we start with y hat = 0 for all samples. The y hat variable is like Z from l(Z) above. And 0 is like a in Z=a. All samples have a known label y. Given y hat and y, we have a loss function like below. This is called the Logistic Loss function. If you have not read it yet, that\u2019s fine, just assume that for binary classification problems, this function sums up the cost or penalty of making mistakes. So we try to minimize it, to get the best possible model.", "So, what's the loss at y hat = 0? If n or number of samples = 5 ?", "Obviously that is just loss from a 0.5 probability given to all samples. From here, we want to do better, by minimizing the loss. So we start building trees. Our objective is to find the next y hats (i.e something better than the default values of 0) using our first tree ft1. However, we have to find it while making sure that the restrictions on ft (Tree boosting parameters) are imposed.", "Let's assume we have found that tree ft=1.", "Now, we can calculate that next better y hat, by adding the leaf scores for each sample, to y hat = 0. That means, the leaf score can be considered as delta y hat. And y hat 0 + delta y hat0 gives us y hat 1.", "If you are following Fig 3, you will now understand that at t=1, the samples have y hat values which is not equal to zero any more. For e.g, sample X1 has a y hat value = 0 + ft1(X1). Samples X3 and X4 being on the same leaf in first tree, have same y hat value = 0 + ft1(X3) = 0 + ft1(X4). And so on.", "Now, we can calculate the loss again. Using our loss formula. Right ?", "Only this time, it won\u2019t be n multiplied by the same ln(0.5) value, because, different samples have different y hats. Let's see an illustration.", "Great, we reduced the loss from 3.465 to 2.925. Don\u2019t worry too much about the leaf scores at this point. We will come back to it.", "At this point, if we are not satisfied with the loss yet, we can try another tree. We will have similar preconditions and challenge. We will have y hat (only it will not be all zero any more). We will have certain tree restrictions. And the objective will be to come out with a delta y hat from ft=2 s.t, the loss after t=2 using y hat 2 is again the minimal achievable given the preconditions.", "This sounds very similar right ? Finding delta Z, which will give us the next best Z=z where l(Z) will be minimal.", "Let\u2019s write the loss function according to Taylor\u2019s expansion illustrated at the beginning of the blog, so that we can approximate both the next best loss and the next best tree which will give us the best delta y hats.", "But first we need to know the first order and second order derivatives of the loss function. They are called the gradient and the hessian respectively. I am skipping the derivation of gradient and hessian, it is a bit non trivial, however, they come out as extremely simple and intuitive. Check out the links in the reference if you want to see step by step derivation.", "Great. So using Loss function in Fig 5, we can calculate the loss at a given value of y hat. We can calculate the first order and second order derivatives at that value because we will know p from y hat (Fig 4). We need to find the next best y hat. We already know what to do. Right? What are we waiting for?", "We can write the loss function as", "Now let's break it up using Taylor\u2019s approximation as we have done before.", "Everything written above is visually explained in Fig 3, so this should be easy to understand. Any confusion, please see \u201cmy pic\u201d again.", "Briefly, loss after tree t, depends on y hat t, which is y hat t-1 added to ft(x). Therefore loss after tree t can be represented as a Taylor\u2019s approximation of loss from y hat t-1, and a delta y hat, which is ft(x). Using first and second order derivatives, we come to the final equation of (1).", "Here omega(ft), represents different constraints and regularization on the tree function/building process. This is actually a very important part of XGBoost. This is what makes it much better than other Boosting algorithms. Like max_depth is a constraint, min_child_weight is a constraint. However it is only the L1 and L2 regularization applied on the leaf score, or leaf weights that need to be considered when formalizing the loss function. Others are imposed by the algorithm when building the tree.", "But, why do we need regularization on the weights? Because, the weights or leaf scores, is what will push the y hats around and we will always have leaves which have a mix of classes in them, meaning misclassified samples. If the weights are too large, then it may result in sudden wrong movement of a sample from more of less OK y hat to a terrible y hat. So keeping the weights under control will make sure that a certain path on a certain tree does not get too much of a say on y hat movement. What if that path in the tree is overfitting on some local condition? More on this later.", "Now, we know that L1 regularization (denoted by the last term above, with alpha as the multiplier), when minimizing a function w.r.t to variable, penalizes the sheer existence of its variable. It will try to make leaf weights or scores wj tend to 0. If wj become too small, our y hats will not change at all -right ?. Actually it will discourage all feature value conditions which give scores or modulus of wj > 0.", "This is not commonly useful in XgBoost. The features are already chosen based on some criterion and we do not want some leaves to vanish just because they result in modulus score > 0. Instead, the L2 regularization (last but one term) is widely used which will allow the leaves, but stop the leaf weights from being too large. More on this later, in Part II.", "Back home, let's use only L2 regularization and break the above function according to leaves. Then by minimizing the loss function, let's find the optimum delta y hat or ft(Xi) or leaf score or leaf weight in each leaf node.", "First we need to rearrange the summation across all samples, into summation across leaves (of summation of samples within each leaf). Why ? Because the regularization is once for each leaf. Whereas the loss is per sample. So, we sum over all leaves for the regularization factor, and then within each leaf, we sum over the samples. See below.", "OK, sweet. So we have optimum wj defined, per leaf. Now we can also find the minimal loss as shown below. Note above, the L2 regularization is per leaf, not per sample. Which makes sense right ? We do not want to regularize each sample within a leaf. All of them will have the same weight.", "Cool. Do we have everything to get that value ? We always know y hats from the previous state (or previous tree) so we know pi; and we also know the label yi; thus we have everything to construct a Tree and get the weights or scores of the leaves. Note, the above loss is a summation across all leaves L and the expression within the third bracket is the loss within a single leaf.", "OK, I hoodwinked you, a bit!. Where? When differentiating the function and equating to 0 in Fig 11 above. Where did I cheat ? L of course. The number of leaves. I mean, if that is a variable, I cannot just dismiss it while differentiating and equating the overall loss to 0. If you caught that already. Smart!! \ud83d\ude0e.", "That was indeed not right. But we did that considering L as a constant. Meaning, the number of leaves in all possible trees as fixed. But this is not possible, right?. From a bunch of features, and let's say a max depth of 6, we can have so many different types of trees with a different number of leaf nodes.", "You are right, but that makes it almost impossible to find a solution. So we consider building a Tree in best first approach. Or the exact (greedy) approach. Which, for most data-sets, is the default (auto) tree_method for XGBoost. In this method, first, we assume a tree has 1 node. Which has a bunch of samples and the loss is calculated based on the previous state of y hat. Then we split that node into two nodes L=2 and follow the below process.", "Let's say the loss at L=1 node is lp or loss parent. Now, for all samples in that node, we iterate through all possible columns (using the column selection constraints specified in tree booster parameters) and their values, and find THE (column,value) combination to split it into two nodes, with L=2 and loss lc = (lc1 + lc2) or Loss children s.t, lp-lc is positive and maximum. Meaning, the loss from lp to lc reduced maximally. Or lc is the minimal loss.", "If even after considering all column value tuples, the loss from lp to lc cannot reduce, then we have already reached the optimum loss for a node. That node itself, becomes a leaf. Else, we expand the tree further into the next sub tree (till we reach max depth) till we get all the leaves.", "OK, also remember ?, not so long ago when we reduced the loss from 3.465 to 2.925 in Fig 3 after the first tree, I said, that if we are not satisfied with the loss reduction, we try the next tree ?. Fine, but why can\u2019t we get the best loss reduction in a single tree, with infinite depth. Or the best delta z hence z and l(Z=z) in one shot? Why do we need to build so many trees?", "Well theoretically we can try, with infinite depth, iterating through all possible trees. But, there is no surety we can reach the absolute minima using only one function. Also it makes THE tree and solution extremely sensitive to variance. And then it becomes too sensitive to the way that a single tree is built, and the whole training data. Which is not good.", "That is why boosting is based on the philosophy of learning in small steps. The idea being, keep depth low (preferably below 10) and create hundreds of trees if possible, learning one after another, little by little, till we can no longer get a substantial loss reduction within the next t trees (early stopping).", "Additionally we should use sub-sampling and column sampling thoroughly so that, each new tree has a different set of samples and columns and hence there is no affiliation to certain column, value, samples tuple values, causing similar splits in each tree.", "Well, eta is really something synthetic. It did not appear anywhere above because it is not mandatory. It is just a clever way of controlling the delta step without going into the complication of lambda (L2) or hessian or gradient.", "Eta is a fraction between 0 and 1. Default 0.3. Which decreases the delta y hat or leaf score, by that fraction.", "By keeping the delta y hat low, we make sure that there is no sudden jump in a samples y hat and thus probability, due to local loss minimization in some leaf, based on some locally sensitive column value condition.", "And that\u2019s all! there is to understanding the math of XGBoost. Let\u2019s quickly look at a visual representation.", "So, above is a quick look at what we learnt. At left, on top, we have all samples at the same point when t=0, y hat = 0, pi = 0.5. Tree by tree the y hats of individual samples keep changing by small deltas. These deltas can be positive or negative based on an overall minimization of loss. After each tree, during training, the log loss should keep decreasing. However, after a point, it is possible that the LogLoss starts increasing. Some y hats may go opposite to the desired direction leading to misclassified samples. At right we have the LogLoss starting from a larger value and then decreasing. These are the values you see when XGBoost trains using a watch list of train and eval_metric of logloss. The rest of the picture should talk for itself.", "Finally, when such a model is used for predicting, for an unknown label sample, all the leaf scores on which the sample lands on every tree, is added up to give the y hat score for that sample. From that y hat score, using Sigmoid function we arrive at the probability value and hence label based on a threshold cutoff.", "If those questions bring out the explorer in you \u2014 then that's what Part II will be all about.", "All resource files can be found at", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A famous Bengali bard once said \u201cKoto ki korar ache baki\u201d \u2014 meaning \u201cThere is so much more to do in life\u201d. Just trying to catch up on all those things .."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff37c5e64ec8e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@b.debanjan?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@b.debanjan?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Deb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c41b57ddc0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&user=Deb&userId=9c41b57ddc0f&source=post_page-9c41b57ddc0f----f37c5e64ec8e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@b.debanjan/de-mystifying-xgboost-part-ii-175252dcdbc5", "anchor_text": "Part II"}, {"url": "https://medium.com/@b.debanjan/container-for-almost-all-ml-use-cases-de-mystifying-xgboost-iii-434310b782e8?source=friends_link&sk=c19c4e76d35a1706ed1fb9ed74f33404", "anchor_text": "Part III"}, {"url": "https://github.com/run2/demystify-xgboost/blob/master/MyBoard.jpg.", "anchor_text": "https://github.com/run2/demystify-xgboost/blob/master/MyBoard.jpg."}, {"url": "https://github.com/run2/demystify-xgboost/tree/master", "anchor_text": "https://github.com/run2/demystify-xgboost/tree/master"}, {"url": "https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x", "anchor_text": "Derivative of sigmoid function $\\sigma (x) = \\frac{1}{1+e^{-x}}$Thanks for contributing an answer to Mathematics Stack Exchange! Please be sure to answer the question. Provide details\u2026math.stackexchange.com"}, {"url": "https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based", "anchor_text": "How to compute the gradient and hessian of logarithmic loss? (question is based on a numpy example\u2026begingroup$ I would like to understand how the gradient and hessian of the logloss function are computed in an xgboost\u2026stats.stackexchange.com"}, {"url": "https://www.quora.com/Logistic-Regression-Why-sigmoid-function", "anchor_text": "Logistic Regression: Why sigmoid function?Answer (1 of 12): In a Binomial regression we want to model the response variable as a linear combination of the\u2026www.quora.com"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html", "anchor_text": "Logistic Regression - ML Glossary documentationSay we're given data on student exam results and our goal is to predict whether a student will pass or fail based on\u2026ml-cheatsheet.readthedocs.io"}, {"url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "anchor_text": "Understanding binary cross-entropy / log loss: a visual explanationHave you ever thought about what exactly does it mean to use this loss function?towardsdatascience.com"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----f37c5e64ec8e---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/math?source=post_page-----f37c5e64ec8e---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/tag/boosting?source=post_page-----f37c5e64ec8e---------------boosting-----------------", "anchor_text": "Boosting"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&user=Deb&userId=9c41b57ddc0f&source=-----f37c5e64ec8e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&user=Deb&userId=9c41b57ddc0f&source=-----f37c5e64ec8e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff37c5e64ec8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f37c5e64ec8e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f37c5e64ec8e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@b.debanjan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@b.debanjan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Deb"}, {"url": "https://medium.com/@b.debanjan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "49 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c41b57ddc0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&user=Deb&userId=9c41b57ddc0f&source=post_page-9c41b57ddc0f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F18cf8d30b680&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fde-mystifying-xgboost-part-i-f37c5e64ec8e&newsletterV3=9c41b57ddc0f&newsletterV3Id=18cf8d30b680&user=Deb&userId=9c41b57ddc0f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}