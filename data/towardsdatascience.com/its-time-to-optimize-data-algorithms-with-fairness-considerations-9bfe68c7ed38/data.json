{"url": "https://towardsdatascience.com/its-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38", "time": 1683007576.845321, "path": "towardsdatascience.com/its-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38/", "webpage": {"metadata": {"title": "It\u2019s Time to Optimize Data Algorithms with Fairness Considerations | by Nicha Ruchirawat | Towards Data Science", "h1": "It\u2019s Time to Optimize Data Algorithms with Fairness Considerations", "description": "It is undeniable that our society is becoming increasingly governed by big data. As our lives become more heavily influenced by data-driven algorithms, it is important to be aware of their ethical\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.theverge.com/2014/2/19/5419854/the-minority-report-this-computer-predicts-crime-but-is-it-racist", "anchor_text": "Chicago police team developed a list of people most likely to commit violent crime", "paragraph_index": 11}, {"url": "https://www.nbcnews.com/feature/in-plain-sight/shift-change-just-in-time-scheduling-creates-chaos-workers-n95881", "anchor_text": "Workers\u2019 schedules are now being controlled by optimization softwares", "paragraph_index": 13}, {"url": "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm", "anchor_text": "recidivism model used in the police department", "paragraph_index": 14}, {"url": "https://www.huffpost.com/entry/teacher-evaluations_b_1328456", "anchor_text": "Sarah Wysocki was fired because an algorithm by DCPS\u2019 IMPACT", "paragraph_index": 15}, {"url": "https://www.usatoday.com/story/money/2018/04/29/student-loans-corinthian-colleges-court-fight-debts/557436002/", "anchor_text": "with Corinthian College", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/1905.01989.pdf", "anchor_text": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search", "paragraph_index": 22}, {"url": "https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html?", "anchor_text": "Google has started to address gender bias in their language translation [5].", "paragraph_index": 23}, {"url": "https://projects.propublica.org/emails/", "anchor_text": "roPublica\u2019s Message Machine", "paragraph_index": 27}, {"url": "https://webtap.princeton.edu/", "anchor_text": "Princeton\u2019s Web Transparency and Accountability Project", "paragraph_index": 27}, {"url": "https://www.ajlunited.org/", "anchor_text": "Algorithmic Justice League", "paragraph_index": 27}, {"url": "https://www.slideshare.net/KrishnaramKenthapadi/fairness-and-privacy-in-aiml-systems-187923831", "anchor_text": "LinkedIn\u2019s Fairness and Privacy in AI/ML Systems", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1905.01989.pdf", "anchor_text": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search", "paragraph_index": 31}, {"url": "https://www.slideshare.net/NeelimaJadhav1/bias-in-artificial-intelligence", "anchor_text": "Bias in Artificial Intelligence", "paragraph_index": 32}, {"url": "https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html", "anchor_text": "A Scalable Approach to Reducing Gender Bias in Google Translate", "paragraph_index": 33}], "all_paragraphs": ["It is undeniable that our society is becoming increasingly governed by big data. As our lives become more heavily influenced by data-driven algorithms, it is important to be aware of their ethical implications. One widespread issue is that these models tend to perpetuate existing societal biases and discrimination. Although often an unintentional process, this can result in products or services that are not equally inclusive for all groups of people, and in many cases even cause unintended consequences in their lives.", "For example, facial recognition systems that exist today are trained heavily with datasets that weigh more on certain demographics, resulting in accuracy that varies greatly across race, regions, and age. A person\u2019s face from a minority group may not be recognized by the system, or misinterpreted in a harmful manner. The following shows accuracy level by gender and race for facial recognition algorithms developed by three major companies:", "Another example is Google search results for certain keywords. Below is a comparison of Google image search results for \u2018professional hairdo\u2019 vs. image search for \u2018unprofessional hairdo\u2019.", "\u2018Professional hairstyles\u2019 dominantly showed pictures of white women. It also showed the associated keyword, \u2018male\u2019 with a white man icon. Whereas, \u2018unprofessional hairstyles\u2019 showed more images of black women. Associated keywords are \u2018black hair\u2019 and \u2018natural hair\u2019, with black women icons. The logo for associated keyword \u2018men\u2019 was of a black man instead.", "Bias can be incorporated in all steps of modeling [1,2,4], as illustrated in the diagram below:", "Certain models that embed bias are highly destructive. The book, Weapon of Mass Destruction by Cathy O\u2019Neil [2], summarizes this very well so I will draw largely from there for this section. In her book, she describes harmful models, or Weapon of Math Destruction, as those that satisfy the following criteria:", "Many models output decisions that may have harmful consequences. For example, it may cause people to lose their jobs, keep them trapped in poverty, or charge for products/services at prices they simply cannot afford.", "Some models govern society on a large scale, so the effects have large consequences on our lives. For example, the credit score model controls our financial options, which plays a large role in the ability to lead a stable life.", "3. Lack of transparency and accountability:", "Many models are \u2018black boxes\u2019. The model\u2019s decision-making process goes largely unquestioned and is not understood. The model\u2019s underlying assumption may even be wrong, but there is not enough transparency for people to be able to call out on its unfairness and defend against it.", "Key factors that contribute to destructive and discriminatory models:", "It may be obvious that the model is biased if it explicitly uses demographic variables. Hiring algorithms may rank females lower because motherhood often disrupts career performance. Nonetheless, a lot of models implicitly discriminate. It is usually difficult to obtain explicit data signals, so modelers turn to proxies that implicitly provide the same signals. A model may just use zip code, but zip code correlates with race and income. P2P lenders commonly develop risk scores based on correlations with neighborhoods, zip codes, stores customers shop at. These proxies do not just indicate responsibility that better guarantees loan payback. They indicate wealth, which is also correlated with race. Poor, minority groups may have higher borrowing costs despite already struggling, even if they are responsible. In another instance, a Chicago police team developed a list of people most likely to commit violent crime, not based on their actions but on their neighborhood and acquaintances who committed crimes. These people are not judged based on their own actions, but punished for belonging in a subgroup of society.", "2. Sole focus on optimization for a business goal, such as profit", "Most businesses build these models to help optimize profits. We never see a model with 100% accuracy in a real life because data is noisy, but we can tune it to optimize certain metrics, such as profit. Yet, model \u2018mistakes\u2019 can have high costs on people\u2019s lives. A stellar employee can mistakenly be predicted by a model as underperforming at their job and get fired. Furthermore, models often trade off humanity to optimize profits. Workers\u2019 schedules are now being controlled by optimization softwares to maximize profits. They employ the principles of \u2018Just in Time\u2019 principles popularized in the manufacturing industry, where items arrive just in time to minimize costs. Similarly, these models predict when stores will need staffing, and place employees in ever-changing schedule. Such an erratic, business-driven schedule makes it difficult to get a second job or go back to school. This, in turn, lessens their chances of escaping this cycle via better employment.", "Biased models produce outputs that help foster an environment reinforcing that bias. For example, a recidivism model used in the police department may output a decision that predicts inmates from certain racial profiles as riskier and should be kept in jail longer. However, because they are in jail longer, it is harder for them to find a job upon return, resulting in an increased likelihood to commit a crime again. When they do, it reinforces the model that it did indeed predict correctly based on these factors. There is very little effort to evaluate whether the end users being impacted received fair treatment, and how to adjust the model if not.", "Models tend to be based on assumptions. The police department\u2019s recidivism risk model explained above assumes that society and even the inmates themselves will benefit from being locked in longer. Yet, a later study actually claims that locking them up longer actually increases the likelihood for recidivism. As another illustration, a teacher named Sarah Wysocki was fired because an algorithm by DCPS\u2019 IMPACT \u2014 used to assess teacher performance \u2014 scored her as \u2018underperforming\u2019. Nonetheless, most students rated her highly. The model\u2019s decision was driven by her student\u2019s lower year to year standardized test scores. Students came in with falsely high scores from the previous year despite having inadequate skills. This hurts Sarah\u2019s ability to raise the scores, regardless of her teaching skills. The assumptions behind score increases as indicator for a teacher\u2019s performance was unvalidated.", "5. Reverse engineering enable gaming the system", "Because models make decisions based on rules, those who can reverse engineer these rules can beat the system and become false positives. U.S. News College Ranking rankings are driven by metrics such as acceptance rates, % graduations, etc. Arguably, this pressures schools to improve in quality. Nevertheless, these rankings often do not reflect the school\u2019s quality. One way to game the ranking is to lower acceptance rates by turning away top students who likely won\u2019t matriculate. Also, schools can cheat % employment by counting students as employed even if they are baristas. Or, they can raise income averages by focusing more on engineering and science departments, while less on education and social work departments.", "6. Taking advantage of end users\u2019 vulnerabilities", "Algorithms can be used to expose people\u2019s vulnerabilities and take advantage of their needs or ignorance. Take the example of a scandal with for-profit colleges, such as with Corinthian College. They used algorithms to target students from poorest zip codes and have clicked on ads such as payday loans offers. This group is desperate for means to earn more money, so it\u2019s easier to sell to them how private education can help. Clickstream data can provide information on their interests, Natural Language Processing can analyze words they put out on social media about their concerns. A/B testing can be used to optimize ads to get their attention. These colleges charge many times more than community colleges, optimizing to attract students who can land government loans. These students end up with a gigantic debt, without quality education to enable more income.", "Increase awareness:Many simply just do not know this issue exists. Awareness is not only important for data scientists or modelers. Other roles such as investors, business strategists, product managers, consumers \u2014 anyone who is involved in a data-driven product or company (which is probably the majority % by now) are just as responsible in shaping the industry.", "Incorporating fairness considerations in modeling processes:Modelers need to evaluate fairness at each step of the modeling process [2,4]. Data collection needs to ensure diverse datasets where minority groups have equal chance at training. Feature engineering needs consideration for explicit or implicit bias. Models need to be evaluated with fairness metrics on top of other traditional metrics, ensuring equal performance and impacts across all groups. Feedbacks when models make wrongful judgements need to go back into the model for readjustments. Workforce building out these models to be diverse. This way, minorities who will pick up on unfairness affecting people similar to them will push to address the issue.", "LinkedIn has put some effort in introducing fairness metrics when optimizing their ranking model for candidate search results. As explained in their paper, Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search [3], they introduce additional constraints to minimize deviation from desired proportions of representation for each attribute (such as race, gender, etc.) that show up via search results to ensure equal opportunity across groups for profile exposure. They validated using A/B testing, which showed that >95% of their searches are representative, while there are no significant changes in business metrics such as number of InMails sent/accepted.", "Similarly, Google has started to address gender bias in their language translation [5]. Many of their translations tend to choose a gender, such as choosing pronoun \u2018She\u2019 when talking about babysitters and \u2018He\u2019 when talking about doctors. They built a model to detect if the default translation made a gender-specific choice, and another language model to rewrite into a version that includes both gender.", "Evaluation of this approach is done by incorporating a fairness metric \u2014 bias reduction. This measures how many % fewer translations made a gender choice from previous models. The new approach reduced over 90% bias in translations from Hungarian, Finnish, Persian, Turkish to English.", "Making transparency and accountability a standard:Consumers should be able to demand more transparency from big corporations on how scores are used to judge them, as well as understand what goes into their score, with rights to fix what is inaccurate. There are some cases of well-governed scores. Credit scores, although can sometimes be misused, are transparent and accountable [2]. They incorporate feedback to adjust for model inaccuracies. They are relatively transparent \u2014 consumers have legal rights to know what their score is, what goes into their scores, and are given instructions on how to improve scores (e.g. reduce debt). Consumers can request to get mistakes fixed. Models that govern at a big scale need to steer more towards this direction.", "Tools for consumer transparency:We would benefit from more initiatives to build tools that are open and available to the public to simulate what would happen to their scores from various models if they took a certain action (e.g. what would happen to their credit score if they had an unpaid bill, and how much the lower score would affect her plans to buy a car) [2].", "Initiatives to expose biases in models:Biases in models can be more exposed through initiatives like ProPublica\u2019s Message Machine crowdsourcing campaign [2]. This crowdsourced what messaging different people were receiving from Obama campaign, used to reverse engineer the model for targeted political ads. Another example is Princeton\u2019s Web Transparency and Accountability Project. The project provides a platform to test automated systems such as search engines and job placement sites how they would treat different personas (e.g. rich, poor, male, female, mental illness). One more initiative is Algorithmic Justice League, which is a platform discussing model bias, where people can share biases they experienced and companies can request for bias audits on their models.", "Policy & Regulation:Regulations protecting the use of data need to protect usage of new types of scores [2]. For instance, we need to regulate the use of personality tests, health scores, reputation scores in hiring decisions. Additionally, data collection regulations need to be reconsidered. Currently, the European policy calls for any data collected to be approved by the user, as an opt-in, and prohibits reuse of data for other purposes. Moreover, regulations should require third party audits of large-scale models.", "Business models need to incorporate fairness & inclusivity goals:Although this may cost companies some of their profit, this is no different than today\u2019s CSR efforts or increasingly ethical business models (e.g. environmentally friendly) that actually add value to brands. Businesses might even become more profitable from serving a wider population. If businesses want to enter emerging markets that hold so much growth potential, it needs products that are accessible and inclusive for that population. As written in the book Factfulness by Hans Rosling, \u201cinvestors focusing solely on wealthy, traditional markets are actually missing out. There are billions of potential new consumers out there with growing purchasing power. Investors can miss out on biggest economic opportunity from continuing to think this group is the \u2018minority\u2019 or are \u2018still too poor\u2019\u201d.", "[1] Krishnaram Kenthapadi, LinkedIn\u2019s Fairness and Privacy in AI/ML Systems (2019) Pinterest Tech Talk", "[3] Sahin Cem Geyik, Stuart Ambler, Krishnaram Kenthapadi, Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search (2019)", "[4] Neelima Kumar, Bias in Artificial Intelligence (2017) Grace Hopper Conference", "[5] Melvin Johnson, A Scalable Approach to Reducing Gender Bias in Google Translate (2020) Google AI Blog", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9bfe68c7ed38&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nicharuch?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Nicha Ruchirawat"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ecd6cdacf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=post_page-f8ecd6cdacf9----9bfe68c7ed38---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf", "anchor_text": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"}, {"url": "https://www.theverge.com/2014/2/19/5419854/the-minority-report-this-computer-predicts-crime-but-is-it-racist", "anchor_text": "Chicago police team developed a list of people most likely to commit violent crime"}, {"url": "https://www.nbcnews.com/feature/in-plain-sight/shift-change-just-in-time-scheduling-creates-chaos-workers-n95881", "anchor_text": "Workers\u2019 schedules are now being controlled by optimization softwares"}, {"url": "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm", "anchor_text": "recidivism model used in the police department"}, {"url": "https://www.huffpost.com/entry/teacher-evaluations_b_1328456", "anchor_text": "Sarah Wysocki was fired because an algorithm by DCPS\u2019 IMPACT"}, {"url": "https://www.usatoday.com/story/money/2018/04/29/student-loans-corinthian-colleges-court-fight-debts/557436002/", "anchor_text": "with Corinthian College"}, {"url": "https://arxiv.org/pdf/1905.01989.pdf", "anchor_text": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search"}, {"url": "https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html?", "anchor_text": "Google has started to address gender bias in their language translation [5]."}, {"url": "https://projects.propublica.org/emails/", "anchor_text": "roPublica\u2019s Message Machine"}, {"url": "https://webtap.princeton.edu/", "anchor_text": "Princeton\u2019s Web Transparency and Accountability Project"}, {"url": "https://www.ajlunited.org/", "anchor_text": "Algorithmic Justice League"}, {"url": "https://www.slideshare.net/KrishnaramKenthapadi/fairness-and-privacy-in-aiml-systems-187923831", "anchor_text": "LinkedIn\u2019s Fairness and Privacy in AI/ML Systems"}, {"url": "https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality-ebook/dp/B019B6VCLO", "anchor_text": "Weapon of Mass Destruction"}, {"url": "https://arxiv.org/pdf/1905.01989.pdf", "anchor_text": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search"}, {"url": "https://www.slideshare.net/NeelimaJadhav1/bias-in-artificial-intelligence", "anchor_text": "Bias in Artificial Intelligence"}, {"url": "https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html", "anchor_text": "A Scalable Approach to Reducing Gender Bias in Google Translate"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9bfe68c7ed38---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9bfe68c7ed38---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9bfe68c7ed38---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ethical-ai?source=post_page-----9bfe68c7ed38---------------ethical_ai-----------------", "anchor_text": "Ethical Ai"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9bfe68c7ed38---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=-----9bfe68c7ed38---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=-----9bfe68c7ed38---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9bfe68c7ed38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9bfe68c7ed38---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9bfe68c7ed38--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nicha Ruchirawat"}, {"url": "https://medium.com/@nicharuch/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "199 Followers"}, {"url": "https://github.com/nicharuc/", "anchor_text": "https://github.com/nicharuc/"}, {"url": "https://www.linkedin.com/in/nicharuchirawat/", "anchor_text": "https://www.linkedin.com/in/nicharuchirawat/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ecd6cdacf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=post_page-f8ecd6cdacf9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F93c3fdcb8688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fits-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38&newsletterV3=f8ecd6cdacf9&newsletterV3Id=93c3fdcb8688&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}