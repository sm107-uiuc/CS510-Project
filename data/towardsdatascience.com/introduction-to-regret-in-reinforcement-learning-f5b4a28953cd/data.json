{"url": "https://towardsdatascience.com/introduction-to-regret-in-reinforcement-learning-f5b4a28953cd", "time": 1683007697.1827881, "path": "towardsdatascience.com/introduction-to-regret-in-reinforcement-learning-f5b4a28953cd/", "webpage": {"metadata": {"title": "Introduction to Regret in Reinforcement Learning | by Ziad SALLOUM | Towards Data Science", "h1": "Introduction to Regret in Reinforcement Learning", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com It is almost sure that every human has regretted something (actually many things) during\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Rock_paper_scissors", "anchor_text": "Rock Paper Scissors", "paragraph_index": 10}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "\u201cIn the end, we only regret the chances we didn\u2019t take\u201dLewis Carroll", "It is almost sure that every human has regretted something (actually many things) during his/her lifetime. Regretting not to buy a ticket when the price was still affordable, regretting not to take a career decision, regretting personal or social move, etc\u2026 Of course, regretting has a bitter taste, and even though it can be instructive, the reality is that the opportunity is often lost and there is no turning back.", "But that might not be quite the case when training a machine or an algorithm.", "The action you regret the most is the one that should have been (more likely) used or taken. So the probability of taking this action is proportional to how deep you regret you haven\u2019t taken it.", "Mathematically speaking, the regret is expressed as the difference between the payoff (reward or return) of a possible action and the payoff of the action that has been actually taken. If we denote the payoff function as u the formula becomes:", "regret = u(possible action) - u(action taken)", "Clearly we are interested in cases where the payoff of \u2018possible action\u2019 outperforms the payoff of the \u2018action taken\u2019, so we consider positive regrets and ignore zero and negative regrets.", "As said earlier the probability of using an action other than the one actually used is proportional to the regret it generates.", "Obviously, you might be asking, why not explicitly give action a4 a probability of 1 (\u03c3(a4) = 1)? Simply because the notion of regret is used when facing another actor, such as in games. Playing in a deterministic manner in a game will give your opponent a chance to counter measure your strategy and win.", "Consider a game of Rock Paper Scissors (RPS) with the system of points as follows:", "The table below gives the different combinations for playing this game as well as the results and how to improve the strategy.", "The first part of the table (Actual Game) shows your play against an opponent, and the \u2018Your Result\u2019 at each episode. The \u2018Iterations\u2019 column is the number of episodes that have occurred with the same combination for ex: R vs R, or S vs P, etc\u2026", "The second part (Your other game scenarios) contains the scenarios that you could have played in order to enhance (or not) your result, assuming that the opponent played the same way. It also shows the Regret for not playing a given action.The column Cumul Regret contains the cumulative Regret which is the sum of Regrets.", "The need for Cumulative Regret stems from the fact that independently computing regret, does not capture what has happened in other games or episodes. This means that the algorithm is not learning from its experience.", "As a human, you keep in memory what you have played previously and how to get the advantage of those past experiences. But in order for an algorithm to do the same, there should be a computation that takes into account what has happened before.", "The third part (Strategy Adjustment), computes probabilities for each action (Rock, Paper, Scissors) that should be used in order to maximize your result, always assuming that the opponent played in the same way.", "These probabilities are computed as (Cumulative Regret for Action) / Total Regret. Where Total Regret is the sum of positive Cumulative Regrets of the same row. In case the Total Regret is zero, we assign equal probabilities for each action (check the 2nd row).", "In the first row of the table above, you played R and the opponent played R, so the result is a draw (0). It would have been better if you have played P and the opponent played R, so your regret is 1 for not playing P. The Strategy Adjustment shows that you don\u2019t regret Rock, nor Scissors but you regret not using Paper.In the second row, you win by using R against S, and the Strategy Adjustment section shows that after 2 games, you have no regret.", "As the episodes continue, we see that the strategies vary to reach equilibrium in which each action should be used 1/3.PS. this is the best strategy in RPS because it makes all 3 actions equally probable and thus the move is unpredictable by the opponent.", "Now, what happens if one scenario is played more than others?For example, in the table below the episode S vs S occurred 1000 times, which results of a thousand draws. This leads to a regret of 1000 on these scenarios, and the strategy shifts towards using Rock 100% of the time.", "In the example below, P vs S occurred 1000 times resulting in thousand losses and 2000 regrets for not using Rock and 1000 regrets for not using Scissors.So the strategy is adjusted to use Rock 67% of the time and Scissors 33% of the time.", "However, there is a catch in the following example, where R vs S occurs 1000 times, resulting in thousand victories and no regrets. Since there is no regret the algorithm does not update the strategy.", "Below is the link to Google Colab book that contains the code to a simple Regret algorithm.Important: In order to run or edit the code you need to make a copy of the book.", "So far we have assumed that the opponent keeps playing in the same way, using the same strategy. However, this can\u2019t be true! Any opponent will eventually detect any bias in your strategy and move to exploit it.", "So the training can\u2019t be done against a fixed strategy. To remedy this situation we use Self Play.Instead of training one actor against a fixed strategy, Self Play trains all actors against each other. This is done by \u201creplicating\u201d the sequence that the first actor does, and applying it to the other actors. So each actor now maintains its own data structures which contain its own strategy, regrets etc\u2026 After each episode, each actor computes the result from its own perspective, as well what it could have done to (possibly) enhance the outcome.", "The following link leads to a Google Colab book implementing a simple implementation of Regret Self Play.", "Important: In order to run or edit the code you need to make a copy of the book.", "It is interesting to notice that no matter what strategy the two actors start with, they both converge to the best strategy of the RPS game which is a uniform probability of 1/3 for each action. This ensures that all actions are equally likely, and prevents any bias that can be exploited by the opponent.", "This article introduces the Regret technique that is simple and intuitive. it permits players to reach equilibrium play by tracking regrets for past plays, making future plays proportional to positive regrets. This technique is the foundation of more elaborate ones such as Counterfactual Regret Minimization (CFR) and Deep CFR.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff5b4a28953cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----f5b4a28953cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@freetousesoundscom?utm_source=medium&utm_medium=referral", "anchor_text": "Free To Use Sounds"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://en.wikipedia.org/wiki/Rock_paper_scissors", "anchor_text": "Rock Paper Scissors"}, {"url": "https://en.wikipedia.org/wiki/Rock_paper_scissors", "anchor_text": "Rock Paper Scissors"}, {"url": "https://colab.research.google.com/drive/1FB57Jfi1llITSL6DUyUr_9P7lsXmOP-8#scrollTo=_9p-AnRTy8rI", "anchor_text": "Simple RPS Examplecolab.research.google.com"}, {"url": "https://colab.research.google.com/drive/1FB57Jfi1llITSL6DUyUr_9P7lsXmOP-8#scrollTo=-ej-kJVb8j0y", "anchor_text": "RPS Self Play Examplecolab.research.google.com"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f5b4a28953cd---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f5b4a28953cd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/regret?source=post_page-----f5b4a28953cd---------------regret-----------------", "anchor_text": "Regret"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f5b4a28953cd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----f5b4a28953cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----f5b4a28953cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff5b4a28953cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f5b4a28953cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f5b4a28953cd--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-regret-in-reinforcement-learning-f5b4a28953cd&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}