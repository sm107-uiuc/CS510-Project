{"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0", "time": 1682997634.176888, "path": "towardsdatascience.com/reinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Generalisation of Continuing Tasks | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Generalisation of Continuing Tasks", "description": "Till now we have been through many reinforcement learning examples, from on-policy to off-policy, discrete state space to continuous state space. All these examples vary in some way, but you might\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/AccessControl/ServerAccess.py", "anchor_text": "Full implementation", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/AccessControl/ServerAccess.py", "anchor_text": "Full implementation", "paragraph_index": 17}], "all_paragraphs": ["Till now we have been through many reinforcement learning examples, from on-policy to off-policy, discrete state space to continuous state space. All these examples vary in some way, but you might have noticed that they have at least one shared trait \u2014 Episodic, that is all of which have a clear starting point and ending point, and whenever an agent reaches the goal, it starts over again and again until reaching certain number of loops. In this article, we will extend the idea to non-episodic task, that is task which has no clear ending point and the agent goes on forever in the environment setting. In this article, we will", "The main concept that will be applied to non-episodic task is average reward. The average reward setting also applies to continuing problems, problems for which the interaction between agent and environment goes on and on forever without termination or start states. Unlike that setting, however, there is no discounting\u2014the agent cares just as much about delayed rewards as it does about immediate reward. So the key is", "Let dive directly into the algorithm and I will explain why it goes in this way.", "Firstly, the algorithm applies to continuous state space, and in fact, the agent\u2019s exploring process and weight updating process are the same with other algorithms we talked before, the difference lies in the average reward term introduced here.", "The key difference is the definition of \u03b4, in continuing task, reward is defined as R(S, A) \u2014 R(average) , which is the difference between reward received at the current state and the average reward until the current state. I will give you an intuitive explanation of why the average reward is introduced here, recall the definition of R(S, A) + Q(S', A') , it is the estimation of the value of Q(S, A) , which equals to the rewards the agent going to collect all the way to the end of the game. However, in a continuing task, the game never ends, thus the collected reward could go to infinity, it needs a term to restrain the estimation value, then there comes the average reward!", "As the step goes on, the average reward needs to be updated as well(Note that \u03b2 is the learning rate dedicated for the reward update). With the average reward setting, it is like setting a baseline for the agent that only when the agent conducts an action that gives a reward above the average reward can the weight be updated positively, otherwise negatively (disregarding the q value of next state). If you are still not convinced by the explanation and doubt what would happen without the average reward term, I will show you the learning result without average reward of the server access example at the end of the post.", "Now let\u2019s apply the algorithm on a concrete continuing task example cited from Sutton\u2019s book called Access-Control Queuing Task.", "This is a decision task involving access control to a set of 10 servers. Customers of four different priorities arrive at a single queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to the server, depending on their priority, with higher priority customers paying more. In each time step, the customer at the head of the queue is either accepted (assigned to one of the servers) or rejected (removed from the queue, with a reward of zero). In either case, on the next time step the next customer in the queue is considered. The queue never empties, and the priorities of the customers in the queue are equally randomly distributed. Of course a customer cannot be served if there is no free server; the customer is always rejected in this case. Each busy server becomes free with probability p = 0.06 on each time step. The task is to decide on each step whether to accept or reject the next customer, on the basis of his priority and the number of free servers, so as to maximize long-term reward without discounting.", "To recap, our agent needs to decide whether to accept a customer based on its priority and current number of free servers in order to gain maximum long-term reward. This game setting is definitely a continuing task, as the process never ends since the waiting queue would never be empty. The state is (numberOfFreeServers, customerPriority) , action is either reject or accept and reward is 1, 2, 4, 8 based on the customer\u2019s priority. Now if you are clear with the rules, let\u2019s go head and implement it.(Full implementation)", "This is a discrete state task, but as we have already intensively talked about applying tile coding on continuous state, we are going to apply it again in representation of the state space even it is discrete(The tile coding function we applied here is discussed here).", "This is in effect a Q value class, inside which we set 8 tilings and in total 2048 grids. Action 1 stands for accept, 0 for reject. The value function gives the value based on current state(n_server, priority) and action, and update function updates the weights accordingly. The stateValue function returns the maximum value of the state(this function will only be used in the visualisation part), and note that when the number of free servers is 0, it returns the value of action 0 (always reject).", "Now let\u2019s get into the main class:", "All these initialisations are self evident and for each server the free probability is 0.06.", "The numFreeServers function gives the current number of free servers based on current state. The chooseAction function chooses the action based on current state and the value function we discussed above, and again the action selection method is \u03f5-greedy.", "After the agent takes an action, we will need to judge the next state, where the number of free servers will be subtracted by 1 if the action was accept, and remains the same if the action was reject. The next customer\u2019s priority is randomly generated.", "The giveReward function simply gives the reward based on a customer\u2019s priority. In the case of reject action, the reward is always 0.", "In the final run function, we will assemble the whole process and let the agent learn.", "The function is straight forward, as everything follows exactly the steps in the algorithm.(Full implementation)", "Now let\u2019s see how our agent performs:", "This is the result after 50,000 rounds and with \u03b1=0.01, \u03b2=0.01 and exp_rate=0.1. The drop on the right of the graph is probably due to insufficient data; many of these states were never experienced.", "Lastly, let me show you what would happen if goes without the average reward term.", "The state value is way larger than the actual value, and in fact, the more rounds we run, the larger the state value would be as the \u03b4 is not well limited.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fffb9a89d57d0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----ffb9a89d57d0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b", "anchor_text": "tile coding"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/AccessControl/ServerAccess.py", "anchor_text": "Full implementation"}, {"url": "https://towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/AccessControl/ServerAccess.py", "anchor_text": "Full implementation"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ffb9a89d57d0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ffb9a89d57d0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----ffb9a89d57d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----ffb9a89d57d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fffb9a89d57d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ffb9a89d57d0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ffb9a89d57d0--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}