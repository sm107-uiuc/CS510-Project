{"url": "https://towardsdatascience.com/understanding-nlp-and-topic-modeling-part-1-257c13e8217d", "time": 1683001200.2876558, "path": "towardsdatascience.com/understanding-nlp-and-topic-modeling-part-1-257c13e8217d/", "webpage": {"metadata": {"title": "Understanding NLP and Topic Modeling Part 1 | by Tony Yiu | Towards Data Science", "h1": "Understanding NLP and Topic Modeling Part 1", "description": "Natural language processing (NLP) is one of the trendier areas of data science. Its end applications are many \u2014 chatbots, recommender systems, search, virtual assistants, etc. So it would be\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e", "anchor_text": "If you want to understand why in more detail, here is a previous post I wrote on The Curse of Dimensionality.", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "euclidean distance", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e", "anchor_text": "the Curse of Dimensionality", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/understanding-pca-fae3e243731d", "anchor_text": "something like PCA, which identifies the key quantitative trends (that explain the most variance) within your features.", "paragraph_index": 24}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership", "paragraph_index": 36}], "all_paragraphs": ["Natural language processing (NLP) is one of the trendier areas of data science. Its end applications are many \u2014 chatbots, recommender systems, search, virtual assistants, etc.", "So it would be beneficial for budding data scientists to at least understand the basics of NLP even if their career takes them in a completely different direction. And who knows, some topics extracted through NLP might just give your next model that extra analytical boost. Today, in this post, we seek to understand why topic modeling is important and how it helps us as data scientists.", "Topic modeling, just as it sounds, is using an algorithm to discover the topic or set of topics that best describes a given text document. You can think of each topic as a word or a set of words.", "The first time I worked with NLP, I wondered to myself:", "\u201cIs NLP just another form of EDA (exploratory data analysis)?\u201d", "That\u2019s because up until then, I had been mainly building models with a clear objective in mind \u2014 use X to forecast or explain Y. NLP was much less structured and clear. Even when I finally successfully ran my topic modeling algorithm, the topics that fell out produced more questions than answers. Here, take a look at some of the topics that came out of my NLP analysis of Reddit:", "Some of the topics make sense, some of them do not. And what exactly should I be doing with these topics anyways?", "Such is life as a data scientist \u2014 often the real work begins only after you finally get your data cleaned and your code debugged. Then, at long last, it\u2019s time to find those annoyingly elusive insights. And that\u2019s precisely the point of NLP and topic modeling. It might not be an end unto itself, but extracting topics via NLP gets us that much closer to generating something useful in much the same way that dimensionality reduction techniques help us on the numerical side of the data science world.", "Topic modeling allows us to cut through the noise (deal with the high dimensionality of text data) and identify the signal (the main topics) of our text data.", "And with this distilled signal, we can start the real work of generating insights. Let\u2019s go through this step by step.", "High dimensional data is regarded as a curse in many data science applications. If you want to understand why in more detail, here is a previous post I wrote on The Curse of Dimensionality. But for those short on time here\u2019s the TLDR:", "When are we most likely to run into high dimensional data (a.k.a. too many features)? Text data. To see why, imagine how we would encode the data for the following sentence so that an algorithm can do something with it:", "\u201cThe man was wearing a jacket with a gold star.\u201d", "A natural way is what\u2019s called the bag of words approach \u2014 bag of words represents a given document as a list of distinct words and their frequencies. So the sentence above would look like this:", "So in order to capture a given document, we need a feature for each unique word in it. And for a given document, the value for each feature is the number of times the word appears in the document (so for our earlier example, every word appears once besides \u201ca\u201d, which appears twice).", "Now imagine that our document is not an isolated one but rather part of a much larger corpus. Let\u2019s first get the lingo out of the way:", "If your corpus is large, then you will probably have at least tens of thousands of unique words in it (more if your corpus includes a lot of names). Just attempting to picturing that bag of words makes my head hurt. And attempting to run algorithms on it would be both extremely slow and probably unhelpful \u2014 it\u2019s highly likely that you will have more features (distinct words) than observations (documents).", "Now let\u2019s use a practical example to see how NLP helps sift through the dimensionality to reveal signal. Imagine that we want to recommend a few books to a friend. How would we go about doing that?", "One way would be to ask:", "\u201cHey, name a few books that you read recently that you really liked.\u201d", "And then based on the reply, recommend a few of our favorite books that are most similar to the ones that he or she listed. We just described a simple recommender system.", "In order to do what we just described algorithmically, we need to be able to figure out how to measure whether two books are similar or not. We could represent both books as bags of words and try comparing them using a distance measure like euclidean distance, but is that actually helpful? The answer is no for several reasons:", "The first reason is stop words (really common words like \u201cthe\u201d, \u201ca\u201d, \u201cit\u201d, \u201cand\u201d, etc.) \u2014 these words occur very frequently in pretty much all documents and they would inject a lot of meaningless noise into our similarity score (knowing that both books contain many instances of the word \u201cthe\u201d would not be helpful at all).", "And even if we removed all the stop words, the Curse of Dimensionality still affects us. There are so many distinct words in a book and many of them have zero correlation to the actual topic of the book. Thus, it\u2019s highly likely for our similarity measure to latch onto one of these noise words \u2014 this is basically the text version of spurious correlation. For example, we could have a book about fire fighters and a book about salmon fishing, but rank them as highly similar because our algorithm noticed that the words \u201cpole\u201d and \u201cengine\u201d occur frequently in both. This is an accidental and meaningless similarity and it would be problematic if we acted upon it.", "This is where topic modeling comes in. Topic modeling is the practice of using a quantitative algorithm to tease out the key topics that a body of text is about. It bears a lot of similarities with something like PCA, which identifies the key quantitative trends (that explain the most variance) within your features. The outputs of PCA are a way of summarizing our features \u2014 for example, it allows us to go from something like 500 features to 10 summary features. These 10 summary feature are basically topics.", "In NLP, it works almost exactly the same way. We want to distill our total corpus of books and its 100,000 features (distinct words) into 7 topics (I decided on 7 topics arbitrarily). And once we know the topics along with what they consist of, we can transform each book in our corpus from a noisy bag of words to a clean portfolio of topic loadings:", "Now we\u2019re in business. Similarity scores calculated using each book\u2019s topic loadings are a lot more useful than ones calculated using the raw bag of words because spurious similarities are now much less likely.", "Even the descriptive statistics of a book are more meaningful in \u201ctopics space\u201d than in \u201cbag of words space\u201d \u2014 we can now say a book loads heavily on the data science topic instead of puzzling over why the two most frequent words in our bag of words are \u201cforest\u201d and \u201crandom\u201d.", "In our previous example, we decided to express our book as loadings on 7 topics. But we could have gone with any number of topics (picking the number of topics is more art than science and depends heavily on your data \u2014 thus knowing your data well is critical). 10 works too, so does 100. But think about what happens as we keep increasing the number of topics and each topic becomes increasingly granular \u2014 the algorithm begins to lose the ability to see the big picture.", "For example, let\u2019s say we have three books \u2014 Book 1 is a French travel guide, Book 2 is a Chinese travel guide, and Book 3 is an economic history of the urbanization of China. With our 7 topics NLP model, we would classify Books 1 and 2 as travel books (and score them as similar to each other) and Book 3 as a business book (and score it as not similar to the others).", "With 5,000 topics, we might classify Book 1 as \u201cCycling Rural France\u201d, Book 2 as \u201cTraveling Urban China\u201d, and Book 3 as \u201cHistory Urban China\u201d. Now it is much less clear how we would score them \u2014 the algorithm might just throw up its hands and rate all 3 books as equally similar/different. That\u2019s not necessarily wrong (depending on the application) but it does show how high dimensional data (which 5,000 topics definitely is) can inject noise that distorts our analysis in unintended ways.", "A general rule of thumb when topic modeling, is to be only as specific as your end application requires you to be, never more.", "I realize that so far I\u2019ve been suitably vague on how we actually come up with our topics. That\u2019s because I wanted to fully explore why NLP is important. Next time, I will cover (with Python code) two topic modeling algorithms \u2014 LDA (latent Dirichlet allocation) and NMF (non-negative matrix factorization).", "Until then, thanks for reading and cheers!", "More Data Science and Analytics Related Posts By Me:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist. Founder Alpha Beta Blog. Doing my best to explain the complex in plain English. Support my writing: https://tonester524.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F257c13e8217d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tonester524.medium.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7----257c13e8217d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@ivo-rainha-527110?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Ivo Rainha"}, {"url": "https://www.pexels.com/photo/assorted-books-on-shelf-1290141/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e", "anchor_text": "If you want to understand why in more detail, here is a previous post I wrote on The Curse of Dimensionality."}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "such as Euclidean distance"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "euclidean distance"}, {"url": "https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e", "anchor_text": "the Curse of Dimensionality"}, {"url": "https://towardsdatascience.com/understanding-pca-fae3e243731d", "anchor_text": "something like PCA, which identifies the key quantitative trends (that explain the most variance) within your features."}, {"url": "https://towardsdatascience.com/got-data-science-jobs-552e39d48da2", "anchor_text": "The Curse of Dimensionality"}, {"url": "https://towardsdatascience.com/understanding-pca-fae3e243731d?source=post_page---------------------------", "anchor_text": "Understanding PCA"}, {"url": "https://towardsdatascience.com/business-strategy-for-data-scientists-25e3ca0af5ee", "anchor_text": "Business Strategy For Data Scientists"}, {"url": "https://towardsdatascience.com/business-simulations-with-python-a70d6cba92c8", "anchor_text": "Business Simulations With Python"}, {"url": "https://towardsdatascience.com/understanding-bayes-theorem-7e31b8434d4b", "anchor_text": "Understanding Bayes\u2019 Theorem"}, {"url": "https://towardsdatascience.com/understanding-the-naive-bayes-classifier-16b6ee03ff7b", "anchor_text": "Understanding The Naive Bayes Classifier"}, {"url": "https://towardsdatascience.com/fun-with-the-binomial-distribution-96a5ecabf65b", "anchor_text": "The Binomial Distribution"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----257c13e8217d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----257c13e8217d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----257c13e8217d---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/programming?source=post_page-----257c13e8217d---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----257c13e8217d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&user=Tony+Yiu&userId=840a3210fbe7&source=-----257c13e8217d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&user=Tony+Yiu&userId=840a3210fbe7&source=-----257c13e8217d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F257c13e8217d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----257c13e8217d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----257c13e8217d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----257c13e8217d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----257c13e8217d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----257c13e8217d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----257c13e8217d--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://tonester524.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "102K Followers"}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F78d3e392d884&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nlp-and-topic-modeling-part-1-257c13e8217d&newsletterV3=840a3210fbe7&newsletterV3Id=78d3e392d884&user=Tony+Yiu&userId=840a3210fbe7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}