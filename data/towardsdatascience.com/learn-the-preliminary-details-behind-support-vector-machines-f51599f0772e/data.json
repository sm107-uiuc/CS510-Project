{"url": "https://towardsdatascience.com/learn-the-preliminary-details-behind-support-vector-machines-f51599f0772e", "time": 1683017494.632284, "path": "towardsdatascience.com/learn-the-preliminary-details-behind-support-vector-machines-f51599f0772e/", "webpage": {"metadata": {"title": "Learn the Preliminary Details Behind Support Vector Machines | by Robby Sneiderman | Towards Data Science", "h1": "Learn the Preliminary Details Behind Support Vector Machines", "description": "Support Vector Machines are a popular tool used in several branches of Machine Learning. In particular, they are extremely useful for binary classification. Support Vector Machines have their basis\u2026"}, "outgoing_paragraph_urls": [{"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning II edition (Hastie).", "paragraph_index": 2}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/data.html", "anchor_text": "this link", "paragraph_index": 3}, {"url": "http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf", "anchor_text": "Rosenblatt 1958", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron", "paragraph_index": 15}, {"url": "https://www.linkedin.com/in/robert-sneiderman-359710188/", "anchor_text": "LinkedIn", "paragraph_index": 50}, {"url": "https://www.sciencedirect.com/topics/neuroscience/support-vector-machine", "anchor_text": "[", "paragraph_index": 53}], "all_paragraphs": ["Support Vector Machines are a popular tool used in several branches of Machine Learning. In particular, they are extremely useful for binary classification. Support Vector Machines have their basis in the concept of separating hyperplanes, so it is useful to first be introduced to this concept.", "In this article, I introduce the method of classification via separating hyperplanes. We start off simple and describe how even linear regression can be used to make simple binary classifications. We then move on to using separating hyperplanes and \u2018optimal\u2019 separating hyperplanes. Finally, with this background, we are in the position to introduce support vectors and the powerful Support Vector Machine. We also include examples using R, and discuss how one can fit kernel based algorithms in R using the kernlab (Kernel Based Machine Learning) package.", "Much of the mathematical details, and the toy example datasets in this article come from The Elements of Statistical Learning II edition (Hastie). The data, and more information on the subject can be found on the textbook's website. The data can be downloaded from the data tab of the above link, where it is called \u201cESL.mixture\u201d.", "Let\u2019s start with a simple. You can load in the following toy dataset into R from this link. The dataset consists of observations each with a pair of coordinates (x1,x2). Each observation also has an outcome, y, which we denote in the plot by colour (red (y=1) or blue (y=0) denoting its class).", "Figure 1 is a plot of the toy example dataset. It appears that red and blue points are somewhat separated, but clearly there would be no way to perfectly separate the classes with just a simple line.", "Suppose we were given a new observation with coordinates (x1,x2). How should we classify this new point? Clearly, the location of the coordinates will help inform our choice of assigning our new observation to red or blue.", "One simple method would be to fit a linear regression line to the data and classify the new point based off if the point lied above 0.5, or below it.", "If we want to classify based on the 0.5 threshold(the midpoint between the two classes), we could fit the linear regression line and solve in terms of x2; as seen in the below code and plot.", "We could then classify a point based on whether it was located above (red) or below (blue) the regression line.", "But is this the optimal way to make classifications? Well, we don\u2019t know the true underlying distribution, so it would be impossible to conclude definitively. But in reality, it would be extremely unlikely that this simple classification would be ideal.", "We come back to this toy dataset later on, but for now, we move to the more general problem.", "As we noted in the toy example dataset, no line could perfectly separate our two classes. But consider a more simple case, where such a line could actually be fit.", "Let's go back to Rosenblatt 1958 and the Perceptron. The perceptron algorithm can be used to find separating hyperplanes (provided one exists). The method for which they find the plane involves stochastic gradient descent.", "Figure 3 demonstrates three lines fit to a simple two class dataset. The orange line is the least squares solution fit to the data, and it doesn\u2019t even separate the two classes. The two blue lines both perfectly separate the classes, but are not identical. This is due to the fact that the perceptron algorithm finds one solution and then stops (due to random starts it can obtain different solutions).", "Using the perceptron algorithm is better then just using simple linear regression, and can actually separate the classes (provided it is possible). However, it still encounters problems. For one, it doesn\u2019t tell us anything about the multiple different lines it can find (are some \u2018better\u2019)? And also, if no perfect separating hyperplane exists, the algorithm does not converge.", "We saw that the perceptron can find a separating hyperplane (provided one exists). However, it doesn\u2019t tell us anything special about the one it happens to converge on. The next big improvement is from Vapnik (1996), which forms the basis for the support vector machine. Vapnik described not only how to find the unique separating hyperplane, but how to find the optimal separating hyperplane or the \u2018perceptron of optimal stability\u2019. This has been shown to outperform simple hyperplane separation and often results in lower test error.", "Figure 4 illustrates the optimal separating hyperplane classifier fit to the simple dataset. Three points lie directly on the outside of the margin, and are known as the support points. The red line is the solution from logistic regression, which is very similar, although not identical.", "Figure 4 was a simple way to visualise the basic support vector machine. We now describe the details surrounding the support vector classifier, and what can be done when there is no way to perfectly separate the data.", "In simple terms, SVMs find an optimal hyperplane to separate distinct classes. In the most basic case, this would be finding the line that best separates two classes. By \u2018best separates\u2019, we mean that the margin is maximised. The margin is the distance from the closest point in each of the two classes. Classifying new observations then depends on their location with respect to the decision boundary.", "Consider a set of N training examples:", "Where there are only two classes for which y belongs to, -1 or 1. That is;", "The support vector works by creating a decision boundary (hyperplane) such that the \u2018margin\u2019 is maximised:", "Points that lie directly on the outside lines (and thus are key in determining the margin), are known as the Support Vectors.", "New classes are thus predicted based on if they lie above or below the boundary (the solid middle blue line); i.e simply off their sign.", "When the data is linearly separable, we can perfectly separate the training data with a maximum margin hyperplane.", "In cases in which we can\u2019t separate perfectly, we have to allow for some violations. We then fit the model by using some cost function that allows for some misclassifications.", "For those interested in more background details please read along, but feel free to skip this section if you are satisfied with knowing that the algorithm is solved using quadratic programming. The method in which this optimal hyperplane is found is not simple; It is a quadratic programming problem that involves Lagrange Multipliers. Thankfully, multiple packages in R and Python can do this automatically.", "Essentially, we want to maximise the Margin M subject to the constraints that we lie on one side of the hyperplane or the other. Which can be simply written as a maximisation problem:", "This is equivalent to the minimisation in terms of the parameters;", "In fact, it is the hyperplane which produces the maximum margin.", "Let\u2019s return to the mixture example from the toy dataset we discussed above. In R, the package Kernlab allows us to easily fit support vector machines. We have to specify a few parameters.", "Type: We specify C-svc (the default setting) for SVM classification.", "Kernel: We want to first fit a linear kernel (linear decision boundary) so we use the vanilladot linear kernel", "C: The Cost Parameter (Determines how points around margin are treated). We try first with C=10000. Larger values of C focus the margin on points close to it, while smaller C focuses more on points in the outliers, far from the margin.", "The above plot visualises our linear support vector machine which aims to separate the two classes. Since it is not perfectly separable, we have a parameter C that acts as a regularisation parameter", "The model obtains a training error of 0.27. Considering that the toy dataset was visually non-separable, that isn\u2019t too bad. There are a total of 125 support vectors. These are points that line on the boundary or within the margin.", "We try the same thing but with C=0.01;", "This produces a similar plot, but the training error is slightly reduced to 0.265. We have more support vectors now, 172. This is due to the fact that we have a wider margin for this particular C value.", "Above we fit some linear Support Vector Machines. But our data didn\u2019t look linearly separable. Can we improve on our 0.265 training error? The answer is yes. SVMs are popular for this reason, as they can be greatly improved and extended via what is known as the \u2018kernel trick\u2019 (which are just a form of basis expansions).", "Clearly our toy dataset was not linearly separable. One of the main applications of support vector machines is that we can easily extend them via the \u2018kernel trick\u2019. This involves transforming the initial observations, using some sort of dot-product like transformation and THEN finding the optimal separable hyperplane. Above, we simply worked with the observations as they came in the form (x1,x2).", "But, suppose for each observation we instead worked with", "This is known as the Radial Basis Kernel and allows us to fit more complicated models.", "Look what happens when we apply the simple radial basis function to our original data:", "Which can then be fit linearly and back transformed to the original dimension. We specify our kernel now as \u2018rbfdot\u2019 (instead of linear as before).", "Which has an even lower training error of 0.155. The number of support vectors is reduced to 115 as less points lie inside the margin. The Cost parameter C can be tuned using cross-validation if you want to search a grid of possible values, but C=1 is the default.", "Support Vector Machines are an important and useful tool that was originally used for binary classification. However, they can be applied to many machine learning algorithms. The history and method by which they were developed is also interesting. The mathematical details behind the SVM are solid and have a firm foundation in linear algebra. They are worthwhile algorithm for those interested in Data Science and Machine learning to study in more detail.", "SVMs extend the idea of separating hyperplanes and find the optimal separating hyperplane. In low dimensions, this has an intuitive way to understand, in higher dimension, we can\u2019t visualise it, but the math is a simple extension.", "SVMs can be extended to a non-linear decision boundary using \u2018kernel tricks\u2019 (or basis expansions) to fit more complicated boundaries. Common other kernels include the Gaussian (Radial Basis Function). These methods work by transforming the data first so that we can fit what appears to be a linear boundary in the transformed input space.", "Despite the theoretical background and more difficult interpretation in words, SVMs are widely used in industry. In particular, they find usage in medical related data, neurological imaging and more.", "In this article, we considered the initial use of SVMs, which is to binary classification. However, SVMs are a topic of ongoing research and several extensions have been developed. You can use SVMs to perform regression and even use them to perform clustering via unsupervised learning.", "Did you enjoy this article or learn something new? If so, please check out my other Medium articles and consider sharing the article or leaving a clap. If you would like to connect, add me on LinkedIn. Also please leave any comments/corrections/questions below, and check out the sources.", "[1] Hastie, Tibshirani, Friedman (2009). The Elements of Statistical Learning II.", "[2] F Rosenblatt (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.", "[6] Geron, Aurelien (2020). Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff51599f0772e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rob-sneiderman.medium.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Robby Sneiderman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd494db13357&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&user=Robby+Sneiderman&userId=cd494db13357&source=post_page-cd494db13357----f51599f0772e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/uPJzWWhPcP4", "anchor_text": "Unsplash"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning II edition (Hastie)."}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/data.html", "anchor_text": "this link"}, {"url": "http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf", "anchor_text": "Rosenblatt 1958"}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron"}, {"url": "https://www.linkedin.com/in/robert-sneiderman-359710188/", "anchor_text": "LinkedIn"}, {"url": "https://github.com/Robby955/LearningSVM", "anchor_text": "Robby955/LearningSVMRepo for practising with Support Vectors and Separating Hyperplanes GitHub is home to over 50 million developers\u2026github.com"}, {"url": "http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf", "anchor_text": "Support vector regression machines"}, {"url": "https://www.sciencedirect.com/topics/neuroscience/support-vector-machine", "anchor_text": "["}, {"url": "https://medium.com/tag/data-science?source=post_page-----f51599f0772e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f51599f0772e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f51599f0772e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/statistics?source=post_page-----f51599f0772e---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&user=Robby+Sneiderman&userId=cd494db13357&source=-----f51599f0772e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&user=Robby+Sneiderman&userId=cd494db13357&source=-----f51599f0772e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff51599f0772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f51599f0772e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f51599f0772e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f51599f0772e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f51599f0772e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f51599f0772e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f51599f0772e--------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robby Sneiderman"}, {"url": "https://rob-sneiderman.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd494db13357&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&user=Robby+Sneiderman&userId=cd494db13357&source=post_page-cd494db13357--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2e6ea218c897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-the-preliminary-details-behind-support-vector-machines-f51599f0772e&newsletterV3=cd494db13357&newsletterV3Id=2e6ea218c897&user=Robby+Sneiderman&userId=cd494db13357&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}