{"url": "https://towardsdatascience.com/probabilistic-matrix-factorization-b7852244a321", "time": 1683001501.252151, "path": "towardsdatascience.com/probabilistic-matrix-factorization-b7852244a321/", "webpage": {"metadata": {"title": "Probabilistic Matrix Factorization | by Benjamin Draves, PhD Candidate | Towards Data Science", "h1": "Probabilistic Matrix Factorization", "description": "In this post we introduce probability matrix factorization from a Bayesian Statistics perspective. We also draw connections between optimization and regularization in posterior inference."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/93b225681c92?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Kevin Liao", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1", "anchor_text": "here", "paragraph_index": 28}, {"url": "http://jmlr.org/papers/volume16/hastie15a/hastie15a.pdf", "anchor_text": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares", "paragraph_index": 32}, {"url": "https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf", "anchor_text": "Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1804.05090.pdf", "anchor_text": "Regularized Singular Value Decomposition and Application to Recommender System", "paragraph_index": 34}], "all_paragraphs": ["\u201cYou are given a design matrix X, how would you predict Y?\u201d This question is at the heart of supervised learning and regression based tasks. This question assumes perfect knowledge of the design matrix X. In X, we store records in the rows and features describing these records in the columns. But what happens when this matrix is corrupted? What happens when some observations are missing?", "Consider the setting where we have a data matrix X with n rows and p columns where X has missing values throughout.", "No fancy machine learning model is saving us here. In fact, fancy supervised learning techniques rely on X to perform as well as they do. In some sense this is the worst case of the adage garbage in; garbage out- we don\u2019t even have garbage to put in!", "This problem is quite typical in most data analyses and has been revisited recently as many modern data science problems can be posed within this framework. We\u2019ll discuss a few of these problems in the Applications section below.", "In this post we look at a model based approach to overcome this missing data problem. We\u2019ll discuss some surprising applications where this methodology is useful beyond matrix completion. We\u2019ll walk through a fully Bayesian perspective on this problem and analyze how a typical SGD framework is closely connected with posterior maximization under this model.", "Three applications of particular interest range from the basics of data analysis to state of the art methods in recommendation systems.", "These are just a sampling of problems that fit into the matrix completion framework. In a later post, I will consider the image completion task and construct a recommendation system using the model introduced below.", "One way to formalize this task is the matrix completion problem where we try to replace the missing data (blue tiles) with knowledge of the known values (white tiles). I think of this as \u201cplugging the holes\u201d in the matrix.", "A popular model based approach is to assume that the data matrix X has low rank\u00b9 and hence can be factorized into the product of two low rank matrices U, V. Hopefully, d = rank(X) is much less than min(n,p) to reduce computational complexity.", "You can think of the vector u\u1d62 as the vector describing the behavior of the i-th observation and v\u2c7c as describing the behavior of the j-th feature. Necessarily, we assume that the entry X\u1d62\u2c7c = \u27e8u\u1d62, v\u2c7c\u27e9. Our problem reduces to attaining estimates of U and V.", "A common solution to this problem is an optimization approach which minimizes a least squares loss. This problem can be solved efficiently (and in parallel) using an Alternating Least Squares algorithm\u00b2. Regularization extensions and solutions have also been characterized at length in the literature.", "Another approach which offers a more robust statistical framework is a Bayesian perspective on the model matrix X. This approach, entitled Probabilistic Matrix Factorization was introduced by Salakhutdinov and Mnih in 2006\u00b3. Under this model, we assume that the entries of X are normally distributed around the inner product \u27e8u\u1d62, v\u2c7c\u27e9 with a common variance. Letting I\u1d62\u2c7c be 1 if the entry was observed and 0 if the data value was missing we can write the likelihood of the entries of X as follows.", "The core assumptions of this likelihood is that (a) the entries of X are independent (b) each entry is normally distributed and (c) the entries share a common variance \u03c3\u00b2. These assumptions may or may not be appropriate for certain applications and will need to be considered more closely in practice.", "To embrace the full Bayesian paradigm, we place prior distributions in the matrices U and V of the following form.", "In these priors we assume (a) the rows of U and V are uncorrelated, (b) are normally distributed, and (c) have common variance. With additional prior information, we can construct more informative prior distributions to capture known correlation between observations or features in the design matrix X.", "Having introduced the prior distributions as well as the likelihood for the matrix X we can derive the full posterior up to a normalization constant.", "In their work [3] suggest deriving the maximum a posteriori estimate (MAP) by finding the matrices U and V that maximize this posterior distribution. In future we look at exploiting the full posterior distribution by building a MCMC sampler for these model parameters\u2074.", "Maximizing this posterior is equivalent to maximizing the log-posterior. The log posterior has the form given below.", "With this, our objective is to minimize the loss function L(U, V) which can be written as follows", "where I is the n \u00d7 p matrix with I\u1d62\u2c7c defined above, the circle product denotes the element wise matrix product, and the norm used is the Frobenius norm.", "From this loss, we see that we balance the goodness of fit of the product of U and V with the \u201csize\u201d of these matrices with respect to the Frobenius norm. We balance the tradeoff between goodness of fit and regularization by tuning the prior variance \u03c3\u00b2, \u03c3\u1d64\u00b2, and \u03c3\u1d65\u00b2. These parameters can be chosen in a data dependent way with cross validation or with a Bayesian approach by placing priors on these hyerparameters.", "From here we see that maximizing the log posterior is equivalent to the regularized optimization approach considered in [2]. Therefore we see that with the correct choice of priors, the Probabilistic Matrix Factorization approach generalizes standard optimization techniques in matrix factorization.", "By choosing more complex prior distributions for the rows of U and V we can devise optimization problems reminiscent of the elastic net and the lasso where more sparse solutions are preferable.", "Having characterized the MAP estimate of U and V in terms of an optimization problem, we now consider optimization approaches to solve this problem. Unfortunately, this optimization problem is not convex so we have no guarantees on global solutions with any of the methods that follow. Recent work\u2075 has worked to characterize properties of the global solution which will, hopefully, enhance our understanding of properties of any local solutions.", "To attain estimates of U and V we consider a (stochastic) gradient descent update scheme. While we could also consider an alternating least squares update which could be solved by using iterative Ridge Solutions, the SGD approach will be able to scale more readily to larger datasets.", "Differentiating the loss function L(U,V) with respect to u\u1d62 or v\u2c7c will provide the gradients of interest. The gradients can be written as", "which have clear connections to the Ridge Regression estimates from regularized regression settings. Notice that each gradient includes a sum of n or p elements. These sums can be cumbersome to calculate in practice and it may be necessary to use stochastic gradients by summing over subsets (mini-batches) of the n observations or of the p features.", "Using these gradients, we iteratively update our estimates until convergence. The full algorithm is formalized below.", "Several improvements to this algorithm can be made to improve runtime. In particular, use of stochastic gradients can speed up computational time significantly. Moreover, each nested for-loop can be done in parallel and distributed across units. Kevin Liao has a nice post on this with PySpark which I\u2019ll link here.", "With that, we\u2019ve basically covered the basics of probabilistic matrix factorization! Most of the ideas discussed here were introduced in [3] and I really encourage you giving their paper a read. Hopefully this post drew some connections between this problem and ideas from Bayesian statistics, regularized regression, and regularized matrix factorization.", "Now that we\u2019ve got all the math worked out, it\u2019s time to implement the SGD algorithm introduced above. I\u2019ll be looking at the problem of image completion as well as a recommendation system in my next post.", "This assumption is quite common in many statistical and machine learning contexts. Check out this 2019 work by Udell and Townsend for an interesting discussion on why this assumption is reasonable.", "[2]: T. Hastie et al. Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares. (2015), Journal of Machine Learning Research.", "[4]: R. Salakhutdino and A. Mnihv. Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo. (2008), International Conference on Machine Learning 2008.", "[5]: S. Zheng et al. Regularized Singular Value Decomposition and Application to Recommender System. (2018), ArXiv.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD Candidate in Statistics at Boston University. Interested in spectral methods in ML and Statistics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb7852244a321&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7852244a321--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@benjamin.draves?source=post_page-----b7852244a321--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamin.draves?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Benjamin Draves, PhD Candidate"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ef4513395dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&user=Benjamin+Draves%2C+PhD+Candidate&userId=5ef4513395dc&source=post_page-5ef4513395dc----b7852244a321---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/93b225681c92?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Kevin Liao"}, {"url": "https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1", "anchor_text": "here"}, {"url": "https://epubs.siam.org/doi/pdf/10.1137/18M1183480", "anchor_text": "Why Are Big Data Matrices Approximately Low Rank?"}, {"url": "http://jmlr.org/papers/volume16/hastie15a/hastie15a.pdf", "anchor_text": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares"}, {"url": "https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf", "anchor_text": "Probabilistic Matrix Factorization."}, {"url": "https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf", "anchor_text": "Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo"}, {"url": "https://arxiv.org/pdf/1804.05090.pdf", "anchor_text": "Regularized Singular Value Decomposition and Application to Recommender System"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b7852244a321---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/matrix-factorization?source=post_page-----b7852244a321---------------matrix_factorization-----------------", "anchor_text": "Matrix Factorization"}, {"url": "https://medium.com/tag/data-imputation?source=post_page-----b7852244a321---------------data_imputation-----------------", "anchor_text": "Data Imputation"}, {"url": "https://medium.com/tag/recommendation-system?source=post_page-----b7852244a321---------------recommendation_system-----------------", "anchor_text": "Recommendation System"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&user=Benjamin+Draves%2C+PhD+Candidate&userId=5ef4513395dc&source=-----b7852244a321---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&user=Benjamin+Draves%2C+PhD+Candidate&userId=5ef4513395dc&source=-----b7852244a321---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7852244a321--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb7852244a321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b7852244a321---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b7852244a321--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b7852244a321--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b7852244a321--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b7852244a321--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b7852244a321--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamin.draves?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamin.draves?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Benjamin Draves, PhD Candidate"}, {"url": "https://medium.com/@benjamin.draves/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "19 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ef4513395dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&user=Benjamin+Draves%2C+PhD+Candidate&userId=5ef4513395dc&source=post_page-5ef4513395dc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F5ef4513395dc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobabilistic-matrix-factorization-b7852244a321&user=Benjamin+Draves%2C+PhD+Candidate&userId=5ef4513395dc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}