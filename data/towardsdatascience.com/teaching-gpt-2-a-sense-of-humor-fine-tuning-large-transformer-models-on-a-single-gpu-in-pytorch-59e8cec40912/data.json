{"url": "https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912", "time": 1683002150.127703, "path": "towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912/", "webpage": {"metadata": {"title": "Teaching GPT-2 transformer a sense of humor | by Martins Frolovs | Towards Data Science", "h1": "Teaching GPT-2 transformer a sense of humor", "description": "In this post, I demonstrate how you can use pre-trained GPT-2 to generate text and then fine-tune it on a specific language modeling task using a single GPU. In this case, I try to teach the model to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models", "paragraph_index": 1}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models Are Unsupervised Multitask Learners", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformer-based", "paragraph_index": 2}, {"url": "http://wiki.fast.ai/index.php/Fine_tuning", "anchor_text": "fine-tuning", "paragraph_index": 2}, {"url": "https://openai.com/blog/better-language-models/#sample1", "anchor_text": "very impressive", "paragraph_index": 3}, {"url": "https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/", "anchor_text": "in some cases, even hundreds of thousands", "paragraph_index": 5}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "pre-trained Transformer", "paragraph_index": 6}, {"url": "https://www.kaggle.com/abhinavmoudgil95/short-jokes", "anchor_text": "Short Jokes dataset", "paragraph_index": 6}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "XL", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization", "paragraph_index": 16}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "feature dimension", "paragraph_index": 16}, {"url": "https://github.com/mf1024/transformers/blob/master/generated_2_jokes.txt", "anchor_text": "the full generated jokes list", "paragraph_index": 23}, {"url": "https://mf1024.github.io/2019/11/12/Fun-With-GPT-2_/", "anchor_text": "original blog", "paragraph_index": 25}], "all_paragraphs": ["In this post, I demonstrate how you can use pre-trained GPT-2 to generate text and then fine-tune it on a specific language modeling task using a single GPU. In this case, I try to teach the model to be funny by fine-tuning it on a jokes dataset.", "Recently OpenAI team published an article Better Language Models, and a technical paper Language Models Are Unsupervised Multitask Learners about training bigger and better language models. They research language model abilities to generate coherent text and solve NLP tasks in a zero-shot setting, which means using the model to solve tasks that it was not explicitly trained for.", "They created a transformer-based language model that they called GPT-2 and trained it on a huge 40GB internet text dataset. They trained the model on a language modeling task, which is predicting probabilities of the next word in a word sequence. Training NLP models for language modeling and then fine-tuning for a specific task is one of the most common paths for training NLP models. Pre-training a model for language modeling is convenient because it does not require labeled data to learn the structure of language \u2014 it only requires plain text, which is openly available in vast amounts. Most publicly available pre-trained NLP models are trained for language modeling.", "The results they got at generating text after the training are very impressive; the fragments feel very human and coherent that it\u2019s almost creepy. Also, the model achieved state-of-the-art scores in zero-shot settings on a variety of language modeling tasks, including summarization, reading comprehension, and translation.", "So I decided to experiment a little with the GPT-2. I thought it would be fun to teach the model to crack some jokes. To do that, I need a jokes dataset and a pre-trained GPT-2 model for fine-tuning.", "Thanks to the generosity of the AI community and some specific teams who publish pre-trained neural network models, relatively cheap solutions for solving challenging tasks like this one are possible. Training such large neural-network models from scratch would costs tens of thousands of dollars, in some cases, even hundreds of thousands. Fine-tuning a pre-trained model on a new task might take a few hours on a single GPU. And I\u2019ll do just that.", "Huggingface has made many pre-trained Transformer models available for easy use in PyTorch. I\u2019ll use their pre-trained GPT-2 and fine-tune it on this Short Jokes dataset published on Kaggle.", "GPT-2 comes in 4 different sizes \u2014 small, medium, large, and XL, with 124M, 355M, 774M, and 1.5B parameters, respectively. I found that a medium-size GPT-2 model is the largest of the models that I could fine-tune with reasonable input sequence length on a single GPU.", "Before fine-tuning the model on jokes, I\u2019ll test it on generating a text.", "In the following gist, I demonstrate how to generate a text by using pre-trained medium-size GPT-2 from huggingface. I\u2019ll feed the model the following text fragments to start with and let it generate the rest:", "\u2018 The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work\u2026 when you go to church\u2026 when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth\u2026 \u2018", "\u2018 Artificial general intelligence is\u2026 \u2018", "\u2018 The Godfather: \u201cI\u2019m going to make him an offer he can\u2019t refuse.\u201d\u2026 \u2018", "Judging by the generated conspiracy theories about technology, threatening predictions about the AI industry, and The Godfather dialogue with himself, I would say that the text generation is working.", "Large Transformer models are usually trained in multi-GPU(or TPU) settings because training on reasonable batch size and sequence length on a large model requires lots of tensor/graphical processing unit memory. My machine is equipped with a single GeForce 1080 Ti, which has 11 GB of memory. By empirical tests on the medium-size GPT-2 model, I found that the maximum total sequence element count in a batch for my GPU to process is approximately 550, which is not a lot and might not be sufficient for successful fine-tuning.", "But there are some things we can take into account to improve the situation.", "The first thing to notice is that the batch size in a forward-backward pass of a transformer-based model does not play a role because Layer Normalization is used instead of Batch Normalization. In Layer Normalization, each feature is normalized across the feature dimension, and the batch dimension is not involved.", "Second, we can accumulate gradients over multiple forward-backward passes, and only then do the model weight update. This way, we don\u2019t have to store the computational graph of a whole batch in the memory, but we can process sequence by sequence and achieve the same result as if the whole batch would have been processed in a single forward-backward pass.", "Taking it all into account, I\u2019ll process one sequence at a time with a maximum length of 550 and do model weight update every BATCH_SIZE processed sequences.", "The length of jokes varies a lot in the dataset \u2014 there are many short sequences. To make the total sequence element count in one optimization step more consistent, I\u2019ll try to fit in as many jokes as possible in each 550 element sequence.", "It is a hard problem to teach AI to generate a text that\u2019ll seem funny to a human, and I think that it is much harder than to generate a coherent text. Even for a human, it is not easy to do \u2014 it takes a special kind of creativity, understanding of the context, and even understanding of human psychology. Feeding many jokes to a language model might not be sufficient for the model actually to learn what makes something funny. It might require more sophisticated techniques and a lot more data to train human-level joking models.", "Nevertheless, it is hilarious to see this language model trying. Once in awhile, the model manages to generate a funny human-level joke.", "*When I started the experiment, I did not notice that a significant portion of the jokes in the dataset are racist and rude, which means you can expect the same in the generated joke list from the model. I apologize for that and be prepared.", "Here is the full generated jokes list.", "If you see something good and funny in the generated jokes list, post it in the comments. :) I didn\u2019t read through all of them myself.", "This is a repost from my original blog."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F59e8cec40912&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@martins.frolovs?source=post_page-----59e8cec40912--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Martins Frolovs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea0d808aff08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=post_page-ea0d808aff08----59e8cec40912---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59e8cec40912&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=-----59e8cec40912---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59e8cec40912&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=-----59e8cec40912---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@benwhitephotography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ben White"}, {"url": "https://unsplash.com/s/photos/laugh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models Are Unsupervised Multitask Learners"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformer-based"}, {"url": "http://wiki.fast.ai/index.php/Fine_tuning", "anchor_text": "fine-tuning"}, {"url": "https://openai.com/blog/better-language-models/#sample1", "anchor_text": "very impressive"}, {"url": "https://unsplash.com/@helloquence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Helloquence"}, {"url": "https://unsplash.com/s/photos/planning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/", "anchor_text": "in some cases, even hundreds of thousands"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "pre-trained Transformer"}, {"url": "https://www.kaggle.com/abhinavmoudgil95/short-jokes", "anchor_text": "Short Jokes dataset"}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "XL"}, {"url": "https://jalammar.github.io/", "anchor_text": "Jay Alammar"}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "The Illustrated GPT-2"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization"}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "feature dimension"}, {"url": "https://unsplash.com/@marchuri?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Marcela Rogante"}, {"url": "https://unsplash.com/s/photos/laugh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/mf1024/transformers/blob/master/generated_2_jokes.txt", "anchor_text": "the full generated jokes list"}, {"url": "https://mf1024.github.io/2019/11/12/Fun-With-GPT-2_/", "anchor_text": "original blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----59e8cec40912---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----59e8cec40912---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----59e8cec40912---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/gpt-2?source=post_page-----59e8cec40912---------------gpt_2-----------------", "anchor_text": "Gpt 2"}, {"url": "https://medium.com/tag/nlp?source=post_page-----59e8cec40912---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59e8cec40912&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=-----59e8cec40912---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59e8cec40912&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=-----59e8cec40912---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59e8cec40912&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=post_page-----59e8cec40912--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea0d808aff08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=post_page-ea0d808aff08----59e8cec40912---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1c1460b44e8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&newsletterV3=ea0d808aff08&newsletterV3Id=1c1460b44e8b&user=Martins+Frolovs&userId=ea0d808aff08&source=-----59e8cec40912---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Written by Martins Frolovs"}, {"url": "https://medium.com/@martins.frolovs/followers?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "64 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mf1024.github.io", "anchor_text": "https://mf1024.github.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea0d808aff08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&user=Martins+Frolovs&userId=ea0d808aff08&source=post_page-ea0d808aff08----59e8cec40912---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1c1460b44e8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912&newsletterV3=ea0d808aff08&newsletterV3Id=1c1460b44e8b&user=Martins+Frolovs&userId=ea0d808aff08&source=-----59e8cec40912---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-scrape-the-imagenet-f309e02de1f4?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Martins Frolovs"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-scrape-the-imagenet-f309e02de1f4?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "How to scrape the ImageNetI wrote a software tool which creates new datasets from ImageNet. Here is a post of why I did it and also little analysis of ImageNet."}, {"url": "https://towardsdatascience.com/how-to-scrape-the-imagenet-f309e02de1f4?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "\u00b76 min read\u00b7Jun 14, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff309e02de1f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-scrape-the-imagenet-f309e02de1f4&user=Martins+Frolovs&userId=ea0d808aff08&source=-----f309e02de1f4----0-----------------clap_footer----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-scrape-the-imagenet-f309e02de1f4?source=author_recirc-----59e8cec40912----0---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff309e02de1f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-scrape-the-imagenet-f309e02de1f4&source=-----59e8cec40912----0-----------------bookmark_preview----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----59e8cec40912----1---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----59e8cec40912----1-----------------bookmark_preview----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----59e8cec40912----2---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----59e8cec40912----2-----------------bookmark_preview----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----3-----------------clap_footer----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----59e8cec40912----3---------------------6120e7f7_d910_4b64_88fb_e62ac8b23add-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----59e8cec40912----3-----------------bookmark_preview----6120e7f7_d910_4b64_88fb_e62ac8b23add-------", "anchor_text": ""}, {"url": "https://medium.com/@martins.frolovs?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "See all from Martins Frolovs"}, {"url": "https://towardsdatascience.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----59e8cec40912----0-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@thomas.rochefort.beaudoin?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@thomas.rochefort.beaudoin?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Thomas Rochefort-Beaudoin"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Training a Language Model To Give (Non) Legal AdviceIn this article, I go through the basics for finetuning large language models like BLOOM on a legal text dataset. You can try it here !"}, {"url": "https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b78 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Fb9f6d7d11016&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Ftraining-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016&user=Thomas+Rochefort-Beaudoin&userId=6a1d4b98f29c&source=-----b9f6d7d11016----1-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9f6d7d11016&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Ftraining-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016&source=-----59e8cec40912----1-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----0-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----59e8cec40912----0---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----59e8cec40912----0-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----1-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----59e8cec40912----1---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----59e8cec40912----1-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/building-your-own-mini-chatgpt-b2e9716ab119?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://thelatestnow.medium.com/?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://thelatestnow.medium.com/?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\ud835\ude83\ud835\ude91\ud835\ude8e \ud835\ude7b\ud835\ude8a\ud835\ude9d\ud835\ude8e\ud835\ude9c\ud835\ude9d \ud835\ude7d\ud835\ude98\ud835\udea0 ~ \ud835\ude70\ud835\ude78"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/building-your-own-mini-chatgpt-b2e9716ab119?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Building Your Own Mini ChatGPTTake a dive into NLP by creating your own small-scale language model with code examples and technical details"}, {"url": "https://medium.com/mlearning-ai/building-your-own-mini-chatgpt-b2e9716ab119?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b77 min read\u00b7Jan 19"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Fb2e9716ab119&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fbuilding-your-own-mini-chatgpt-b2e9716ab119&user=%F0%9D%9A%83%F0%9D%9A%91%F0%9D%9A%8E+%F0%9D%99%BB%F0%9D%9A%8A%F0%9D%9A%9D%F0%9D%9A%8E%F0%9D%9A%9C%F0%9D%9A%9D+%F0%9D%99%BD%F0%9D%9A%98%F0%9D%9A%A0+%7E+%F0%9D%99%B0%F0%9D%99%B8&userId=30a9c8621de5&source=-----b2e9716ab119----2-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/building-your-own-mini-chatgpt-b2e9716ab119?source=read_next_recirc-----59e8cec40912----2---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2e9716ab119&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fbuilding-your-own-mini-chatgpt-b2e9716ab119&source=-----59e8cec40912----2-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "Nitin Kishore"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "How to solve CUDA Out of Memory error**Freeze frame, scratch that record and cue \u2014 \u2018The Who\u2019 intro**"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": "\u00b77 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&user=Nitin+Kishore&userId=ef6a1cf849e2&source=-----850bb247cfb2----3-----------------clap_footer----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----59e8cec40912----3---------------------36162c76_7e95_434e_8d57_ac7a4e9f1faf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&source=-----59e8cec40912----3-----------------bookmark_preview----36162c76_7e95_434e_8d57_ac7a4e9f1faf-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----59e8cec40912--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----59e8cec40912--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}