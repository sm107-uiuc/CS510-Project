{"url": "https://towardsdatascience.com/flexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c", "time": 1683007950.7256541, "path": "towardsdatascience.com/flexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c/", "webpage": {"metadata": {"title": "Flexible, Declarative Dataset Sampling in PyTorch | by Stefan Schroedl | Towards Data Science", "h1": "Flexible, Declarative Dataset Sampling in PyTorch", "description": "A configurable tree-structured PyTorch sampler to take advantage of any useful example metadata"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 0}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST", "paragraph_index": 0}, {"url": "http://image-net.org/", "anchor_text": "Imagenet", "paragraph_index": 0}, {"url": "https://www.atomwise.com/", "anchor_text": "Atomwise", "paragraph_index": 1}, {"url": "https://redis.io/", "anchor_text": "Redis", "paragraph_index": 3}, {"url": "http://www.atomwise.com", "anchor_text": "Atomwise", "paragraph_index": 34}], "all_paragraphs": ["When you are building your awesome deep learning application with PyTorch, the torchvision package provides convenient interfaces to many existing datasets, such as MNIST and Imagenet. Stochastic gradient descent proceeds by continually sampling instances into mini batches. In many cases, you don\u2019t have to lose sleep over it and can stick with the default behavior: Go through the list of images one-by-one, and reshuffle the list after every epoch. If you never had reason to modify that in your modeling experience, you can stop reading here.", "However, when you are working on your own custom dataset that you aggregated and curated, you might end up writing your own subclasses of DataSet, DataLoader, or Sampler. This was the case for us at Atomwise, where we try to predict bioactivity of possible medical drugs based on structural models. Without going into too much detail, for the purpose of the following exhibition let me briefly describe a simplified version of our data schema. Each example consists of a pair of files with spatial coordinates, for a protein target and a ligand molecule binding to it. There are multiple labels corresponding to different endpoints and confidences, according to their assay (the type of chemical or biological experiment that was performed to determine activity). Some negative examples can be synthetically generated, in a number of different ways. We are continually asking ourselves the question, \u201cWhat is the best way to use this data?\u201d And you guessed it, the answer ranges from \u201cit depends\u201d to \u201cit\u2019s complicated\u201d. In the drug discovery domain, it can be tricky to come up with canonical, universal recipes for benchmark construction. Hence, over time, we have been playing with numerous variations, such as:", "I think you get the idea: There is a large hypothesis space of constructing our training and test distributions.", "Similarly as in other datasets such as ImageNet, the raw data is too voluminous to be completely loaded into main memory. Typically it resides on disk, and is accessed using clever caching and compression schemes. We can also provide it through a distributed in-memory server such as Redis. What we can keep in memory, however, is a directory table containing references to the raw data, the target labels, and any other useful metadata to support sampling schemes as outlined above. Here is a hypothetical metadata file:", "A straightforward way to vary sampling is to grab a torch.utils.data.WeightedRandomSampler, and apply a script to calculate a weight column in the metadata table. Or, to include only some instances but not others, we could determine the desired row indices on the fly and apply a torch.utils.data.SubsetRandomSampler. These are perfectly good options, and that is exactly what we started with. But the complexity of our experiment specification grew over time; data preparation bugs crept in, and we wanted to enable experimentation without having to write code. Hence, we started thinking about a more general, flexible sampling mechanism. What we came up with is a subclass of Sampler configured by a YAML specification. And since our entire machine learning framework is governed by a YAML file, sampling now constitutes one section of it. Over several rounds of refinements, we ended up with a TreeSampler whose design I will sketch in the rest of this post.", "To dive right into the matter, imagine positive examples in general are rare, but to balance the classes, we want to oversample them to a 30% fraction. So our initial tree contains only a root and two leaf nodes:", "Observe that a node naturally encompasses both the aspects of a WeightedRandomSampler and a SubsetRandomSampler. We can can associate subsets of rows that satisfy the selection condition with child nodes. Each node has the task of enumerating one child at a time. There are different ways it can do that. Of course the familiar stochastic concepts of sampling \u201cwith replacement\u201d or \u201cwithout replacement\u201d come to mind. Strictly speaking, only the former constitutes sampling in the true sense of the word; the latter can be realized by a simple list iterator. The list is typically reshuffled after all of its element have been enumerated \u2014 but there are cases where it is useful not to. So, in summary, it is sensible to define three possible sampling modes: replacement, sequential, or shuffle.", "In the following, I will formalize a little more the specification of the tree and its nodes, and demonstrate a couple of use cases based on our toy data schema.", "Sampling weights induce probabilities via normalization over all sibling nodes. In the picture of the tree, we could label the branches between parent and child nodes. But to keep things clean, it is equivalent to attribute a weight to each child node. Note that these weights then pertain to the parent\u2019s sampling procedure; this means they are meaningless if the parent applies sequential or shuffle sampling. So let us write above tree as follows:", "The node names appear in brackets, and are only for reference. column refers to the header row in the metadata file; value to a specific content. As mentioned above, mode can be one of replacement, sequential, or shuffle. Weights are given as constants here. To obtain instance weights in the standard sense, we also allow an expression involving any column name, sich as weight: proportional(conf), or weight: proportional(count) for the number of rows.", "Implicitly, the subset selection compares the column with the specified value using the equality-operator __eq__(); a straightforward, but maybe less common, generalization of this selection mechanism is to allow any of the other comparison operators.", "The ability to express multiple, hierarchical sampling conditions is the reason to have a tree in the first place. In our toy data set, we have two types of negative examples, measured and synthetic; here is how to balance them in an 60%:40% ratio, in addition to the same active/inactive class balance as before:", "The astute reader will of course recognize equivalence to a standard WeightedRandomSampler, with weights set to .3, .42, and .28 for active, measured negative, and synthetic negative examples, respectively.", "Case 3: Undersampling the Majority Class", "The previous trees are generating infinite sequences of replacement sampling. It is valid to train in this way, though traditionally, training in epochs is more common \u2014 complete scans through all available data, followed by reshuffling. Say, we wanted to do that for the positive examples, but we have a huge number of synthetic negative examples and don\u2019t care about enumerating each single one of them, in every epoch. Assume further that we also want to balance positives and negatives evenly. Then we can undersample the majority class by setting the modes as follows:", "How would we describe to sample every protein with the same frequency, regardless of how much data our metadata contains for each one? The straightforward solution is to construct a tree containing one node for each protein species, and to assign equal weights:", "However, that list might be very long, and extremely tedious to write manually! So let\u2019s invent the following shortcut:", "At tree construction time, the \u201cmagic\u201d for_each value acts like a wildcard in regular expressions: it clones one sibling node, for each unique value encountered in the file.", "Test time augmentation is a common practice that can give a slight boost to predictive accuracy. We average prediction scores over multiple copies of the same instance, but with different augmentations applied. The node attribute repeat is designed to do exactly that. Say, we want to aggregate over 4 augmentations of every example:", "This will generate sequences of four identical active instances, followed by 4 identical inactives.", "Now let me come to the final and most complex example. Assume we are training a Siamese network: each instance consists of 2 successive protein/molecule pairs, with identical protein but different molecules; one of the pair is active, the other one is not.", "As we have seen above, we can use sequential mode to alternate between actives and inactives. But we need to do more here; namely, constrain the protein between the two. Here again the repeat feature comes in handy:", "For illustration, let us walk through the sequence of steps for the first request:", "2. node_prot_1 is a sequential node, so it picks its first child, node_active.", "3. node_active is a leaf node, so it randomly chooses some row in the metatable with protein=prot1 and active=1. This is the example the TreeSampler returns.", "The third request will then again follow in the footsteps of the first one.", "We are currently working on an arrangement to share our code; nevertheless, it should (hopefully!) not be too difficult to imagine the TreeSampler implementation by now. Initialization reads a metadata file and a configuration in a format similar to the ones shown above. The tree is recursively constructed from the root to the leaves; child nodes are created and associated with example subsets filtered according to the given conditions.", "I would like to point out one quirk that might not be obvious at first glance: It is possible that some combinations of configuration and dataset leads to empty (unsatisfiable) nodes. For instance, training for a target-specific classification metric requires at least one active and one inactive example per protein; however, for some proteins, the metadata might contain only actives or only inactives. This is where our pruning algorithm comes in. With the node attribute prune_method: individual, we can just ignore (delete) this individual empty node. But in this scenario, it makes more sense to drop the protein entirely from consideration, by specifying prune_method: parent. Note that pruning can propagate multiple steps up the tree, until a prune_method: individual is reached (or the root, at which point the entire tree is empty). Notice also that pruning can change the aggregated node weights, so the final sampling probabilities are calculated in a final bottom-up pass.", "In our first implementation version, we used to store example subsets using pandas.DataFrame, but a later optimization led to considerable speedup: The actual examples can be thrown away after construction. All we need to maintain are index arrays, pointing to children in the case of an internal node, and to row numbers in the metadata file in the case of a leaf. These can be directly handled using NumPy functions, without having to resort to pandas.", "Exactly as in other Sampler classes, at runtime, every call to TreeSampler:__next__() returns the index of one metadata row. It does so by following a path from the root to a leaf. Every node maintains a local enumeration state and samples according to its specified properties, independently of each other. Rather than generating a random number for each instance request, it is more efficient to let nodes of all types create a complete buffer of child indices whenever they run out. The difference between the three sampling modes lies only in the replenishment method: Bootstrap sampling for replacement mode, reshuffling for shuffle mode, or doing nothing but rewinding for sequential mode. In our experience, the sampler is very efficient and requires only a fraction of our total time per iteration, even for files with millions of rows.", "To make model training runs reproducible, it is often recommended to initialize random seeds to known values (that means all the random values that can be used \u2014 the random number generators of the python library, NumPy, PyTorch, and cudNN). But what can easily be overlooked is the fact that the PyTorch DataLoader uses multiple worker processes (based on the multiprocessing package) for IO-intensive prefetching of examples together with applying transforms and augmentations. All of these processes depend on their own, separate random number generators. Even initializing them all to the same value does not guarantee determinism, since over time the assignment order of mini batches to processes varies due to race conditions.", "The only way we found to make data set transforms truly deterministic, without introducing too much code complication, was to generate sequences of all needed random seeds and parameters in parallel to the sequence of sampled row indices. E.g., for random (2D) rotation, we can generate one angle together with each sampled row index. Recall that the sampler is a singleton object that lives in the main process \u2014 thus, there is no ambiguity in the random number generator used. We allow the user to configure a variable-size list of named iterators. Their results are packaged together into a data class structure passed to the transforms. Our transform pipeline has its dedicated section in the overall YAML configuration as well, and our transform classes know how to pick the values they need out of the generated data record. In this way, all randomness local to the worker processes is bypassed.", "Of course, this mechanism can control any iterator sequence, not just random values. Assume that for test time augmentation (case 5), we would like to average over 4 different crops, and the CropTransform class takes a string loc argument. Then we can control this centrally from the sampler configuration by specifying a parameter loc: itertools.cycle([\u2018lower_right\u2019, \u2018lower_left\u2019, \u2018upper_right\u2019, and \u2018upper_left\u2019]). These values do not depend on specifics of the chosen example, and will occur successively in a row.", "Custom data sets often come with relevant metadata fields; it can pay off to explore the best way of using them for determining the sampling scheme to train and evaluate machine learning models. There can be a variety of plausible choices for weighting, subsetting, and constraining examples during sampling. To avoid having to repeatedly hardcode these or to change the metadata file for each such experiment, we proposed and implemented a PyTorch TreeSampler class. At initialization, it is dynamically constructed from a configuration and a metadata file. Each node selects an example subset based on a logical condition; it samples its children, independently of each other, in either sequential, shuffle, or replacement mode. In the latter case, children can be assigned frequency weights as constants, from metadata columns, or from row count. To control data augmentations and transformations across multiple workers, it helps to have a way of generating sequences of auxiliary parameters and random numbers, in parallel with the row indices produced by the sampler.", "This work has been a collaboration with my awesome (ex-)colleagues Misko Dzamba and Bastiaan Bergman from Atomwise \u2014 thank you!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Machine Learning @ Atomwise \u2014 Deep Learning for Better Medicines, Faster. Formerly Amazon, Yahoo, DaimlerChrysler."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F613c6d5db10c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://stefan-schroedl.medium.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": ""}, {"url": "https://stefan-schroedl.medium.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Stefan Schroedl"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c3954a88def&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&user=Stefan+Schroedl&userId=4c3954a88def&source=post_page-4c3954a88def----613c6d5db10c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@christina_winter?utm_source=medium&utm_medium=referral", "anchor_text": "Christina Winter"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST"}, {"url": "http://image-net.org/", "anchor_text": "Imagenet"}, {"url": "https://www.atomwise.com/", "anchor_text": "Atomwise"}, {"url": "https://redis.io/", "anchor_text": "Redis"}, {"url": "http://www.atomwise.com", "anchor_text": "Atomwise"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----613c6d5db10c---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/sampling?source=post_page-----613c6d5db10c---------------sampling-----------------", "anchor_text": "Sampling"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----613c6d5db10c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/dataset?source=post_page-----613c6d5db10c---------------dataset-----------------", "anchor_text": "Dataset"}, {"url": "https://medium.com/tag/software-engineering?source=post_page-----613c6d5db10c---------------software_engineering-----------------", "anchor_text": "Software Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&user=Stefan+Schroedl&userId=4c3954a88def&source=-----613c6d5db10c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&user=Stefan+Schroedl&userId=4c3954a88def&source=-----613c6d5db10c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F613c6d5db10c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----613c6d5db10c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----613c6d5db10c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----613c6d5db10c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----613c6d5db10c--------------------------------", "anchor_text": ""}, {"url": "https://stefan-schroedl.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://stefan-schroedl.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Stefan Schroedl"}, {"url": "https://stefan-schroedl.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "245 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c3954a88def&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&user=Stefan+Schroedl&userId=4c3954a88def&source=post_page-4c3954a88def--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4c3954a88def%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fflexible-declarative-dataset-sampling-in-pytorch-613c6d5db10c&user=Stefan+Schroedl&userId=4c3954a88def&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}