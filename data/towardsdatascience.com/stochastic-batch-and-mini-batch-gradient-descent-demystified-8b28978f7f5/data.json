{"url": "https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5", "time": 1683006418.031095, "path": "towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5/", "webpage": {"metadata": {"title": "Stochastic-, Batch-, and Mini-Batch Gradient Descent Demystified | Towards Data Science", "h1": "Stochastic-, Batch-, and Mini-Batch Gradient Descent", "description": "This is a detailed guide that should answer the questions of why and when we need Stochastic-, Batch-, and Mini-Batch Gradient Descent when implementing Deep Neural Networks."}, "outgoing_paragraph_urls": [{"url": "https://www.deeplearning-academy.com/p/ai-wiki-what-is-deep-learning", "anchor_text": "please refer to this article", "paragraph_index": 7}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com", "paragraph_index": 32}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 63}], "all_paragraphs": ["This is a detailed guide that should answer the questions of why and when we need Stochastic-, Batch-, and Mini-Batch Gradient Descent when implementing Deep Neural Networks.", "In Short: We need these different ways of implementing gradient descent to address several issues we will most certainly encounter when training Neural Networks which are local minima and saddle points of the loss function and noisy gradients.", "More on that will be explained in the following article \u2014 nice ;)", "Before we address the different approaches to implement gradient descent, I think it would be a good idea to refresh your memory on what gradient descent actually is.", "When we train a neural network, we want it to learn to perform a specific task. This task can be as simple as predicting the expected demand for a product in a particular market or performing the classification of skin cancer.", "Regardless of this task, our only goal during training is to minimize the objective/ loss function. For predictions of the expected demand, which is a regression task, this loss function would be the Mean Squared Error (MSE) loss function:", "For classification tasks, we want to minimize the Cross-Entropy loss function:", "Before we can minimize a loss function however, the neural network must compute an output. This is accomplished during the forward propagation step when the network receives an input feature vector, performs several dot-products, and non-linear operations in the hidden layers and outputs a prediction. (For a more detailed explanation of the forward propagation step please refer to this article). This step is represented in the following image:", "The equations for the computation of the hidden values as well as the final prediction vector y looks as follows:", "The prediction y and the ground truth label (the value we actually want to predict) are both included in the loss function to compute a quantitative metric that indicates how accurate the network prediction is.", "A lower value of the loss function results in a lower error between the prediction and the label, and vise versa.", "To get the lowest possible value of the loss function, we must adjust the parameters of the neural network, which are the weights and biases.", "And this is where gradient descent comes into play.", "We must compute the negative derivative (gradient) of the loss function with respect to these weights and biases. In the next step, these parameters are updated towards the directions of this gradient. The following equation represents the update step for an arbitrary weight matrix.", "Each time an update is performed, the weights and biases move closer and closer to the optimal set of values for which the loss function will have a global minimum. And this the predictions will be as close as possible to the ground truth label we want to predict.", "Gradient descent is the backbone of neural networks training and the entire field of deep learning. This method enables us to teach neural networks to perform arbitrary tasks without explicitly program them for it.", "As long as the neural network can minimize a loss function the network will eventually be able to do what we want it to do.", "Gradient Descent will encounter Problems during Training", "Of course, as usual, it is easier said than done. While descending along the negative gradient of the loss function to the optimal weights, we will most certainly face multiple problems such as local minima, saddle points, and noisy gradients which can make the training process more problematic for us.", "Different approaches to regular gradient descent, which are Stochastic-, Batch-, and Mini-Batch Gradient Descent can properly handle these problems \u2014 although not every method solves every problem. It is up to you to decide which methods work best for your current problem.", "Because of this, we will discuss in the following different approaches to implementing the gradient descent algorithm in more detail as well as their distinct advantages and disadvantages.", "But first I would like to briefly address the problem of local minima, saddle points, and noisy gradients to give you a better understanding of why we need these different kinds of gradient descent in the first place.", "Unfortunately, the loss functions do not always have a nice shape. In fact, they can take on very complex shapes, such as:", "As a result, the loss functions usually have local minima and saddle points.", "These 2D graphics show schematically what a local minimum and a saddle point would look like in a two-dimensional space:", "The x-axis is an arbitrary weight value, while the y-axis is the value of the loss function. Suppose we obtain weights during the training of a neural network that results in a value of the loss function at which either a saddle point or a local minimum is located.", "In this case, you can clearly see that the slope, or in other words the gradient of the loss function, becomes infinitely small since the loss function is flat at this point. The direct consequence of this is that the gradient gets stuck in a local minimum or saddle point and learning does not progress further because the weights would remain the same.", "In practice, saddle points are a much bigger problem than the local minima, especially when dealing with hundreds of thousands of weight parameters.", "The corresponding dimensional space where the loss function lives has the same number of dimensions as the number of weight parameters. A saddle point means that at my current point in the high dimensional space the loss goes down in one direction, while goes up in another direction. If you are in a hundred thousand dimensional space, as you can imagine this would happen very often.", "On the contrary, a local minimum means that the loss function increases in all directions at my current point.", "As you can imagine, this is unlikely in high-dimensional spaces. Regardless of whether you have a saddle point or a local minimum, the regular implementation of the gradient descent would encounter difficulties during training.", "Coming soon: Advanced Deep Learning Education for software developers, data analysts, academics and industry experts to speed up the transition to a career in Artificial Intelligence.", "For more details check out: www.deeplearning-academy.com", "Another problem and limiting factor when training neural networks is the fact that the gradients that we calculate can become very noisy. When doing gradient descent we (usually) compute the loss for each single feature-label instance pair in the training set and get the gradients by deriving the loss function with respect to the weight parameter.", "The result of this is that each computed gradient on a single data sample is only a rough estimate of the true gradient that points towards the highest rate of increase of the loss function.", "This causes the computed gradients to have slightly different directions and values for each features-label instance pair in the dataset. We say that these gradients are noisy or have a high variance.", "As a result during the training, we don\u2019t go directly towards the global minimum of the loss function, rather the gradient is doing some zig-zag movements. The noisiness of the gradients can result in longer training time of the network.", "In order to better understand this, let\u2019s take a look at a visual example of noisy gradients. The following video shows the process of gradient descent with noisy gradients.", "The gradients are moving towards the global minimum of the loss function which lives in a 3-dimensional space. You can notice that the negative gradients which are computed on each single data instance in the training set do not point directly towards the global minimum of the loss function.", "Rather the gradients differ a little bit in terms of their directions and values. Because of this we are doing these zig-zag movements and do not move directly towards the global minimum.", "In the next section, we will look at different variations of how we can use gradient descent to update the weight of a neural network. Although we are still going to use the gradient descent that we have learned about previously, there are several ways how we can use the calculated gradient to update network weights.", "In the following, I\u2019ll introduce you to three techniques known as Stochastic, , and Mini Batch Gradient Descent. Each weight update technique has its advantages and disadvantages.", "Depending on the problem, you may prefer one method over another. Let\u2019s start with batch gradient descent.", "Please consider a dataset where we have N=6 labeled data instances. Each instance has 4 features (age, job, education, martial) and a label y.", "During the training process, the neural network computes for each instance a prediction that is compared to the ground truth label. Given the prediction and the label, we can put both into the loss function and calculate the gradient of the loss function for that given sample.", "So far, so good. At this point we could use the calculated gradient to update our network weights towards the optimal weights, thereby minimizing the loss function.", "However, during batch gradient descent we don\u2019t do it right away.", "Instead, the weights are updated only once after all data instances of the dataset have been processed. Specifically, during the batch gradient descent, the gradients for each instance in the dataset are calculated and summed.", "In the end, the accumulated gradient is divided by the number of data instances, which is 6. In this way, we get an averaged gradient across all data instances in the dataset. The weights are now updated in the negative direction of this averaged gradient.", "For our dataset, we must calculate and sum the gradients for each of the six samples in that data set. Then we divide the sum of the gradients by 6 and perform single gradient descent with this averaged gradient to update the weights of the neural network.", "In this equation, \u03b8 represents an arbitrary weight value.", "In contrast to batch gradient descent, we can perform the stochastic gradient descent. This method can be considered as the opposite of the batch gradient.", "Please consider the dataset I introduced earlier. For each instance, in the data, we again make a prediction, compare the prediction with the label, and calculate the gradient of the loss function.", "In this case, however, we update the weights after each data instance has been processed by the neural network. This method is also often called as online learning.", "For our small data set, we calculate the gradients for each data instance and update the weights of the neural network six times:", "In other words: This equation is performed six times \u2014 one for each data instance.", "The third and final weight update technique is called mini-batch gradient descent. Imagine that this method combines the best of the other two methods. For the mini-batch gradient descent, we must divide our training set into batches of size n", "Analogous to the batch gradient descent we compute and average the gradients across the data instance in a mini-batch. The gradient descent step is performed after each mini-batch of samples has been processed.", "Please consider once more our small dataset. In the case of mini-batch gradient descent, we may divide these six data instances into batches of size n=2t, leaving us with three mini-batches in total:", "We calculate two gradients for the two data instances in each mini-batch, sum them, and divide them by two to get the gradient average over that mini-batch:", "We would use this average gradient to perform a gradient descent step. In this case, we would do the gradient step a total of three times.", "The mini-batch approach is the default method to implement the gradient descent algorithm in Deep Learning", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8b28978f7f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----8b28978f7f5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.deeplearning-academy.com/p/ai-wiki-what-is-deep-learning", "anchor_text": "please refer to this article"}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com"}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com"}, {"url": "https://www.deeplearning-academy.com/p/ai-wiki-gradient-descent-variations", "anchor_text": "https://www.deeplearning-academy.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8b28978f7f5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8b28978f7f5---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8b28978f7f5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8b28978f7f5---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8b28978f7f5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&user=Artem+Oppermann&userId=619319ac8220&source=-----8b28978f7f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&user=Artem+Oppermann&userId=619319ac8220&source=-----8b28978f7f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8b28978f7f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8b28978f7f5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8b28978f7f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8b28978f7f5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8b28978f7f5--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}