{"url": "https://towardsdatascience.com/counterfactual-policy-gradients-explained-40ac91cef6ae", "time": 1683012670.9419892, "path": "towardsdatascience.com/counterfactual-policy-gradients-explained-40ac91cef6ae/", "webpage": {"metadata": {"title": "Counterfactual Policy Gradients Explained | by Austin Nguyen | Towards Data Science", "h1": "Counterfactual Policy Gradients Explained", "description": "Among many of its challenges, multi-agent reinforcement learning has one obstacle that is overlooked: \u201ccredit assignment.\u201d To explain this concept, let\u2019s first take a look at an example\u2026 Say we have\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/b24112d01863?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Chris Yoon", "paragraph_index": 13}, {"url": "https://medium.com/u/54f2f2975136?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Andre Violante", "paragraph_index": 17}], "all_paragraphs": ["Among many of its challenges, multi-agent reinforcement learning has one obstacle that is overlooked: \u201ccredit assignment.\u201d To explain this concept, let\u2019s first take a look at an example\u2026", "Say we have two robots, robot A and robot B. They are trying to collaboratively push a box into a hole. In addition, they both receive a reward of 1 if they push it in and 0 otherwise. In the ideal case, the two robots would both push the box towards the hole at the same time, maximizing the speed and efficiency of the task.", "However, suppose that robot A does all the heavy lifting, meaning robot A pushes the box into the hole while robot B stands idly on the sidelines. Even though robot B simply loitered around, both robot A and robot B would receive a reward of 1. In other words, the same behavior is encouraged later on even though robot B executed a suboptimal policy. This is when the issue of \u201ccredit assignment\u201d comes in. In multi-agent systems, we need to find a way to give \u201ccredit\u201d or reward to agents who contribute to the overall goal, not to those who let others do the work.", "Okay so what\u2019s the solution? Maybe we only give rewards to agents who contribute to the task itself.", "It seems like this easy solution may just work, but we have to keep several things in mind.", "First, state representation in reinforcement learning might not be expressive enough to properly tailor rewards like this. In other words, we can\u2019t always easily quantify whether an agent contributed to a given task and dole out rewards accordingly.", "Secondly, we don\u2019t want to handcraft these rewards, because it defeats the purpose of designing multi-agent algorithms. There\u2019s a fine line between telling agents how to collaborate and encouraging them to learn how to do so.", "Counterfactual policy gradients address this issue of credit assignment without explicitly giving away the answer to its agents.", "The main idea behind the approach? Let\u2019s train agent policies by comparing its actions to other actions it could\u2019ve taken. In other words, an agent will ask itself:", "\u201c Would we have gotten more reward if I had chosen a different action?\u201d", "By putting this thinking process into mathematics, counterfactual multi-agent (COMA) policy gradients tackle the issue of credit assignment by quantifying how much an agent contributes to completing a task.", "COMA is an actor-critic method that uses centralized learning with decentralized execution. This means we train two networks:", "In addition, the critic is only used during training and is removed during testing. We can think of the critic as the algorithm\u2019s \u201ctraining wheels.\u201d We use the critic to guide the actor throughout training and give it advice on how to update and learn its policies. However, we remove the critic when it\u2019s time to execute the actor\u2019s learned policies.", "For more background on actor-critic methods in general, take a look at Chris Yoon\u2019s in-depth article here:", "Let\u2019s start by taking a look at the critic. In this algorithm, we train a network to estimate the joint Q-value across all agents. We\u2019ll discuss the critic\u2019s nuances and how it\u2019s specifically designed later in this article. However, all we need to know now is that we have two copies of the critic network. One is the network we are trying to train and the other is our target network, used for training stability. The target network\u2019s parameters are copied from the training network periodically.", "To train the networks, we use on-policy training. Instead of using one-step or n-step lookahead to determine our target Q-values, we use TD(lambda), which uses a mixture of n-step returns.", "where gamma is the discount factor, r denotes a reward at a specific time step, f is our target value function, and lambda is a hyper-parameter. This seemingly infinite horizon value is calculated using bootstrapped estimates by a target network.", "For more information on TD(lambda), Andre Violante\u2019s article provides a fantastic explanation:", "Finally, we update the critic\u2019s parameters by minimizing this function:", "Now, you may be wondering: this is nothing new! What makes this algorithm special? The beauty behind this algorithm comes with how we update the actor networks\u2019 parameters.", "In COMA, we train a probabilistic policy, meaning each action in a given state is chosen with a specific probability that is changed throughout training. In typical actor-critic scenarios, we update the policy by using a policy gradient, typically using the value function as a baseline to create advantage actor-critic:", "However, there\u2019s a problem here. This fails to address the original issue we were trying to solve: \u201ccredit assignment.\u201d We have no notion of \u201chow much any one agent contributes to the task.\u201d Instead, all agents are being given the same amount of \u201ccredit,\u201d considering our value function estimates joint value functions. As a result, COMA proposes using a different term as our baseline.", "To calculate this counterfactual baseline for each agent, we calculate an expected value over all actions that agent can take while keeping the actions of all other agents fixed.", "Let\u2019s take a step back here and dissect this equation. The first term is just the Q-value associated with the joint state and joint action (all agents). The second term is an expected value. Looking at each individual term in that summation, there are two values being multiplied together. The first is the probability this agent would\u2019ve chosen a specific action. The second is the Q-value of taking that action while all other agents kept their actions fixed.", "Now, why does this work? Intuitively, by using this baseline, the agent knows how much reward this action contributes relative to all other actions it could\u2019ve taken. In doing so, it can better distinguish which actions will better contribute to the overall reward across all agents.", "COMA proposes using a specific network architecture helps make computing the baseline more efficient [1]. Furthermore, the algorithm can be extended to continuous action spaces by estimating the expected value using Monte Carlo Samples.", "COMA was tested on StarCraft unit micromanagement, pitted against various central and independent actor critic variations, estimating both Q-values and value functions. It was shown that the approach outperformed others significantly. For official reported results and analysis, check out the original paper [1].", "Nobody likes slackers. Neither do robots.", "Properly allowing agents to recognize their personal contribution to a task and optimizing their policies to best use this information is an essential part of making robots collaborate. In the future, better decentralized approaches may be explored, effectively lowering the learning space exponentially. However, this is easier said than done, as with all problems of these sorts. But of course, this is a strong milestone to letting multi-agents function at a far higher, more complex level.", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F40ac91cef6ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----40ac91cef6ae---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alesnesetril?utm_source=medium&utm_medium=referral", "anchor_text": "Ales Nesetril"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@kadh?utm_source=medium&utm_medium=referral", "anchor_text": "Kira auf der Heide"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@bmowinkel?utm_source=medium&utm_medium=referral", "anchor_text": "Brandon Mowinkel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/u/b24112d01863?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "Understanding Actor Critic MethodsPreliminariestowardsdatascience.com"}, {"url": "https://medium.com/u/54f2f2975136?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Andre Violante"}, {"url": "https://medium.com/@violante.andre/simple-reinforcement-learning-temporal-difference-learning-e883ea0d65b0", "anchor_text": "Simple Reinforcement Learning: Temporal Difference LearningSo recently I\u2019ve been doing a lot of reading on reinforcement learning and watching David Silver\u2019s Introduction to\u2026medium.com"}, {"url": "https://unsplash.com/@jf3380?utm_source=medium&utm_medium=referral", "anchor_text": "Jose Morales"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral", "anchor_text": "JESHOOTS.COM"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1705.08926.pdf", "anchor_text": "Counterfactual Multi-Agent Policy Gradients"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82", "anchor_text": "OpenAI\u2019s MADDPG AlgorithmAn actor-critic approach to multi-agent RL problemstowardsdatascience.com"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----40ac91cef6ae---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----40ac91cef6ae---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----40ac91cef6ae---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----40ac91cef6ae---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----40ac91cef6ae---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----40ac91cef6ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----40ac91cef6ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F40ac91cef6ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----40ac91cef6ae---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----40ac91cef6ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "208 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcounterfactual-policy-gradients-explained-40ac91cef6ae&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}