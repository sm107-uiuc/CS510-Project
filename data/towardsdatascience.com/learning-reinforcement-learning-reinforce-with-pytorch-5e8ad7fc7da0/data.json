{"url": "https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0", "time": 1683001783.280488, "path": "towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0/", "webpage": {"metadata": {"title": "Learning Reinforcement Learning: REINFORCE with PyTorch! | by Christian Hubbs | Towards Data Science", "h1": "Learning Reinforcement Learning: REINFORCE with PyTorch!", "description": "The REINFORCE algorithm is one of the first policy gradient algorithms in reinforcement learning and a great jumping off point to get into more advanced approaches. Policy gradients are different\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.datahubbs.com/reinforcement-learning/", "anchor_text": "links to other articles here", "paragraph_index": 1}, {"url": "https://www.datahubbs.com/intro-to-q-learning/", "anchor_text": "simple primer on the topic here", "paragraph_index": 1}, {"url": "https://www.datahubbs.com/reinforce-with-pytorch/", "anchor_text": "some previous posts,", "paragraph_index": 8}, {"url": "https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability", "anchor_text": "log of the probability", "paragraph_index": 11}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "Cart-Pole", "paragraph_index": 12}, {"url": "http://pytorch.org", "anchor_text": "pytorch.org", "paragraph_index": 12}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P", "paragraph_index": 26}], "all_paragraphs": ["The REINFORCE algorithm is one of the first policy gradient algorithms in reinforcement learning and a great jumping off point to get into more advanced approaches. Policy gradients are different than Q-value algorithms because PG\u2019s try to learn a parameterized policy instead of estimating Q-values of state-action pairs. So the policy output is represented as a probability distribution over actions rather than a set of Q-value estimates. If any of this is confusing or unclear, don\u2019t worry, we\u2019ll break it down step-by-step!", "In this post, we\u2019ll look at the REINFORCE algorithm and test it using OpenAI\u2019s CartPole environment with PyTorch. We assume a basic understanding of reinforcement learning, so if you don\u2019t know what states, actions, environments and the like mean, check out some of the links to other articles here or the simple primer on the topic here.", "We can distinguish policy gradient algorithms from Q-value approaches (e.g. Deep Q-Networks) in that policy gradients make action selection without reference to the action values. Some policy gradients learn an estimate of values to help find a better policy, but this value estimate isn\u2019t required to select an action. The output of a DQN is going to be a vector of value estimates while the output of the policy gradient is going to be a probability distribution over actions.", "For example, consider we have two networks, a policy network and a DQN network that have learned the CartPole task with two actions (left and right). If we pass a state s to each, we might get the following from the DQN:", "And this from the policy gradient:", "The DQN gives us estimates of the discounted future rewards of the state and we make our selection based on these values (typically taking the maximum value according to some \u03f5-greedy rule). The policy gradient, on the other hand, gives us probabilities of our actions. The way we make our selection, in this case, is by choosing action 0 28% of the time and action 1 72% of the time. These probabilities will change as the network gains more experience.", "To get these probabilities, we use a simple function called softmax at the output layer. The function is given below:", "This squashes all of our values to be between 0 and 1, and ensures that all of the outputs sum to 1 (\u03a3 \u03c3(x) = 1). Because we\u2019re using the exp(x) function to scale our values, the largest ones tend to dominate and get more of the probability assigned to them.", "If you\u2019ve followed along with some previous posts, this shouldn\u2019t look too daunting. However, we\u2019ll walk through it anyway for clarity.", "The requirements are rather straightforward, we need a differentiable policy, for which we can use a neural network, and a few hyperparameters like our step size (\u03b1), discount rate (\u03b3), batch size (K) and max episodes (N). From there, we initialize our network, and run our episodes. After each episode, we discount our rewards, which is the sum of all of the discounted rewards from that reward onward. We\u2019ll update our policy after each batch (e.g. K episodes) is complete. This tends to help provide stability for training.", "The policy loss (L(\u03b8)) looks a bit complicated at first but isn\u2019t that difficult to understand if you look at it closely. Recall that the output of the policy network is a probability distribution. What we\u2019re doing with the \u03c0(a | s, \u03b8), is just getting the probability estimate of our network at each state. We then multiply that by the sum of the discounted rewards (G) to get the network\u2019s expected value.", "For example, say we\u2019re at a state s the network is split between two actions, so the probability of choosing a=0 is 50% and a=1 is also 50%. The network randomly selects a=0, we get a reward of 1 and the episode ends (let\u2019s assume discount factor is 1). When we go back and update our network, this state-action pair gives us (1)(0.5)=0.5, which translates into the network\u2019s expected value of that action taken at that state. From here, we take the log of the probability and sum over all of the steps in our batch of episodes. Finally, we average this out and take the gradient of this value to make our updates.", "Just for a quick refresher, the goal of Cart-Pole is to keep the pole in the air for as long as possible. Your agent needs to determine whether to push the cart to the left or the right to keep it balanced while not going over the edges on the left and right. If you don\u2019t have OpenAI\u2019s library installed yet, just run pip install gym and you should be set. Also, grab the latest off of pytorch.org if you haven\u2019t already.", "Go ahead and import some packages:", "With our packages imported, we\u2019re going to set up a simple class called policy_estimator that will contain our neural network. It\u2019s going to have two hidden layers with a ReLU activation function and softmax output. We\u2019ll also give it a method called predict that enables us to do a forward pass through the network.", "Note that calling the predict method requires us to convert our state into a FloatTensor for PyTorch to work with it. Actually, the predict method itself is somewhat superfluous in PyTorch as a tensor could be passed directly to our network to get the results, but I include it here just for clarity.", "The other thing we need is our discounting function to discount future rewards based on the discount factor \u03b3 we use.", "One thing I\u2019ve done here that\u2019s a bit non-standard is subtract the mean of the rewards at the end. This helps to stabilize the learning, particularly in cases such as this one where all the rewards are positive because the gradients change more with negative or below-average rewards than they would if the rewards weren\u2019t normalized like this.", "Now for the REINFORCE algorithm itself.", "For the algorithm, we pass our policy_estimator and env objects, set a few hyperparameters and we\u2019re off.", "A few points on the implementation, always be certain to ensure your outputs from PyTorch are converted back to NumPy arrays before you pass the values to env.step() or functions like np.random.choice() to avoid errors. Also, we use torch.gather() to separate the actual actions taken from the action probabilities to ensure we\u2019re calculating the loss function properly as discussed above. Finally, you can change the ending so that the algorithm stops running once the environment is \u201csolved\u201d instead of running for a preset number of steps (CartPole is solved after an average score of 195 or more for 100 consecutive episodes).", "To run this, we just need a few lines of code to put it all together.", "Plotting the results, we can see that it works quite well!", "Even simple policy gradient algorithms can work quite nicely and they have less baggage than DQN\u2019s which often employ additional features like memory replay to learn effectively.", "See what you can do with this algorithm on more challenging environments!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI/ML researcher writing about technology, economics, and business. Connect with me: https://bit.ly/2scbU1P"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5e8ad7fc7da0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b----5e8ad7fc7da0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@vantorin?utm_source=medium&utm_medium=referral", "anchor_text": "Nikita Vantorin"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.datahubbs.com/reinforcement-learning/", "anchor_text": "links to other articles here"}, {"url": "https://www.datahubbs.com/intro-to-q-learning/", "anchor_text": "simple primer on the topic here"}, {"url": "https://www.datahubbs.com/reinforce-with-pytorch/", "anchor_text": "some previous posts,"}, {"url": "https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability", "anchor_text": "log of the probability"}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "Cart-Pole"}, {"url": "http://pytorch.org", "anchor_text": "pytorch.org"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5e8ad7fc7da0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5e8ad7fc7da0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/reinforce?source=post_page-----5e8ad7fc7da0---------------reinforce-----------------", "anchor_text": "Reinforce"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----5e8ad7fc7da0---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/python?source=post_page-----5e8ad7fc7da0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----5e8ad7fc7da0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----5e8ad7fc7da0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5e8ad7fc7da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5e8ad7fc7da0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5e8ad7fc7da0--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://christiandhubbs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "657 Followers"}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd00e5d4c3897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0&newsletterV3=8c5ed989fb1b&newsletterV3Id=d00e5d4c3897&user=Christian+Hubbs&userId=8c5ed989fb1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}