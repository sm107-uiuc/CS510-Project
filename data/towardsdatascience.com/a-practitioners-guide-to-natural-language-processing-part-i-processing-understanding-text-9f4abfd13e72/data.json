{"url": "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72", "time": 1682993454.204182, "path": "towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72/", "webpage": {"metadata": {"title": "A Practitioner's Guide to Natural Language Processing (Part I) \u2014 Processing & Understanding Text | by Dipanjan (DJ) Sarkar | Towards Data Science", "h1": "A Practitioner's Guide to Natural Language Processing (Part I) \u2014 Processing & Understanding Text", "description": "Unstructured data, especially text, images and videos contain a wealth of information. However, due to the inherent complexity in processing and analyzing this data, people often refrain from\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dipanjanS/text-analytics-with-python", "anchor_text": "\u201cText Analytics with Python\u201d", "paragraph_index": 7}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python", "anchor_text": "\u201cPractical Machine Learning with Python\u201d", "paragraph_index": 7}, {"url": "https://inshorts.com/", "anchor_text": "inshorts", "paragraph_index": 10}, {"url": "https://inshorts.com/", "anchor_text": "inshorts", "paragraph_index": 14}, {"url": "https://github.com/dipanjanS/text-analytics-with-python/tree/master/Chapter-3", "anchor_text": "Chapter 3 of \u2018Text Analytics with Python\u2019", "paragraph_index": 19}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py", "anchor_text": "contractions.py", "paragraph_index": 21}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "my repository", "paragraph_index": 21}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py", "anchor_text": "contractions.py", "paragraph_index": 27}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "my repository", "paragraph_index": 27}, {"url": "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf", "anchor_text": "Penn Treebank notation", "paragraph_index": 50}, {"url": "https://www.ghostscript.com/download/gsdnld.html", "anchor_text": "ghostscript", "paragraph_index": 62}, {"url": "http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip", "anchor_text": "here", "paragraph_index": 69}, {"url": "http://nlp.stanford.edu/software/lex-parser.shtml#Download", "anchor_text": "this website", "paragraph_index": 69}, {"url": "https://web.archive.org/web/20130517134339/http://bulba.sdsu.edu/jeanette/thesis/PennTags.html", "anchor_text": "Penn Treebank reference", "paragraph_index": 72}, {"url": "https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf", "anchor_text": "Universal Stanford Dependencies: A Cross-Linguistic Typology by de Marneffe et al, 2014", "paragraph_index": 75}, {"url": "http://universaldependencies.org/u/dep/index.html", "anchor_text": "here", "paragraph_index": 75}, {"url": "https://spacy.io/api/annotation#section-dependency-parsing", "anchor_text": "here", "paragraph_index": 77}, {"url": "http://universaldependencies.org/u/dep/index.html", "anchor_text": "Universal Dependencies Scheme", "paragraph_index": 77}, {"url": "http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf", "anchor_text": "CLEAR Style Dependency Scheme", "paragraph_index": 77}, {"url": "https://emorynlp.github.io/nlp4j/components/dependency-parsing.html", "anchor_text": "NLP4J", "paragraph_index": 77}, {"url": "https://emorynlp.github.io/nlp4j/components/dependency-parsing.html", "anchor_text": "CLEAR dependency scheme", "paragraph_index": 78}, {"url": "https://spacy.io/api/annotation#named-entities", "anchor_text": "the documentation", "paragraph_index": 83}, {"url": "http://nlp.stanford.edu/software/stanford-ner-2014-08-27.zip", "anchor_text": "Stanford NER resources", "paragraph_index": 89}, {"url": "https://nlp.stanford.edu/software/CRF-NER.shtml", "anchor_text": "Stanford\u2019s Named Entity Recognizer", "paragraph_index": 90}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment", "anchor_text": "Chapter 7: Analyzing Movie Reviews Sentiment", "paragraph_index": 95}, {"url": "https://www.springer.com/us/book/9781484232064", "anchor_text": "Practical Machine Learning with Python, Springer\\Apress, 2018", "paragraph_index": 95}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/", "anchor_text": "AFINN lexicon", "paragraph_index": 98}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/", "anchor_text": "official GitHub repository", "paragraph_index": 98}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/.", "anchor_text": ".", "paragraph_index": 98}, {"url": "http://textblob.readthedocs.io/en/dev/", "anchor_text": "TextBlob", "paragraph_index": 105}, {"url": "http://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis", "anchor_text": "sentiment analysis", "paragraph_index": 105}, {"url": "https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml", "anchor_text": "sentiment lexicon", "paragraph_index": 105}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "GitHub", "paragraph_index": 112}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb", "anchor_text": "Jupyter notebook", "paragraph_index": 113}, {"url": "https://www.springboard.com/", "anchor_text": "Springboard", "paragraph_index": 114}, {"url": "https://www.springboard.com/", "anchor_text": "them", "paragraph_index": 114}, {"url": "https://www.springboard.com/workshops/data-science-career-track", "anchor_text": "Springboard\u2019s DSC bootcamp", "paragraph_index": 114}, {"url": "https://www.springer.com/us/book/9781484223871", "anchor_text": "\u201cText Analytics with Python\u201d", "paragraph_index": 115}, {"url": "https://github.com/dipanjanS/text-analytics-with-python", "anchor_text": "GitHub", "paragraph_index": 115}, {"url": "https://www.springer.com/us/book/9781484232064", "anchor_text": "\u201cPractical Machine Learning with Python\u201d", "paragraph_index": 116}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python", "anchor_text": "GitHub", "paragraph_index": 116}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn", "paragraph_index": 117}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba", "paragraph_index": 118}], "all_paragraphs": ["Unstructured data, especially text, images and videos contain a wealth of information. However, due to the inherent complexity in processing and analyzing this data, people often refrain from spending extra time and effort in venturing out from structured datasets to analyze these unstructured sources of data, which can be a potential gold mine.", "Natural Language Processing (NLP) is all about leveraging tools, techniques and algorithms to process and understand natural language-based data, which is usually unstructured like text, speech and so on. In this series of articles, we will be looking at tried and tested strategies, techniques and workflows which can be leveraged by practitioners and data scientists to extract useful insights from text data. We will also cover some useful and interesting use-cases for NLP. This article will be all about processing and understanding text data with tutorials and hands-on examples.", "The nature of this series will be a mix of theoretical concepts but with a focus on hands-on techniques and strategies covering a wide variety of NLP problems. Some of the major areas that we will be covering in this series of articles include the following.", "Feel free to suggest more ideas as this series progresses, and I will be glad to cover something I might have missed out on. A lot of these articles will showcase tips and strategies which have worked well in real-world scenarios.", "This article will be covering the following aspects of NLP in detail with hands-on examples.", "This should give you a good idea of how to get started with analyzing syntax and semantics in text corpora.", "Formally, NLP is a specialized field of computer science and artificial intelligence with roots in computational linguistics. It is primarily concerned with designing and building applications and systems that enable interaction between machines and natural languages that have been evolved for use by humans. Hence, often it is perceived as a niche area to work on. And people usually tend to focus more on machine learning or statistical learning.", "When I started delving into the world of data science, even I was overwhelmed by the challenges in analyzing and modeling on text data. However, after working as a Data Scientist on several challenging problems around NLP over the years, I\u2019ve noticed certain interesting aspects, including techniques, strategies and workflows which can be leveraged to solve a wide variety of problems. I have covered several topics around NLP in my books \u201cText Analytics with Python\u201d (I\u2019m writing a revised version of this soon) and \u201cPractical Machine Learning with Python\u201d.", "However, based on all the excellent feedback I\u2019ve received from all my readers (yes all you amazing people out there!), the main objective and motivation in creating this series of articles is to share my learnings with more people, who can\u2019t always find time to sit and read through a book and can even refer to these articles on the go! Thus, there is no pre-requisite to buy any of these books to learn NLP.", "When building the content and examples for this article, I was thinking if I should focus on a toy dataset to explain things better, or focus on an existing dataset from one of the main sources for data science datasets. Then I thought, why not build an end-to-end tutorial, where we scrape the web to get some text data and showcase examples based on that!", "The source data which we will be working on will be news articles, which we have retrieved from inshorts, a website that gives us short, 60-word news articles on a wide variety of topics, and they even have an app for it!", "In this article, we will be working with text data from news articles on technology, sports and world news. I will be covering some basics on how to scrape and retrieve these news articles from their website in the next section.", "I am assuming you are aware of the CRISP-DM model, which is typically an industry standard for executing any data science project. Typically, any NLP-based problem can be solved by a methodical workflow that has a sequence of steps. The major steps are depicted in the following figure.", "We usually start with a corpus of text documents and follow standard processes of text wrangling and pre-processing, parsing and basic exploratory data analysis. Based on the initial insights, we usually represent the text using relevant feature engineering techniques. Depending on the problem at hand, we either focus on building predictive supervised models or unsupervised models, which usually focus more on pattern mining and grouping. Finally, we evaluate the model and the overall success criteria with relevant stakeholders or customers, and deploy the final model for future usage.", "We will be scraping inshorts, the website, by leveraging python to retrieve news articles. We will be focusing on articles on technology, sports and world affairs. We will retrieve one page\u2019s worth of articles for each category. A typical news category landing page is depicted in the following figure, which also highlights the HTML section for the textual content of each article.", "Thus, we can see the specific HTML tags which contain the textual content of each news article in the landing page mentioned above. We will be using this information to extract news articles by leveraging the BeautifulSoup and requests libraries. Let\u2019s first load up the following dependencies.", "We will now build a function which will leverage requests to access and get the HTML content from the landing pages of each of the three news categories. Then, we will use BeautifulSoup to parse and extract the news headline and article textual content for all the news articles in each category. We find the content by accessing the specific HTML tags and classes, where they are present (a sample of which I depicted in the previous figure).", "It is pretty clear that we extract the news headline, article text and category and build out a data frame, where each row corresponds to a specific news article. We will now invoke this function and build our dataset.", "We, now, have a neatly formatted dataset of news articles and you can quickly check the total number of news articles with the following code.", "There are usually multiple steps involved in cleaning and pre-processing textual data. I have covered text pre-processing in detail in Chapter 3 of \u2018Text Analytics with Python\u2019 (code is open-sourced). However, in this section, I will highlight some of the most important steps which are used heavily in Natural Language Processing (NLP) pipelines and I frequently use them in my NLP projects. We will be leveraging a fair bit of nltk and spacy, both state-of-the-art libraries in NLP. Typically a pip install <library> or a conda install <library> should suffice. However, in case you face issues with loading up spacy\u2019s language models, feel free to follow the steps highlighted below to resolve this issue (I had faced this issue in one of my systems).", "Let\u2019s now load up the necessary dependencies for text pre-processing. We will remove negation words from stop words, since we would want to keep them as they might be useful, especially during sentiment analysis.", "\u2757 IMPORTANT NOTE: A lot of you have messaged me about not being able to load the contractions module. It\u2019s not a standard python module. We leverage a standard set of contractions available in the contractions.py file in my repository.Please add it in the same directory you run your code from, else it will not work.", "Often, unstructured text contains a lot of noise, especially if you use techniques like web or screen scraping. HTML tags are typically one of these components which don\u2019t add much value towards understanding and analyzing text.", "It is quite evident from the above output that we can remove unnecessary HTML tags and retain the useful textual information from any document.", "Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example \u2014 converting \u00e9 to e.", "The preceding function shows us how we can easily convert accented characters to normal English characters, which helps standardize the words in our corpus.", "Contractions are shortened version of words or syllables. They often exist in either written or spoken forms in the English language. These shortened versions or contractions of words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. Examples would be, do not to don\u2019t and I would to I\u2019d. Converting each contraction to its expanded, original form helps with text standardization.", "We leverage a standard set of contractions available in the contractions.py file in my repository.", "We can see how our function helps expand the contractions from the preceding output. Are there better ways of doing this? Definitely! If we have enough examples, we can even train a deep learning model for better performance.", "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them.", "I\u2019ve kept removing digits as optional, because often we might need to keep them in the pre-processed text.", "To understand stemming, you need to gain some perspective on what word stems represent. Word stems are also known as the base form of a word, and we can create new words by attaching affixes to them in a process known as inflection. Consider the word JUMP. You can add affixes to it and form new words like JUMPS, JUMPED, and JUMPING. In this case, the base word JUMP is the word stem.", "The figure shows how the word stem is present in all its inflections, since it forms the base on which each inflection is built upon using affixes. The reverse process of obtaining the base form of a word from its inflected form is known as stemming. Stemming helps us in standardizing words to their base or root stem, irrespective of their inflections, which helps many applications like classifying or clustering text, and even in information retrieval. Let\u2019s see the popular Porter stemmer in action now!", "The Porter stemmer is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules.", "Do note that usually stemming has a fixed set of rules, hence, the root stems may not be lexicographically correct. Which means, the stemmed words may not be semantically correct, and might have a chance of not being present in the dictionary (as evident from the preceding output).", "Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word, but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary. Both nltk and spacy have excellent lemmatizers. We will be using spacy here.", "You can see that the semantics of the words are not affected by this, yet our text is still standardized.", "Do note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.", "Words which have little or no significance, especially when constructing meaningful features from text, are known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are a, an, the, and the like.", "There is no universal stopword list, but we use a standard English language stopwords list from nltk. You can also add your own domain-specific stopwords as needed.", "While we can definitely keep going with more techniques like correcting spelling, grammar and so on, let\u2019s now bring everything we learnt together and chain these operations to build a text normalizer to pre-process text data.", "Let\u2019s now put this function in action! We will first combine the news headline and the news article text together to form a document for each piece of news. Then, we will pre-process them.", "Thus, you can see how our text pre-processor helps in pre-processing our news articles! After this, you can save this dataset to disk if needed, so that you can always load it up later for future analysis.", "For any language, syntax and structure usually go hand in hand, where a set of specific rules, conventions, and principles govern the way words are combined into phrases; phrases get combines into clauses; and clauses get combined into sentences. We will be talking specifically about the English language syntax and structure in this section. In English, words usually combine together to form other constituent units. These constituents include words, phrases, clauses, and sentences. Considering a sentence, \u201cThe brown fox is quick and he is jumping over the lazy dog\u201d, it is made of a bunch of words and just looking at the words by themselves don\u2019t tell us much.", "Knowledge about the structure and syntax of language is helpful in many areas like text processing, annotation, and parsing for further operations such as text classification or summarization. Typical parsing techniques for understanding text syntax are mentioned below.", "We will be looking at all of these techniques in subsequent sections. Considering our previous example sentence \u201cThe brown fox is quick and he is jumping over the lazy dog\u201d, if we were to annotate it using basic POS tags, it would look like the following figure.", "Thus, a sentence typically follows a hierarchical structure consisting the following components,", "sentence \u2192 clauses \u2192 phrases \u2192 words", "Parts of speech (POS) are specific lexical categories to which words are assigned, based on their syntactic context and role. Usually, words can fall into one of the following major categories.", "Besides these four major categories of parts of speech , there are other categories that occur frequently in the English language. These include pronouns, prepositions, interjections, conjunctions, determiners, and many others. Furthermore, each POS tag like the noun (N) can be further subdivided into categories like singular nouns (NN), singular proper nouns (NNP), and plural nouns (NNS).", "The process of classifying and labeling POS tags for words called parts of speech tagging or POS tagging . POS tags are used to annotate words and depict their POS, which is really helpful to perform specific analysis, such as narrowing down upon nouns and seeing which ones are the most prominent, word sense disambiguation, and grammar analysis. We will be leveraging both nltk and spacy which usually use the Penn Treebank notation for POS tagging.", "We can see that each of these libraries treat tokens in their own way and assign specific tags for them. Based on what we see, spacy seems to be doing slightly better than nltk.", "Based on the hierarchy we depicted earlier, groups of words make up phrases. There are five major categories of phrases:", "Shallow parsing, also known as light parsing or chunking , is a popular natural language processing technique of analyzing the structure of a sentence to break it down into its smallest constituents (which are tokens such as words) and group them together into higher-level phrases. This includes POS tags as well as phrases from a sentence.", "We will leverage the conll2000 corpus for training our shallow parser model. This corpus is available in nltk with chunk annotations and we will be using around 10K records for training our model. A sample annotated sentence is depicted as follows.", "From the preceding output, you can see that our data points are sentences that are already annotated with phrases and POS tags metadata that will be useful in training our shallow parser model. We will leverage two chunking utility functions, tree2conlltags , to get triples of word, tag, and chunk tags for each token, and conlltags2tree to generate a parse tree from these token triples. We will be using these functions to train our parser. A sample is depicted below.", "The chunk tags use the IOB format. This notation represents Inside, Outside, and Beginning. The B- prefix before a tag indicates it is the beginning of a chunk, and I- prefix indicates that it is inside a chunk. The O tag indicates that the token does not belong to any chunk. The B- tag is always used when there are subsequent tags of the same type following it without the presence of O tags between them.", "We will now define a function conll_tag_ chunks() to extract POS and chunk tags from sentences with chunked annotations and a function called combined_taggers() to train multiple taggers with backoff taggers (e.g. unigram and bigram taggers)", "We will now define a class NGramTagChunker that will take in tagged sentences as training input, get their (word, POS tag, Chunk tag) WTC triples, and train a BigramTagger with a UnigramTagger as the backoff tagger. We will also define a parse() function to perform shallow parsing on new sentences", "The UnigramTagger , BigramTagger , and TrigramTagger are classes that inherit from the base class NGramTagger , which itself inherits from the ContextTagger class , which inherits from the SequentialBackoffTagger class .", "We will use this class to train on the conll2000 chunked train_data and evaluate the model performance on the test_data", "Our chunking model gets an accuracy of around 90% which is quite good! Let\u2019s now leverage this model to shallow parse and chunk our sample news article headline which we used earlier, \u201cUS unveils world\u2019s most powerful supercomputer, beats China\u201d.", "Thus you can see it has identified two noun phrases (NP) and one verb phrase (VP) in the news article. Each word\u2019s POS tags are also visible. We can also visualize this in the form of a tree as follows. You might need to install ghostscript in case nltk throws an error.", "The preceding output gives a good sense of structure after shallow parsing the news headline.", "Constituent-based grammars are used to analyze and determine the constituents of a sentence. These grammars can be used to model or represent the internal structure of sentences in terms of a hierarchically ordered structure of their constituents. Each and every word usually belongs to a specific lexical category in the case and forms the head word of different phrases. These phrases are formed based on rules called phrase structure rules.", "Phrase structure rules form the core of constituency grammars, because they talk about syntax and rules that govern the hierarchy and ordering of the various constituents in the sentences. These rules cater to two things primarily.", "The generic representation of a phrase structure rule is S \u2192 AB , which depicts that the structure S consists of constituents A and B , and the ordering is A followed by B . While there are several rules (refer to Chapter 1, Page 19: Text Analytics with Python, if you want to dive deeper), the most important rule describes how to divide a sentence or a clause. The phrase structure rule denotes a binary division for a sentence or a clause as S \u2192 NP VP where S is the sentence or clause, and it is divided into the subject, denoted by the noun phrase (NP) and the predicate, denoted by the verb phrase (VP).", "A constituency parser can be built based on such grammars/rules, which are usually collectively available as context-free grammar (CFG) or phrase-structured grammar. The parser will process input sentences according to these rules, and help in building a parse tree.", "We will be using nltk and the StanfordParser here to generate parse trees.", "Prerequisites: Download the official Stanford Parser from here, which seems to work quite well. You can try out a later version by going to this website and checking the Release History section. After downloading, unzip it to a known location in your filesystem. Once done, you are now ready to use the parser from nltk , which we will be exploring soon.", "The Stanford parser generally uses a PCFG (probabilistic context-free grammar) parser. A PCFG is a context-free grammar that associates a probability with each of its production rules. The probability of a parse tree generated from a PCFG is simply the production of the individual probabilities of the productions used to generate it.", "We can see the constituency parse tree for our news headline. Let\u2019s visualize it to understand the structure better.", "We can see the nested hierarchical structure of the constituents in the preceding output as compared to the flat structure in shallow parsing. In case you are wondering what SINV means, it represents an Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal. Refer to the Penn Treebank reference as needed to lookup other tags.", "In dependency parsing, we try to use dependency-based grammars to analyze and infer both structure and semantic dependencies and relationships between tokens in a sentence. The basic principle behind a dependency grammar is that in any sentence in the language, all words except one, have some relationship or dependency on other words in the sentence. The word that has no dependency is called the root of the sentence. The verb is taken as the root of the sentence in most cases. All the other words are directly or indirectly linked to the root verb using links , which are the dependencies.", "Considering our sentence \u201cThe brown fox is quick and he is jumping over the lazy dog\u201d , if we wanted to draw the dependency syntax tree for this, we would have the structure", "These dependency relationships each have their own meaning and are a part of a list of universal dependency types. This is discussed in an original paper, Universal Stanford Dependencies: A Cross-Linguistic Typology by de Marneffe et al, 2014). You can check out the exhaustive list of dependency types and their meanings here.", "If we observe some of these dependencies, it is not too hard to understand them.", "Spacy had two types of English dependency parsers based on what language models you use, you can find more details here. Based on language models, you can use the Universal Dependencies Scheme or the CLEAR Style Dependency Scheme also available in NLP4J now. We will now leverage spacy and print out the dependencies for each token in our news headline.", "It is evident that the verb beats is the ROOT since it doesn\u2019t have any other dependencies as compared to the other tokens. For knowing more about each annotation you can always refer to the CLEAR dependency scheme. We can also visualize the above dependencies in a better way.", "You can also leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree. We showcase the dependency tree both in its raw and annotated form as follows.", "You can notice the similarities with the tree we had obtained earlier. The annotations help with understanding the type of dependency among the different tokens.", "In any text document, there are particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities , which more specifically refer to terms that represent real-world objects like people, places, organizations, and so on, which are often denoted by proper names. A naive approach could be to find these by looking at the noun phrases in text documents. Named entity recognition (NER) , also known as entity chunking/extraction , is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.", "SpaCy has some excellent capabilities for named entity recognition. Let\u2019s try and use it on one of our sample news articles.", "We can clearly see that the major named entities have been identified by spacy. To understand more in detail about what each named entity means, you can refer to the documentation or check out the following table for convenience.", "Let\u2019s now find out the most frequent named entities in our news corpus! For this, we will build out a data frame of all the named entities and their types using the following code.", "We can now transform and aggregate this data frame to find the top occuring entities and types.", "Do you notice anything interesting? (Hint: Maybe the supposed summit between Trump and Kim Jong!). We also see that it has correctly identified \u2018Messenger\u2019 as a product (from Facebook).", "We can also group by the entity types to get a sense of what types of entites occur most in our news corpus.", "We can see that people, places and organizations are the most mentioned entities though interestingly we also have many other entities.", "Another nice NER tagger is the StanfordNERTagger available from the nltk interface. For this, you need to have Java installed and then download the Stanford NER resources. Unzip them to a location of your choice (I used E:/stanford in my system).", "Stanford\u2019s Named Entity Recognizer is based on an implementation of linear chain Conditional Random Field (CRF) sequence models. Unfortunately this model is only trained on instances of PERSON, ORGANIZATION and LOCATION types. Following code can be used as a standard workflow which helps us extract the named entities using this tagger and show the top named entities and their types (extraction differs slightly from spacy).", "We notice quite similar results though restricted to only three types of named entities. Interestingly, we see a number of mentioned of several people in various sports.", "Sentiment analysis is perhaps one of the most popular applications of NLP, with a vast number of tutorials, courses, and applications that focus on analyzing sentiments of diverse datasets ranging from corporate surveys to movie reviews. The key aspect of sentiment analysis is to analyze a body of text for understanding the opinion expressed by it. Typically, we quantify this sentiment with a positive or negative value, called polarity. The overall sentiment is often inferred as positive, neutral or negative from the sign of the polarity score.", "Usually, sentiment analysis works best on text that has a subjective context than on text with only an objective context. Objective text usually depicts some normal statements or facts without expressing any emotion, feelings, or mood. Subjective text contains text that is usually expressed by a human having typical moods, emotions, and feelings. Sentiment analysis is widely used, especially as a part of social media analysis for any domain, be it a business, a recent movie, or a product launch, to understand its reception by the people and what they think of it based on their opinions or, you guessed it, sentiment!", "Typically, sentiment analysis for text data can be computed on several levels, including on an individual sentence level, paragraph level, or the entire document as a whole. Often, sentiment is computed on the document as a whole or some aggregations are done after computing the sentiment for individual sentences. There are two major approaches to sentiment analysis.", "For the first approach we typically need pre-labeled data. Hence, we will be focusing on the second approach. For a comprehensive coverage of sentiment analysis, refer to Chapter 7: Analyzing Movie Reviews Sentiment, Practical Machine Learning with Python, Springer\\Apress, 2018. In this scenario, we do not have the convenience of a well-labeled training dataset. Hence, we will need to use unsupervised techniques for predicting the sentiment by using knowledgebases, ontologies, databases, and lexicons that have detailed information, specially curated and prepared just for sentiment analysis. A lexicon is a dictionary, vocabulary, or a book of words. In our case, lexicons are special dictionaries or vocabularies that have been created for analyzing sentiments. Most of these lexicons have a list of positive and negative polar words with some score associated with them, and using various techniques like the position of words, surrounding words, context, parts of speech, phrases, and so on, scores are assigned to the text documents for which we want to compute the sentiment. After aggregating these scores, we get the final sentiment.", "Various popular lexicons are used for sentiment analysis, including the following.", "This is not an exhaustive list of lexicons that can be leveraged for sentiment analysis, and there are several other lexicons which can be easily obtained from the Internet. Feel free to check out each of these links and explore them. We will be covering two techniques in this section.", "The AFINN lexicon is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis. Developed and curated by Finn \u00c5rup Nielsen, you can find more details on this lexicon in the paper, \u201cA new ANEW: evaluation of a word list for sentiment analysis in microblogs\u201d, proceedings of the ESWC 2011 Workshop. The current version of the lexicon is AFINN-en-165. txt and it contains over 3,300+ words with a polarity score associated with each word. You can find this lexicon at the author\u2019s official GitHub repository along with previous versions of it, including AFINN-111. The author has also created a nice wrapper library on top of this in Python called afinn, which we will be using for our analysis.", "The following code computes sentiment for all our news articles and shows summary statistics of general sentiment per news category.", "We can get a good idea of general sentiment statistics across different news categories. Looks like the average sentiment is very positive in sports and reasonably negative in technology! Let\u2019s look at some visualizations now.", "We can see that the spread of sentiment polarity is much higher in sports and world as compared to technology where a lot of the articles seem to be having a negative polarity. We can also visualize the frequency of sentiment labels.", "No surprises here that technology has the most number of negative articles and world the most number of positive articles. Sports might have more neutral articles due to the presence of articles which are more objective in nature (talking about sporting events without the presence of any emotion or feelings). Let\u2019s dive deeper into the most positive and negative sentiment news articles for technology news.", "Looks like the most negative article is all about a recent smartphone scam in India and the most positive article is about a contest to get married in a self-driving shuttle. Interesting! Let\u2019s do a similar analysis for world news.", "Interestingly Trump features in both the most positive and the most negative world news articles. Do read the articles to get some more perspective into why the model selected one of them as the most negative and the other one as the most positive (no surprises here!).", "TextBlob is another excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. Typically, the scores have a normalized scale as compare to Afinn. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. Let\u2019s use this now to get the sentiment polarity and labels for each news article and aggregate the summary statistics per news category.", "Looks like the average sentiment is the most positive in world and least positive in technology! However, these metrics might be indicating that the model is predicting more articles as positive. Let\u2019s look at the sentiment frequency distribution per news category.", "There definitely seems to be more positive articles across the news categories here as compared to our previous model. However, still looks like technology has the most negative articles and world, the most positive articles similar to our previous analysis. Let\u2019s now do a comparative analysis and see if we still get similar articles in the most positive and negative categories for world news.", "Well, looks like the most negative world news article here is even more depressing than what we saw the last time! The most positive article is still the same as what we had obtained in our last model.", "Finally, we can even evaluate and compare between these two models as to how many predictions are matching and how many are not (by leveraging a confusion matrix which is often used in classification). We leverage our nifty model_evaluation_utils module for this.", "In the preceding table, the \u2018Actual\u2019 labels are predictions from the Afinn sentiment analyzer and the \u2018Predicted\u2019 labels are predictions from TextBlob. Looks like our previous assumption was correct. TextBlob definitely predicts several neutral and negative articles as positive. Overall most of the sentiment predictions seem to match, which is good!", "This was definitely one of my longer articles! If you are reading this, I really commend your efforts for staying with me till the end of this article. These examples should give you a good idea about how to start working with a corpus of text documents and popular strategies for text retrieval, pre-processing, parsing, understanding structure, entities and sentiment. We will be covering feature engineering and representation techniques with hands-on examples in the next article of this series. Stay tuned!", "All the code and datasets used in this article can be accessed from my GitHub", "The code is also available as a Jupyter notebook", "I often mentor and help students at Springboard to learn essential skills around Data Science. Thanks to them for helping me develop this content. Do check out Springboard\u2019s DSC bootcamp if you are interested in a career-focused structured path towards learning Data Science.", "A lot of this code comes from the research and work that I had done during writing my book \u201cText Analytics with Python\u201d. The code is open-sourced on GitHub. (Python 3.x edition coming by end of this year!)", "\u201cPractical Machine Learning with Python\u201d, my other book also covers text classification and sentiment analysis in detail. The code is open-sourced on GitHub for your convenience.", "If you have any feedback, comments or interesting insights to share about my article or data science in general, feel free to reach out to me on my LinkedIn social media channel.", "Thanks to Durba for editing this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9f4abfd13e72&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://djsarkar.medium.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682----9f4abfd13e72---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/dipanjanS/text-analytics-with-python", "anchor_text": "\u201cText Analytics with Python\u201d"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python", "anchor_text": "\u201cPractical Machine Learning with Python\u201d"}, {"url": "https://inshorts.com/", "anchor_text": "inshorts"}, {"url": "https://inshorts.com/", "anchor_text": "Inshorts, news in 60 words !Edit descriptioninshorts.com"}, {"url": "https://inshorts.com/", "anchor_text": "inshorts"}, {"url": "https://github.com/dipanjanS/text-analytics-with-python/tree/master/Chapter-3", "anchor_text": "Chapter 3 of \u2018Text Analytics with Python\u2019"}, {"url": "https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.0.0", "anchor_text": "https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.0.0"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py", "anchor_text": "contractions.py"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "my repository"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py", "anchor_text": "contractions.py"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "my repository"}, {"url": "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf", "anchor_text": "Penn Treebank notation"}, {"url": "https://www.ghostscript.com/download/gsdnld.html", "anchor_text": "ghostscript"}, {"url": "http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip", "anchor_text": "here"}, {"url": "http://nlp.stanford.edu/software/lex-parser.shtml#Download", "anchor_text": "this website"}, {"url": "https://web.archive.org/web/20130517134339/http://bulba.sdsu.edu/jeanette/thesis/PennTags.html", "anchor_text": "Penn Treebank reference"}, {"url": "https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf", "anchor_text": "Universal Stanford Dependencies: A Cross-Linguistic Typology by de Marneffe et al, 2014"}, {"url": "http://universaldependencies.org/u/dep/index.html", "anchor_text": "here"}, {"url": "https://spacy.io/api/annotation#section-dependency-parsing", "anchor_text": "here"}, {"url": "http://universaldependencies.org/u/dep/index.html", "anchor_text": "Universal Dependencies Scheme"}, {"url": "http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf", "anchor_text": "CLEAR Style Dependency Scheme"}, {"url": "https://emorynlp.github.io/nlp4j/components/dependency-parsing.html", "anchor_text": "NLP4J"}, {"url": "https://emorynlp.github.io/nlp4j/components/dependency-parsing.html", "anchor_text": "CLEAR dependency scheme"}, {"url": "https://spacy.io/api/annotation#named-entities", "anchor_text": "the documentation"}, {"url": "http://nlp.stanford.edu/software/stanford-ner-2014-08-27.zip", "anchor_text": "Stanford NER resources"}, {"url": "https://nlp.stanford.edu/software/CRF-NER.shtml", "anchor_text": "Stanford\u2019s Named Entity Recognizer"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment", "anchor_text": "Chapter 7: Analyzing Movie Reviews Sentiment"}, {"url": "https://www.springer.com/us/book/9781484232064", "anchor_text": "Practical Machine Learning with Python, Springer\\Apress, 2018"}, {"url": "https://github.com/fnielsen/afinn", "anchor_text": "AFINN lexicon"}, {"url": "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html", "anchor_text": "Bing Liu\u2019s lexicon"}, {"url": "http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/", "anchor_text": "MPQA subjectivity lexicon"}, {"url": "http://sentiwordnet.isti.cnr.it/", "anchor_text": "SentiWordNet"}, {"url": "https://github.com/cjhutto/vaderSentiment", "anchor_text": "VADER lexicon"}, {"url": "https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/en/en-sentiment.xml", "anchor_text": "TextBlob lexicon"}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/", "anchor_text": "AFINN lexicon"}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/", "anchor_text": "official GitHub repository"}, {"url": "https://github.com/fnielsen/afinn/blob/master/afinn/data/.", "anchor_text": "."}, {"url": "http://textblob.readthedocs.io/en/dev/", "anchor_text": "TextBlob"}, {"url": "http://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis", "anchor_text": "sentiment analysis"}, {"url": "https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml", "anchor_text": "sentiment lexicon"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach", "anchor_text": "GitHub"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb", "anchor_text": "Jupyter notebook"}, {"url": "https://www.springboard.com/", "anchor_text": "Springboard"}, {"url": "https://www.springboard.com/", "anchor_text": "them"}, {"url": "https://www.springboard.com/workshops/data-science-career-track", "anchor_text": "Springboard\u2019s DSC bootcamp"}, {"url": "https://www.springboard.com/workshops/data-science-career-track", "anchor_text": "Data Science Career Track | SpringboardData Science Career Track is your springboard to a data science career. Online, mentor-guided bootcamp, designed to get\u2026www.springboard.com"}, {"url": "https://www.springer.com/us/book/9781484223871", "anchor_text": "\u201cText Analytics with Python\u201d"}, {"url": "https://github.com/dipanjanS/text-analytics-with-python", "anchor_text": "GitHub"}, {"url": "https://www.springer.com/us/book/9781484223871", "anchor_text": "Text Analytics with Python - A Practical Real-World Approach to Gaining Actionable Insights from\u2026Derive useful insights from your data using Python. You will learn both basic and advanced concepts, including text and\u2026www.springer.com"}, {"url": "https://www.springer.com/us/book/9781484232064", "anchor_text": "\u201cPractical Machine Learning with Python\u201d"}, {"url": "https://github.com/dipanjanS/practical-machine-learning-with-python", "anchor_text": "GitHub"}, {"url": "https://www.springer.com/us/book/9781484232064", "anchor_text": "Practical Machine Learning with Python - A Problem-Solver's Guide to Building Real-World\u2026Master the essential skills needed to recognize and solve complex problems with machine learning and deep learning in\u2026www.springer.com"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "Dipanjan Sarkar - Data Scientist - Intel Corporation | LinkedInView Dipanjan Sarkar's profile on LinkedIn, the world's largest professional community. Dipanjan has 6 jobs listed on\u2026www.linkedin.com"}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9f4abfd13e72---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9f4abfd13e72---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----9f4abfd13e72---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9f4abfd13e72---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9f4abfd13e72---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----9f4abfd13e72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----9f4abfd13e72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9f4abfd13e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9f4abfd13e72---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9f4abfd13e72--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://djsarkar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa34c887aa0f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72&newsletterV3=6278d12b0682&newsletterV3Id=a34c887aa0f4&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}