{"url": "https://towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c", "time": 1682993633.3748338, "path": "towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c/", "webpage": {"metadata": {"title": "GAN Objective Functions: GANs and Their Variations | by Hunter Heidenreich | Towards Data Science", "h1": "GAN Objective Functions: GANs and Their Variations", "description": "If you haven\u2019t already, you should definitely read my previous post about what a GAN is (especially if you don\u2019t know what I mean when I say GAN!). That post should give you a starting point to dive\u2026"}, "outgoing_paragraph_urls": [{"url": "http://hunterheidenreich.com/blog/what-is-a-gan/", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://www.courses.psu.edu/for/for466w_mem14/Ch11/HTML/Sec1/ch11sec1_ObjFn.htm", "anchor_text": "objective function", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence", "anchor_text": "Jensen Shannon Divergence", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullbach-Liebler Divergence", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1701.07875.pdf", "anchor_text": "Wasserstein GAN", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Earth_mover's_distance", "anchor_text": "Earth-Mover (EM) distance", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Wasserstein_metric", "anchor_text": "Wasserstein distance", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1704.00028.pdf", "anchor_text": "Improved Training of Wasserstein GANs", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1611.04076.pdf", "anchor_text": "LSGAN", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "L2 loss", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1705.07164.pdf", "anchor_text": "RWGAN", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1702.08398.pdf", "anchor_text": "Mean and Covariance Feature Matching GAN", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1502.02761.pdf", "anchor_text": "Generative Moment Matching Networks", "paragraph_index": 26}, {"url": "http://alex.smola.org/teaching/iconip2006/iconip_3.pdf", "anchor_text": "maximum mean discrepancy", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Kernel_method", "anchor_text": "kernel trick", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1705.08584.pdf", "anchor_text": "Maximum Mean Discrepancy GAN", "paragraph_index": 29}, {"url": "https://arxiv.org/pdf/1705.10743.pdf", "anchor_text": "Cramer GAN", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1705.09675.pdf", "anchor_text": "Fisher GAN", "paragraph_index": 33}, {"url": "http://www.statisticshowto.com/mahalanobis-distance/", "anchor_text": "Mahalanobis distance", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1609.03126.pdf", "anchor_text": "Energy Based GAN", "paragraph_index": 36}, {"url": "https://arxiv.org/pdf/1703.10717.pdf", "anchor_text": "Boundary Equilibrium GAN", "paragraph_index": 39}, {"url": "https://arxiv.org/pdf/1704.03817.pdf", "anchor_text": "Margin Adaptation GAN", "paragraph_index": 41}, {"url": "https://www.gofundme.com/hunter-heidenreich-research-fund", "anchor_text": "a dollar or two", "paragraph_index": 46}], "all_paragraphs": ["If you haven\u2019t already, you should definitely read my previous post about what a GAN is (especially if you don\u2019t know what I mean when I say GAN!). That post should give you a starting point to dive into the world of GANs and how they work. It\u2019s a solid primer for any article on GANs, not to mention this one where we will be discussing objective functions of GANs and other variations of GANs currently out there that use twists on defining their objectives for different results.", "Don\u2019t have time to read the whole thing? Here\u2019s the TL;DR", "In our introductory post, we talked about generative models. We discussed how the goal of a generative model is to come up with a way of matching their generated distribution to a real data distribution. Minimizing the distance between the two distributions is critical for creating a system that generates content that looks good, new, and like it is from the original data distribution.", "But how do we measure the difference between our generated data distribution and our original data distribution? That\u2019s what we call an objective function and it is the focus of this article today! We are going to look at some variations of GANs to understand how we can alter the measure of the divergence between our generated data distribution and the actual distribution and the effect that that will have.", "The objective function of our original GAN is essentially the minimization of something called the Jensen Shannon Divergence (JSD). Specifically it is:", "The JSD is derived from the Kullbach-Liebler Divergence (KLD) that we mentioned in the previous post.", "We are already familiar with our friend, the original GAN. Instead of discussing it any further, let\u2019s just admire its performance in all its glory:", "The Wasserstein GAN (WGAN) is a GAN you may have heard about, since it got a lot of attention. It did so for a lot of practical reasons (in general, when you train a GAN the loss values returned don\u2019t mean anything except that with WGAN they can), but what made WGAN different?", "WGAN doesn\u2019t use the JSD to measure divergence, instead it uses something called the Earth-Mover (EM) distance (AKA Wasserstein distance). EM distance is defined as:", "Let\u2019s try and understand the intuition behind the EM distance. A probability distribution is essentially a collection of mass, with the distribution measuring the amount of mass at a given point. We give EM distance two distributions. Since the cost to move a mass a certain distance is equivalent to the product of the mass and the distance, the EM distance basically calculates the minimal cost of transforming one probability distribution into the other. This can be seen as the minimal effort needed.", "But why do we care? Well, we care about EM distance because it oftentimes measures a distance of a straight line for transforming one distribution to the other. This is helpful with gradients in optimization. Not to mention, there are also a set of functions that do not converge when distance is measured with something like KLD or JSD that do actually converge for the EM distance.", "This is because EM distance has guarantees of continuity and differentiability, something that distance functions like KLD and JSD lack. We want these guarantees for a loss function, making EM distance better suited to our needs. More than that, everything that would converge under JSD or KLD also converge under EM distance. It\u2019s just that EM distance encompasses that much more.", "Stepping away from all these thoughts about math and into the practical application of such things, how do we use this new distance when we can\u2019t directly calculate it? Well, we first take a critic function that is parameterized and train it to approximate the EM distance between our data distribution and our generated distribution. When we have achieved that, we have a good approximator for the EM distance. From there, we then optimize our generator function to reduce this EM distance.", "In order to guarantee that our function lies in a compact space (this helps ensure that we meet the theoretical guarantees needed to do our calculations), we clip the weights that parametrize our critic function f.", "Just a side note: Our critic function f is called a critic because it is not an explicit discriminator. A discriminator will classify its inputs as real or fake. The critic doesn\u2019t do that. The critic function just approximates a distance score. However, it plays the discriminator role in the traditional GAN framework, so its worth highlighting how it is similar and how it is different.", "With all those good things proposed with WGAN, what still needs to be improved? Well, Improved Training of Wasserstein GANs highlights just that.", "WGAN got a lot of attention, people started using it, and the benefits were there. But people began to notice that despite all the things WGAN brought to the table, it still can fail to converge or produce pretty bad generated samples. The reasoning that Improved WGAN gives is that weight clipping is an issue. It does more harm than good in some situations. We noted that the reason why we weight clip has to do with maintaining the theoretical guarantees of the critic function. But in practice, what clipping actually does is encourage very simple critic functions that are pushed to the extremes of their boundaries. This is not good.", "What Improved WGAN proposes instead is that you don\u2019t clip weights but rather add a penalization term to the norm of the gradient of the critic function. They found that this produces better results and, when plugged into a bunch of different GAN architectures, produces stable training.", "LSGAN has a setup similar to WGAN. However, instead of learning a critic function, LSGAN learns a loss function. The loss for real samples should be lower than the loss for fake samples. This allows the LSGAN to put a high focus on fake samples that have a really high margin.", "Like WGAN, LSGAN tries to restrict the domain of their function. They take a different approach instead of clipping. They introduce regularization in the form of weight decay, encouraging the weights of their function to lie within a bounded area that guarantee the theoretical needs.", "Another point to note is that the loss function is setup more similarly to the original GAN, but where the original GAN uses a log loss, the LSGAN uses an L2 loss (which equates to minimizing the Pearson X\u00b2 divergence). The reason for this has to do with the fact that a log loss will basically only care about whether or not a sample is labeled correctly or not. It will not heavily penalize based on the distance of said sample from correct classification. If a label is correct, it doesn\u2019t worry about it further. In contrast, L2 loss does care about distance. Data far away from where it should be will be penalized proportionally. What LSGAN argues is that this produces more informative gradients.", "Or RWGAN for short is another variation of the WGAN paper. They describe their RWGAN as the happy medium between WGAN and Improved WGAN (WGAN-GP as they cite it in the paper). Instead of symmetric clamping of weights (like in WGAN) or a gradient penalty (like proposed for Improved WGAN), RWGAN utilizes an asymmetric clamping strategy.", "Beyond the specific GAN architecture they put forth, they also go onto describe what they call a statistical class of divergences (dubbed Relaxed Wasserstein divergences or RW divergences). RW divergences take the Wasserstein divergence from the WGAN paper and make it more general, outlining some key probabilistic properties that are needed in order to hold some of theoretical guarantees of our GANs.", "They specifically show that RWGAN parameterized with KL divergence is extremely competitive against other state-of-the-art GANs, but with better convergence properties than even the regular WGAN. They also open their framework up to defining new loss functions and thus new cost functions for designing a GAN scheme.", "The Mean and Covariance Feature Matching GAN (McGAN) is part of the same family of GAN\u2019s that WGAN is. This family is dubbed the Integral Probability Metric (IPM) family. These GANs are the ones that use a critic architecture instead of an explicit discriminator.", "The critic function for McGAN has to do with measuring the mean or the covariance features of the generated data distribution and the target data distribution. This seems pretty straight forward when looking at the name too. They define two different ways of creating a critic function, one for the mean and one for the covariance and demonstrate how to actually use them. Like WGAN, they also use clipping on their model, which ends up restricting the capacity of the model. No super eventful conclusions were drawn from this paper.", "Generative Moment Matching Networks (GMMN) focuses on minimizing something called the maximum mean discrepancy (MMD). MMD is essentially the mean of the embedding space of two distributions, and we are trying to minimize the difference between the two means here. We can use something called the kernel trick which allows us to cheat and use a Gaussian kernel to calculate this distance.", "They argue that this allows for a simple objective that can easily be trained with backpropagation, and produces competitive results with a standard GAN. They also showed how you could add an auto-encoder into the architecture of this GAN to ease the amount of training needed to accurately estimate the MMD.", "An additional note: Though they claim competitive results, from what I\u2019ve read elsewhere, it seems that their empirical results are often lacking. What\u2019s more, this model is fairly computationally heavy, so the computational resource and performance trade-off doesn\u2019t really seem to be there in my opinion.", "Maximum Mean Discrepancy GAN or MMD GAN is, you guessed it, an improvement of GMMN. Their major contributions come in the form of not using static Gaussian kernels to calculate the MMD, and instead use adversarial techniques to learn kernels. It combines ideas from the original GAN and GMMN papers to create a hybrid of the ideas of the two. The benefits it claims are an increase in performance and run time.", "Cramer GAN starts by outlining an issue with the popular WGAN. It claims that there are three properties that a probability divergence should satisfy:", "Of these properties, they argue that the Wasserstein distance lacks the final property, unlike KLD or JSD which both have it. They demonstrate that this is actually an issue in practice, and propose a new distance: the Cramer distance.", "Now if we look at the Cramer distance, we can actually see it looks somewhat similar to the EM distance. However, due to its mathematical differences, it actually doesn\u2019t suffer from the biased sample gradients that EM distance will. This is proven in the paper, if you really wish to dig into the mathematics of it.", "The Fisher GAN is yet another iteration on IPM GAN\u2019s claiming to surpass McGAN, WGAN, and Improved WGAN in a number of aspects. What it does is sets up its objective function to have a critic that has a data dependent constraint on its second order moment (AKA its variance).", "Because of this objective the Fisher GAN boasts the following:", "What makes Fisher GAN\u2019s distance different? It has to do with the fact that it is essentially measure what is called the Mahalanobis distance which in simple terms is the distance between two points that have correlated variables, relative to a centroid that is believed to be the mean of the distribution of the multivariate data. This actually assures that the generator and critic will be bounded like we desire. As the parameterized critic approaches infinite capacity, it actually estimates the Chi-square distance.", "Energy Based GAN (EBGAN) is an interesting one in our collection of GANs here today. Instead of using a discriminator like how the original GAN does, it uses an autoencoder to estimate reconstruction loss. The steps to setting this up:", "This is a really cool approach to setting up the GAN, and with the right regularization to prevent mode collapse (the generator just producing the same sample over and over again), it seems to be fairly decent.", "So why even do this? Well, what was empirically shown is that using the autoencoder in this fashion actually produces a GAN that is fast, stable, and robust to parameter changes. What\u2019s more, there isn\u2019t a need to try and pull a bunch of tricks to balance the training of the discriminator and the generator.", "Boundary Equilibrium GAN (BEGAN) is an iteration on EBGAN. It instead uses the autoencoder reconstruction loss in a way that is similar to WGAN\u2019s loss function.", "In order to do this, a parameter needs to be introduced to balance the training of the discriminator and generator. This parameter is weighted as a running mean over the samples, dancing at the boundary between improving the two halves (thus where it gets its name: \u201cboundary equilibrium\u201d).", "Margin Adaptation GAN (MAGAN) is the last on our list. It is another variation of EBGAN. EBGAN has a margin as a part of its loss function to produce a hinge loss. What MAGAN does is reduce that margin monotonically over time, instead of keeping it constant. The result of this is that the discriminator will autoencode real samples better.", "The result that we care about: better samples and more stability in training.", "That was a lot of different GANs! And a lot of content! I think it\u2019s worth a summary in a table just to keep us organized:", "Whew\u2026 Pat yourself on the back, that was a lot of GAN content.", "If I missed something or misinterpreted something, please correct me!", "If you enjoyed this article or found it helpful in any way, I would love you forever if you passed me a long a dollar or two to help fund my machine learning education and research! Every dollar helps me get a little closer and I\u2019m forever grateful.", "And stay tuned for more GAN blogs in the near future!", "CS Undergrad at Drexel University | AI | ML | NLP | DS"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fad77340bce3c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----ad77340bce3c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad77340bce3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----ad77340bce3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad77340bce3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=-----ad77340bce3c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://hunterheidenreich.com/blog/what-is-a-gan/", "anchor_text": "previous post"}, {"url": "https://www.courses.psu.edu/for/for466w_mem14/Ch11/HTML/Sec1/ch11sec1_ObjFn.htm", "anchor_text": "objective function"}, {"url": "https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence", "anchor_text": "Jensen Shannon Divergence"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullbach-Liebler Divergence"}, {"url": "https://arxiv.org/pdf/1701.07875.pdf", "anchor_text": "Wasserstein GAN"}, {"url": "https://en.wikipedia.org/wiki/Earth_mover's_distance", "anchor_text": "Earth-Mover (EM) distance"}, {"url": "https://en.wikipedia.org/wiki/Wasserstein_metric", "anchor_text": "Wasserstein distance"}, {"url": "https://arxiv.org/pdf/1704.00028.pdf", "anchor_text": "Improved Training of Wasserstein GANs"}, {"url": "https://arxiv.org/pdf/1611.04076.pdf", "anchor_text": "LSGAN"}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "L2 loss"}, {"url": "https://arxiv.org/pdf/1705.07164.pdf", "anchor_text": "RWGAN"}, {"url": "https://arxiv.org/pdf/1702.08398.pdf", "anchor_text": "Mean and Covariance Feature Matching GAN"}, {"url": "https://arxiv.org/pdf/1502.02761.pdf", "anchor_text": "Generative Moment Matching Networks"}, {"url": "http://alex.smola.org/teaching/iconip2006/iconip_3.pdf", "anchor_text": "maximum mean discrepancy"}, {"url": "https://en.wikipedia.org/wiki/Kernel_method", "anchor_text": "kernel trick"}, {"url": "https://arxiv.org/pdf/1705.08584.pdf", "anchor_text": "Maximum Mean Discrepancy GAN"}, {"url": "https://arxiv.org/pdf/1705.10743.pdf", "anchor_text": "Cramer GAN"}, {"url": "https://arxiv.org/pdf/1705.09675.pdf", "anchor_text": "Fisher GAN"}, {"url": "http://www.statisticshowto.com/mahalanobis-distance/", "anchor_text": "Mahalanobis distance"}, {"url": "https://arxiv.org/pdf/1609.03126.pdf", "anchor_text": "Energy Based GAN"}, {"url": "https://arxiv.org/pdf/1703.10717.pdf", "anchor_text": "Boundary Equilibrium GAN"}, {"url": "https://arxiv.org/pdf/1704.03817.pdf", "anchor_text": "Margin Adaptation GAN"}, {"url": "https://www.gofundme.com/hunter-heidenreich-research-fund", "anchor_text": "a dollar or two"}, {"url": "http://hunterheidenreich.com/blog/what-is-a-gan/", "anchor_text": "hunterheidenreich.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ad77340bce3c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ad77340bce3c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----ad77340bce3c---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/generative-models?source=post_page-----ad77340bce3c---------------generative_models-----------------", "anchor_text": "Generative Models"}, {"url": "https://medium.com/tag/gans?source=post_page-----ad77340bce3c---------------gans-----------------", "anchor_text": "Gans"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad77340bce3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----ad77340bce3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad77340bce3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----ad77340bce3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad77340bce3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----ad77340bce3c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a81cf6d293c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&newsletterV3=66c914ddeac8&newsletterV3Id=8a81cf6d293c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----ad77340bce3c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Written by Hunter Heidenreich"}, {"url": "https://medium.com/@hunterheidenreich/followers?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "915 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----ad77340bce3c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a81cf6d293c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgan-objective-functions-gans-and-their-variations-ad77340bce3c&newsletterV3=66c914ddeac8&newsletterV3Id=8a81cf6d293c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----ad77340bce3c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "NEAT: An Awesome Approach to NeuroEvolutionNeuroEvolution can optimize and evolve neural network structure, and the NEAT algorithm was one of the first to show it as a viable\u2026"}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "8 min read\u00b7Jan 4, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3eca5cc7930f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneat-an-awesome-approach-to-neuroevolution-3eca5cc7930f&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----3eca5cc7930f----0-----------------clap_footer----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----ad77340bce3c----0---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3eca5cc7930f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneat-an-awesome-approach-to-neuroevolution-3eca5cc7930f&source=-----ad77340bce3c----0-----------------bookmark_preview----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ad77340bce3c----1---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----ad77340bce3c----1-----------------bookmark_preview----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ad77340bce3c----2---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ad77340bce3c----2-----------------bookmark_preview----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "Introduction to Word EmbeddingsWhat is a word embedding?"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": "10 min read\u00b7Aug 16, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf857b12edc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-word-embeddings-4cf857b12edc&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----4cf857b12edc----3-----------------clap_footer----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----ad77340bce3c----3---------------------13f4414a_a42d_42e6_aee5_5020edef3eba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cf857b12edc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-word-embeddings-4cf857b12edc&source=-----ad77340bce3c----3-----------------bookmark_preview----13f4414a_a42d_42e6_aee5_5020edef3eba-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "See all from Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@sunil7545/variational-autoencoders-ce7fe921cce7?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@sunil7545?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@sunil7545?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Sunil Yadav"}, {"url": "https://medium.com/@sunil7545/variational-autoencoders-ce7fe921cce7?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Variational AutoencodersIn this article, we will continue our discussion with variational autoencoders (VAEs) after covering DGM basics and AGM. Variational\u2026"}, {"url": "https://medium.com/@sunil7545/variational-autoencoders-ce7fe921cce7?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b76 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fce7fe921cce7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40sunil7545%2Fvariational-autoencoders-ce7fe921cce7&user=Sunil+Yadav&userId=9ee175d244fd&source=-----ce7fe921cce7----0-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@sunil7545/variational-autoencoders-ce7fe921cce7?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce7fe921cce7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40sunil7545%2Fvariational-autoencoders-ce7fe921cce7&source=-----ad77340bce3c----0-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----ad77340bce3c----1-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Steins"}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Diffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art."}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b77 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&user=Steins&userId=a36be384d77d&source=-----cd331bd41166----0-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----ad77340bce3c----0---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&source=-----ad77340bce3c----0-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Getting Started With Stable DiffusionStable Diffusion is a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION. It\u2019s\u2026"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b712 min read\u00b7Nov 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&user=Youssef+Hosni&userId=859af34925b7&source=-----f343639e4931----1-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----ad77340bce3c----1---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&source=-----ad77340bce3c----1-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----2-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----ad77340bce3c----2---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----ad77340bce3c----2-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----3-----------------clap_footer----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----ad77340bce3c----3---------------------afdee899_3b06_4ca4_b037_1b47cc1220ab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----ad77340bce3c----3-----------------bookmark_preview----afdee899_3b06_4ca4_b037_1b47cc1220ab-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----ad77340bce3c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}