{"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "time": 1683011310.357443, "path": "towardsdatascience.com/monte-carlo-methods-9b289f030c2e/", "webpage": {"metadata": {"title": "Monte Carlo Methods. Exploration-Explanation Dilemma | Towards Data Science", "h1": "Monte Carlo Methods", "description": "In this new post of the \u201cDeep Reinforcement Learning Explained\u201d series, we will introduce the Monte Carlo Methods and the Exploration-Explanation Dilemma"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction", "paragraph_index": 5}, {"url": "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py", "anchor_text": "BlackJack Environment", "paragraph_index": 14}, {"url": "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py", "anchor_text": "Blackjack-v0", "paragraph_index": 19}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_Monte_Carlo.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 22}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_Monte_Carlo.ipynb", "anchor_text": "can be run as a Colab google notebook using this link.", "paragraph_index": 22}, {"url": "https://github.com/udacity/deep-reinforcement-learning/blob/master/monte-carlo/plot_utils.py", "anchor_text": "this code borrowed from Udacity", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/mc-control-methods-50c018271553", "anchor_text": "In the next post", "paragraph_index": 61}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 62}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 62}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 63}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 64}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 67}], "all_paragraphs": ["In this new post of the \u201cDeep Reinforcement Learning Explained\u201d series, we will introduce another of the classical methods of Reinforcement Learning to estimate the value of a policy \u03c0. The most straightforward approach it\u2019s just to run several episodes with this policy collecting hundreds of trajectories, and then calculate averages for every state. This method of estimating value functions is called Monte Carlo prediction (MC).", "In this post we will also introduce how to estimate the optimal policy and the Exploration-Exploitation Dilemma.", "In Part 1 of this series, we presented a solution to MDP called dynamic programming, pioneered by Richard Bellman. Remember that the Bellman equation allows us to define the value function recursively and can be solved with the Value Iteration algorithm. To summarize, Dynamic Programming provides a foundation for reinforcement learning, but we need to loop through all the states on every iteration (they can grow exponentially in size, and the state space can be very large or infinite). Dynamic programming also requires a model of the Environment, specifically knowing the state-transition probability p(s\u2032,r|s,a).", "In contrast, Monte Carlo methods are all about learning from experience. Any expected value can be approximated by sample means \u2014 in other words, all we need to do is play a bunch of episodes, gather the returns, and average them. Monte Carlo methods are actually a set of alternatives to the basic algorithm. These are only for episodic tasks, where the interaction stops when the Agent encounters a terminal state. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected.", "It\u2019s important to note that Monte Carlo methods only give us a value for states and actions we\u2019ve encountered, and if we never encounter a state its value is unknown.", "This post will provide a practical approach to Monte Carlo used in Reinforcement Learning. For a more formal explanation of the methods, I invite the reader to read the Chapter 5 of the textbook Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.", "Recall that the optimal policy \u03c0\u2217\u200b specifies, for each Environment state s, how the Agent should select an action a towards its goal of maximising reward G. We also learned that the Agent could structure its search for an optimal policy by first estimating the optimal action-value function q\u2217\u200b; then, once q\u2217\u200b is known, \u03c0\u2217\u200b is quickly obtained.", "The Agent starts taking a basic policy, like the equiprobable random policy, a stochastic policy where from each state the Agent randomly selects from the set of available actions, and each action is selected with equal probability. The Agent uses the current policy \u03c0 will interact with the environment to collect some episodes, and then consolidate the results to arrive at a better policy.", "The way to do it is by estimating the action-value function with a table we will call Q-table. This core table in Monte Carlo Methods has a row for each state and a column for each action. The entry corresponding to state s and action a is denoted Q(s,a).", "We refer to this as the prediction problem: Given a policy, how might the Agent estimate the value function for that policy?. We refer to Monte Carlo (MC) approaches to the prediction problem as MC prediction methods.", "We will focus our explanation to the action-value function Q, but MC can also be used to estimate the state-value function V.", "In the algorithm for MC prediction, we begin by collecting many episodes with the policy. Then, we note that each entry in the Q-table corresponds to a particular state and action. To populate an entry of the Q-table, we use the return that followed when the Agent was in that state and chose the action.", "We define every occurrence of a state in an episode as a visit to that state-action pair. It can happen that a state-action pair is visited more than once in an episode. This leads us to have two versions of MC prediction algorithm:", "Both the first visit method and each visit are considered to guarantee convergence to the true action-value function.", "In this post, we will implement the first-visit for our working example from OpenAI Gym: BlackJack Environment. But actually what happens in this example first-visit and every-visit MC return the same results. Note that the same state never recurs within one episode, so there is no difference between first-visit and every-visit MC methods. The pseudocode for the First-Visit MC Prediction can be found below:", "In this pseudocode, the variable num_episodes indicates the number of episodes the Agent collects and there are three relevant tables:", "After each episode, N and returns_sum tables are updated to store the information contained in the episode. The final estimate for Q (the Q-table) is done after all of the episodes have been collected.", "To better understand how Monte Carlo works in practice, let\u2019s perform a step-by-step coding with the game of Blackjack using OpenAI\u2019s gym Environment Blackjack-v0.", "To begin with, let\u2019s define the rules and conditions of our game:", "We\u2019ll use OpenAI\u2019s gym Environment Blackjack-v0. Each state is a 3-tuple of:", "The Agent has two potential actions:", "Each game of blackjack is an episode. Rewards of +1, \u22121, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are 0, and we do not discount (gamma = 1); therefore these terminal rewards are also the returns.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "We begin by importing the necessary packages:", "We can see that there are 704 different states in the Environment corresponding to 32 times 11 times 2 and two possible actions corresponding to selecting to stick or to hit:", "To see a sample games each time step with each episode the Agent interacts with the Environment we can run the following code (multiple times):", "One example of a sample game is:", "where the Agent lost the play. I suggest the reader to run this code several times to see different plays.", "Consider the policy where the player decides an action based only on her/his current score. For instance, if the sum of the cards we have is 18 or less we figure that probably it\u2019s okay if we ask for a new car. We do that with a 75% probability. If the sum of the cards is bigger than 18, we consider that it is too dangerous to accept a new card, and we don\u2019t do that with a 75% probability.", "In summary, we select action stick with a 75% probability if the sum is greater than 18; and, if the sum is 18 or below, we select action hit with a 75% probability.", "Notice that in this first approach of policy, there\u2019s information in the state that we are ignoring, for instance, the dealer\u2019s face-up card or whether we have a usable ace. This is to simplify the example a focus the explanation of the code. The following function generate_episode samples an episode using this policy:", "This code returns an episode (using the policy \u03c0 decided) as a list of (state, action, reward) tuples of tuples. episode[i][0], episode[i][1], and episode[i][2] correspond to the state at time-step \ud835\udc56, action at time-step i, and reward at time-step \ud835\udc56+1, respectively.", "In order to play (10 games), we can execute the previous function as follows:", "The output will be (in my run):", "The rewards are received only at the end of the game and are 1.0 if we win and -1.0 if we lose. We see that sometimes we win and sometimes we lose.", "Let\u2019s start to program a code based on the previous pseudocode. First, we need to initialize the dictionaries N, return_sum, and Q:", "Next, the algorithm loops over episodes generated using the provided function generate_episodethat uses the policy. Each episode is going to be a list of state, action, and reward tuples. Then we use the zip command to separate the states, actions, and rewards into separate quantities:", "Let\u2019s back to de pseudocode to see that we loop over time steps to look at the corresponding state action pair for each time step. If that\u2019s the first time that we visited that pair we increment the corresponding position of table N by one and add the return at this time step in the corresponding entry of the table return_sum.", "Remember that in this Blackjack example, first visit and every visit MC prediction are equivalent, so we will do this update for every time step. Then, once we have the corresponding updated values for return_sum and N we can use them to update our Q table estimated. The code for this part will be the following:", "The complete code of our MC prediction method is as follows:", "The algorithm has as arguments the instance of an OpenAI Gym Environment, the number of episodes that are generated, and the discount rate (default value 1). The algorithm returns as output the Q-table (estimate of the action-value function), a dictionary (of one-dimensional arrays).", "Will be interesting to plot the corresponding state value function to see which states have more value. We can do it from the Q-table with this simple code that obtains the value of the state by weighting the value of each action according to how we have defined the problem:", "We could plot this with this code borrowed from Udacity. There are two plots corresponding to whether we have a usable ace or we don\u2019t have a usable ace. But in either case, we see that the highest state values correspond to when the player sum is something like 20 or 21, and it seems obvious because in this case we are most likely to win the game.", "So far, we have learned how an Agent can take a policy like the equal probable random policy, use that to interact with the Environment, and then use that experience to populate the corresponding Q-table, which becomes an estimate of that policies action-value function. So, now the question is how can we use this in our search for an optimal policy?", "Well, to get a better policy, that is not necessarily the optimal one, we need only select for each state the action that maximizes the Q-table. Let\u2019s call that new policy \u03c0\u2019. When we take a Q-table and use the action that maximizes each row to come up with the policy, we say that we are constructing the policy that\u2019s greedy with respect to the Q-table.", "It is common to refer to the selected action as the greedy action. In the case of a finite MDP, the action-value function estimate is represented in a Q-table. Then, to get the greedy action, for each row in the table, we need only select the action corresponding to the column that maximize the row.", "However, instead of always constructing a greedy policy (always select the greedy action)what we will do instead is construct a so-called Epsilon-Greedy policy that\u2019s most likely to pick the greedy action, but with some small but nonzero probability picks one of the other actions instead. In this case, some small positive number epsilon \u03f5, which must be between zero and one is used. This is motivated, as we will explain in more detail later, due an Agent must find a way to balance the drive to behave optimally based on their current knowledge and the need to acquire knowledge to attain better future behavior.", "We have learned how the Agent can take a policy \u03c0, use it to interact with the Environment for many episodes, and then use the results to estimate the action-value function with a Q-table. Once the Q-table closely approximates the action-value function\u200b, the Agent can construct the policy \u03c0\u2032 that is \u03f5-greedy with respect to the Q-table, which will yield a policy that is better than the original policy \u03c0. Then we can improve the policy by changing it to to be \u03f5-greedy with respect to the Q-table.", "So, we will eventually obtain the optimal policy \u03c0\u2217\u200b if the Agent alternates between these two steps over and over until we got successively better and better policies in the hope that we converge to the optimal policy:", "This proposed algorithm is so close to giving us the optimal policy, as long as we run it for long enough.", "We call this algorithm Monte Carlo control method, to estimate the optimal policy. It is common to refer to Step 1 as policy evaluation since it is used to determine the action-value function of the policy. Likewise, since Step 2 is used to improve the policy, we also refer to it as a policy improvement step. Schematically we could represent it as:", "Therefore, using this new terminology, we can summarize that our Monte Carlo control method alternates between policy evaluation and policy improvement steps to find the optimal policy \u03c0\u2217\u200b:", "where the \u201cE\u201d in the arrow denotes a complete policy evaluation and \u201cI\u201d denotes a complete policy improvement. In the next post we will present an implementation of the Monte Carlo Control Algorithm.", "However, some questions arise, how to set the value of \u03f5 when constructing \u03f5-greedy policies? In the next section, we will see how.", "Recall that the Environment\u2019s dynamics are initially unknown to our Agent and due the goal is maximizing return, the Agent must learn about the Environment through interaction. Then, at every time step, when the Agent selects an action, it bases its decision on past experience with the Environment. And, the instinct could be to select the action that based on past experience will maximize return. As we discussed in a previous section, this policy that is greedy with respect to the action-value function estimate can easily lead to convergence to a sub-optimal policy.", "This may happen because, in the early episodes, the Agent\u2019s knowledge is quite limited and dismisses considering non-greedy actions that are better in terms of future rewards than known actions. That means that a successful Agent cannot act greedily at every time step; instead, in order to discover the optimal policy, it has to continue to refine the estimated return for all state-action pairs. But at the same time maintain a certain level of greedy actions to maintain the goal of maximizing return as quickly as possible. This motivated the idea of an \u03f5-greedy policy which we have present before.", "We refer to the need to balance these two competing requirements as the Exploration-Exploitation Dilemma, where an Agent must find a way to balance the drive to behave optimally based on their current knowledge (exploitation) and the need to acquire knowledge to attain better judgment (exploration).", "One potential solution to this dilemma is implemented by gradually modifying the value of \u03f5 when constructing \u03f5-greedy policies. It makes sense for the Agent to begin its interaction with the Environment by opting for exploration rather than exploitation trying various strategies for maximizing return. With this in mind, the best starting policy is the equiprobable random policy, as it is equally likely to explore all possible actions from each state. Setting \u03f5=1 yields an \u03f5-greedy policy that is equivalent to the equiprobable random policy.", "At later time steps, it makes sense to foster exploitation over exploration, where the policy gradually becomes more greedy with respect to the action-value function estimate. Setting \u03f5=0 yields the greedy policy. It has been shown that initially favoring exploration by exploitation and gradually preferring exploitation to exploring is an optimal strategy.", "In order to guarantee that MC control converges to the optimal policy \u03c0\u2217\u200b, we need to ensure that two conditions are met: every state-action pair is visited infinitely many times, and the policy converges to a policy that is greedy with respect to the action-value function estimate Q. We refer to these conditions as Greedy in the Limit with Infinite Exploration that ensure the Agent continues to explore for all time steps, and the Agent gradually exploits more and explores less.", "One way to satisfy these conditions is to modify the value of \u03f5 , making it gradually decay, when specifying an \u03f5-greedy policy. However, one has to be very careful with setting the decay rate for \u03f5. Determining the best decay is not trivial, requiring a bit of alchemy, that is, experience. We will see an example of implementation in the next post.", "So far, in our current algorithm for Monte Carlo control, we collect a large number of episodes to build the Q-table. Then, after the values in the Q-table have converged, we use the table to come up with an improved policy. However, Monte Carlo prediction methods can be implemented incrementally, on an episode-by-episode basis. In the next post, we will present how to build better MC control algorithms following this idea. See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9b289f030c2e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----9b289f030c2e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINE \u2014 13"}, {"url": "https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/5-evaluaci%C3%B3n-de-pol%C3%ADticas-con-monte-carlo-a6d70d1db7d4", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/5-evaluaci%C3%B3n-de-pol%C3%ADticas-con-monte-carlo-a6d70d1db7d4", "anchor_text": "5. Evaluaci\u00f3n de pol\u00edticas con Monte CarloAcceso abierto al cap\u00edtulo 5 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py", "anchor_text": "BlackJack Environment"}, {"url": "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py", "anchor_text": "Blackjack-v0"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_Monte_Carlo.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_Monte_Carlo.ipynb", "anchor_text": "can be run as a Colab google notebook using this link."}, {"url": "https://github.com/udacity/deep-reinforcement-learning/blob/master/monte-carlo/plot_utils.py", "anchor_text": "this code borrowed from Udacity"}, {"url": "https://towardsdatascience.com/mc-control-methods-50c018271553", "anchor_text": "In the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9b289f030c2e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----9b289f030c2e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9b289f030c2e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----9b289f030c2e---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9b289f030c2e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----9b289f030c2e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----9b289f030c2e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9b289f030c2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9b289f030c2e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9b289f030c2e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9b289f030c2e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9b289f030c2e--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-methods-9b289f030c2e&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}