{"url": "https://towardsdatascience.com/logistic-regression-fa1a86270eb", "time": 1682997462.1017978, "path": "towardsdatascience.com/logistic-regression-fa1a86270eb/", "webpage": {"metadata": {"title": "Logistic Regression. A simpler intuitive explanation. | by Abhishek Kumar | Towards Data Science", "h1": "Logistic Regression", "description": "Logistic regression is a popular classification algorithm. Given an input example, a logistic regression model assigns the example to a relevant class."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Contrary to its name logistic regression is a classification algorithm. Given an input example, a logistic regression model assigns the example to a relevant class.", "For the sake of understanding let\u2019s assume John, has been assigned the job of predicting whether a given email is spam or not spam. He collects a dataset represented in the table below. The data set consists of n examples, where each example x_{i} is represented by a 2-dimensional feature vector, comprising of the number of words and the number of links in that email. The target variable y, is equal to 1 if the corresponding example is a spam and is 0 otherwise.", "John first decides to build a linear regression model to solve this problem. However having built the linear regression model he is quite unhappy with his results.", "Remember, that linear regression is used to predict a real-valued output where the output may vary between +\u221e to -\u221e whereas in this case y, the target variable takes only two discrete values, 0 and 1.", "John\u2019s decides to extend the concepts of linear regression to fulfil his requirement. One approach is to take the output of linear regression and map it between 0 and 1, if the resultant output is below a certain threshold, classify the example as a negative class whereas if the resultant output is above a certain threshold, classify the example as a positive class. In fact, this is the logistic regression learning algorithm.", "We will crunch the real-valued output obtained from a linear regression model between 0 and 1 and classify a new example based on a threshold value. The function used to perform this mapping is the sigmoid function.", "The graph shows that the sigmoid function has successfully mapped values from +\u221e to -\u221e between 0 and 1.", "Note that the value of sigmoid function is 0.5 at x=0. Also, if x>0 then the value of sigmoid function is >0.5, on the other hand if x<0 then the value of sigmoid function is <0.5.", "Following the above discussion, the model of logistic regression is represented as \u2014", "In above equation, the output wx+b, from the linear regression model has been passed as an input to the sigmoid function, thereby crunching values ranging from +\u221e to -\u221e between 0 and 1 and, giving us the model for logistic regression. The output of the model is interpreted as the probability of an example x falling into the positive class. Concretely, if for an unseen x the model outputs 0.8, then it implies that, the probability of x being classified as positive is 0.8.", "It is apparent that different values of w and b correspond to different models of logistic regression. But, we want an optimal set of values, w* and b* which will minimize the error between the predictions made by the model f(x) and the actual results y for the training set.", "The cost function for a logistic regression model, called the log-likelihood function is given by \u2014", "The goal is to find values of w and b for the model f(x) which will minimise the above cost function. On observing the cost function we notice that there are two terms inside summation where, the first term will be 0 for all the examples where y=0, and the second term will be zero for all the examples where y=1. So, for any given example one of the summation terms is always zero. Also, the range of f(x) is [0,1], which implies that -log(f(x)) ranges from (+\u221e,0].", "Let\u2019s try to understand the cost function better by considering an arbitrary example x_{^i} for which y=1. The second term will evaluate to zero as (1-y_{^i}) becomes zero. As far as the first term is concerned, if our model f(x) predicts 1, then the first term will also become zero because log(1) is equal to 0, therefore both the first and the second term become zero, subsequently the cost also becomes zero, which makes sense as our model has predicted the correct value for the target variable, y. Had our model predicted 0, the value of -log(0) would have approached +\u221e, signifying the high penalty imposed by the cost function on our model.", "The average over summation signifies that we want to the minimise the overall error for all the training examples.", "Having developed this intuition John now decides to build a logistic regression model for the spam classifier problem. He starts by plotting his data set.", "It is apparent that a linear decision boundary is unlikely to fit the data set. Just like linear regression we may transform features to higher order comprising of quadratic or cubic terms to fit a non-linear decision boundary to our dataset.", "Let\u2019s say, John decides to fit a quadratic decision boundary to his data set, therefore he includes the second degree terms such as (number of links)\u00b2, (number of words)\u00b2 and (number of links)*(number of words). He finally ends up with the following decision boundary.", "The above decision boundary corresponds to the circle with the centre at (55,5) and a radius of 4 unit \u2014", "Which gives John the following logistic regression model \u2014", "For an unseen example falling inside the circle (decision boundary) the equation of circle returns a negative value therefore the above model returns a value less than 0.5, implying the probability of that example being spam is less than 0.5. If the threshold is also 0.5, the target variable, y takes the value 0, implying the example to be a regular email.", "Choosing a threshold value is in itself a topic for discussion \u2014 one interpretation is that, the threshold value signifies how tolerant the model is. Take an example of a model which classifies patients as being affected with a heart disease \u2014 in such a scenario it is preferred to have a lower value of threshold which captures the idea that\u2014 whenever in doubt, classify the patient as being effected by the disease and have him/her checked just to be sure \u2014 it means your are willing to accept more false positives (classifying a healthy patient as unhealthy or classifying a not spam email as a spam email).", "Technically, sensitivity is the ability to correctly identify patients with the disease (true positive), whereas specificity is the ability to correctly identify patients without the disease (true negative). Ideally, one would want a threshold value that balances sensitivity and specificity however, the value of threshold is problem specific and depending on how tolerant you want the model to be, the value has to tuned.", "Consider a classification problem with three arbitrary classes represented in the plot below \u2014", "Now the goal is to extend the ideas of binary classification to multi-class classification. The following plots illustrate how to convert our original problem of multi-class classification to a binary classification problem.", "Each plot considers only one class at a time and predicts the probability of an example falling into that class, finally the class with the highest probability is assigned to a new example. This method is called as one versus all classification.", "In general, to solve a multi-class classification problem with n classes, n logistic regression models are trained each considering only a single class. Whenever a new example has to be classified, all the n models predict the probability of the example falling into its respective class. Finally, the example is assigned to the class with the highest probability as predicted by one of the n models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I enjoy to read, write, develop, and listen to music."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa1a86270eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhieshekumar.medium.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Abhishek Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1245ce379ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&user=Abhishek+Kumar&userId=e1245ce379ff&source=post_page-e1245ce379ff----fa1a86270eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fa1a86270eb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----fa1a86270eb---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----fa1a86270eb---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&user=Abhishek+Kumar&userId=e1245ce379ff&source=-----fa1a86270eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&user=Abhishek+Kumar&userId=e1245ce379ff&source=-----fa1a86270eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffa1a86270eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fa1a86270eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa1a86270eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fa1a86270eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fa1a86270eb--------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Kumar"}, {"url": "https://abhieshekumar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "143 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1245ce379ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&user=Abhishek+Kumar&userId=e1245ce379ff&source=post_page-e1245ce379ff--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F919dadc87f64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-fa1a86270eb&newsletterV3=e1245ce379ff&newsletterV3Id=919dadc87f64&user=Abhishek+Kumar&userId=e1245ce379ff&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}