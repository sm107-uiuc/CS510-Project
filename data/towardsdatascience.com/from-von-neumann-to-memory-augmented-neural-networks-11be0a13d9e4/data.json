{"url": "https://towardsdatascience.com/from-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4", "time": 1683013589.2741199, "path": "towardsdatascience.com/from-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4/", "webpage": {"metadata": {"title": "From von Neumann to Memory-Augmented Neural Networks | by Julia Turc | Towards Data Science", "h1": "From von Neumann to Memory-Augmented Neural Networks", "description": "Memory-Augmented Neural Networks (MANNs) were introduced in 2014 by two concurrent research efforts: Neural Turing Machines and Memory Networks. Since then, they expanded into a broader topic that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1410.5401.pdf", "anchor_text": "Neural Turing Machines", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1410.3916", "anchor_text": "Memory Networks", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Von_Neumann_architecture", "anchor_text": "von Neumann architecture", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Differentiable_function", "anchor_text": "Wikipedia", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Derivative", "anchor_text": "Wikipedia", "paragraph_index": 7}, {"url": "http://storia.ai", "anchor_text": "storia.ai", "paragraph_index": 22}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc", "paragraph_index": 22}], "all_paragraphs": ["Memory-Augmented Neural Networks (MANNs) were introduced in 2014 by two concurrent research efforts: Neural Turing Machines and Memory Networks. Since then, they expanded into a broader topic that spans beyond these original implementations. However, I will stick to a high-level intuitive overview. This article is meant to distill the last 7 years of research into a 7-minute read, removing paper-specific terminology and implementation details that didn\u2019t pass the test of time.", "Memory-Augmented Neural Networks (MANNs) are differentiable versions of the von Neumann architecture. The neural memory is separate from the rest of the model parameters and, similarly to the RAM, stores long-term information.", "The traditional von Neumann architecture differentiates between a CPU (Central Processing Unit) and three levels of memory: registers \u2014 very fast, but with storage capability limited to a few values; main memory (e.g. RAM)\u2014 faster, with enough storage to accommodate for instructions and data to run a program, and external memory (e.g. hard drive) \u2014 slow, but with room for virtually all data used by a computer.", "Memory-Augmented Neural Networks (MANNs) are differentiable versions of the von Neumann architecture (more on this in the next section). The bulk of the neural network can be thought of as the CPU. Certain architectures like RNNs (Recurrent Neural Networks) have built-in memory that is analogous to the registers, storing short-term information. The neural memory is separate from the rest of the model parameters and, similarly to the RAM, stores long-term information. It consists of an array of memory slots (i.e., a matrix) and, most commonly, stores continuous representations of information (text, images, etc.)", "The component that directly interacts with the neural memory via read and write operations is called a controller. In early work, the controller coincided with the rest of the model (i.e. all the parameters outside the memory), so it acted as the interface between the memory and the \u201cexternal world\u201d. It was often implemented as a recurrent neural network. More recently, with the advent of massive Transformer-based architectures, the controller is only a small subset of the model, and mediates the communication between the memory and the rest of the network.", "MANNs are differentiable, von Neumann architectures are not \u2014 but what exactly does this mean? You might recall the following definitions:", "A differentiable function of a real variable is a function whose derivative exists at each point in its domain \u2014 Wikipedia.", "The derivative of a function of a real variable measures the sensitivity of the function value (output value) with respect to a change in its argument (input value) \u2014 Wikipedia.", "The von Neumann architecture performs non-differentiable operations. For instance, consider a read operation: when the CPU fetches its next instruction from RAM, it specifies an address (the input) and receives back an instruction (the output). The input domain is thus unsigned integers, so the operation is not defined over a real variable. According to the definition above, differentiability is out of the question.", "The core reason why RAM reads are not differentiable is that they operate over a discrete space of addresses. Neural memories propose an adjustment:", "Instead of reading from a single entry, perform a weighted read from all entries.", "For each memory slot i, the controller specifies a real-valued weight w\u1d62 such that all weights sum up to 1. This changes the input of the read operation from a single integer value (the address) to a vector of real values (the per-slot weights), which is the first requirement for differentiability. Note that this revised operation is strictly more general: when a single weight is set to 1.0 and all others to 0.0, we are effectively reading from a single entry. The same reasoning applies to writes: instead of writing a value x to a single memory slot, we update each entry i by a weighted value w\u1d62 * x.", "These operations are called soft reads and writes, due to the continuous nature of the weights w\u1d62. This contrasts with the hard reads and writes to RAM, where the controller makes a hard decision regarding the memory slot to operate on.", "RAM is accessed based on location \u2014 read operations specify the exact address to read from. Neural memories are typically accessed based on content \u2014 queries specify what to read, not where to read from.", "How does the controller compute the per-slot weights w\u1d62?", "First, a note on terminology: the mechanism to compute weights w\u1d62 is often referred to as memory addressing, since it determines which memory slots are addressed, and how much attention is paid to each. Addressing neural memories can be done based on content or location.", "With content-based addressing, the weights w\u1d62 reflect how relevant the content of slot i is in resolving an incoming query. For instance, for a question answering task, the memory query could be an embedding of the actual question. The controller must then upweight the memory slots that are good answer candidates. Most commonly, w\u1d62 is the dot product or cosine similarity between the embeddings of the content in slot i and the query. Finally, all weights are normalized via softmax so that they sum up to 1.", "With location-based addressing, the weights w\u1d62 reflect how much attention should be paid to location i, irrespective of its content. This technique is less common; it was introduced in 2014 by Google DeepMind\u2019s Neural Turing Machine [1] and then dropped in their 2016 iteration of MANNs [3]. The original justification was that, for certain tasks that require arithmetic operations like adding two variables x and y, it is important that the controller is able to retrieve the operands x and y from memory regardless of their exact value.", "Making reads and writes differentiable comes with a computational cost. Each query is now resolved in linear time O(N), where N is the number of memory slots (in contrast, hard reads and writes take constant O(1) time). When the input to a network is a sequence (e.g. a text document) of length L, it is common to make one query for each element in the sequence \u2014 which brings the cost up to O(N*L). During training, soft reads and writes are also memory-inefficient; computing gradients for the entire memory requires making a copy of it.", "Follow-up research focused on reducing the O(N) cost to either O(log N) (Rae et al. [2]) or O(sqrt N) (Lample et al. [4]). While the two approaches are quite different, their common ground is to operate on a subset of the memory, as opposed to all entries. In other words, they limit the number of non-zero weights w\u1d62 to a small constant K (somewhere between 2 and 8), and apply gradient descent only on the slots with non-zero weight.", "Memory-Augmented Neural Networks have shown promising results in artificial tasks (e.g. they learn to copy a sequence a given number of times), some natural language tasks (question answering, machine translation) and some computer vision tasks (character recognition). However, they are yet to become mainstream. There are interesting research opportunities in multiple directions: reducing their computational cost, speeding up training, understanding under what circumstances they are most useful, and integrating them with the state-of-the-art Transformers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building storia.ai | Ex-Google Research | twitter.com/juliarturc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F11be0a13d9e4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@turc.raluca?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc----11be0a13d9e4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1410.5401.pdf", "anchor_text": "Neural Turing Machines"}, {"url": "https://arxiv.org/abs/1410.3916", "anchor_text": "Memory Networks"}, {"url": "https://blog.ed.ted.com/2015/08/11/12-amazing-facts-about-elephants/", "anchor_text": "12 amazing facts about elephants"}, {"url": "https://unsplash.com/@tobiasadam?utm_source=medium&utm_medium=referral", "anchor_text": "Tobias Adam"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Von_Neumann_architecture", "anchor_text": "von Neumann architecture"}, {"url": "https://arxiv.org/pdf/1410.5401.pdf", "anchor_text": "Graves et al."}, {"url": "https://en.wikipedia.org/wiki/Differentiable_function", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Derivative", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Derivative", "anchor_text": "Wikipedia"}, {"url": "https://arxiv.org/pdf/1410.5401.pdf", "anchor_text": "Neural Turing Machines"}, {"url": "https://arxiv.org/pdf/1610.09027.pdf", "anchor_text": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"}, {"url": "https://arxiv.org/pdf/1605.06065.pdf", "anchor_text": "One-shot Learning with Memory-Augmented Neural Networks"}, {"url": "https://arxiv.org/pdf/1907.05242.pdf", "anchor_text": "Large Memory Layers with Product Keys"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----11be0a13d9e4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----11be0a13d9e4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----11be0a13d9e4---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----11be0a13d9e4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----11be0a13d9e4---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&user=Julia+Turc&userId=f758859396fc&source=-----11be0a13d9e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&user=Julia+Turc&userId=f758859396fc&source=-----11be0a13d9e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F11be0a13d9e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----11be0a13d9e4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----11be0a13d9e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/@turc.raluca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "392 Followers"}, {"url": "http://storia.ai", "anchor_text": "storia.ai"}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F779df2fab045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-von-neumann-to-memory-augmented-neural-networks-11be0a13d9e4&newsletterV3=f758859396fc&newsletterV3Id=779df2fab045&user=Julia+Turc&userId=f758859396fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}