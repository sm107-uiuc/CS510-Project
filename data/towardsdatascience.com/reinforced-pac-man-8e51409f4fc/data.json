{"url": "https://towardsdatascience.com/reinforced-pac-man-8e51409f4fc", "time": 1683010919.069746, "path": "towardsdatascience.com/reinforced-pac-man-8e51409f4fc/", "webpage": {"metadata": {"title": "Reinforced Pac-man. In-depth analysis of AI in a fun\u2026 | by Sarthak Das | Towards Data Science", "h1": "Reinforced Pac-man", "description": "Escaping the harsh realities of the year 2020 has taken on various forms. For some, the year 2020 has been rife with reading, podcast listening, or late-night binging of the latest Netflix series\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Escaping the harsh realities of the year 2020 has taken on various forms. For some, the year 2020 has been rife with reading, podcast listening, or late-night binging of the latest Netflix series. For me, nothing says departure from reality better than building an artificially intelligent agent to outperform a duo of ghosts who are hell-bent on eating it. Yes, you guessed correctly, I\u2019m talking Pac-man and I\u2019m talking AI. Let\u2019s get started!", "The game will be played on a tiled game board containing both our characters and objects. Time in this context works in a discrete manner rather than continuous, and as a consequence, the following discussion will refer to time in step (e.g. the kth time step). Within this world, there exists:", "The interactions between these characters and the world around them are encapsulated within a complete board representation of the game \u2014 otherwise known as the state. Each state represents the world at a specific point in time and any action performed by either Pac-man or the ghosts will result in a change of state. The actions available to either Pac-man or the ghosts are movement-based actions that dictate whether the entity moves up, down, left, or right. Furthermore, like any good game, there exists a point system that is altered slightly from the traditional game.", "Inky and Clyde, in this version of Pac-man, are not particularly bright and traverse the game board with random behavior. Although this does make it easier for Pac-man to win the game, the objective is two-fold:", "The solution below will tackle both of these goals without running a single search algorithm.", "A Markov Decision Process (MDP) is a formal way of modeling a scenario where there exists a set of states and actions. Committing to an action results in a different state with some specified probability. That is to say, there exists a transition function that describes the probability of arriving at a state, s\u2019, given both the previous state, s, and the action chosen, a.", "Each state also has a reward associated with it and hence a reward function. The reward value depends on the previous state, action, and resulting state.", "To be able to describe the beginning and end of this process, there is also a start and terminal state. There need not always be a terminal state but that is a construction for a different context.", "As the name suggests, an MDP has the Markov property which says that the conditional probability of the future state depends strictly on the present state. Within this construction, this can be written as:", "The goal for these types of processes is to find the most optimal policy. A policy is a set of instructions that the AI agent follows when in any given state. This is done by an iterative method known as value iteration. The algorithm is quite an elegant solution for search problems due to the fact that it is guaranteed to converge to an optimal policy. The algorithm hinges on the computation of the Bellman equation:", "The interpretation of the above equation is a lot simpler than it looks. What it is being computed is the value of a state, s, at the (k+1)th time step. The value of a state is the reward associated with the transition to a new state, s\u2019, and the consequent rewards that will be received from the new state. However, since these rewards are future rewards, they are worth less to Pac-man right now, and hence it uses a discount factor, \ud835\udefe, to represent the diminishing return. Since the new state, s\u2019, is uncertain, the equation takes the average of each possible new state and weights them based on the transition probability. Finally, the reason to take the maximum over each possible action is that there exists an optimistic view by defining the value of a state not by it\u2019s worst outcome but by its best.", "The algorithm stems out of the above explanation and is given as follows:", "Over time, the optimal choice of action will converge to identify each state\u2019s optimal policy.", "So you might be asking yourself, how does this relate to Pac-man? The Pac-man game is set up akin to the MDP. There exist states that encapsulating the game at any given point in time, actions that Pac-man can choose to take and rewards in the form of points as the agent arrives at a new state. However, the above construction of iterating to find an optimal policy is not true learning but rather a method to simulate a scenario and find the best plan. In that sense, it is not that different from a search algorithm and I am not about to break my promise of no searches behind a fancy facade. So, the construction needs a bit more\u2026", "What was described above is known as an \u2018offline\u2019 solution which formulates an optimal policy given a transition function and rewards. Typically, it\u2019s either impossible or very difficult to accurately define and as such, the agent is required to learn the true distribution rather than have it unrealistically provided. So, there is a need for an \u2018online\u2019 solution that knows nothing beyond the state it is in and the actions it can take.", "Optimizing a Q-function, as opposed to the value iteration above, is a solution to the problem stated. Why? The equation for Value Iteration is the expected cost and reward peer state. A Q-function on the other hand is the expected cost and reward when you are in a state, s, and apply an action, a. By doing so, as the agent explores the actions available at each state, the consequent reward will be understood for next time. The implementation takes a new sample whose value adheres to the Bellman equation and nudges the previous Q-value towards the new experienced value. Mathematically this is described as follows:", "In the implementation, the \u237a is the learning rate and the value can be interpreted as how sensitive the model is to the presence of new samples. The higher the learning the rate the more new information is valued because the value is altered considerably more when a new sample is observed.", "Although this method does approximate the transition probabilities well after high enough repetition, one clear problem is the choice between exploration and exploitation. If the protagonist decides on an action, a, at a state s, and realises a positive reward then what will stop Pac-man from repeating this same action at a later point in time. Within this current construction, nothing. This is why an exploration rate is introduced, . It forces Pac-man to choose a random move every \u03f5 proportion of the time. Initially, \u03f5=1, which means that Pac-man will always choose an action at random because it does not know anything about the game board, and hence it will allow Pac-man to make a fair number of good and bad decisions. Over time, \u03f5 will converge to 0 to reflect growing knowledge of the surrounding game and the lack of requirement to explore new actions. It is important to be wary of the implications of \u03f5 converging too fast or too slow. Converging too fast will result in Pac-man not taking enough actions at a specific state and as a result, the estimated transition function will be inaccurate. On the other hand, converging too slow will result in sub-optimal decisions being made even after many iterations of learning as a result of random decisions being made.", "In a typical fixed state scenario where each state has a fixed reward and the terminal states are fixed, the above construction will be enough to traverse the board and optimise rewards over time. However, Pac-man is a more dynamic game. There are a total of 3 terminal cases:", "Furthermore, once Pac-man eats a Pellet or Power Pellet and realises the reward of that action it can no longer devour that same Pellet or Power Pellet. These facts give Pac-man a much more dynamic nature and hence requires us to encode these \u2018features\u2019 of the game into the learning model. In doing so, it can also reduce the complexity of the model from a state-based learning model to a feature-based learning model.", "The features used to simplify the game:", "In doing so, approximate Q-learning is approximated and summarises all the possible scenarios into 4 features.", "As a result, an appropriate alteration to the algorithm is made to learn each \u03b2 weight for each feature. The weight is representative of the importance that Pac-man will place on each feature. As a result, we make the appropriate alterations to the algorithm such that the importance of each feature, \u03b2, is being learned.", "Upon conclusion, we now have a protagonist that traverses the world around it and builds the intuition to evade the antagonist while maximising the rewards in its lifetime.", "Suppose we train this model with 1000 iterations and the following parameters:", "I hope the above demonstration of our yellow friend is equally as satisfying to you as it is to me. None of this would be possible without the Pac-man game engine, which has been adapted to be compatible with Python 3, from UC Berkley. The process of building and explaining my solution has been invaluable despite there being plenty of next steps. These include:", "I hope that readers have been captivated by my first blog and been able to process the overall idea of reinforcement learning as well as the details. Please leave your comments and feedback. Thank you!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Application Development Analyst at Accenture | BSc from the University of Melbourne (Data Science and Mathematics Major)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8e51409f4fc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sarthakdaswork?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sarthakdaswork?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Sarthak Das"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe18a34f1c795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&user=Sarthak+Das&userId=e18a34f1c795&source=post_page-e18a34f1c795----8e51409f4fc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8e51409f4fc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----8e51409f4fc---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/pacman?source=post_page-----8e51409f4fc---------------pacman-----------------", "anchor_text": "Pacman"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----8e51409f4fc---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----8e51409f4fc---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&user=Sarthak+Das&userId=e18a34f1c795&source=-----8e51409f4fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&user=Sarthak+Das&userId=e18a34f1c795&source=-----8e51409f4fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8e51409f4fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8e51409f4fc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8e51409f4fc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8e51409f4fc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8e51409f4fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sarthakdaswork?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sarthakdaswork?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sarthak Das"}, {"url": "https://medium.com/@sarthakdaswork/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe18a34f1c795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&user=Sarthak+Das&userId=e18a34f1c795&source=post_page-e18a34f1c795--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe18a34f1c795%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforced-pac-man-8e51409f4fc&user=Sarthak+Das&userId=e18a34f1c795&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}