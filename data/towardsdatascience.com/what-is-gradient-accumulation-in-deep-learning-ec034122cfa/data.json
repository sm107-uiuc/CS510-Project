{"url": "https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa", "time": 1683003186.249964, "path": "towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa/", "webpage": {"metadata": {"title": "What is Gradient Accumulation in Deep Learning? | by Raz Rotenberg | Towards Data Science", "h1": "What is Gradient Accumulation in Deep Learning?", "description": "Examining the backpropagation process of neural networks and the algorithm of gradient accumulation mechanism"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce?source=friends_link&sk=74a7a2793da909c1194c0add818c7fd3", "anchor_text": "another article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/how-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60?source=friends_link&sk=ff9137c1c7fa5bbfc4c4e09bacc0273b", "anchor_text": "another article", "paragraph_index": 19}], "all_paragraphs": ["In another article, we addressed the problem of batch size being limited by GPU memory, and how gradient accumulation helps in overcoming this.", "In this post, we will first examine the backpropagation process of a neural network and then go through the technical and algorithmic details of gradient accumulation. We will discuss how it works, and iterate through an example.", "Gradient accumulation is a mechanism to split the batch of samples \u2014 used for training a neural network \u2014 into several mini-batches of samples that will be run sequentially.", "Before further going into gradient accumulation, it will be good to examine the backpropagation process of a neural network.", "A deep-learning model consists of many layers, connected to each other, in all of which the samples are propagating through the forward pass in every step. After propagating through all the layers, the network generates predictions for the samples and then calculates the loss value for every sample, which specifies \u201chow wrong was the network for this sample?\u201d. The neural network then computes the gradients of those loss values with respect to the model parameters. Then, these gradients are used for calculating the updates for the respective variables.", "When building the model, we choose an optimizer, which is responsible for the algorithm used for minimizing the loss. The optimizer can be one of the common optimizers that are already implemented in the framework (SGD, Adam, etc\u2026), or a custom optimizer, implementing the desired algorithm. Along with the gradients, there may be more parameters that the optimizer would manage and use for calculating the updates, such as the learning rate, the current step index (for adaptive learning rate), momentums, etc\u2026", "The optimizer represents a mathematical formula that computes the parameter updates. A simple example would be the stochastic gradient descent (SGD) algorithm: V = V \u2014 (lr * grad), where V is any trainable model parameter (weight or bias), lr is the learning rate, and grad is the gradients of the loss with respect to the model parameter:", "Gradient accumulation means running a configured number of steps without updating the model variables while accumulating the gradients of those steps and then using the accumulated gradients to compute the variable updates.", "Running some steps without updating any of the model variables is the way we \u2014 logically \u2014 split the batch of samples into a few mini-batches. The batch of samples that is used in every step is effectively a mini-batch, and all the samples of those steps combined are effectively the global batch.", "By not updating the variables at all those steps, we cause all the mini-batches to use the same model variables for calculating the gradients. This is mandatory to ensure the same gradients and updates are calculated as if we were using the global batch size.", "Accumulating the gradients in all of these steps results in the same sum of gradients as if we were using the global batch size.", "So, let\u2019s say we are accumulating gradients over 5 steps. We want to accumulate the gradients of the first 4 steps, without updating any variable. At the fifth step, we want to use the accumulated gradients of the previous 4 steps combined with the gradients of the fifth step to compute and assign the variable updates. Let\u2019s see it in action:", "Starting at the first step, all the samples of the first mini-batch propagate through the forward and backward passes, resulting in computed gradients for each trainable model variable. We don\u2019t want to actually update the variables, so there is no need in computing the updates at this point. What we need, though, is a place to store the gradients of the first step, in order for them to be accessible in the following steps, and we will use another variable for each trainable model variable, to hold the accumulated gradients. So, after computing the gradients of the first step, we will store them in the variables we created for the accumulated gradients.", "Now the second step starts, and again, all the samples of the second mini-batch propagate through all the layers of the model, computing the gradients of the second step. Just like the step before, we don\u2019t want to update the variables yet, so there is no need in computing the variable updates. What\u2019s different than the first step though, is that instead of just storing the gradients of the second step in our variables, we are going to add them to the values stored in the variables, which currently hold the gradients of the first step.", "Steps 3 and 4 are pretty much the same as the second step, as we are not yet updating the variables, and we are accumulating the gradients by adding them to our variables.", "Then, in step 5, we do want to update the variables, as we intended to accumulate the gradients over 5 steps. After computing the gradients of the fifth step, we will add them to the accumulated gradients, resulting in the sum of all the gradients of those 5 steps.", "We\u2019ll then take this sum and insert it as a parameter to the optimizer, resulting in the updates computed using all the gradients of those 5 steps, computed over all the samples in the global batch.", "If we will take the SGD optimizer as an example, let\u2019s see the variables after the updates at the end of the fifth steps, computed using the gradients of those 5 steps (N=5 in the following example):", "It is possible to implement a gradient-accumulated version of any optimizer. Each optimizer has a different formula and therefore will require a different implementation. This is not optimal, as gradient accumulation is a general approach and should be optimizer-independent.", "In another article, we cover the way in which we implemented a generic gradient accumulation mechanism and show you how you could use it in your own models using any optimizer of your choice.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Programmer. I like technology, music, and too many more things."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fec034122cfa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec034122cfa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----ec034122cfa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327----ec034122cfa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@austris_a?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Austris Augusts"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce?source=friends_link&sk=74a7a2793da909c1194c0add818c7fd3", "anchor_text": "another article"}, {"url": "https://towardsdatascience.com/how-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60?source=friends_link&sk=ff9137c1c7fa5bbfc4c4e09bacc0273b", "anchor_text": "another article"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ec034122cfa---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ec034122cfa---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ec034122cfa---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gpu?source=post_page-----ec034122cfa---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/keras?source=post_page-----ec034122cfa---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&user=Raz+Rotenberg&userId=4f915c067327&source=-----ec034122cfa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&user=Raz+Rotenberg&userId=4f915c067327&source=-----ec034122cfa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fec034122cfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ec034122cfa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ec034122cfa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ec034122cfa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ec034122cfa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ec034122cfa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ec034122cfa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/@raz.rotenberg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F68478d8b203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-accumulation-in-deep-learning-ec034122cfa&newsletterV3=4f915c067327&newsletterV3Id=68478d8b203&user=Raz+Rotenberg&userId=4f915c067327&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}