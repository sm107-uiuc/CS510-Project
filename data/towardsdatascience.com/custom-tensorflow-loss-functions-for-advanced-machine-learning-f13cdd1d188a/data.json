{"url": "https://towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a", "time": 1682994428.197948, "path": "towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a/", "webpage": {"metadata": {"title": "Custom TensorFlow Loss Functions for Advanced Machine Learning | by Haihan Lan | Towards Data Science", "h1": "Custom TensorFlow Loss Functions for Advanced Machine Learning", "description": "[NOTE: Tensorflow 2.0 is now released and is quite different from the 1.x version of TF that this article is based on. I am still researching TF 2.0 w.r.t custom losses and tensor manipulation as\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/community/roadmap", "anchor_text": "Tensorflow 2.0", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier", "anchor_text": "Gaussian Process Classifier", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler Divergence", "paragraph_index": 12}], "all_paragraphs": ["In this article, we\u2019ll look at:", "[NOTE: Tensorflow 2.0 is now released and is quite different from the 1.x version of TF that this article is based on. I am still researching TF 2.0 w.r.t custom losses and tensor manipulation as outlined in this article and may write a new article or update this one. Please keep this in mind going forward.]", "The beaten path of machine learning involves journeying to familiar landmarks and scenic locales. One set of familiar landmarks are predefined loss functions that give you a suitable loss value for the problem you are trying to optimize over. We\u2019re familiar with the cross-entropy loss for classification and the mean squared error (MSE) or root-mean square error (RMSE) for regression problems. Popular ML packages including front-ends such as Keras and back-ends such as Tensorflow, include a set of basic loss functions for most classification and regression tasks. But off the beaten path there exist custom loss functions you may need to solve a certain problem, which are constrained only by valid tensor operations.", "In Keras you can technically create your own loss function however the form of the loss function is limited to some_loss(y_true, y_pred) and only that. If you tried to add additional parameters to the loss in the form of some_loss_1(y_true, y_pred, **kwargs), Keras will throw a runtime exception and you lose the compute time that went into aggregating the datasets. There are hacks to work around this, but in general we want a scalable way to write a loss function accepting any valid arguments we pass to it, and operates on our tensors in an standard and expected way. We\u2019ll see how to use Tensorflow directly to write a neural network from scratch and build a custom loss function to train it.", "Tensorflow (TF) is a symbolic and numeric computation engine that allows us to string tensors* together into computational graphs and do backpropogation over them. Keras is an API or front-end running on top of Tensorflow that conveniently packages standard constructs built using Tensorflow (such as various pre-defined neural net layers) and abstracts many of the low level mechanics of TF from the programmer (Keras can run on top of Theano as well, the same concepts apply). However, in the process of making these constructs \u2018off-the-shelf\u2019, granular level control and the power to do very specific things is lost.", "*For simplicity\u2019s sake, tensors are multi-dimensional arrays with a shape tuple like (feature_dim, n_features)", "One example is the ability to define custom loss functions accepting an arbitrary number of parameters, and can compute losses with arbitrary tensors internal to the network and input tensors external to the network. Strictly speaking, a loss function in TF does not even need to be a python function, but only a valid combination of operations on TF tensor objects. The previous point is important because the power of custom losses comes from the ability to compute losses over arbitrary tensors, not strictly just your supervised target tensor and the network output tensor, in the form of (y_true, y_pred).", "Before we get to custom losses, let\u2019s briefly review a basic 2-layer dense net (MLP) and see how it\u2019s defined and trained in TF. Although there are predefined TF layers, let\u2019s define the layers from scratch from weights and bias tensors. We want to get familiar with the placeholder and variable tensors in TF.", "That was pretty cool. The code above can be modified for multi-class classification by replacing the loss with softmax_cross_entropy_with_logits and the final sigmoid activation replaced with a tf.nn.softmax.", "Next to demonstrate how to use custom loss functions with arbitrary tensors, let\u2019s implement a knowledge distillation model which optimizes over the binary classification loss as well as a loss between the model being trained and a reference model. Knowledge distillation is a form of transfer learning where we learn with our target model (model we want to train) but also indirectly transfer knowledge representations from a reference model. We\u2019ll use a Gaussian Process Classifier (GPC) from sklearn as our reference model. We\u2019ll also make the problem more interesting by reducing our training data to only 1% of the samples from the 569 in the sklearn breast cancer dataset\u2026 for both the reference and the target, and train them both from scratch.* This is known as a few-shot learning problem.", "*In traditional transfer learning the reference model is usually a wide and/or deep network pre-trained on many examples/classes and the target is a narrower/shallower network to be trained on few available specific examples/classes.", "The loss for this knowledge distillation scheme looks something like", "The binary cross entropy loss is just the regular binary classification loss, the second term involves another loss D between the outputs of the target f(x) and the reference g(x). We\u2019ll let D be the Kullback-Leibler Divergence (DKL) between f(x) and g(x):", "The DKL in a nutshell quantifies how different a distribution f is from g, in terms of information (roughly information is inversely proportional to certainty); it can be thought of as a cross entropy between distributions, and is an asymmetric loss that can take negative values*. By minimizing the DKL between f and g we basically want to to increase the information content of f relative to g. When f and g have the same amount of information, the log term above is 0 and the DKL loss is 0 as well. Using the DKL as the loss makes sense when using GPCs as a reference model, since when making predictions form the GPC, we are sampling from its posterior distribution (a softmax) and although our neural net is a crude approximation of this posterior, it is also a distribution.", "*In our implementation below, we take abs( D_KL( f(x), g(x) ) ) in our combined loss. Theoretically D_KL will always be non-negative due to the log-sum inequality but computing D_KL with round-off errors on a real computer can lead to negative values.", "Notice how we need to now feed external inputs g(x) to our loss. In Keras this process is contrived and unscalable. But in TF, it is as simple as creating a new placeholder tensor, adding the necessary terms to the combined loss, and feeding the inputs when running a training or prediction session.", "Having only 5 training samples, the above example converges faster and gives better raw test accuracy than a net where the DKL loss is set to zero (i.e. no transfer learning). Note, test set imbalance was not accounted for! The astute reader should add an F1 score to the accuracy readout. Thankfully our pseudo-random sample of the dataset gives a 2:3 split of positive vs negative classes.", "Also interesting to note is the softmax outputs of the transfer learning model tested on 100 examples from the holdout:", "The predictions of the transfer learning model reflect the uncertainty given the limited information it was trained on. There is a very good reason why a GPC was chosen as the reference model, after all how can we be so sure a new patient has cancer when we trained a classifier on only 5 examples from scratch?", "We saw how to implement a neural net from scratch in TF, how to combine tensor operations into loss functions, and touched on an interesting application of transfer learning. In general TF is far more flexible for the Data Scientist in advanced or niche supervised learning applications. If you enjoyed reading this article and using the code, please check out my other articles!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff13cdd1d188a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hhl60492?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhl60492?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Haihan Lan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70a936a57085&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&user=Haihan+Lan&userId=70a936a57085&source=post_page-70a936a57085----f13cdd1d188a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/deep-kernels-and-gaussian-processes-for-few-shot-learning-38a4ac0b64db", "anchor_text": "Deep Kernel Transfer and Gaussian Processes"}, {"url": "https://towardsdatascience.com/decision-trees-and-random-forests-for-classification-and-regression-pt-1-dbb65a458df", "anchor_text": "Random forests"}, {"url": "https://towardsdatascience.com/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932", "anchor_text": "Softmax classification"}, {"url": "https://towardsdatascience.com/analyzing-climate-patterns-with-self-organizing-maps-soms-8d4ef322705b", "anchor_text": "Climate analysis"}, {"url": "https://www.tensorflow.org/community/roadmap", "anchor_text": "Tensorflow 2.0"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier", "anchor_text": "Gaussian Process Classifier"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler Divergence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f13cdd1d188a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----f13cdd1d188a---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f13cdd1d188a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----f13cdd1d188a---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f13cdd1d188a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&user=Haihan+Lan&userId=70a936a57085&source=-----f13cdd1d188a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&user=Haihan+Lan&userId=70a936a57085&source=-----f13cdd1d188a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff13cdd1d188a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f13cdd1d188a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f13cdd1d188a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhl60492?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhl60492?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Haihan Lan"}, {"url": "https://medium.com/@hhl60492/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "371 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70a936a57085&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&user=Haihan+Lan&userId=70a936a57085&source=post_page-70a936a57085--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F376c65fc851a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a&newsletterV3=70a936a57085&newsletterV3Id=376c65fc851a&user=Haihan+Lan&userId=70a936a57085&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}