{"url": "https://towardsdatascience.com/an-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d", "time": 1683011612.3165922, "path": "towardsdatascience.com/an-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d/", "webpage": {"metadata": {"title": "An Implementation of DBSCAN on PySpark | by Salil Jain | Towards Data Science", "h1": "An Implementation of DBSCAN on PySpark", "description": "DBSCAN is a well-known clustering algorithm that has stood the test of time. Though the algorithm is not included in Spark MLLib. There are a few implementations (1, 2, 3) though they are in scala\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/docs/latest/ml-clustering.html", "anchor_text": "Spark MLLib", "paragraph_index": 0}, {"url": "https://github.com/alitouka/spark_dbscan", "anchor_text": "1", "paragraph_index": 0}, {"url": "https://github.com/mraad/dbscan-spark", "anchor_text": "2", "paragraph_index": 0}, {"url": "https://github.com/irvingc/dbscan-on-spark", "anchor_text": "3", "paragraph_index": 0}, {"url": "https://github.com/htleeab/DBSCAN-pyspark", "anchor_text": "PySpark", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Spatial_index", "anchor_text": "indexing structure", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Fixed-radius_near_neighbors", "anchor_text": "neighborhood query", "paragraph_index": 2}, {"url": "https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html", "anchor_text": "sklearn", "paragraph_index": 34}], "all_paragraphs": ["DBSCAN is a well-known clustering algorithm that has stood the test of time. Though the algorithm is not included in Spark MLLib. There are a few implementations (1, 2, 3) though they are in scala. Implementation in PySpark uses the cartesian product of rdd to itself which results in O(n\u00b2) complexity and possibly O(n\u00b2) memory before the filter.", "For a quick primer on the complexity of the DBSCAN algorithm:", "DBSCAN visits each point of the database, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighborhood query in O(log n), an overall average runtime complexity of O(n log n) is obtained (if parameter \u03b5 is chosen in a meaningful way, i.e. such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g. all points within a distance less than \u03b5), the worst case run time complexity remains O(n\u00b2). The distance matrix of size (n\u00b2-n)/2 can be materialized to avoid distance recomputations, but this needs O(n\u00b2) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.", "In this post, we will explore how we can implement DBSCAN in PySpark efficiently without using O(n\u00b2) operations by reducing the number of distance calculations. We would implement an indexing/partitioning structure based on Triangle Inequality to achieve this.", "Let\u2019s refresh triangle inequality. If there are three vertices of the triangle a, b and c, and given distance metric d. Then", "In DBSCAN there is a parameter \u03b5, which is used to find the linkage between points. Now, let us use this parameter to see if we can use triangle inequality to reduce the number of operations.", "Let\u2019s say there are four points x, y, z, and c.", "What we can deduce from above is that if we compute distances of all points from c then we can filter points y and z using above criteria. We can compute distances from c and partition points in concentric rings (center being c).", "From the above lemmas, we can see that if", "Here is a depiction how it would look like. Two-dimensional space is divided into quantiles of \u03b5 euclidean distance. Green rings show the partition. x1 is at the (m+1/2)\u03b5 distance from c (center of (m-1)\u03b5 \u2014 (m+2)\u03b5 partition). x2 and x3 are at m\u03b5 and (m+1)\u03b5 distance from c. It is clear that for x1, x2, and x3 all points of relevance (within the circle of radius \u03b5) are within the partition.", "If we create mutually exclusive partitions and compute distances among points within that partition, it would be incomplete. For example, x4 and x5\u2019s range circle will overlap two partitions. Hence the need for overlapping partition. One strategy could be to move the partition by \u03b5. Though in that case if partition width is 3\u03b5, then one point may occur in three different partitions. Instead, partitions are created of 2\u03b5 width and move them by \u03b5. In this case, one point may occur only in two partitions.", "The above two images show how this partition scheme works. Two partitions in combination allow a range query of \u03b5 radius for all the points from m\u03b5 to (m+1)\u03b5. The first partition covers all the points from m\u03b5 to (m+1/2)\u03b5 (x2 covered but x3 not) and the second covers (m+1/2)\u03b5 to (m+1)\u03b5 (x3 covered but x2 not).", "Let us see how these partitions look like on some generated data.", "The above data and image are generated by the following code:", "The above partitions from data are generated by the following code:", "partition_index identifies each partition. Each data point is put into two partitions as discussed before based on the distance from c (pivot) and \u03b5. distance method handles one point at a time. In PySpark flatMap method is used to map every point into an array of tuples (out).", "All data points within a partition are merged before generating the visualization. They also need to be merged for further processing DBSCAN on PySpark.", "reduceByKey method is used to merge partition data in one. The use of word partition may confuse with PySpark partitions but those are two separate things. Though partitionBy method could be used to reconcile that as well.", "After reduceByKey, we will get each row of the rdd as a ring depicted in figure 4. As you can see there is an overlap so points would be in two rows of rdd and that is intentional.", "The above code computes distance within each partition. The output of the method is a list of tuples. Each tuple has the id of point and set of its neighbors within \u03b5 distance. As we know that point would occur in two partitions so we need to combine the sets for a given point so we get all of its neighbors within \u03b5 distance in whole data. reduceByKey is used to combine the sets by doing union operation on them.", "Combined code till now looks as follows:", "Once we have neighbors within \u03b5 distance for a point we can identify if it is a core or border point.", "Core Point: There are at least min_pts within \u03b5 distance", "Border Point: There are less than min_pts within \u03b5 distance but one of them is the core point.", "To identify core and border points, first, we assign them to a cluster. For each point which is a core point, we create a cluster label the same to its id (assuming the ids are unique). We create a tuple for each core point and its neighbors of the form (id, [(cluster_label, is_core_point)]). All the neighbors in this scenario would be labeled as a base point. Let's take an example", "Input is a tuple where 3 is the id of point and (4, 6, 2) are its neighbors within \u03b5 distance.", "As one can see all points are assigned cluster label 3. While 3 is assigned True for is_core_point as a core point and all other points are considered base points and assigned False for is_core_point.", "We may have similar input tuples for 4, 6, and 2 where they may or may not be assigned as core points. The idea is to eventually combine all cluster labels for a point and if at least one of the assignment for is_core_point is True then its a core point otherwise its a border point.", "We combine all (cluster_label, is_core_point) tuples for a point using reduceByKey method and then investigate if its a core point or not while combining all clusters labels for that point. If its a border point then we would only leave one cluster label for it.", "The above method is used to combine all cluster label for a point. Again if it is a border point then we return only 1st cluster label.", "Code in PySpark till now looks like the following:", "Now for each point, we have cluster labels. If a point has more than one cluster label then it means those clusters are connected. Those connected clusters are the final clusters we need to solve DBSCAN. We solve this by creating a graph with vertices as cluster labels and edges between cluster labels if they are assigned to the same point.", "In the above code, combine_cluster_rdd is a collection of rows where each row is a tuple (point, cluster_labels). Each cluster label is vertices and combinations of cluster labels for a point are the edges. Connected components of this graph give a mapping between each cluster label and a connected cluster. which we can apply to points to get final clusters.", "Above is how the final method looks like which returns a Spark Dataframe with point id, cluster component label, and a boolean indicator if its a core point.", "Now I compare the results in terms of accuracy with sklearn implementation of DBSCAN.", "The make_blobs method creates blobs around three input centers. Running DBSCAN using sklearn and my implementation with \u03b5=0.3 and min_pts=10 gives the following results.", "Core points are bigger circles while border points are smaller ones. Noise points are colored black which is the same in both implementations. One thing that jumps out is the border points are assigned different clusters that speak to the non-deterministic nature of DBSCAN. My other post also talks about it.", "For \u03b5=0.2 we get the border points assigned to the same clusters. Following is some code and results on data in rings.", "For n=750, the number of distance operations required by a simple implementation of DBSCAN would be n(n-1)/2 which is 280875. As we create the partition based on \u03b5, the smaller the \u03b5 less the number of distance operations would be needed. In this case, there were a total of 149716(\u03b5=.2) and 217624(\u03b5=0.3) operations needed.", "A pyspark implementation which would be efficient based on the value of \u03b5 with the following steps:", "A comparison with the existing implementation shows the accuracy of the algorithm and the implementation of this post.", "On the local machine with both driver and worker nodes, implementation is slower than sklearn. There could be a couple of reasons which need to be investigated:", "Complete PySpark Implementation can be fount at:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3e2be646f57d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://salilkjain.medium.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": ""}, {"url": "https://salilkjain.medium.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Salil Jain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcf373ef1b80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&user=Salil+Jain&userId=fcf373ef1b80&source=post_page-fcf373ef1b80----3e2be646f57d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://spark.apache.org/docs/latest/ml-clustering.html", "anchor_text": "Spark MLLib"}, {"url": "https://github.com/alitouka/spark_dbscan", "anchor_text": "1"}, {"url": "https://github.com/mraad/dbscan-spark", "anchor_text": "2"}, {"url": "https://github.com/irvingc/dbscan-on-spark", "anchor_text": "3"}, {"url": "https://github.com/htleeab/DBSCAN-pyspark", "anchor_text": "PySpark"}, {"url": "https://github.com/htleeab/DBSCAN-pyspark/blob/master/DBSCAN.py", "anchor_text": "https://github.com/htleeab/DBSCAN-pyspark/blob/master/DBSCAN.py"}, {"url": "https://en.wikipedia.org/wiki/DBSCAN#Complexity", "anchor_text": "https://en.wikipedia.org/wiki/DBSCAN#Complexity"}, {"url": "https://en.wikipedia.org/wiki/Spatial_index", "anchor_text": "indexing structure"}, {"url": "https://en.wikipedia.org/wiki/Fixed-radius_near_neighbors", "anchor_text": "neighborhood query"}, {"url": "https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html", "anchor_text": "sklearn"}, {"url": "https://medium.com/@salilkjain/some-notes-on-dbscan-algorithm-61a2e9acce29", "anchor_text": "Some Notes on DBSCAN AlgorithmIn this post, I would like to discuss some of the insights on DBSCAN algorithm. Usually, when I look at an algorithm I\u2026medium.com"}, {"url": "https://github.com/SalilJain/pyspark_dbscan", "anchor_text": "SalilJain/pyspark_dbscanAn \"Efficient\" Implementation of DBSCAN on PySpark - SalilJain/pyspark_dbscangithub.com"}, {"url": "https://medium.com/tag/dbscan?source=post_page-----3e2be646f57d---------------dbscan-----------------", "anchor_text": "Dbscan"}, {"url": "https://medium.com/tag/clustering?source=post_page-----3e2be646f57d---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3e2be646f57d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3e2be646f57d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3e2be646f57d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&user=Salil+Jain&userId=fcf373ef1b80&source=-----3e2be646f57d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&user=Salil+Jain&userId=fcf373ef1b80&source=-----3e2be646f57d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3e2be646f57d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3e2be646f57d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3e2be646f57d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3e2be646f57d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3e2be646f57d--------------------------------", "anchor_text": ""}, {"url": "https://salilkjain.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://salilkjain.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Salil Jain"}, {"url": "https://salilkjain.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "160 Followers"}, {"url": "https://www.linkedin.com/in/jainsalil/", "anchor_text": "https://www.linkedin.com/in/jainsalil/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcf373ef1b80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&user=Salil+Jain&userId=fcf373ef1b80&source=post_page-fcf373ef1b80--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56fa7ca9e190&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-efficient-implementation-of-dbscan-on-pyspark-3e2be646f57d&newsletterV3=fcf373ef1b80&newsletterV3Id=56fa7ca9e190&user=Salil+Jain&userId=fcf373ef1b80&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}