{"url": "https://towardsdatascience.com/expectation-maximization-algorithm-explained-ffaf0655357e", "time": 1683015439.0470378, "path": "towardsdatascience.com/expectation-maximization-algorithm-explained-ffaf0655357e/", "webpage": {"metadata": {"title": "Expectation-Maximization Algorithm, Explained | by YANG Xiaozhou | Towards Data Science", "h1": "Expectation-Maximization Algorithm, Explained", "description": "A comprehensive guide to the EM algorithm with intuitions, examples, Python implementation, and maths. Read this article to understand EM and how to use it."}, "outgoing_paragraph_urls": [{"url": "https://yangxiaozhou.github.io/data/2020/10/20/EM-algorithm-explained.html", "anchor_text": "this article", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture", "anchor_text": "implementation", "paragraph_index": 10}, {"url": "https://askabiologist.asu.edu/peppered-moths-game/play.html", "anchor_text": "game", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Law_of_total_probability", "anchor_text": "total probability", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution#Probability_mass_function", "anchor_text": "distribution PDF", "paragraph_index": 50}, {"url": "https://amstat.tandfonline.com/doi/abs/10.1198/106186001317115045", "anchor_text": "EM", "paragraph_index": 65}, {"url": "https://academic.oup.com/biomet/article-abstract/80/2/267/251605", "anchor_text": "ECM", "paragraph_index": 65}, {"url": "http://yangxiaozhou.github.io/", "anchor_text": "http://yangxiaozhou.github.io/", "paragraph_index": 68}], "all_paragraphs": ["Yes! Let\u2019s talk about the expectation-maximization algorithm (EM, for short). If you are in the data science \u201cbubble\u201d, you\u2019ve probably come across EM at some point in time and wondered: What is EM, and do I need to know it?", "It\u2019s the algorithm that solves Gaussian mixture models, a popular clustering approach. The Baum-Welch algorithm essential to hidden Markov models is a special type of EM.", "It works with both big and small data; it thrives when there is missing information while other techniques fail.", "It\u2019s such a classic, powerful, and versatile statistical learning technique that it\u2019s taught in almost all computational statistics classes. After reading this article, you could gain a strong understanding of the EM algorithm and know when and how to use it.", "We start with two motivating examples (unsupervised learning and evolution). Next, we see what EM is in its general form. We jump back in action and use EM to solve the two examples. We then explain both intuitively and mathematically why EM works like a charm. Lastly, a summary of this article and some further topics are presented.", "This article is adapted from my blog post with derivations, proofs, and Python codes omitted. If you prefer LaTex-formatted maths or would like to get the Python codes for all the problems here, you can read this article on my blog.", "Maybe you already know why you want to use EM, or maybe you don\u2019t. Either way, let me use two motivating examples to set the stage for EM. These are quite lengthy, I know, but they perfectly highlight the common feature of the problems that EM is best at solving: the presence of missing information.", "Suppose you have a data set with n number of data points. It could be a group of customers visiting your website (customer profiling) or an image with different objects (image segmentation). Clustering is the task of finding out k natural groups for your data when you don\u2019t know (or don\u2019t specify) the real grouping. This is an unsupervised learning problem because no ground-truth labels are used.", "Such clustering problem can be tackled by several types of algorithms, e.g., combinatorial type such as k-means or hierarchical type such as Ward\u2019s hierarchical clustering. However, if you believe that your data could be better modeled as a mixture of normal distributions, you would go for Gaussian mixture model (GMM).", "The underlying idea of GMM is that you assume there\u2019s a data generating mechanism behind your data. This mechanism first chooses one of the k normal distributions (with a certain probability) and then delivers a sample from that distribution. Therefore, once you have estimated each distribution\u2019s parameters, you could easily cluster each data point by selecting the one that gives the highest likelihood.", "However, estimating the parameters is not a simple task since we do not know which distribution generated which points (missing information). EM is an algorithm that can help us solve exactly this problem. This is why EM is the underlying solver in scikit-learn\u2019s GMM implementation.", "Have you heard the phrase \u201cindustrial melanism\u201d before? Biologists coined the term in the 19th century to describe how animals change their skin color due to the massive industrialization in the cities. They observed that previously rare dark peppered moths started to dominate the population in coal-fueled industrialized towns. Scientists at the time were surprised and fascinated by this observation. Subsequent research suggests that the industrialized cities tend to have darker tree barks that disguise darker moths better than the light ones. You can play this peppered moth game to understand the phenomenon better.", "As a result, dark moths survive the predation better and pass on their genes, giving rise to a predominantly dark peppered moth population. To prove their natural selection theory, scientists first need to estimate the percentage of black-producing and light-producing genes/alleles present in the moth population. The gene responsible for the moth\u2019s color has three types of alleles: C, I, and T. Genotypes CC, CI, and CT produce dark peppered moth ( Carbonaria); TT produces light peppered moth ( Typica); II and IT produce moths with intermediate color ( Insularia).", "Here\u2019s a hand-drawn graph that shows the observed and missing information.", "We wish to know the percentages of C, I, and T in the population. However, we can only observe the number of Carbonaria, Typica, and Insularia moths by capturing them, but not the genotypes ( missing information). The fact that we do not observe the genotypes and multiple genotypes produce the same subspecies make the calculation of the allele frequencies difficult. This is where EM comes in to play. With EM, we can easily estimate the allele frequencies and provide concrete evidence for the micro-evolution happening on a human time scale due to environmental pollution.", "How does EM tackle the GMM problem and the peppered moth problem in the presence of missing information? We will illustrate these in the later section. But first, let\u2019s see what EM is really about.", "At this point, you must be thinking (I hope): All these examples are wonderful, but what is really EM? Let\u2019s dive into it.", "The EM algorithm is an iterative optimization method that finds the maximum likelihood estimate (MLE) of parameters in problems where hidden/missing/latent variables are present.", "It was first introduced in its full generality by Dempster, Laird, and Rubin (1977) in their famous paper\u00b9 (currently 62k citations). It has been widely used for its easy implementation, numerical stability, and robust empirical performance.", "Let\u2019s set up the EM for a general problem and introduce some notations. Suppose that Y are our observed variables, X are hidden variables, and we say that the pair (X, Y) is the complete data. We also denote any unknown parameter of interest as \u03b8 \u2208 \u0398. The objective of most parameter estimation problems is to find the most probable \u03b8 given our model and data, i.e.,", "where the term being maximized is the incomplete-data likelihood. Using the law of total probability, we can also express the incomplete-data likelihood as", "where the term being integrated is known as the complete-data likelihood.", "What\u2019s with all these complete- and incomplete-data likelihoods?", "In many problems, the maximization of the incomplete-data likelihood is difficult because of the missing information. On the other hand, it\u2019s often easier to work with complete-data likelihood.", "The EM algorithm is designed to take advantage of this observation. It iterates between an expectation step (E-step) and a maximization step (M-step) to find the MLE.", "Assuming \u03b8 with a superscript of (n) is the estimate obtained at the nth iteration, the algorithm iterates between the two steps as follows:", "The above definitions might seem hard-to-grasp at first. Some intuitive explanation might help:", "The algorithm iterates between these two steps until a stopping criterion is reached, e.g., when either the Q function or the parameter estimate has converged. The entire process can be illustrated in the following flowchart.", "That\u2019s it! With two equations and a bunch of iterations, you have just unlocked one of the most elegant statistical inference techniques!", "What we\u2019ve seen above is the general framework of EM, not the actual implementation of it. In this section, we will see step-by-step just how EM is implemented to solve the two previously mentioned examples. After verifying that EM does work for these problems, we then see intuitively and mathematically why it works in the next section.", "Suppose we have some data and would like to model the density of them.", "Are you able to see the different underlying distributions? Apparently, these data come from more than one distribution. Thus a single normal distribution would not be appropriate, and we use a mixture approach. In general, GMM-based clustering is the task of clustering (y1, \u2026, yn) data points into k groups. We let", "Thus, x_i is the one-hot coding of data y_i, e.g., x_i = [0, 0, 1] if k = 3 and y_i is from group 3. In this case, the collection of data points y is the incomplete data, and (x, y) is the augmented complete data. We further assume that each group follows a normal distribution, i.e.,", "Following the usual mixture Gaussian model set up, a new point is generated from the kth group with probability w_k, and the probabilities for all groups sum to 1. Suppose we are only working with the incomplete data y. The likelihood of one data point under a GMM is", "where the \u03c6(;\u03bc, \u03a3) is the PDF of a normal distribution with mean \u03bc and covariance \u03a3. The total log-likelihood of n points is", "In our problem, we are trying to estimate three groups of parameters: the group mixing probabilities (w) and each distribution\u2019s mean and covariance matrix (\u03bc, \u03a3). The usual approach to parameter estimation is by maximizing the above total log-likelihood function w.r.t. each parameter (MLE). However, this is difficult to do due to the summation inside the log term.", "Let\u2019s use the EM approach instead! Remember that we first need to define the Q function in the E-step, which is the conditional expectation of the complete-data log-likelihood. Since (x, y) is the complete data, the corresponding likelihood of one data point is", "and only the term with x_{ij} = 1 is active since it\u2019s a one-hot coding. Hence, our total complete-data log-likelihood is", "Denote \u03b8 as the collection of unknown parameters (w, \u03bc, \u03a3). Following the E-step formula in (2), we obtain the Q function as", "The z term above is the probability that data y_i is in class j with the current parameter estimates. This probability is also called responsibility in some texts. It means the responsibility of each class to this data point. It\u2019s also a constant given the observed data and current parameter estimates.", "Recall that the EM algorithm proceeds by iterating between the E-step and the M-step. We have obtained the latest iteration\u2019s Q function in the E-step above. Next, we move on to the M-step and find a new \u03b8 that maximizes the Q function in (6), i.e., we find", "A closer look at the obtained Q function reveals that it\u2019s actually a weighted normal distribution MLE problem. That means, the new \u03b8 has closed-form formulas and can be verified easily using differentiation:", "We go back to the opening problem in this section. I simulated 400 points using four different normal distributions. FIGURE 5 is what we see if we do not know the underlying true groupings. We run the EM procedure as derived above and set the algorithm to stop when the log-likelihood does not change anymore.", "In the end, we found the mixing probabilities and all four group\u2019s means and covariance matrices. FIGURE 6 below shows the density contours of each distribution found by EM superimposed on the data, which are now color-coded by their ground-truth groupings. Both the locations (means) and the scales (covariances) of the four underlying normal distributions are correctly identified. Unlike k-means, EM gives us both the clustering of the data and the generative model (GMM) behind them.", "We return to the population genetics problem mentioned earlier. Suppose we captured n moths and of which there are three different types: Carbonaria, Typica, and Insularia. However, we do not know the genotype of each moth except for Typica moths, see FIGURE 3 above. We wish to estimate the population allele frequencies. Let\u2019s speak in EM terms. Here\u2019s what we know:", "2. Unobserved: the number of different genotypes", "3. But we do know the relationship between them:", "4. Parameter of interest: allele frequencies", "There\u2019s another important modeling principle that we need to use: the Hardy-Weinberg principle, which says that the genotype frequency is the product of the corresponding allele frequency or double that when the two alleles are different. That is, we can expect the genotype frequencies to be", "Good! Now we are ready to plug in the EM framework. What\u2019s the first step?", "Just like the GMM case, we first need to figure out the complete-data likelihood. Notice that this is actually a multinomial distribution problem. We have a population of moths, the chance of capturing a moth of genotype CC is p_{C}\u00b2, similarly for the other genotypes. Therefore, the complete-data likelihood is just the multinomial distribution PDF:", "And the complete-data log-likelihood can be written in the following decomposed form:", "Remember that the E-step is taking a conditional expectation of the above likelihood w.r.t. the unobserved data Y, given the latest iteration\u2019s parameter estimates. The Q function is found to be", "where n_{CC}^(n) is the expected number of CC type moth given the current allele frequency estimates, and similarly for the other types. k() is a function that does not involve \u03b8.", "Since we obtained the expected number of each phenotype, estimating the allele frequencies is easy. Intuitively, the frequency of allele C is calculated as the ratio between the number of allele C present in the population and the total number of alleles. This works for the other alleles as well. Therefore, in the M-step, we obtain", "In fact, we could obtain the same M-step formulas by differentiating the Q function and setting them to zero (usual optimization routine).", "Let\u2019s try solving the peppered moth problem using the above derived EM procedure. Suppose we captured 622 peppered moths. 85 of them are Carbonaria, 196 of them are Insularia, and 341 of them are Typica. We run the EM iterations for 10 steps, FIGURE 7 shows that we obtain converged results in less than five steps.", "Estimating the allele frequencies is difficult because of the missing phenotype information. EM helps us to solve this problem by augmenting the process with exactly the missing information. If we look back at the E-step and M-step, we see that the E-step calculates the most probable phenotype counts given the latest frequency estimates; the M-step then calculates the most probable frequencies given the latest phenotype count estimates. This process is evident in the GMM problem as well: the E-step calculates the class responsibilities for each data given the current class parameter estimates; the M-step then estimates the new class parameters using those responsibilities as the data weights.", "Working through the previous two examples, we see clearly that the essence of EM lies in the E-step/M-step iterative process that augments the observed information with the missing information. And we see that it indeed finds the MLEs effectively. But why does this iterative process work? Is EM just a smart hack, or is it well-supported by theory? Let\u2019s find out.", "We start by gaining an intuitive understanding of why EM works.", "EM solves the parameter estimation problem by transferring the task of maximizing incomplete-data likelihood (difficult) to maximizing complete-data likelihood (easier) in some small steps.", "Imagine you are hiking up Mt. Fuji \ud83d\uddfb for the first time. There are nine stations to reach before the summit, but you do not know the route. Luckily, there are hikers coming down from the top, and they can give you a rough direction to the next station. Therefore, here\u2019s what you can do to reach the top: start at the base station and ask people for the direction to the second station; go to the second station and ask the people there for the path to the third station, and so on. At the end of the day (or start of the day, if you are catching sunrise \ud83c\udf04), there\u2019s a high chance you\u2019ll reach the summit.", "That\u2019s very much what EM does to find the MLEs for problems where we have missing data. Instead of maximizing ln p(x) (find the route to the summit), EM maximizes the Q function and finds the next \u03b8 that also increases ln p(x) (ask the direction to the next station). FIGURE 8 below illustrates this process in two iterations. Note that the G function is just a combination of the Q function and a few other terms constant w.r.t. \u03b8. Maximizing the G function w.r.t. \u03b8 is equivalent to maximizing the Q function.", "In this article, we see that EM converts a difficult problem with missing information to an easy problem through the optimization transfer framework. We also see EM in action by solving step-by-step two problems with Python implementation (Gaussian mixture clustering and peppered moth population genetics). More importantly, we see that EM is not just a smart hack but has solid mathematical groundings on why it would work.", "I hope this introductory article has helped you a little in getting to know the EM algorithm. From here, if you are interested, consider exploring the following topics.", "Digging deeper, the first question you might ask is: So, is EM perfect? Of course, it\u2019s not. Sometimes, the Q function is difficult to obtain analytically. We could use Monte Carlo techniques to estimate the Q function, e.g., check out Monte Carlo EM. Sometimes, even with complete-data information, the Q function is still difficult to maximize. We could consider alternative maximizing techniques, e.g., see expectation conditional maximization ( ECM). Another disadvantage of EM is that it provides us with only point estimates. In case we want to know the uncertainty in these estimates, we would need to conduct variance estimation through other techniques, e.g., Louis\u2019s method, supplemental EM, or bootstrapping.", "Thanks for reading! Please consider leaving feedback for me below. If you are interested in more statistical learning stuff, feel free to take a look at my other articles:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Run, cycle, dive, and blog at http://yangxiaozhou.github.io/. But mostly working on my research in data-driven critical infrastructure system resilience."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fffaf0655357e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yangxiaozhou?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yangxiaozhou?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "YANG Xiaozhou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22b8d9e0d922&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&user=YANG+Xiaozhou&userId=22b8d9e0d922&source=post_page-22b8d9e0d922----ffaf0655357e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "GETTING STARTED"}, {"url": "https://unsplash.com/@hobz?utm_source=medium&utm_medium=referral", "anchor_text": "Gilles Desjardins"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://yangxiaozhou.github.io/data/2020/10/20/EM-algorithm-explained.html", "anchor_text": "this article"}, {"url": "https://commons.wikimedia.org/wiki/File:ClusterAnalysis_Mouse.svg", "anchor_text": "example"}, {"url": "https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture", "anchor_text": "implementation"}, {"url": "https://askabiologist.asu.edu/peppered-moths-game/play.html", "anchor_text": "game"}, {"url": "https://en.wikipedia.org/wiki/Law_of_total_probability", "anchor_text": "total probability"}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution#Probability_mass_function", "anchor_text": "distribution PDF"}, {"url": "https://amstat.tandfonline.com/doi/abs/10.1198/106186001317115045", "anchor_text": "EM"}, {"url": "https://academic.oup.com/biomet/article-abstract/80/2/267/251605", "anchor_text": "ECM"}, {"url": "https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b", "anchor_text": "Linear Discriminant Analysis, ExplainedIntuitions, illustrations, and maths: How it\u2019s more than a dimension reduction tool and why it\u2019s robust for real-world\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-math-free-introduction-to-convolutional-neural-network-ff38fbc4fc76", "anchor_text": "Convolutional Neural Network: How is it different from the other networks?What\u2019s so unique about CNNs and what does convolution really do? This is a math-free introduction to the wonders of\u2026towardsdatascience.com"}, {"url": "https://yangxiaozhou.github.io/data/2020/10/20/EM-algorithm-explained.html", "anchor_text": "https://yangxiaozhou.github.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ffaf0655357e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----ffaf0655357e---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/expectation-maximization?source=post_page-----ffaf0655357e---------------expectation_maximization-----------------", "anchor_text": "Expectation Maximization"}, {"url": "https://medium.com/tag/statistical-learning?source=post_page-----ffaf0655357e---------------statistical_learning-----------------", "anchor_text": "Statistical Learning"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----ffaf0655357e---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&user=YANG+Xiaozhou&userId=22b8d9e0d922&source=-----ffaf0655357e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&user=YANG+Xiaozhou&userId=22b8d9e0d922&source=-----ffaf0655357e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fffaf0655357e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ffaf0655357e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ffaf0655357e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ffaf0655357e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ffaf0655357e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yangxiaozhou?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yangxiaozhou?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "YANG Xiaozhou"}, {"url": "https://medium.com/@yangxiaozhou/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "128 Followers"}, {"url": "http://yangxiaozhou.github.io/", "anchor_text": "http://yangxiaozhou.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22b8d9e0d922&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&user=YANG+Xiaozhou&userId=22b8d9e0d922&source=post_page-22b8d9e0d922--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F49a94dd007ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-algorithm-explained-ffaf0655357e&newsletterV3=22b8d9e0d922&newsletterV3Id=49a94dd007ca&user=YANG+Xiaozhou&userId=22b8d9e0d922&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}