{"url": "https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea", "time": 1683002075.344953, "path": "towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea/", "webpage": {"metadata": {"title": "InDepth: Layer-Wise Relevance Propagation | by Eugen Lindwurm | Towards Data Science", "h1": "InDepth: Layer-Wise Relevance Propagation", "description": "Layer-wise Relevance Propagation (LRP) is one of the most prominent methods in explainable machine learning (XML). This article will give you a good idea about the details of LRP and some tricks for\u2026"}, "outgoing_paragraph_urls": [{"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "this book chapter", "paragraph_index": 0}, {"url": "https://lrpserver.hhi.fraunhofer.de/image-classification", "anchor_text": "interactive demo", "paragraph_index": 1}, {"url": "https://news.mit.edu/2019/using-ai-predict-breast-cancer-and-personalize-care-0507", "anchor_text": "predicts a cancer diagnosis from a mammogram", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/why-how-interpretable-ml-7288c5aa55e4", "anchor_text": "my short primer", "paragraph_index": 3}, {"url": "https://link.springer.com/chapter/10.1007/978-3-030-28954-6_11", "anchor_text": "can be used for LSTMs", "paragraph_index": 12}, {"url": "https://github.com/albermax/innvestigate", "anchor_text": "excellent implementations", "paragraph_index": 19}, {"url": "http://heatmapping.org/tutorial/", "anchor_text": "this short tutorial", "paragraph_index": 20}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "more efficient way", "paragraph_index": 20}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "neat list of the most important rules", "paragraph_index": 24}, {"url": "https://www.sciencedirect.com/science/article/pii/S0031320316303582", "anchor_text": "Deep Taylor Decomposition paper", "paragraph_index": 25}, {"url": "https://lrpserver.hhi.fraunhofer.de/image-classification", "anchor_text": "interactive demo", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1608.00507", "anchor_text": "comparisons", "paragraph_index": 31}, {"url": "https://arxiv.org/abs/1708.07689", "anchor_text": "Lapuschkin et al.", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1810.09945", "anchor_text": "Thomas et al.", "paragraph_index": 34}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142", "anchor_text": "Arras et al.", "paragraph_index": 35}, {"url": "https://www.nature.com/articles/s41598-019-38748-8", "anchor_text": "Horst et al.", "paragraph_index": 36}, {"url": "https://ieeexplore.ieee.org/document/7952445", "anchor_text": "Srinivasan et al.", "paragraph_index": 37}, {"url": "https://www.nature.com/articles/s41467-019-08987-4", "anchor_text": "Montavon et at.", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/why-how-interpretable-ml-7288c5aa55e4", "anchor_text": "many different techniques", "paragraph_index": 39}, {"url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53", "anchor_text": "Zeiler and Fergus", "paragraph_index": 40}, {"url": "https://arxiv.org/pdf/1312.6034.pdf", "anchor_text": "Simonyan et al.", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1412.6806", "anchor_text": "Springenberg et al.", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1608.00507", "anchor_text": "Zhang et al.", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1610.02391", "anchor_text": "Ramprasaath et al.", "paragraph_index": 44}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Sundararajan et al.", "paragraph_index": 45}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140", "anchor_text": "Bach et al.", "paragraph_index": 46}, {"url": "https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub", "anchor_text": "Montavon et al.", "paragraph_index": 46}], "all_paragraphs": ["Layer-wise Relevance Propagation (LRP) is one of the most prominent methods in explainable machine learning (XML). This article will give you a good idea about the details of LRP and some tricks for implementing it. The content is largely based on this book chapter.", "To make your mouth water about the potential of LRP, check out this interactive demo first.", "The purpose of LRP is to provide an explanation of any neural network\u2019s output in the domain of its input. For example, if your network predicts a cancer diagnosis from a mammogram (an image of breast tissue), then the explanation given by LRP would be a map of which pixels in the original image contribute to the diagnosis and to what extent. This method does not interact with the training of the network, so you can easily apply it on already trained classifiers.", "XML methods are especially useful in safety-critical domains where practitioners must know exactly what the network is paying attention to. Other use-cases are the diagnosis of network (mis)behavior, scientific discovery, and improving the network architecture. You can read a high-level intro to XML and why it is useful in my short primer.", "Intuitively, what LRP does, is that it uses the network weights and the neural activations created by the forward-pass to propagate the output back through the network up until the input layer. There, we can visualize which pixels really contributed to the output. We call the magnitude of the contribution of each pixel or intermediate neuron \u201crelevance\u201d values R.", "LRP is a conservative technique, meaning the magnitude of any output y is conserved through the backpropagation process and is equal to the sum of the relevance map R of the input layer. This property holds for any consecutive layers j and k, and by transitivity for the input and output layer.", "With that property in mind, let\u2019s take it step by step. Let\u2019s say our network is a classifier and its output is a vector of size M, where each entry corresponds to one of M classes. In the output layer, we pick one neuron, or class, that we want to explain. For this neuron, the relevance is equal to its activation, the relevance of all other neurons in the output layer is zero. For example, if we want to use LRP to find out the relevance of the network\u2019s neurons and inputs with respect to predicting class c, we start with the output neuron for class c and only look at how the network arrived at this neuron\u2019s value.", "From there on we go backwards through the network by following this basic LPR rule:", "Here, j and k are two neurons of any consecutive layers. We already know the relevance R in the output layer, so we\u2019ll start from there and use this formula iteratively to calculate R for every neuron of the previous layer. a denotes the activation of the respective neuron, and w is the weight between the two neurons.", "This is the simplest LRP rule. Depending on your application, you will perhaps want to use different rules, which will be discussed later. All of them follow the same basic principle.", "Now, what does this formula mean? The numerator of the fraction is the amount to which the neuron j influences the neuron k (this is true for the linear case of an active ReLU). To enforce the conservation property above, this has to be divided by the sum of contributions of all neurons of the lower layer. We typically add a very small constant \u03f5 to the denominator to make the fraction more stable. The outer sum over k means that the relevance of neuron j is determined by the sum of its influence on all neurons k of the following layer, times the relevance of these neurons.", "We can simply follow this through from the last until the first layer. Depending on what output neuron we choose to start from, we can get an explanation for any class. So we can even inspect what the network thought would be relevant for predicting any class A, even though it actually predicted class B!", "The network structure that can be analyzed like this is not strongly restricted. For example, LRP works also perfectly for CNNs and can be used for LSTMs. It is a requirement, though, that the network only contains ReLU activation functions.", "I hope you have seen that the basic principle is very simple. To give you a better idea of the process that is going on when we apply this formula, I will explain it as a 4-step procedure \u2014 following the original authors\u2019 procedure in the table below.", "The first step determines the sum of influences for each neuron in the higher layer and is analogous to a modified forward pass. Note that for ReLU layers it is the same as a normal forward pass through the network, except that we add a small constant \u03f5 and wrap a function \u03c1 around our weights. This \u03c1 is just there to make the formula more general and encompasses all the possible LRP rules we will see later. In the case we had before, \u03c1 would be the identity function, meaning that we can just ignore it.", "Notice that the sum goes over every neuron j in the lower layer, and also over the bias neuron. In all the following steps, the bias will be ignored because we want relevance to only flow to the input neurons and not end up in static bias neurons.", "In the second step, we simply divide each neuron\u2019s relevance in the higher layer by the z value computed before. This ensures that the conservation property holds.", "In the third step, we compute a quantity c for every neuron in the preceding layer, hence it can be seen as a backward pass. This c can be roughly seen as how much relevance trickles down to neuron j from the succeeding layer.", "Lastly, in the fourth step, the relevance coming from above is multiplied with the activation of the neuron to calculate its own relevance. Intuitively, a neuron is relevant if 1) it has a high activation, and 2) it contributes a lot to relevant neurons of the higher layer.", "Now you should already be able to implement LRP yourself! The next section is meant to help with that. If you just want to use it out-of-the-box, be aware that there are excellent implementations out there already.", "When first introduced, the authors of LRP provided a pseudo-code snippet that would go through the network layer-by-layer to calculate the relevances, similar to what we have seen earlier. (Check out this short tutorial to implement this basic version.) As they became more experienced with applying LRP themselves, they published a more efficient way to go about it.", "The key part is to leverage efficient autograd frameworks, such as pytorch or tensorflow to do the backward pass for us.", "The big difference to the formulas we have looked at above is that we calculate the c values as gradient computation.", "This allows us to make use of the efficient automatic backward operation. In order for this to work out and give the correct result, we have to treat s as a constant. This is indicated by the .data in the code above.", "It is important to note that there are different propagation rules for LRP and that you will probably want to combine several of them to get the best results. Luckily there exists a neat list of the most important rules with suggestions on when to use them.", "Input-layer: For images, the rule of choice has been introduced in the Deep Taylor Decomposition paper by the authors of LRP and takes the following form:", "Here l and h are the lowest and highest admissible pixel values, respectively.", "Lower-layers: Here we want the explanation to be more smooth and less noisy since these layers are already very close to the relevance map we humans will see and have to make sense of. For this purpose, we can use the LRP-\u03b3 rule that that disproportionately favors positive evidence over negative evidence:", "Higher-layers: Either LRP-\u03b3 from above or the following LRP-\u03f5 rule can work well here. They remove some of the noise from the relevance map. In particular, the \u03f5 will absorb some small or contradicting evidence.", "Output-layer: Here we ideally use the unmodified LRP-0 rule.", "There are even more rules than these, but the ones you have seen here suffice for most applications. If picking the right rules looks overly complicated to you, don\u2019t worry! Just LRP-0 and LRP-\u03f5 should get you very far, and once you got an explanation going with those rules, you can always decide whether to invest the time to experiment with other rules and make the explanation more pretty. To get a bit of an intuition about the different rules, I recommend playing around with the interactive demo.", "There being so many possible rules also means that you should take the comparisons between LRP and similar techniques with a grain of salt, as these are typically conducted with only the basic LRP-0 rule.", "This section is meant to briefly give some examples of successful applications of LRP.", "Lapuschkin et al. have used the technique to investigate networks predicting gender and age from image data.", "Thomas et al. applied the technique to a large corpus of fMRI neuroimaging data, explaining brain states from 3D data.", "Arras et al. apply LRP to text, to investigate how a neural network classifies text as belonging to one topic or another.", "Horst et al. analyze human gait patterns with LRP.", "Srinivasan et al. use LRP to find out exactly what parts of a video are used by a classifier for human action recognition.", "Montavon et at. apply LRP to Atari games and image classification.", "Now that you have a good idea about the inner workings of LRP, it is only fair to mention related techniques. The umbrella term explainable ML or explainable AI can be used for many different techniques. Here, I will only point to some that are very closely related to LRP.", "Zeiler and Fergus published a seminal paper in 2014, visualizing neuron activations with a deconvolutional network.", "Simonyan et al. published another important paper in 2014, computing saliency maps based on Taylor decompositions.", "Springenberg et al. published Guided Backpropagation soon afterward.", "Zhang et al. introduced Excitation Backprop, casting the backpropagation as a probabilistic process.", "Ramprasaath et al. introduced Grad-Cam for CNNs.", "Sundararajan et al. introduced the Integrated Gradient method.", "Based on LRP by Bach et al., Montavon et al. created Deep Taylor Decomposition.", "Thank you for reading! I hope you learned something useful.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Machine Learning. Interested in social and environmental issues."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F340f95deb1ea&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00----340f95deb1ea---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "this book chapter"}, {"url": "https://lrpserver.hhi.fraunhofer.de/image-classification", "anchor_text": "interactive demo"}, {"url": "https://news.mit.edu/2019/using-ai-predict-breast-cancer-and-personalize-care-0507", "anchor_text": "predicts a cancer diagnosis from a mammogram"}, {"url": "https://towardsdatascience.com/why-how-interpretable-ml-7288c5aa55e4", "anchor_text": "my short primer"}, {"url": "https://link.springer.com/chapter/10.1007/978-3-030-28954-6_11", "anchor_text": "can be used for LSTMs"}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "Layer-Wise Relevance Propagation: An Overview"}, {"url": "https://github.com/albermax/innvestigate", "anchor_text": "excellent implementations"}, {"url": "http://heatmapping.org/tutorial/", "anchor_text": "this short tutorial"}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "more efficient way"}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "Layer-Wise Relevance Propagation: An Overview"}, {"url": "https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10", "anchor_text": "neat list of the most important rules"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0031320316303582", "anchor_text": "Deep Taylor Decomposition paper"}, {"url": "https://lrpserver.hhi.fraunhofer.de/image-classification", "anchor_text": "interactive demo"}, {"url": "https://arxiv.org/abs/1608.00507", "anchor_text": "comparisons"}, {"url": "https://arxiv.org/abs/1708.07689", "anchor_text": "Lapuschkin et al."}, {"url": "https://arxiv.org/abs/1810.09945", "anchor_text": "Thomas et al."}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142", "anchor_text": "Arras et al."}, {"url": "https://www.nature.com/articles/s41598-019-38748-8", "anchor_text": "Horst et al."}, {"url": "https://ieeexplore.ieee.org/document/7952445", "anchor_text": "Srinivasan et al."}, {"url": "https://www.nature.com/articles/s41467-019-08987-4", "anchor_text": "Montavon et at."}, {"url": "https://towardsdatascience.com/why-how-interpretable-ml-7288c5aa55e4", "anchor_text": "many different techniques"}, {"url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53", "anchor_text": "Zeiler and Fergus"}, {"url": "https://arxiv.org/pdf/1312.6034.pdf", "anchor_text": "Simonyan et al."}, {"url": "https://arxiv.org/abs/1412.6806", "anchor_text": "Springenberg et al."}, {"url": "https://arxiv.org/abs/1608.00507", "anchor_text": "Zhang et al."}, {"url": "https://arxiv.org/abs/1610.02391", "anchor_text": "Ramprasaath et al."}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Sundararajan et al."}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140", "anchor_text": "Bach et al."}, {"url": "https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub", "anchor_text": "Montavon et al."}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----340f95deb1ea---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----340f95deb1ea---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----340f95deb1ea---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/interpretable-ai?source=post_page-----340f95deb1ea---------------interpretable_ai-----------------", "anchor_text": "Interpretable Ai"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----340f95deb1ea---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----340f95deb1ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----340f95deb1ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F340f95deb1ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----340f95deb1ea---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----340f95deb1ea--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----340f95deb1ea--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----340f95deb1ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/@pflaenzchen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "135 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56b4244ba55f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Findepth-layer-wise-relevance-propagation-340f95deb1ea&newsletterV3=cb9e52bc6a00&newsletterV3Id=56b4244ba55f&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}