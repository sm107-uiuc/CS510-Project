{"url": "https://towardsdatascience.com/understanding-neural-networks-c5557cfef5b3", "time": 1683007577.6250691, "path": "towardsdatascience.com/understanding-neural-networks-c5557cfef5b3/", "webpage": {"metadata": {"title": "Understanding Neural Networks. This article focuses on in-depth\u2026 | by Ayush Parhi | Towards Data Science", "h1": "Understanding Neural Networks", "description": "In my previous article, I briefly discussed deep learning and how to get started with it. If you haven\u2019t read that article, please read it here to get an intuitive idea about deep learning and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@ayushparhi7/deep-learning-the-state-of-the-art-88153e69a602", "anchor_text": "here", "paragraph_index": 0}], "all_paragraphs": ["In my previous article, I briefly discussed deep learning and how to get started with it. If you haven\u2019t read that article, please read it here to get an intuitive idea about deep learning and machine learning.", "If you have already read it, then let\u2019s get started!", "Perceptron?! Some of you may know about it already. Anyway, a perceptron is the structural building block of a neural network. As simple as that. Combining many perceptrons by forming layers ends up being a deep neural network. A perceptron architecture may look like this:", "Here, there are 2 layers in total: an input layer and an output layer. But, in the machine learning world, developers don\u2019t consider input as a layer and hence they will say, \u201cthis a single layer perceptron model\u201d. So, when someone says, \u201cI have build a 5 layer neural network\u201d, don\u2019t count the input as a layer. So, what does this perceptron model do? As you can see in the above diagram we have 2 inputs and one single node with sigma and integration signs on it and then there is the output. This node computes two mathematical expressions to give the output. First, it takes the weighted sum of the input plus a bias and then the sum is passed through a non-linear activation function. Later the activation function produces predicted output. This whole process is called forward propagation in the neural network.", "Have a look at this image:", "The inspiration for the forward propagation is taken from logistic regression. If you know the logistic regression algorithm, this may seem you familiar, but if you don\u2019t know logistic regression, it\u2019s not necessary. The weights (W) and the biases (b) are the parameters that are \u201ctrained\u201d by the neural network, and by \u201ctrained\u201d I mean they are set to a precise value such that the loss is minimum.", "The output ( \u201cy\u201d in the above diagram) is the prediction made by the neural network. The difference between the actual value and the predicted value is called the loss of the neural network. But it is not as simple as that. We do take the difference between the predicted value and the actual value but not the direct difference. Let\u2019s understand what I want to say.", "One thing you should know before moving on is that the predicted value calculates the loss of the neural network and the neural network do so by the calculation of \u201cZ\u201d which is dependent on \u201cW\u201d and \u201cb\u201d. Ultimately, we can say that the loss is dependent on \u201cW\u201d and \u201cb\u201d. So, the \u201cW\u201d and the \u201cb\u201d should be set to a value that gives minimum loss. To be clear, a neural network always minimizes the loss instead of maximizing the accuracy.", "When solving a deep learning problem, the dataset is huge. For example, let\u2019s say that we have to build an image classifier that classifies the image of cats and dogs (you can consider it as the \u201cHello World!\u201d of computer vision :) ). So for training the neural network, we need as many images of cats and dogs as we can get. In machine learning, the image of a dog or a cat is considered as a \u201ctraining example\u201d. For training a good neural network, we need a good number of training examples. The loss function is the loss calculated for a single training example. So, actually what we optimize for the training of a neural network is the cost function. The cost function can be defined as the average of all losses calculated separately for each training example.", "Let us assume there are \u201cm\u201d number of training examples. Then the cost function is:", "Let\u2019s take the loss of a neural network as: Loss (say, L) = Predicted value ( say, yhat) \u2014 Actual value (say, y)", "Since the loss of a neural network depends on \u201cW\u201d and \u201cb\u201d, let\u2019s plot the above loss function with respect to \u201cW\u201d only ( for the sake of simplicity, if we take \u201cW\u201d and \u201cb\u201d both into consideration, we have to plot a 3-D graph and it would be hard to understand the concept. Also, the bias \u201cb\u201d is used to shift the activation function to right or left, like the intercept in the equation of line)The plot might look something like this:", "The problem with this loss function is that it is a straight line and it becomes impossible to optimize the loss function with the use of optimizing algorithms like the gradient descent (tell you about it in the next section). For now, understand that the loss function should be a curve that has a global minimum.", "Later researchers and came up with another loss function:", "This is called \u201cMean Squared error loss\u201d and it is used in regression type of problems. Also, for one training example, this seems to be a fair equation. It is a parabola. But for \u201cm\u201d training examples, i.e, for the cost function is will be a wavy curve with lots of local minima. We want a bowl-like curve or a convex curve for optimizing the cost.", "So, later we got an accepted equation for the loss function which is:", "This equation is called \u201ccross-entropy loss\u201d and it is widely used for classification problems in deep learning.", "The cost function for the above equation will look something like this:", "Now we know how to compute the cost of a neural network, let\u2019s understand how to optimize the cost function for better performance.", "Backpropagation is the most important task in the neural network building. It is the process where the actual training of the neural network happens. It is a highly computational task. In fact, it is two-third of the whole computational process in a neural network. In forward propagation, we saw how to calculate the cost of a neural network. In backpropagation, we use this cost to set the value of \u201cW\u201d and \u201cb\u201d such that it can minimize the cost of the neural network.In the beginning, we initialize weights (W) and biases (b) to some random small numbers. As the model trains, the weights and biases get updated with new values. This update is done with the help of an optimization algorithm called gradient descent.", "In mathematics, gradient means slope or derivative. Descent means decreasing. So in layman language, gradient descent means decreasing slope. Familiarity with calculus will help to understand gradient descent. Remember when I said that we need a convex shape cost function for optimization. The reason is that while descending, we will not stuck at local minima. If the cost function is wavy, it has chances to stuck at one of its local minima and we will never have the global optimum value of the cost.", "The above representation of the gradient descent algorithm may help you to understand it.", "In gradient descent algorithm we calculate the derivative of computed cost with respect to the weights and biases separately.", "Let us consider a simple perceptron model with 2 inputs:", "Then the weight is updated as:", "The alpha in the above equation is called the learning rate of the neural network. It is a hyperparameter and I will tell you about it later after I tell you about some more hyperparameters. For now, consider it as a constant.", "The weights and the biases will get updated until the cost function reaches its global optimum value. Thus we will get the predicted output will less error.", "One forward propagation and one backpropagation together are counted as 1 iteration or epoch of training. A deep learning practitioner has to set the number of epochs (another hyperparameter) before training the model.", "Until now we have worked on a perceptron model and it is quite easy dealing with a perceptron model. But things get ugly when we get deeper into the network.", "Real-world problems have a huge number of input features and each input feature has its own weight and bias and according to the problem there are many hidden layers in the model and each hidden layer has many nodes that compute Z and A. The process of training is same as I described it earlier, but it is repeated as many times as the number of nodes are present in the neural network.", "In the above GIF, you can guess the complexity of a neural network and the high computational requirement for implementing it.", "Let\u2019s summarize everything we learned till now:1. During Forward Propagation:- Initialize the weights and biases.- Calculate Z and A for each node.- Calculate the cost of the whole model.2. During Backpropagation:- Use gradient descent to optimize the cost.- Compute gradients of the cost with respect to weights and biases separately.- Update the parameters (W & b)3. Repeat steps 1 and 2 until the cost function reaches its global optimum value.", "I hope that I was clear throughout the article and you understand the concepts well. If not, feel free to ask questions in the comments. Also, give your valuable suggestions in the comments so that I can improve my articles.", "You can also suggest what topic you want to learn in the field of deep learning.", "Thank you all for giving this article your valuable time to read it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A BTech student who is a Deep learning enthusiast and practitioner. I have been a Data Science intern and have some knowledge in this field."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc5557cfef5b3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ayushparhi7?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushparhi7?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Ayush Parhi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2087f23b9009&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&user=Ayush+Parhi&userId=2087f23b9009&source=post_page-2087f23b9009----c5557cfef5b3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@ayushparhi7/deep-learning-the-state-of-the-art-88153e69a602", "anchor_text": "here"}, {"url": "https://unsplash.com/@diskander?utm_source=medium&utm_medium=referral", "anchor_text": "David Iskander"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.allaboutcircuits.com/uploads/articles/how-to-perform-classification-using-a-neural-network-a-simple-perceptron-example_rk_aac_image1.jpg", "anchor_text": "All about circuits"}, {"url": "https://gfycat.com/activecourteousamericanindianhorse-mathematics-3b1b", "anchor_text": "Gfycat"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c5557cfef5b3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c5557cfef5b3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c5557cfef5b3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c5557cfef5b3---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c5557cfef5b3---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&user=Ayush+Parhi&userId=2087f23b9009&source=-----c5557cfef5b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&user=Ayush+Parhi&userId=2087f23b9009&source=-----c5557cfef5b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc5557cfef5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c5557cfef5b3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c5557cfef5b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushparhi7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushparhi7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ayush Parhi"}, {"url": "https://medium.com/@ayushparhi7/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "21 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2087f23b9009&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&user=Ayush+Parhi&userId=2087f23b9009&source=post_page-2087f23b9009--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2087f23b9009%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-c5557cfef5b3&user=Ayush+Parhi&userId=2087f23b9009&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}