{"url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "time": 1683010701.3431628, "path": "towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0/", "webpage": {"metadata": {"title": "Why Neural Nets Can Approximate Any Function | by Thomas Hikaru Clark | Towards Data Science", "h1": "Why Neural Nets Can Approximate Any Function", "description": "In this article, I will explain the Universal Approximation Theorem and showcase two quick examples with PyTorch code to demonstrate neural networks learning to approximate functions. Feel free to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab", "anchor_text": "weight decay", "paragraph_index": 20}], "all_paragraphs": ["In this article, I will explain the Universal Approximation Theorem and showcase two quick examples with PyTorch code to demonstrate neural networks learning to approximate functions. Feel free to skip straight to the code and visualizations if you already know the basics of how a neural network works!", "When a lot of people hear the word function, they just think of high school algebra and relations like f(x)=x\u00b2. Although I have nothing against high school algebra (I taught it for two years!), it\u2019s important to keep in mind that a function is just a mapping from inputs to outputs, and these can take many forms.", "Let\u2019s say you want to train a machine learning model that predicts a person\u2019s clothing size. (I recently used such an algorithm to estimate my size for a jacket). The inputs are the person\u2019s height, weight, and age. The output is the size. What we are trying to do is produce a function that converts a person\u2019s height/weight/age combination (a triple of numbers) into a size (perhaps a continuous scalar value or a classification like XS, S, M, L, XL).", "According to machine learning, we can do this by using the following steps:", "Training a model would be easy if the clothing size were just a linear combination of the input variables. A simple linear regression could get you good values for a, b, c, and d in the following equation.", "However, we cannot assume, in general, that an output is a linear combination of input variables. Conditions in real life are complicated. Rules have exceptions and special cases. Tasks like handwriting recognition and image classification clearly require very complicated patterns to be learned from high-dimensional input data.", "Wouldn\u2019t it be great if there were a way to approximate any function? According to the Universal Approximation Theorem, a neural network with a single hidden layer can do exactly that.", "For our purposes, we will be looking only at fully connected neural networks with an input layer, a single hidden layer, and an output layer. In the case of our clothing size predictor, the input layer would have three neurons (the height, weight, and age) and the output layer would have just one (the predicted size). In between, there is a hidden layer with some number of neurons (5 in the image below, but in practice probably something bigger like 1024).", "Each neuron is connected to every neuron in the next layer. Each of these connections has a certain weight. The value of each neuron is passed along each connection, where it gets multiplied by the weight. All the neurons then feed forward into the output layer, which gives you a result. Training the model involves finding good weights for all the connections. The central claim of the Universal Approximation Theorem is that with enough hidden neurons, there exists some set of connection weights that can approximate any function \u2014 even if that function is not something that you could possible write down neatly like f(x)=x\u00b2. Even a crazy, complicated function, like the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this Theorem.", "The key to neural networks\u2019 ability to approximate any function is that they incorporate non-linearity into their architecture. Each layer is associated with an activation function that applies a non-linear transformation to the output of that layer. This means that each layer is not just working with some linear combination of the previous layer. Some common non-linear activation functions are ReLU, Tanh, and Sigmoid, compared in the graphs below.", "We now have all the pieces in place and can dive into an example of the Universal Approximation Theorem at work. To demonstrate this in action, I am going to generate random sets of inputs and outputs, and make a neural network learn the \u201cfunction\u201d. Of course, in this case, there is no function! But what we should expect is that the model will fit to the random data anyway, as if it was generated by some deterministic function.", "For the demonstrations in this article, I use the high-level neural network functionalities provided in PyTorch. The PyTorch libraries make it easy to build a basic neural network in only a few lines of code. It abstracts away a lot of the technical details so you only have to deal with the big picture.", "One of the most basic cases a neural network might face is learning a function from one variable to one variable. For example, let\u2019s say the x-value represents time and the y-coordinate represents the amount of traffic on a certain street. There is no reason why this would be a linear relationship, with peaks and valleys occurring at various points throughout the day.", "The code below generates random points from a normal distribution and trains a network that treats the x-coordinate as the input and the y-coordinate as the output. Please see the code comments for more details on each step.", "Functions don\u2019t have to be the \u201cone number goes in, a different number comes out\u201d type of function that you see in high school algebra. Let\u2019s try a binary classification task instead. Data points have two features and can be classified into one of two labels. Perhaps the two features are latitude and longitude coordinates and the label is the presence of an environmental pollutant. Or perhaps the features are the math and reading test scores of students and the label corresponds to whether they are right- or left-handed. What matters is that the model must learn a function from two inputs to a single output\u2014 either 0 or 1.", "The code below is very similar to the earlier code. The only new things are that the input layer now has two neurons, and that the output layer is followed by a Sigmoid activation which squishes all outputs into the range (0, 1). This gives us a nice binary classification where the distance from the output to either 0 or 1 also gives us a sense of the model\u2019s confidence in that classification.", "I randomly generated some points in the unit square and randomly assigned them to one of two categories, labeled as 0 and 1. As you can see, there is no single line or curve separating the teal and pink points. Nevertheless, the model was able to learn the relationship between input features (x- and y- coordinates) and output label (category/color).", "Before we get too excited about these results, let\u2019s take a step back and ask if the results are actually desirable. Both of these examples illustrate a very important phenomenon in machine learning: overfitting. Overfitting happens when a model learns the peculiarities of the training data so well that it generalizes poorly to unseen data.", "In Example 1, let\u2019s imagine that one of the points is an outlier that resulted from faulty data collection. Given such a small quantity of training data to learn from, the model became overfitted to this data, seeing a signal in what was really just noise. On one hand, it\u2019s actually quite impressive that the model was able to learn a function that takes into account this outlier. On the other hand, this could lead to undesirable outcomes when applying this model to real-world data, producing faulty predictions near that point.", "In Example 2, the model learned a beautiful patchwork quilt of classification values. However, notice the teal point closest to the bottom right corner. Even though it\u2019s the only point there, it resulted in the model labeling the entire bottom right corner as teal. Just a few erroneous data points could seriously skew the model. When we try applying the model to test data, it may work much worse than expected.", "To avoid overfitting, it is important to have a large quantity of training data that is representative of the sample the model is expected to face. If you are building a tool to predict clothing sizes for the general population, don\u2019t gather training data only from your college-age friends. In addition, there are advanced techniques to help reduce overfitting (e.g. weight decay).", "In summary, neural networks are powerful machine learning tools because of their ability to (in theory) learn any function. This is not a guarantee, however, that you will easily find the optimal weights for a given problem! In practice, training an accurate model in a reasonable amount of time depends on many factors, such as optimizer, model architecture, quality of data, and many more. In particular, deep learning involves neural networks with multiple hidden layers, which are very good at learning certain difficult tasks.", "I personally enjoy visualizing how neural networks work with small examples, as having a solid, intuitive understanding of how the basics work helps me to grasp more advanced or abstract concepts. For example, although I knew the technical definition of overfitting, running these examples myself helped me to really understand what it is at a more intuitive level. I hope you learned something as well. I welcome any feedback you have, and thank you for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa878768502f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://thomashikaru.medium.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Thomas Hikaru Clark"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe806fec87c25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=post_page-e806fec87c25----a878768502f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@incedikitti?utm_source=medium&utm_medium=referral", "anchor_text": "Kitti Inc\u00e9di"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab", "anchor_text": "weight decay"}, {"url": "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html", "anchor_text": "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "https://en.wikipedia.org/wiki/Universal_approximation_theorem"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a878768502f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----a878768502f0---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a878768502f0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----a878768502f0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----a878768502f0---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=-----a878768502f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=-----a878768502f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa878768502f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a878768502f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a878768502f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a878768502f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a878768502f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a878768502f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a878768502f0--------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://thomashikaru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Hikaru Clark"}, {"url": "https://thomashikaru.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "352 Followers"}, {"url": "https://podcasts.apple.com/us/podcast/modus-mirandi-podcast-with-thomas-hikaru-clark/id1551675175?uo=4", "anchor_text": "https://podcasts.apple.com/us/podcast/modus-mirandi-podcast-with-thomas-hikaru-clark/id1551675175?uo=4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe806fec87c25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=post_page-e806fec87c25--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F70b012cf892f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-neural-nets-can-approximate-any-function-a878768502f0&newsletterV3=e806fec87c25&newsletterV3Id=70b012cf892f&user=Thomas+Hikaru+Clark&userId=e806fec87c25&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}