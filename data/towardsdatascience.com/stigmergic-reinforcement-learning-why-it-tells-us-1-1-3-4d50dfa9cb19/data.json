{"url": "https://towardsdatascience.com/stigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19", "time": 1683006871.798341, "path": "towardsdatascience.com/stigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19/", "webpage": {"metadata": {"title": "Stigmergic Reinforcement Learning: SIRL | Towards Data Science", "h1": "Unpacking Stigmergic Independent Reinforcement Learning", "description": "Describes Stigmergic Independent Reinforcement Learning using deep neural networks for decentralized multi-agent learning"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1911.12504.pdf?fbclid=IwAR2d19jkD67GlfKW0rt_JvHzGiYaalY-Im0lgEPa43wzTnbtZOr9lGOW-34", "anchor_text": "Stigmergic Independent Reinforcement Learning for Multi-Agent Collaboration", "paragraph_index": 41}, {"url": "https://www.researchgate.net/publication/279058749_Stigmergy_as_a_Universal_Coordination_Mechanism_components_varieties_and_applications", "anchor_text": "Stigmergy as a Universal Coordination Mechanism: Components, Varieties, and Applications", "paragraph_index": 42}], "all_paragraphs": ["Swarm optimization is one of many sub-fields under the \u201cmulti-agent learning\u201d umbrella. It embraces the idea of \u201cstrength in numbers.\u201d The more agents we have, the more we can accomplish. The field attempts to solve problems using an extremely large number of simple agents, each contributing to a goal bigger than themselves. In other words, swarm optimization exploits one concept:", "multi-agent systems have greater flexibility, robustness, and potential than the respective sums of its individual constituents.", "One of the most popular branches of swarm optimization is stigmergy. Stigmergic algorithms allow agents to use local information and indirect coordination through the environment. An agent\u2019s action leaves a \u201ctrace\u201d in the environment and stimulates other agents\u2019 succeeding actions. Then, we repeat.", "We can think of stigmergy as a methodology using simple rules. Each agent is given a very small, comprehensive rule book to follow, instructing them how to tinker with its surroundings or interact with others. If each rule book is designed correctly, the combination of their actions ultimately achieve some task.", "This idea of \u201ccascading actions\u201d can often be found in nature. Ants and termites use pheromones to indirectly communicate with each other, allowing them to build bridges or construct complex nests. Stigmergic behavior has even been observed in bacteria, specifically myxobacteria, where group behavior is guided by complex control systems.", "In this article, we explore Stigmergic Independent Reinforcement Learning, a novel approach to large-scale multi-agent learning.", "Scalability is one of the largest hurdles in multi-agent reinforcement learning (MARL) because state and action spaces grow exponentially as the number of agents increase. While independent learning algorithms have been proposed to combat this challenge, these methods tend to suffer from non-stationarity issues. For more background on MARL and its hurdles, feel free to check out this article:", "Stigmergic Independent Reinforcement Learning (SIRL) proposes an algorithm that provides a balance between explicit joint learning and its independent counterpart. In other words, the algorithm coordinates different independent learning agents on a large scale.", "Stigmergy consists of four main components:", "Using these formulations, we can represent the indirect coordination between agents more easily than classical reinforcement learning.", "Traces are left by agents in the medium after executing actions given certain conditions.", "As these traces are accumulated and mixed in the medium due to many contributing agents, they each respond to these signals accordingly depending on the application. The larger the accumulated trace at a location, the more drastically agents respond to that specific signal.", "In SIRL\u2019s formulations, the environment and medium are separated entities. We can think of the environment as a collection of physical states in which agents can represent, describe, and locate themselves. On the other hand, the medium can be thought of as a specific part of the environment through which communication is funneled. This separation is used to distinguish SIRL from classical RL algorithms. The other parts of the SIRL framework will be overviewed in subsequent sections.", "Conflict avoidance is used to implicitly coordinate agents, even though each only has local information. Here\u2019s an idea: let agents compete for action opportunity, only allowing a subset of agents to execute actions given a timestep. To that end, each agent is given an evaluation module and a behavior module, each with its own set of neural networks.", "The evaluation module measures the agent\u2019s action priority given its current local state. In other words, agents with higher returns from the evaluation module are more likely to execute their actions.", "The behavior module selects the appropriate action to execute given the local state and the evaluation module\u2019s permission. As a result, there are always two steps when determining actions:", "These mathematical formulations will be discussed later in this article.", "In SIRL, a digital pheromone (trace) is used for indirect communication between agents. These signals are stored in a map (medium) containing the distribution of pheromones and providing relevant state space information to the agents. We can imagine this representation as almost a heat map stored centrally by the virtual agent or split between specified agents.", "Digital pheromones are just numbers. Higher numbers represent stronger signals. As discussed before, they can be combined, mixed, and accumulated. SIRL models this by incorporating three properties. It uses linear superposition when different pheromones are combined. Additionally, these pheromones are not restricted to a single space but diffuse into surrounding areas. Lastly, the pheromone\u2019s magnitude decays over time. Diffusion and decay rates are tunable hyperparameters.", "We assume that each agent can sense digital pheromones within a limited range. Let\u2019s define an attractor as any block in the pheromone map that contains a non-zero amount of digital pheromone. This block will have an attractive effect on nearby agents to efficiently explore the local state space. However, for agents to select an action, they first need to select an attractor to conduct its action (ie. moving towards the attractor). The probability that an agent selects a specific attractor is given by:", "where d_ij is the distance from agent i to attractor j. Epsilon is the magnitude of pheromone at location j, xi is the set of all attractors within range, and D is some monotonic function. For example, in the paper\u2019s experiments, the function D looked something like this:", "This probability value intuitively makes sense. We can see that has distance increases, the probability of choosing that attractor decreases. Similarly, as the amount of pheromone magnitude decreases, the probability decreases as well. The location of the selected attractor gets concatenated to the agent\u2019s local state and is passed into both the evaluation and behavior modules.", "However, some may ask why don\u2019t we simply choose the attractor with the best combination of pheromone strength and distance? We don\u2019t want our agents clumping together as that defeats the purpose of conflict avoidance. In effect, it\u2019s ideal to stochastically select attractors.", "After passing in the local state to the evaluation and behavior modules, we decide whether or not to execute an action. If the agent does, we place pheromones in the environment using:", "where a represents a fixed amount of digital pheromone dropped by an agent and b is the decay rate, applied if no agent was there in the first place. The decay helps remove useless attractors as time passes on.", "We assume the traditional RL framework where we attempt to maximize the expected cumulative reward. As a result, we define rewards for a state transition as follows:", "where p is a scaling factor. Given agent i and chosen attractor j, we reward actions that get an agent closer to the attractor.", "Remember, each agent has an evaluation network that outputs values proportional to the action priority it has. The higher the value, the more likely it is permitted to execute an action.", "Since we want to maximize our overall expected reward, we intuitively want to give action priority to agents with higher expected returns given their local states. As a result, we define our action priority as the value function (V):", "The higher the value of an agent\u2019s state, the more action priority it gets. Subsequently, the behavior module chooses an action that has the highest probability given its policy. We also define R_tilde as:", "where this value function (V) denotes the output of a target value network. The target value network copies the value network\u2019s parameters periodically and is used to increase stability when training. Similarly, the behavior module, in addition to its policy network, also has both a value network and a target value network.", "We are almost there! Next, we define the loss functions of both the evaluation and behavior modules in each of the agents. The evaluation module\u2019s value loss is defined as:", "representing agent i\u2019s squared TD error. The behavior module\u2019s value loss is analogous to the above. Furthermore, its policy loss is similar to the Advantage Actor Critic\u2019s loss, where we scale negative probabilities with the calculated advantage function:", "Notice how the variable R no longer has a tilde. These rewards are defined by the behavior module\u2019s target value function instead of the evaluation module\u2019s, making it a different variable from before.", "Now that we have all these losses, we may be tempted to optimize each of the agents\u2019 modules individually. However, this would effectively reduce the algorithm to independent learning between agents. In other words, each agent solely optimizes its policies disregarding the others, losing the notion of coordination we originally set out to optimize.", "But wait! There is one last part of our architecture! Here, we explain the federal training module. While this is a virtual agent, it contains the same architecture and parts as the other agents. It is responsible for optimizing each of the agent\u2019s neural network modules through average optimization. More formally:", "The \u201cv\u201d superscripts denote parameters belonging to the federal training module. Furthermore, \u201cp\u201d represents a momentum term and \u201cl\u201d the learning rate. The update sums only over agents that executed actions in that timestep. As a result, theta represents the parameters of each of those agent\u2019s respective modules. Here, we update the federal trainer\u2019s parameters by averaging over these agents\u2019 gradients. Then, because the federal trainer has the same module structure, we\u2019re able to send these new parameters back to the agents. In effect, we\u2019ve done several things:", "The experiment conducted in the paper involved groups of pixels, each an agent, aligning themselves in the shape of a number. For example, the pixels learned how to form the number \u201c4\u201d after 150 iterations as shown below:", "SIRL was compared with four other methods.", "SIRL stacks up well against its competitors, teaching its agents to form accurate formations within less training epochs.", "SIRL presents a novel approach to tackle the issue of scalability in multi-agent systems. It offers a promising decentralized method for reinforcement learning in extraordinarily large systems and can be considered a starting point for swarm-based RL in more complex domains, ranging from multi-robot locomotion to traffic signal control. Regardless, it\u2019s clear that what agents can accomplish collaboratively is greater than the sum of their individual potentials. It might just tell us that, sometimes, 1 + 1 = 3.", "[1] X. Xu, R. Li, Z. Zhao, and H. Zhang, Stigmergic Independent Reinforcement Learning for Multi-Agent Collaboration (2019).", "[2] F. Heylighen, Stigmergy as a Universal Coordination Mechanism: Components, Varieties, and Applications (2015).", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d50dfa9cb19&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----4d50dfa9cb19---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral", "anchor_text": "JESHOOTS.COM"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@saadx?utm_source=medium&utm_medium=referral", "anchor_text": "Saad Salim"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/swlh/the-gist-multi-agent-reinforcement-learning-767b367b395f", "anchor_text": "Multi-Agent Reinforcement Learning: The GistAs if one robot learning everything wasn\u2019t enough alreadymedium.com"}, {"url": "https://unsplash.com/@ericjamesward?utm_source=medium&utm_medium=referral", "anchor_text": "Eric Ward"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@mero_dnt?utm_source=medium&utm_medium=referral", "anchor_text": "Chinh Le Duc"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@brock222?utm_source=medium&utm_medium=referral", "anchor_text": "Richard Lee"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@kati_ur?utm_source=medium&utm_medium=referral", "anchor_text": "Katarzyna Urbanek"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1911.12504.pdf?fbclid=IwAR2d19jkD67GlfKW0rt_JvHzGiYaalY-Im0lgEPa43wzTnbtZOr9lGOW-34", "anchor_text": "Stigmergic Independent Reinforcement Learning for Multi-Agent Collaboration"}, {"url": "https://www.researchgate.net/publication/279058749_Stigmergy_as_a_Universal_Coordination_Mechanism_components_varieties_and_applications", "anchor_text": "Stigmergy as a Universal Coordination Mechanism: Components, Varieties, and Applications"}, {"url": "https://towardsdatascience.com/multi-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e", "anchor_text": "Introduction to Nash Equilibria: Friend or Foe Q-LearningMaking robots tip the scalestowardsdatascience.com"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4d50dfa9cb19---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4d50dfa9cb19---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4d50dfa9cb19---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4d50dfa9cb19---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/swarm-intelligence?source=post_page-----4d50dfa9cb19---------------swarm_intelligence-----------------", "anchor_text": "Swarm Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----4d50dfa9cb19---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----4d50dfa9cb19---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d50dfa9cb19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d50dfa9cb19---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d50dfa9cb19--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "208 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}