{"url": "https://towardsdatascience.com/clustering-text-with-transformed-document-vectors-1e14c9f0f198", "time": 1682994022.0230389, "path": "towardsdatascience.com/clustering-text-with-transformed-document-vectors-1e14c9f0f198/", "webpage": {"metadata": {"title": "Clustering Text with Transformed Document Vectors | by Ashok Chilakapati | Towards Data Science", "h1": "Clustering Text with Transformed Document Vectors", "description": "Distance and similarity measures cannot be directly compared across transformed spaces. A metric to test transformations for improving clusterability of documents is proposed"}, "outgoing_paragraph_urls": [{"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification of document vectors", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction", "anchor_text": "tf-idf", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "anchor_text": "naive bayes", "paragraph_index": 1}, {"url": "http://xplordat.com/2018/11/05/want-clusters-how-many-will-you-have/", "anchor_text": "distance between objects", "paragraph_index": 2}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "similarity of objects", "paragraph_index": 2}, {"url": "http://xplordat.com/2018/01/23/stacks-of-documents-and-bags-of-words/", "anchor_text": "bag-of-words", "paragraph_index": 2}, {"url": "http://xplordat.com/2018/06/18/reduced-order-models-for-documents/", "anchor_text": "vector space model", "paragraph_index": 4}, {"url": "https://scikit-learn.org/stable/modules/clustering.html#k-means", "anchor_text": "scikit-learn", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans", "anchor_text": "K-means", "paragraph_index": 5}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "Word Embeddings and Document Vectors: Part 2. Classification)", "paragraph_index": 8}, {"url": "https://github.com/ashokc/Clustering-text-with-tranformed-document-vectors", "anchor_text": "github", "paragraph_index": 16}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com", "paragraph_index": 20}], "all_paragraphs": ["A sister task to classification in machine learning is clustering. While classification requires up-front labeling of training data with class information, clustering is unsupervised. There is a large benefit to unattended grouping of text on disk and we would like to know if word-embeddings can help. In fact, once identified, these clusters can then go on to serve as target classification buckets for similar future incoming text. If we succeed in this we would have obviated the need for up-front labeling! Worthy goal but more on that later.", "Word-embeddings yield a linear transformation of n-long (n being the size of the vocabulary making up the text corpus) sparse document vectors to p-long dense vectors, with p << n. In the context of classification of document vectors we concluded that using tf-idf vectors with naive bayes classifier is a great starting point. While we cannot generalize, replacing words with pre-trained/custom-derived numerical vectors did not seem to do much for the classification quality. The general objective of this post (and the upcoming one) is to repeat that exercise for a clustering task. In this post we focus on the following.", "Clustering hinges on the notion of distance between objects, while classification hinges on the similarity of objects. The objects here are documents represented as numerical vectors as per the tested and tried bag-of-words approach and its variations (including word-embeddings). We might choose to use the euclidean distance measure for clustering and cosine similarity for classification. Intuitively we expect similar documents to be close to each other \u2014 distance wise. But that is not guaranteed unless we explicitly require it in the implementation. Things that we did not have to worry about while classifying document vectors, matter for the clustering task. Some of those would be as follows.", "Let us look at each of these in some detail focusing mainly on the distance between vectors.", "The core assumption of the vector space model for documents is that if two documents contain the same words with the same relative frequencies, then they are likely talking about the same thing. Thus the vectors built on that premise reflect it by yielding a cosine similarity of unity. We want the same end result in the clustering exercise as well. That is, we want to build clusters of documents, where the documents in each cluster are more similar to each other than they are to documents in other clusters. Makes sense right?", "But clustering is in general based on distance between vectors in the feature space and not on their cosine similarity. Perhaps we could use cosine similarity itself as the distance measure. Most implementations for K-means clustering (for example scikit-learn) do not allow for a custom distance function. So if we want to use K-means implementation in scikit for clustering documents, we need the distance measure for documents to behave the same way as their cosine similarity does, to get good results for clustering documents. Cosine similarity is unaffected by the magnitude of the vectors while the distance between them is. So we require that all the document vectors be normalized to have the same length prior to a clustering task.", "For illustration, consider the vectors OP, OA, OB, and OQ in Figure 1 below. OA is simply OP but with a larger magnitude. Likewise OB is a larger version of OQ. But the angle between OA, OB and that between OP and OQ is the same, and hence their cosine similarity is unaffected.", "Let us be very clear that we are requiring length normalization in the context of clustering document vectors where we want the distance measure to behave like the cosine similarity measure\u2026 Length normalization is not a blanket recommendation for clustering any set of vectors\u2026 In fact it would ruin the perfect clusters we obtained in the previous post where distances needed to be well distances between the original vectors\u2026", "Consider two vectors X and Y of length n, the size of the vocabulary. We transform them into shorter p-dimensional vectors X\u2019 and Y\u2019 by use of word-embeddings. We know (see for example Word Embeddings and Document Vectors: Part 2. Classification) that the transformation is linear with X\u2019 = WX and Y\u2019 = WY where W is the pxn matrix where p is the length of the word vectors. The cosine similarities and the distances in the new space can be derived as follows.", "Clearly, the angles and distances between vectors in p-space are a function of W. And they would be invariant if and only if W is orthonormal.", "But there is no reason to expect W to be orthonormal, as the word-vectors are derived from digesting text in a variety of ways depending on the underlying algorithm and with no requirement that they be orthonormal. As we are dealing with documents here, it follows from the argument in section 1 that we need to,", "normalize the length of transformed document vectors before proceeding with a clustering task", "We are hoping that a linear transformation of vectors will enable us to better decipher the latent clusters in the data. Each transformation takes the document vectors to a different space. While we can compare distances with-in a single space, we cannot compare them across spaces. See the Figure 2 below where the clustered data in 2A got transformed via linear transformations W_1 and W_2 to produce the clusters we see in 2B and 2C respectively.", "Figure 2B shows larger absolute values for the distances separating the clusters. But Figure 2C demarcates the clusters better. The reason is clear. In Figure 2B both the intracluster and intercluster distances got larger. In Figure 2C while the overall distances got squished, the intracluster distances got squished more than the intercluster distances. That is the key. To get better delineation between the clusters we need the individual clusters as compact as possible, while being as far as possible from the other clusters. Common sense right? We can somewhat formalize this by saying", "we should prefer a linear transformation that increases the ratio of intercluster distances to intracluster distances\u2026 in fact we should expect the transformation that yields the highest ratios to show best results for the clustering task", "As there are many inter & intra cluster distances, here is a plausible procedure to compare different order reducing linear transformations for the clustering task.", "We conclude the post with an example that solidifies our observations from the earlier sections. Consider a 2-d feature space with identifiable clusters shown in Figure 3A below. The code for reproducing these results can be downloaded from github.", "Applying a sample linear transformation to this data we get the clusters shown in Figure 3B. The observed orthogonality in Figure 3B is not surprising because \u2018we cheated\u2019 and picked a linear transformation that aligns somewhat with the dominant directions of these clusters in Figure 3A. We do not have that luxury in general but this is just for magnifying the impact of transformations on cluster detection. Applying a diffrent random transformation yields the squished clusters shown in Figure 3C. The actual distribution of intracluster distances (from the centroid of the cluster) have changed upon these transformations. The squishing of distances in 3C/3D and expansion of distances in 3B/3E is evident.", "Clearly the clusters obtained in Figure 3B are better separated than those in Figures 3A or 3C. The ratio of intercluster distance to the averaged median intracluster distances has the highest value as well for Figure 3B. That is the point we were building towards to in sections 2 thru 4 of this post", "A bit short of a post this one, but it will have to do. We have laid the groundwork for clustering documents with order reducing transformations. Getting into the details and results will make this too long. We will get to that in the next post and conclude this series.", "Originally published at xplordat.com on November 26, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e14c9f0f198&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29----1e14c9f0f198---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification of document vectors"}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction", "anchor_text": "tf-idf"}, {"url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "anchor_text": "naive bayes"}, {"url": "http://xplordat.com/2018/11/05/want-clusters-how-many-will-you-have/", "anchor_text": "distance between objects"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "similarity of objects"}, {"url": "http://xplordat.com/2018/01/23/stacks-of-documents-and-bags-of-words/", "anchor_text": "bag-of-words"}, {"url": "http://xplordat.com/2018/06/18/reduced-order-models-for-documents/", "anchor_text": "vector space model"}, {"url": "https://scikit-learn.org/stable/modules/clustering.html#k-means", "anchor_text": "scikit-learn"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans", "anchor_text": "K-means"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "Word Embeddings and Document Vectors: Part 2. Classification)"}, {"url": "https://github.com/ashokc/Clustering-text-with-tranformed-document-vectors", "anchor_text": "github"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com"}, {"url": "https://medium.com/tag/document-clustering?source=post_page-----1e14c9f0f198---------------document_clustering-----------------", "anchor_text": "Document Clustering"}, {"url": "https://medium.com/tag/transformation?source=post_page-----1e14c9f0f198---------------transformation-----------------", "anchor_text": "Transformation"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----1e14c9f0f198---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/similarity-search?source=post_page-----1e14c9f0f198---------------similarity_search-----------------", "anchor_text": "Similarity Search"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----1e14c9f0f198---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----1e14c9f0f198---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1e14c9f0f198&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1e14c9f0f198---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1e14c9f0f198--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/@ashok.chilakapati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ab4b71672c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclustering-text-with-transformed-document-vectors-1e14c9f0f198&newsletterV3=cc37b40eae29&newsletterV3Id=5ab4b71672c9&user=Ashok+Chilakapati&userId=cc37b40eae29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}