{"url": "https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb", "time": 1682993349.8790112, "path": "towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb/", "webpage": {"metadata": {"title": "Fast Near-Duplicate Image Search using Locality Sensitive Hashing | by Gal Yona | Towards Data Science", "h1": "Fast Near-Duplicate Image Search using Locality Sensitive Hashing", "description": "If you have some education in Machine Learning, the name Nearest Neighbor probably reminds you of the k-nearest neighbors algorithm. It is a very simple algorithm with seemingly no \u201clearning\u201d\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.wikiwand.com/en/K-nearest_neighbors_algorithm", "anchor_text": "k-nearest neighbors", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf", "anchor_text": "results can improve significantly", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/do-gans-really-model-the-true-data-distribution-or-are-they-just-cleverly-fooling-us-d08df69f25eb", "anchor_text": "verify the claim that generative models are really learning the underlying distribution", "paragraph_index": 4}, {"url": "https://github.com/facebookresearch/faiss", "anchor_text": "FAISS", "paragraph_index": 5}, {"url": "https://github.com/spotify/annoy", "anchor_text": "Annoy", "paragraph_index": 5}, {"url": "https://www.wikiwand.com/en/Johnson%E2%80%93Lindenstrauss_lemma", "anchor_text": "Johnson-Lindenstrauss lemma", "paragraph_index": 11}, {"url": "https://www.wikiwand.com/en/Precision_and_recall", "anchor_text": "recall", "paragraph_index": 15}, {"url": "http://www.vision.caltech.edu/Image_Datasets/Caltech101/", "anchor_text": "Caltech101", "paragraph_index": 23}, {"url": "https://galyona.github.io/website", "anchor_text": "https://galyona.github.io/website", "paragraph_index": 32}], "all_paragraphs": ["If you have some education in Machine Learning, the name Nearest Neighbor probably reminds you of the k-nearest neighbors algorithm. It is a very simple algorithm with seemingly no \u201clearning\u201d actually involved: The kNN rule simply classifies each unlabeled example by the majority label among its k-nearest neighbors in the training set.", "This seems like a very naive, even \u201csilly\u201d, classification rule. Is it? Well, depends on what you take as your distance metric, i.e: how do you choose to measure the similarity between examples. Yes, the naive choice \u2014 using simple Euclidean distance in the \u201craw\u201d features \u2014 often usually leads to very poor results in practical applications. For example, here are two examples (images) whose pixel-values are close in Euclidean distance; but arguably, one would be crazy to classify the left image as a flower, solely based on it being a neighbor of the right image.", "But, as it turns out, coupling the kNN rule with the proper choice of a distance metric can actually be extremely powerful. The field of \u201cmetric learning\u201d demonstrated that when machine learning is applied to learning the metric prior to using the kNN rule, results can improve significantly.", "The great thing about our current \u201cDeep Learning era\u201d is the abundance of available pre-trained networks. These networks solve certain classification tasks (predicting an image category, or the text surrounding a word), but the interesting thing is not so much their success on those tasks, but actually the extremely useful by-product they provide us with: dense vector representations, for which simple Euclidean distance actually corresponds to high-level, \u201csemantic\u201d similarity.", "The point is that for many tasks (but namely, general-purpose images and text), we already have a good distance metric, so now we actually can just use the simple kNN rule. I\u2019ve talked about this point a lot in the past \u2014 e.g, in a previous post I attempted to use such searches to verify the claim that generative models are really learning the underlying distribution and not just memorizing examples from the training set.", "This leaves us with the task of actually finding the nearest neighbors (I will refer to this as a NN query). This problem \u2014 now a building block in literally any ML pipeline \u2014 has received a lot of traction, both in the CS-theory literature and from companies that need highly optimized solutions for production environments. Here again, the community benefits, because a couple of the big players in this field have actually open-sourced their solutions. These tools use carefully crafted data structures and optimized computation (e.g on GPUs) to efficiently implement NN queries. Two of my favorites are Facbook\u2019s FAISS and Spotify\u2019s Annoy. This post should hopefully bring you up to speed on what happens \u201cbehind the hood\u201d of these libraries.", "A first distinction when we talk about nearest neighbors queries is between exact and approximate solutions.", "Exact algorithms need to return the k nearest neighbors of a given query point in a dataset. Here, the naive solution is to simply compare the query element to each element in the dataset, and choose the k that had the shortest distances. This algorithm takes O(dN), where N is the size of the dataset and d is the dimensionality of the instances. At first glance this might actually seem satisfactory, but think about it: 1) this is only for a single query! 2) while true that d is fixed, it can often be very large, and most importantly 3) in the \u201cbig data\u201d paradigm, when datasets can be huge, being linear in the dataset size is no longer satisfactory (even if you\u2019re Google or Facebook!). Other approaches for exact queries use tree structures and can achieve better average complexity but their worst-case complexity still approaches something that\u2019s linear in N.", "Approximate algorithms are given some leeway. There are a couple of different formulations, but the main idea is that they only need to return instances whose distance to the query point is almost that of the real nearest neighbors (where \u2018almost\u2019 is the algorithm\u2019s approximation factor). Allowing for approximate solutions opens the door to randomized algorithms, that can perform an ANN (approximate NN) query in sublinear time.", "Generally-speaking, a common and basic building block for implementing sublinear time algorithms are hash functions. A hash function is any function that maps input into data of fixed size (usually of lower dimension). The most famous example, which you might have encountered by simply downloading files off the internet, is that of checksum hashes. The idea behind them is to generate a \u201cfinger-print\u201d \u2014 i.e, some number that is hopefully unique for a particular chunk of data \u2014 that can be used to verify that the data was not corrupted or tampered with when it was transferred from one place to another.", "These hash functions were designed with this sole purpose in mind. This means that they are actually very sensitive to small changes in the input data; even a single bit that\u2019s changed will completely change the hash value. While this is really what we need for exact duplicate detection (e.g, flagging when two files are really the same), it\u2019s actually the opposite of what we need for near duplicate detection.", "This is precisely what Locality Sensitive Hashing (LSH) attempts to address. As it\u2019s name suggest, LSH depends on the spatiality of the data; in particular, data items that are similar in high-dimension will have a larger chance of receiving the same hash value. This is the goal; there are numerous algorithms that construct hash functions with this property. I will describe one approach, that is amazingly simple and demonstrates the incredibly surprising power of random projections (for another example, see the beautiful Johnson-Lindenstrauss lemma).", "The basic idea is that we generate a hash (or signature) of size k using the following procedure: we generate k random hyperplanes; the i-th coordinate of the hash value for an item x is binary: it is equal to 1 if and only if x is above the i-th hyperplane.", "The entire algorithms is just repeating this procedure L times:", "Let\u2019s understand how LSH can be used to perform ANN queries. The intuition is as follows: If similar items have (with high probability) similar hashes, then given a query item, we can replace the \u201cnaive\u201d comparison against all the items in the dataset, with a comparison only against items with similar hashes (in the common jargon, we refer to this as items that landed \u201cin the same bucket\u201d). Here we see that the fact that we were willing to settle for accuracy is precisely what allows for sublinear time.", "Since inside the bucket we compute exact comparisons, the FP probability (i.e, saying that an item is a NN when it truly isn\u2019t) is zero, so the algorithm always has perfect precision; however, we will only return items from that bucket, so if the true NN was not originally hashed to the bucket, we have no way of returning it. This means that in the context of LSH, when we talk about accuracy we really mean recall.", "Formally, an ANN query using LSH is performed as follows: 1) Find the \u201cbucket\u201d (hash value) of the query item 2) Compare against every other item in the bucket.", "Let\u2019s analyze the computational complexity of this algorithm. It will be quick and easy, I promise!", "Stage 1) costs dk; Stage 2) costs N/2^k in expectation (because there are N points in the dataset and 2^K regions in our partitioned space). Since the entire procedure is repeated L times, the total cost is, on average, LDK+LDN/2^k. When k and L are taken to be about logN, we get the desired O(logN).", "Part 4: LSH Hyperparameters, or the accuracy-time tradeoff", "We\u2019ve seen the basic algorithm for LSH. It has two parameters, k (size of each hash) and L (the number of hash-tables) \u2014 different setting of the values for k,L correspond to different LSH configurations, each with its own time complexity and accuracy.", "Analyzing these formally is a little tricky and requires much more math, but the general take-away is this:", "By careful setting of these parameters, you can get a system that is arbitrarily accurate (what-ever you definition of a \u201cnear duplicate\u201d is), BUT some of these can come at a cost of a very large L , i.e a large computational cost.", "Generally a good approach for settling such trade-offs empirically is to quantify them on a well-defined task, which you can hopefully design using minimal manual labor. In this case, I used the Caltech101 dataset (yes, it\u2019s old; yes, there were image datasets that predated ImageNet!), with images of 101 simple objects. As input to my LSH scheme, I used the 4096-dimensional feature-vectors obtained by passing each image through a pre-trained VGG network. To keep things simple, I assumed that the other images from the same category are true NN in the feature space.", "With a \u201cground truth\u201d at hand, we can try out different hyperparameter combinations and measure their accuracy (recall) and runnning time. Plotting the results gives a nice feel for the accuracy-time trade-off:", "We clearly see that better recall comes at the cost of longer run-time. Note that the actual results are task-dependent: generally speaking, the more similar (in high-dimension) the items you consider \u201cnear\u201d are, the easier the task will be. Finding distant neighbors efficiently is a hard task, beware!", "Part 5: Putting it all together for an example application", "I wanted to piece together this pipeline for a personal project, which is to more efficiently browse my personal photo collection. Returning from a trip, I often have photos from several devices, and many of them are so similar \u2014 my appreciation for the views usually leaves me with tens of photos of pretty much the same things. Semantic similarity to the rescue! Here are some of the results.", "Each row represents a single query; on the left is the query image, and on the right are the images that were hashed to the same bucket, with their actual distance in green. Pretty cool stuff!", "We reviewed two really useful ideas:", "Finally, we saw how the combination of these two ideas \u2014 namely, applying LSH not on the raw data (image, text) but on the deep representation \u2014 can be used to perform fast similarity search in huge collections.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI / ML & social aspects. https://galyona.github.io/website"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4c16058efcb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@galyona?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@galyona?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Gal Yona"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc81e4b81bed8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&user=Gal+Yona&userId=c81e4b81bed8&source=post_page-c81e4b81bed8----d4c16058efcb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.wikiwand.com/en/K-nearest_neighbors_algorithm", "anchor_text": "k-nearest neighbors"}, {"url": "https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf", "anchor_text": "results can improve significantly"}, {"url": "https://towardsdatascience.com/do-gans-really-model-the-true-data-distribution-or-are-they-just-cleverly-fooling-us-d08df69f25eb", "anchor_text": "verify the claim that generative models are really learning the underlying distribution"}, {"url": "https://github.com/facebookresearch/faiss", "anchor_text": "FAISS"}, {"url": "https://github.com/spotify/annoy", "anchor_text": "Annoy"}, {"url": "https://www.wikiwand.com/en/Johnson%E2%80%93Lindenstrauss_lemma", "anchor_text": "Johnson-Lindenstrauss lemma"}, {"url": "https://www.wikiwand.com/en/Precision_and_recall", "anchor_text": "recall"}, {"url": "http://www.vision.caltech.edu/Image_Datasets/Caltech101/", "anchor_text": "Caltech101"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d4c16058efcb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----d4c16058efcb---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d4c16058efcb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d4c16058efcb---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&user=Gal+Yona&userId=c81e4b81bed8&source=-----d4c16058efcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&user=Gal+Yona&userId=c81e4b81bed8&source=-----d4c16058efcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd4c16058efcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d4c16058efcb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4c16058efcb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d4c16058efcb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d4c16058efcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@galyona?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@galyona?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gal Yona"}, {"url": "https://medium.com/@galyona/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "381 Followers"}, {"url": "https://galyona.github.io/website", "anchor_text": "https://galyona.github.io/website"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc81e4b81bed8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&user=Gal+Yona&userId=c81e4b81bed8&source=post_page-c81e4b81bed8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Feaf9cb176976&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb&newsletterV3=c81e4b81bed8&newsletterV3Id=eaf9cb176976&user=Gal+Yona&userId=c81e4b81bed8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}