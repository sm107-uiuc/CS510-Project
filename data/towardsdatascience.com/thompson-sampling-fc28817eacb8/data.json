{"url": "https://towardsdatascience.com/thompson-sampling-fc28817eacb8", "time": 1683015885.238174, "path": "towardsdatascience.com/thompson-sampling-fc28817eacb8/", "webpage": {"metadata": {"title": "Thompson Sampling. Multi-Armed Bandits: Part 5 | by Steve Roberts | Towards Data Science", "h1": "Thompson Sampling", "description": "Over the last few parts in this series we\u2019ve been looking at increasingly complex methods of solving the Multi-Armed Bandit problem. We\u2019ve now reached the final and most complex of all the methods\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Beta_distribution", "anchor_text": "Beta distribution", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af", "anchor_text": "Beta Distribution \u2014 Intuition, Examples, and Derivation", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)", "anchor_text": "Uniform distribution", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "Prior Probability", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "Posterior Probability", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "Part 2", "paragraph_index": 23}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "github", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior", "anchor_text": "Conjugate Prior", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution", "anchor_text": "table of conjugate priors on Wikipedia", "paragraph_index": 33}, {"url": "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf", "anchor_text": "shown to have logarithmic regret", "paragraph_index": 51}], "all_paragraphs": ["Over the last few parts in this series we\u2019ve been looking at increasingly complex methods of solving the Multi-Armed Bandit problem. We\u2019ve now reached the final and most complex of all the methods we\u2019re going to look at: Thompson Sampling.", "If you\u2019re not already familiar with the bandit problem and its terminology you may want to first take a look at the earlier parts of this series, which are as follows:", "All code for the bandit algorithms and testing framework can be found on github: Multi_Armed_Bandits", "Baby Robot is lost in the mall. Using Reinforcement Learning we want to help him find his way back to his mum. However, before he can even begin looking for her, he needs to recharge, from a set of power sockets that each give a slightly different amount of charge.", "Using the strategies from the multi-armed bandit problem we need to find the best socket, in the shortest amount of time, to allow Baby Robot to get charged up and on his way.", "Baby Robot has entered a charging room containing 5 different power sockets. Each of these sockets returns a slightly different amount of charge. We want to get Baby Robot charged up in the minimum amount of time, so we need to locate the best socket and then use it until charging is complete.", "This is identical to the Multi-Armed Bandit problem except that, instead of looking for a slot machine that gives the best payout, we\u2019re looking for a power socket that gives the most charge.", "Up until now, all of the methods we\u2019ve seen for tackling the Bandit Problem have selected their actions based on the current averages of the rewards received from those actions. Thompson Sampling (also sometimes referred to as the Bayesian Bandits algorithm) takes a slightly different approach; rather than just refining an estimate of the mean reward it extends this, to instead build up a probability model from the obtained rewards, and then samples from this to choose an action.", "In this way, not only is an increasingly accurate estimate of the possible reward obtained, but the model also provides a level of confidence in this reward, and this confidence increases as more samples are collected. This process of updating your beliefs as more evidence becomes available is known as Bayesian Inference.", "As an introduction, and to make things easier to work with, let\u2019s simplify the power socket problem. Now, instead of each socket returning a varying amount of charge, each socket will either return some charge or no charge; the rewards have only two possible values: 1 when the chosen socket supplies a charge and 0 when it doesn\u2019t. When a random variable has only two possible outcomes its behaviour can be described by the Bernoulli distribution.", "So now, instead of the amount of charge varying per socket, the probability of a socket producing a charge varies with each socket. We want to find the socket with the highest probability of returning a charge, rather than the socket that gives the most charge.", "As already mentioned, Thompson Sampling generates a model of the reward probabilities. When, as in this case, the available rewards are binary (win or lose, yes or no, charge or no charge) then the Beta distribution is ideal to model this type of probability.", "(For more information on the relationship between the Beta and Bernoulli distributions check out this excellent article:Beta Distribution \u2014 Intuition, Examples, and Derivation).", "The Beta distribution takes two parameters, \u2018\u03b1\u2019 (alpha) and \u2018\u03b2\u2019 (beta). In the simplest terms these parameters can be thought of as respectively the count of successes and failures.", "Additionally, a Beta distribution has a mean value given by:", "Initially we have no idea what the probability is of any given socket producing an output, so we can start by setting both \u2018\u03b1\u2019 and \u2018\u03b2\u2019 to one, which produces a flat line Uniform distribution (shown as the flat, red, line in figure 5.1).", "This initial guess at the probability of the socket producing an output is known as the Prior Probability; it is the probability of the specific event occurring before we have collected any evidence and in this case is represented by the Beta distribution Beta(1,1).", "Once we test a socket, and obtain a reward, we can modify our belief in the likelihood of that socket returning some charge. This new probability, after some evidence has been collected, is known as the Posterior Probability. Again this is given by a Beta distribution, but now the values of \u2018\u03b1\u2019 and \u2018\u03b2\u2019 are updated with the value of the returned reward.", "So, if a socket returns some charge, the reward will be 1 and \u2018\u03b1\u2019, the count of the number of successes, will increment by 1. The count of the number of failures, \u2018\u03b2\u2019, will not increase. If instead no reward was obtained, then \u2018\u03b1\u2019 will stay the same and \u2018\u03b2\u2019 will increment by 1. As more data is collected the Beta distribution moves from being a flat line to become an increasingly accurate model of the probability of the mean reward. By maintaining the values of \u2018\u03b1\u2019 and \u2018\u03b2\u2019 a Thompson sampling algorithm is able to describe the estimated mean reward and the level of confidence in this estimate.", "In contrast to the Greedy algorithm, which at each time step selects the action with the highest estimated reward, even if the confidence in that estimate is low, Thompson sampling instead samples from the Beta distribution of each action and chooses the action with the highest returned value. Since actions that have been tried infrequently have wide distributions (see the blue curve in figure 5.1), they have a larger range of possible values. In this way, a socket that currently has a low estimated mean reward, but has been tested fewer times than a socket with a higher estimated mean, can return a larger sample value and therefore become the selected socket at this time step.", "In the graph above, the blue curve has a lower estimated mean reward than the green curve. Therefore, under Greedy selection, green would be chosen and the blue socket would never be selected. In contrast, Thompson Sampling effectively considers the full width of the curve, which for the blue socket can be seen to extend beyond that of the green socket. In this case the blue socket may be selected in preference to the green one.", "As the number of trials of a socket increases the confidence in the estimated mean increases. This is reflected in the probability distribution becoming narrower and the sampled value will then be drawn from a range of values that are closer to the true mean (see the green curve in figure 5.1). As a result, exploration decreases and exploitation increases, since the sockets with a higher probability of returning a reward will begin to be selected with increasing frequency.", "On the other hand, sockets with a low estimated mean will start to be selected less frequently and will tend to be dropped early from the selection process. Consequently, their true mean may never be found. Since we are only interested in finding the socket with the highest probability of returning a reward, and finding it as quickly as possible, we don\u2019t care if full information of poorly performing sockets is never obtained.", "As in the socket experiments we carried out in previous parts of this series, we will be using a basic socket class, on top of which we add the specific functionality for the algorithm being studied. Then, using this new class, we run it through a set of experiments using the same test harness for all bandit algorithms. Full details of the power socket base class and the accompanying test system are given in Part 2 of this series and all the code can be found on github.", "The implementation of Bernoulli Thompson sampling, as described above, is shown in the BernoulliThompsonSocket class:", "In this class we initialise \u2018\u03b1\u2019 and \u2018\u03b2\u2019 to one, to give the Uniform Distribution. Then, when updating, we simply increment \u2018\u03b1\u2019 if the socket returned a reward, otherwise we update \u2018\u03b2\u2019.", "The \u201csample\u201d function draws a value from the Beta distribution, using the current values of \u2018\u03b1\u2019 and \u2018\u03b2\u2019 as its parameters.", "The evolution of the Beta distribution for each power socket, where we are using the simpler probabilistic power sockets, can be seen in Figure 5.2 below.", "To keep things simple, we\u2019ve reduced the number of sockets to three and these have true probabilities 0.3 (green), 0.7(red) and 0.8 (blue) of returning some power when tested.", "In Figure 5.2 above, the true means of 0.3, 0.7 and 0.8 are shown by the dashed lines. The legend displays the number of trials for each socket and the number of successes that have resulted from these trials.", "The main points to note from Figure 5.2 are the following:", "The simplified socket problem we\u2019ve used so far is a good way to grasp the concepts of Bayesian Thompson Sampling. However, to use this method with our actual socket problem, in which the sockets aren\u2019t binary, but instead return a variable amount of charge, we need to change things slightly.", "In the previous problem we modelled the socket\u2019s behaviour using a Beta distribution. This was chosen because the simplified socket output had only two possible outcomes, some charge or no charge, and could therefore be described using a Bernoulli distribution. When a value drawn from a Bernoulli distribution (the likelihood value) is multiplied by a value drawn from a Beta distribution (the prior probability), then the resultant value (the posterior probability) also has a Beta distribution. When this occurs, such that the likelihood multiplied by the prior results in a posterior with the same distribution type as the prior, the prior is referred to as a Conjugate Prior.", "With our standard socket problem, each socket returns a real value described by a normal distribution. If we assume we know the variance of our socket (which is actually 1, since we use an unmodified version of the numpy randn function in our code), then from the table of conjugate priors on Wikipedia we can see that the conjugate prior also has a normal distribution. If we don\u2019t know the variance of our distribution, or we\u2019re using a different distribution, then we simply need to pick one of other conjugate priors from the table and adjust our algorithm accordingly.", "So we can model the output of a socket using a normal distribution and gradually refine this model by updating its mean and variance parameters. If, instead of using the variance, we use the precision \u2018\u03c4\u2019 (tau), where precision is just one over the variance (precision \u03c4 = 1/variance), then we can use the simple update rules for the mean \u2018\u03bc\u2080\u2019 and total precision \u2018\u03c4\u2080\u2019 given by:", "At a first glance this looks rather intimidating, but all it\u2019s basically saying is that we have 2 parameters \u2018\u03bc\u2080\u2019 and \u2018\u03c4\u2080\u2019 that we\u2019ll update each time we test a socket, just as we did with \u2018\u03b1\u2019 and \u2018\u03b2\u2019 for the Bernoulli socket. Except in that case those parameters represented the number of successes and failures of the socket, whereas \u2018\u03bc\u2080\u2019 and \u2018\u03c4\u2080\u2019 represent the estimated mean and the precision, representing the confidence in the estimated mean.", "Additionally we can make a couple of other simplifications:", "With these simplifications we tame the scary mathematics! It\u2019s now clear that all we need to do is to keep estimates of the mean and precision of the reward from each socket and then use 2 simple rules to update these values. Things become even clearer when these equations are translated into code.", "The associated code for a Gaussian Thompson sampling socket is shown below. This retains all of the basic functionality we\u2019ve used in previous socket types and adds the parameters and update function for the posterior distribution that is used to model the socket output.", "Note that in this update function we\u2019ve replaced the summation over all of the observed rewards with \u2018self.n * self.Q\u2019. This gives us exactly the same value without having to retain the sum of rewards which, as described previously, could potentially grow to an unmanageable size.", "The two update functions, given in the equations above, translate into the simple lines of code shown in the update function.", "The other main points to note are:", "If you look back at the code for the base power socket, when a socket is sampled it returns an amount of charge given by a normal distribution around its true mean value:", "In the charge function above, The numpy \u201crandn\u201d function returns a random value from a normal distribution of mean 0 and variance 1. By adding the true socket reward value \u2018q\u2019 to this, we shift the mean to get the distribution to be centred on the actual output of the socket.", "In the sample function of the Thompson socket a very similar function can be seen:", "Except in this case the normal is centred on self.\u03bc_0, the posterior mean. Additionally, it can be seen that randn is now divided by the square root of the posterior precision self.\u03c4_0. Remember that the precision is just one over the variance and that variance is the standard deviation squared. Therefore dividing by the square root of the precision is identical to multiplying by the standard deviation. This is what changes the width of the distribution, reducing it as more samples occur and we become more confident in our estimated mean.", "As with the Bernoulli experiment done previously, we\u2019ve observed the socket selection over 1000 trials, as shown by the probability density curves below. Due to the posterior distributions starting out with almost flat curves, each of the 5 sockets gets tested once during the first 5 trials. After this, socket 4 (shown as the red curve) dominates the further trials. By the end of the tests it has a tall, thin, curve centred on a value of 12 (the true socket reward value) indicating a high level of confidence in this value.", "The only other socket that is tested more than once, over the first 200 tests, is socket 5 (the purple curve, which has a true socket reward of 10). However, it is only tested 3 times and therefore has a small, fat, distribution curve, indicating a low confidence in its value.", "From Figure 5.3 its clear to see how Thompson sampling quickly locates and then exploits the best socket, with the other sockets being left largely untested. In this way the algorithm manages to return a large, and nearly optimal, accumulated reward.", "The regret obtained when using the Thompson Sampling algorithm with our standard socket selection problem is shown in Figure 5.4 below.", "As was seen for the UCB algorithm, the regret is practically zero, meaning that the best socket was nearly always chosen. This can also be seen in the plot of Cumulative Reward vs Time, in which the actual obtained reward is such a close match for the optimal that it obscured by this curve on the graph. As was seen with the probability density curves for Gaussian Thompson Sampling, the algorithm quickly locks onto the best action and then ruthlessly exploits this, resulting in a very low level of regret.", "As with the UCB algorithm, Thompson Sampling can be shown to have logarithmic regret, where the value of the regret falls to almost zero as time progresses.", "(Due to the very small number of actions, and the distinct reward values of each socket, this logarithmic decline in the regret isn\u2019t seen in our experiment, since we\u2019re already down near to zero regret.)", "For a more in-depth look at Thompson Sampling, its uses and mathematical framework check out the following:", "Algorithms that solve the bandit problem need to find a way to balance the trade-off between exploitation and exploration. They need to look for the best actions to take while at the same time trying to make use of the information they\u2019ve already gained.", "In simple approaches, such as Epsilon-Greedy, this trade-off is achieved by mainly using the action that currently gives the most reward and adding simple exploration by now and again randomly trying some of the other actions. In more complex solutions, such as UCB, again the actions with the highest mean reward are selected most often but this is balanced by a confidence measure. This ensures that actions that have not been selected often will get tested.", "Thompson Sampling takes a different approach to these other methods. Instead of simply maintaining an estimate of the reward, it gradually refines a model of the probability of the reward for each action and actions are chosen by sampling from this distribution. It is therefore possible to get an estimate for the mean reward value of an action, plus a measure of confidence for that estimate. As we saw in our experiments, this allows it to quickly locate and lock onto the optimal action, to give a near optimal accumulated return.", "But is Thompson Sampling the best bandit algorithm and, more importantly, is it the one we should use to charge Baby Robot? To know the answers to these questions you\u2019ll need to wait until the final part of this series, when we have the bandit algorithms go head-to-head in a final showdown!", "We\u2019ve looked at the theory behind Thompson Sampling and investigated how it can be used on a couple of simple problems. To see how this can be extended further, to work when the data comes from a normal distribution with unknown mean and variance, check out the article below\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D., \"The evolution of artificial neural networks\""], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffc28817eacb8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652----fc28817eacb8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/baby-robot-guide", "anchor_text": "A Baby Robot\u2019s Guide To Reinforcement Learning"}, {"url": "https://unsplash.com/@nci?utm_source=medium&utm_medium=referral", "anchor_text": "National Cancer Institute"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697", "anchor_text": "Part 1: Mathematical Framework and Terminology"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "Part 2: The Bandit Framework"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "Part 3: Bandit Algorithms"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Greedy Algorithm"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Optimistic-Greedy Algorithm"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Epsilon-Greedy Algorithm (\u03b5-Greedy)"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "Regret"}, {"url": "https://towardsdatascience.com/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f", "anchor_text": "Part 4: The Upper Confidence Bound (UCB) Bandit Algorithm"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution"}, {"url": "https://en.wikipedia.org/wiki/Beta_distribution", "anchor_text": "Beta distribution"}, {"url": "https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af", "anchor_text": "Beta Distribution \u2014 Intuition, Examples, and Derivation"}, {"url": "https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)", "anchor_text": "Uniform distribution"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "Prior Probability"}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "Posterior Probability"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "Part 2"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "github"}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior", "anchor_text": "Conjugate Prior"}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution", "anchor_text": "table of conjugate priors on Wikipedia"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697#23ee", "anchor_text": "Sample Average Estimates"}, {"url": "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf", "anchor_text": "shown to have logarithmic regret"}, {"url": "http://A Tutorial on Thompson Sampling Daniel J. Russo1", "anchor_text": "A Tutorial on Thompson Sampling"}, {"url": "https://towardsdatascience.com/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f", "anchor_text": "UCB Bandit Algorithm"}, {"url": "https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb", "anchor_text": "A Comparison of Bandit Algorithms"}, {"url": "https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d", "anchor_text": "Thompson Sampling using Conjugate PriorsMulti-Armed Bandits: Part 5btowardsdatascience.com"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----fc28817eacb8---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-armed-bandit?source=post_page-----fc28817eacb8---------------multi_armed_bandit-----------------", "anchor_text": "Multi Armed Bandit"}, {"url": "https://medium.com/tag/thompson-sampling?source=post_page-----fc28817eacb8---------------thompson_sampling-----------------", "anchor_text": "Thompson Sampling"}, {"url": "https://medium.com/tag/baby-robot-guide?source=post_page-----fc28817eacb8---------------baby_robot_guide-----------------", "anchor_text": "Baby Robot Guide"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&user=Steve+Roberts&userId=6b6735266652&source=-----fc28817eacb8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&user=Steve+Roberts&userId=6b6735266652&source=-----fc28817eacb8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffc28817eacb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fc28817eacb8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fc28817eacb8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fc28817eacb8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fc28817eacb8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d5f26d16450&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthompson-sampling-fc28817eacb8&newsletterV3=6b6735266652&newsletterV3Id=1d5f26d16450&user=Steve+Roberts&userId=6b6735266652&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}