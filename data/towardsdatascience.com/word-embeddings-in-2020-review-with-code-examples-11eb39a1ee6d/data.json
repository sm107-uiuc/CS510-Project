{"url": "https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d", "time": 1683011481.208291, "path": "towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d/", "webpage": {"metadata": {"title": "Word embeddings in 2020. Review with code examples | by Rostyslav Neskorozhenyi | Towards Data Science", "h1": "Word embeddings in 2020. Review with code examples", "description": "In this article we will study word embeddings \u2014 digital representation of words suitable for processing by machine learning algorithms."}, "outgoing_paragraph_urls": [{"url": "http://ai-labs.org/", "anchor_text": "AI Labs", "paragraph_index": 1}, {"url": "https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb", "anchor_text": "Google Colab Notebook", "paragraph_index": 1}, {"url": "https://github.com/dav/word2vec/", "anchor_text": "word2vec", "paragraph_index": 10}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "distance", "paragraph_index": 11}, {"url": "https://github.com/facebookresearch/fastText", "anchor_text": "FastText", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/N-gram", "anchor_text": "n-grams", "paragraph_index": 21}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 27}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 30}, {"url": "https://github.com/google-research/bert", "anchor_text": "BERT", "paragraph_index": 30}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "CTRL", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "RNN", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradient problem", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "attention mechanizm", "paragraph_index": 31}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "article", "paragraph_index": 31}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Transformers", "paragraph_index": 32}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "BERT", "paragraph_index": 32}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "XLNet", "paragraph_index": 32}, {"url": "https://huggingface.co/transformers/model_doc/dialogpt.html", "anchor_text": "DialoGPT", "paragraph_index": 32}, {"url": "https://huggingface.co/transformers/model_doc/gpt2.html", "anchor_text": "GPT-2", "paragraph_index": 32}, {"url": "https://github.com/AliOsm/simplerepresentations", "anchor_text": "simplerepresentations", "paragraph_index": 45}], "all_paragraphs": ["In this article we will study word embeddings \u2014 digital representation of words suitable for processing by machine learning algorithms.", "Originally I created this article as a general overview and compilation of current approaches to word embedding in 2020, which our AI Labs team could use from time to time as a quick refresher. I hope that my article will be useful to a wider circle of data scientists and developers. Each word embedding method in the article has a (very) short description, links for further study, and code examples in Python. All code is packed as Google Colab Notebook. So let\u2019s begin.", "According to Wikipedia, Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.", "The most basic method for transforming words into vectors is to count occurrence of each word in each document. Such approach is called countvectorizing or one-hot encoding.", "The main principle of this method is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in each document. Strictly speaking, the columns of the resulting matrix are words and the rows are documents.", "Another approach in countvectorizing is just to place 1 if the word is found in the document (no matter how often) and 0 if the word is not found in the document. In this case we get real \u2018one-hot\u2019 encoding.", "With a large corpus of documents some words like \u2018a\u2019, \u2018the\u2019, \u2018is\u2019, etc. occur very frequently but they don\u2019t carry a lot of information. Using one-hot encoding approach we can decide that these words are important because they appear in many documents. One of the ways to solve this problem is stopwords filtering, but this solution is discrete and not flexible.", "TF-IDF (term frequency \u2014 inverse document frequency) can deal with this problem better. TF-IDF lowers the weight of commonly used words and raises the weight of rare words that occur only in current document. TF-IDF formula looks like this:", "Where TF is calculated by dividing number of times the word occurs in the document by the total number of words in the document", "IDF (inverse document frequency), interpreted like inversed number of documents, in which the term we\u2019re interested in occurs. N \u2014 number of documents, n(t) \u2014 number of documents with current word or term t.", "The most commonly used models for word embeddings are word2vec and GloVe which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).", "Word2Vec word embeddings are vector representations of words, that are typically learnt by an unsupervised model when fed with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that distance between vectors for words with close meanings (\u201cking\u201d and \u201cqueen\u201d for example) are closer than distance for words with complety different meanings (\u201cking\u201d and \u201ccarpet\u201d for example).", "Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:", "king \u2014 man + woman = queen", "Check how similar are vectors for words \u2018woman\u2019 and \u2018man\u2019.", "Check how similar are vectors for words \u2018king\u2019 and \u2018woman\u2019.", "Another word embedding method is Glove (\u201cGlobal Vectors\u201d). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each \u201cword\u201d (the rows), you count how frequently we see this word in some \u201ccontext\u201d (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a \u201creconstruction loss\u201d. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.", "Find similarity between King and Queen (higher value is better).", "Find similarity between King and carpet.", "Check if king \u2014 man + woman = queen. We will multiply vectors for \u2018man\u2019 and \u2018woman\u2019 by two, because subtracting one vector for \u2018man\u2019 and adding the vector for \u2018woman\u2019 will do little to the original vector for \u201cking\u201d, likely because those \u201cman\u201d and \u201cwoman\u201d are related themselves.", "FastText is an extension of word2vec. FastText was developed by the team of Tomas Mikolov who created the word2vec framework in 2013.", "The main improvement of FastText over the original word2vec vectors is the inclusion of character n-grams, which allows computing word representations for words that did not appear in the training data (\u201cout-of-vocabulary\u201d words).", "Create an embedding for the word \u2018king\u2019.", "Get most similar words for the word \u2018king\u2019.", "Test model ability to create vectors for unknown words.", "Get most similar words for unknown word \u2018king-warrior\u2019.", "Unlike traditional word embeddings such as word2vec and GLoVe, the ELMo vector assigned to a token or a word depends on current context and is actually a function of the entire sentence containing that word. So the same word can have different word vectors under different contexts. Also ELMo representations are purely character based so they are not limited to any predefined vocabulary.", "ELMo is a deep contextualized word representation that models both (1) complex characteristics of the word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.", "In order to send sentences to the model we need to split them into the arrays of words and pad arrays to the same length. Also we will create \u2018mask\u2019 array that will show whether element is a real word or a padding symbol (in our case \u2014 \u2018_\u2019). We will use \u2018mask\u2019 array for visualization later to show only real words.", "Convert Tensorflow tensors to numpy array.", "At last it\u2019s time for current state-of-the-art approach \u2014 Transformers. Famous GPT-2, BERT, CTRL are all Transformers-based and produce context-sensitive embeddings like ELMo. But unlike ELMo Transformers do not use RNN, they do not require to process words in sentence sequentially one-by-one. All words in the sentence are processed in parallel, this approach speeds up processing and solves vanishing gradient problem.", "Transformers use the attention mechanizm to describe the connections and dependencies of each specific word with all other words in the sentence. This mechanism and the main principles of Transformers described in detail in a beautifully illustrated article by Jay Alammar.", "For our example we will use brilliant Transformers library which contains the latest Transformers-based models (such as BERT, XLNet, DialoGPT or GPT-2).", "Let\u2019s make some embeddings with BERT. Firstly we will need to install Transformers library.", "Now we import pytorch, the pretrained BERT model, and a BERT tokenizer that will do all the needed work of converting sentences into format appropriate for BERT (tokenizing itself and adding special tokens like [SEP] and [CLS]).", "Enter some sentences and tokenize them.", "Note that some tokens may look like this: [\u2018aa\u2019, \u2018##th\u2019, \u2018##ur\u2019, \u2018pen\u2019, \u2018##dra\u2019, \u2018##gon\u2019]. This is because of the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. BERT tokenizer uses vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. So, if the word is not mentioned in a vocabulary, that words is splitted into subwords and characters. The two hash signs (##) before some subwords shows that subword is part of a larger word and preceded by another subword.", "We will use tokenizer.encode_plus function, that will:", "Segment ID. BERT is trained on and expects sentence pairs using 1s and 0s to distinguish between the two sentences. We will encode each sentence separately so we will just mark each token in each sentence with 1.", "Now we can call BERT model and finally get model hidden states from which we will create word embeddings.", "We will use last four hidden layers to create each word embedding.", "Concatenate four layers for each token to create embeddings", "Let\u2019s examine embeddings for the first sentence. Firstly we need to get ids of tokens we need to compare.", "We can see that word \u2018king\u2019 is placed at indexes 1 and 17. We will check distance between embeddings 1 and 17. Also, we will check if embedding for the word \u2018arthur\u2019 is closer to \u2018king\u2019 then to the word \u2018table\u2019.", "So we see that embeddings for two \u2018kings\u2019 are quite similar but not the same, and Archtur is closer to be a king than a table.", "Things may be simplier with simplerepresentations module. This module does all the work we did earlier \u2014 extracts needed hidden states from BERT and creates embeddings in a few lines of code.", "Check distaces between Archtur, king and table.", "I hope that after reading this article you have formed an idea of the current approaches to word embeddings and began to understand how to quickly implement these approaches in Python. The world of NLP is diverse and there are many more models and methods for embeddings. In my article I focused on the most common and those that we ourselves often use in our work. You can find additional information in the References section."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F11eb39a1ee6d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@slanjr?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Rostyslav Neskorozhenyi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c7f264e81e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=post_page-8c7f264e81e5----11eb39a1ee6d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11eb39a1ee6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----11eb39a1ee6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11eb39a1ee6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=-----11eb39a1ee6d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://github.com/amueller/word_cloud", "anchor_text": "WordCloud"}, {"url": "http://ai-labs.org/", "anchor_text": "AI Labs"}, {"url": "https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb", "anchor_text": "Google Colab Notebook"}, {"url": "https://seaborn.pydata.org/", "anchor_text": "seaborn"}, {"url": "https://seaborn.pydata.org/", "anchor_text": "seaborn"}, {"url": "https://seaborn.pydata.org/", "anchor_text": "seaborn"}, {"url": "https://github.com/dav/word2vec/", "anchor_text": "word2vec"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "distance"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "anchor_text": "developers.google.com"}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "https://code.google.com/archive/p/word2vec/"}, {"url": "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", "anchor_text": "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"}, {"url": "https://github.com/facebookresearch/fastText", "anchor_text": "FastText"}, {"url": "https://en.wikipedia.org/wiki/N-gram", "anchor_text": "n-grams"}, {"url": "https://fasttext.cc/docs/en/crawl-vectors.html", "anchor_text": "https://fasttext.cc/docs/en/crawl-vectors.html"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "https://tfhub.dev/google/elmo/3"}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "https://tfhub.dev/google/elmo/3"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA"}, {"url": "https://plotly.com/", "anchor_text": "plotly"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://github.com/google-research/bert", "anchor_text": "BERT"}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "CTRL"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "RNN"}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradient problem"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "attention mechanizm"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "article"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "http://jalammar.github.io"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Transformers"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "BERT"}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "XLNet"}, {"url": "https://huggingface.co/transformers/model_doc/dialogpt.html", "anchor_text": "DialoGPT"}, {"url": "https://huggingface.co/transformers/model_doc/gpt2.html", "anchor_text": "GPT-2"}, {"url": "https://github.com/AliOsm/simplerepresentations", "anchor_text": "simplerepresentations"}, {"url": "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/", "anchor_text": "BERT Word Embeddings Tutorial"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "The Illustrated GPT-2 (Visualizing Transformer Language Models)"}, {"url": "https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598", "anchor_text": "FROM Pre-trained Word Embeddings TO Pre-trained Language Models \u2014 Focus on BERT"}, {"url": "https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30", "anchor_text": "Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuning"}, {"url": "https://medium.com/swlh/playing-with-word-vectors-308ab2faa519", "anchor_text": "Playing with word vectors"}, {"url": "https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010", "anchor_text": "Intuitive Guide to Understanding GloVe Embeddings"}, {"url": "https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/", "anchor_text": "Word Embeddings in Python with Spacy and Gensim"}, {"url": "https://medium.com/analytics-vidhya/brief-review-of-word-embedding-families-2019-b2bbc601bbfe", "anchor_text": "Brief review of word embedding families (2019)"}, {"url": "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795", "anchor_text": "Word embeddings: exploration, explanation, and exploitation (with code in Python)"}, {"url": "https://medium.com/tag/data-science?source=post_page-----11eb39a1ee6d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----11eb39a1ee6d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----11eb39a1ee6d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----11eb39a1ee6d---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----11eb39a1ee6d---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11eb39a1ee6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----11eb39a1ee6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11eb39a1ee6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----11eb39a1ee6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11eb39a1ee6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c7f264e81e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=post_page-8c7f264e81e5----11eb39a1ee6d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3835a199cf2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&newsletterV3=8c7f264e81e5&newsletterV3Id=3835a199cf2e&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----11eb39a1ee6d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Written by Rostyslav Neskorozhenyi"}, {"url": "https://medium.com/@slanjr/followers?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "134 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/slanj/", "anchor_text": "https://www.linkedin.com/in/slanj/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c7f264e81e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=post_page-8c7f264e81e5----11eb39a1ee6d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3835a199cf2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d&newsletterV3=8c7f264e81e5&newsletterV3Id=3835a199cf2e&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----11eb39a1ee6d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Rostyslav Neskorozhenyi"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuningTeach GPT-2 to write like a mad scientist"}, {"url": "https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "\u00b77 min read\u00b7Jun 8, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff85e6d1f4e30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----f85e6d1f4e30----0-----------------clap_footer----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30?source=author_recirc-----11eb39a1ee6d----0---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff85e6d1f4e30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30&source=-----11eb39a1ee6d----0-----------------bookmark_preview----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----11eb39a1ee6d----1---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----11eb39a1ee6d----1-----------------bookmark_preview----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----11eb39a1ee6d----2---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----11eb39a1ee6d----2-----------------bookmark_preview----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/tile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Rostyslav Neskorozhenyi"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/tile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "Tile (Slice) YOLO Dataset for Small Objects DetectionImplement a simple script that can cut images and corresponding labels into tiles of a specified size"}, {"url": "https://towardsdatascience.com/tile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": "\u00b75 min read\u00b7Mar 9, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa75bf26f7fa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----a75bf26f7fa2----3-----------------clap_footer----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/tile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2?source=author_recirc-----11eb39a1ee6d----3---------------------2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa75bf26f7fa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftile-slice-yolo-dataset-for-small-objects-detection-a75bf26f7fa2&source=-----11eb39a1ee6d----3-----------------bookmark_preview----2d29e1c6_4d3a_4ea3_946b_e0311e5355dd-------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "See all from Rostyslav Neskorozhenyi"}, {"url": "https://towardsdatascience.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----0-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----11eb39a1ee6d----0-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----1-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----11eb39a1ee6d----1-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----0-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----11eb39a1ee6d----0---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----11eb39a1ee6d----0-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----11eb39a1ee6d----1---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----11eb39a1ee6d----1-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----11eb39a1ee6d----2---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----11eb39a1ee6d----2-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----11eb39a1ee6d----3---------------------1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----11eb39a1ee6d----3-----------------bookmark_preview----1a06cf0f_a2ea_4f6a_a1a9_412276c3fb06-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----11eb39a1ee6d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}