{"url": "https://towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9", "time": 1683012660.890302, "path": "towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9/", "webpage": {"metadata": {"title": "Build a bidirectional text generator with XLNet | by Rostyslav Neskorozhenyi | Towards Data Science", "h1": "Build a bidirectional text generator with XLNet", "description": "Generate text in a new way with Permutation Language Modeling"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers", "paragraph_index": 0}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "text-generation", "paragraph_index": 0}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "XLNet", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://colab.research.google.com/drive/1RhHiKTp0os2_q5z6pKS6vQUz0SM1EXrM", "anchor_text": "link", "paragraph_index": 4}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Transformers", "paragraph_index": 4}, {"url": "https://huggingface.co/transformers/tokenizer_summary.html", "anchor_text": "link", "paragraph_index": 6}, {"url": "https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e", "anchor_text": "proposed", "paragraph_index": 7}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html#xlnetlmheadmodel", "anchor_text": "here", "paragraph_index": 8}], "all_paragraphs": ["Current Transformers based models, like GPT-2 or even GPT-3 show incredible achievements in the task of text-generation (prediction of the next probable word based on the previous sequence of words). These models can create long, creative and cohesive texts, but usually they can generate text only in one direction, from left to right. I was wondering if there is a way to generate text in both directions and having some start phrase (for example \u201ctext generation is cool\u201d) to see what story will unfold around it. XLNet was the solution: due to its using of all permutations of the input sequence factorization order this model can help to generate text in any direction.", "In this article we will not study in detail the internal principles of XLNet (excellent brief explanation you can find here). Instead, we\u2019ll start experimenting right away: we will practice a little bit in masked word prediction with XLNet, try to implement top-K bidirectional generation, and then implement a more efficient approach that combines beam search and top-K sampling.", "At the end of the article we will get a generator capable of creating such text based on the start phrase (which is highlighted in bold):", "Following up on my initial thoughts: text generation is cool! It works great for creating blog header, title etc. You will need Word 2013", "Let\u2019s begin. We will conduct all our experiments in Google Collab Notebook (with GPU environment), which is available by this link, so the only module we will need to install is the excellent Transformers library. This library provides a simple interface to XLNet, as well as to many other transformers based models.", "One of the advantages of XLNet is that this model can perfectly cope with the prediction of several related masked words while taking into account the previous context. For example, I will mention in the text that I gave you three apples, and then ask the model to tell me who now owns some apples by feeding the model a sentence with masked words: \u201c<mask> have <mask> apples in hands\u201d. As a result, we will see that the model perfectly understands who has apples and how many.", "Before we can start communicating with the model, we need to load it, as well as load a tokenizer that processes the incoming text into a digital form understandable for the model. In the basic form, tokenization is splitting of the text into words or subwords, which then are converted to ids. Each model requires text to be tokenized in a specific way. XLNet uses SentencePiece method. You can read more about the tokenization process at the link.", "Also we need to add a padding text to help XLNet with short texts as was proposed by Aman Rusia.", "Predict top 5 words for each <mask> token. To make a prediction we need to feed the model with tokenized text, masked words indexes and permutation masks. Permutation masks are needed to disable input tokens to attend to masked tokens. You can read more about model parameters here.", "Now when we know how to predict masked words with XLNet it\u2019s time to create a top-k bidirectional text generator. Its work principles are simple. We will create a loop and at each iteration the model will predict top-k tokens for a masked word on the right or on the left side of start phrase. After that we add random token from topK to the start phrase and repeat iteration for n times.", "Not too impressive. There is a lot of repetitions and whole text looks meaningless. But we will find a better solution.", "As we can see, it is still quite difficult for the model to generate text right-to-left. We often get a word that does not fit into the context well, which leads to an even less suitable next word. As a result, the generated text becomes incoherent.", "We can increase the chances of finding connected word sequences by generating words not by one on each side of the starting phrase, but by creating a certain number of beams of word sequences and choosing one of the most probable beams of a certain length.", "Thus, we get some kind of combination of top-k sampling and beam search. The principle of the resulting method is shown in the diagram.", "The bidirectional generation process consists of n iterations. I split each iteration into four steps for better understanding:", "I hope that the description was clear enough and the diagram will help you figure it out. The main thing is that, based on my experiments, this method in most cases allows you to generate quite coherent text bidirectionally.", "Let\u2019s implement the method in code. Firstly we will create a function that will take tokenized start sentence, a sequence of token candidates with their probabilities and generate next n probable sequences of token candidates on the right or on the left side. We will use this function iteratively, so generated token sequences from previous iteration will serve as input on the next iteration.", "Now we will create beam_gen function that will generate a list of token beams of given length (depth) using token candidates proposed by candidates_gen.", "beam_gen function will return final beams list sorted by probability.", "Let\u2019s gather all parts together in a bi_gen function. bi_gen will be able to generate text left-to-right (parameter direction=\u2019right\u2019), right-to-left (parameter direction=\u2019left\u2019), or in both directions (parameter direction=\u2019both\u2019)", "If both directions are selected, generator will work in the following way: generate n_tokens on the left side, after that \u2014 n tokens in the right side, then again n tokens on the left side and so on. It will repeat number of times, that is saved in iterations parameter.", "We will separately indicate in first_sample_size parameter the number of candidates in the first stage of beam search. This number can be higher than the number of candidates in the next stages (specified in the variable sample_size), since it is important to get enough candidates for the first token, on which all subsequent sequences will be based. According to my observations, this approach increases the likelihood of generating a coherent and reasonably probable sequence of tokens.", "We will use high temperature parameter to lower model confidence in its top token choices. This allows to make the generation more varied and not get stuck with the most likely repeating sequences of tokens.", "And finally we will try our bidirectional text generator with start phrase \u201ctext generation is cool\u201d.", "It\u2019s also starting to seem somewhat like we are embarking into a new world largely controlled by artificial intelligence based on its ability over a long period to manipulate, manage and adapt our daily lives.", "The entire previous paragraph was generated by our new text generator. The text is pretty convincing, isn\u2019t it? Therefore, please accept my congratulations. We have created almost the first of its kind transformers based bidirectional text generator. And while it still makes a lot of mistakes, it can be used to create a lot of interesting and fun stories that will grow around any phrase that comes to your mind.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F49d9d37b48a9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@slanjr?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Rostyslav Neskorozhenyi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c7f264e81e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=post_page-8c7f264e81e5----49d9d37b48a9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@bdchu614?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Brendan Church"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "text-generation"}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "XLNet"}, {"url": "https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0", "anchor_text": "here"}, {"url": "https://colab.research.google.com/drive/1RhHiKTp0os2_q5z6pKS6vQUz0SM1EXrM", "anchor_text": "link"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Transformers"}, {"url": "https://huggingface.co/transformers/tokenizer_summary.html", "anchor_text": "link"}, {"url": "https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e", "anchor_text": "proposed"}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html#xlnetlmheadmodel", "anchor_text": "here"}, {"url": "https://draw.io/", "anchor_text": "draw.io"}, {"url": "https://medium.com/tag/data-science?source=post_page-----49d9d37b48a9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----49d9d37b48a9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----49d9d37b48a9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----49d9d37b48a9---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/programming?source=post_page-----49d9d37b48a9---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----49d9d37b48a9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=-----49d9d37b48a9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F49d9d37b48a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----49d9d37b48a9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----49d9d37b48a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slanjr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rostyslav Neskorozhenyi"}, {"url": "https://medium.com/@slanjr/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "134 Followers"}, {"url": "https://www.linkedin.com/in/slanj/", "anchor_text": "https://www.linkedin.com/in/slanj/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c7f264e81e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=post_page-8c7f264e81e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3835a199cf2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9&newsletterV3=8c7f264e81e5&newsletterV3Id=3835a199cf2e&user=Rostyslav+Neskorozhenyi&userId=8c7f264e81e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}