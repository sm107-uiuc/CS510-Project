{"url": "https://towardsdatascience.com/building-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb", "time": 1683015537.900949, "path": "towardsdatascience.com/building-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb/", "webpage": {"metadata": {"title": "Building an Event-Driven Data Pipeline to Copy Data from Amazon S3 to Azure Storage | by Yi Ai | Towards Data Science", "h1": "Building an Event-Driven Data Pipeline to Copy Data from Amazon S3 to Azure Storage", "description": "It may be a requirement of your business to move a good amount of data periodically from one public cloud to another. More specifically, you may face mandates requiring a multi-cloud solution. This\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob/layers", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://github.com/yai333/DataPipelineS32Blob", "anchor_text": "Github Repo", "paragraph_index": 35}, {"url": "https://console.aws.amazon.com/cloudformation", "anchor_text": "console", "paragraph_index": 36}, {"url": "https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob", "anchor_text": "GitHub repo", "paragraph_index": 41}], "all_paragraphs": ["It may be a requirement of your business to move a good amount of data periodically from one public cloud to another. More specifically, you may face mandates requiring a multi-cloud solution. This article covers one approach to automate data replication from AWS S3 Bucket to Microsoft Azure Blob Storage container using Amazon S3 Inventory, Amazon S3 Batch Operations, Fargate, and AzCopy.", "Your company produces new CSV files on-premises every day with a total size of around 100GB after compression. All files have a size of 1\u20132 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3 am, and 5 am. Your business has decided to copy those CSV files from S3 to Microsoft Azure Storage after all files uploaded to S3. You have to find an easy and fast way to automate the data replication workflow.", "To accomplish this task, we can build a data pipeline to copy data periodically from S3 to Azure Storage using AWS Data Wrangler, Amazon S3 Inventory, Amazon S3 Batch Operations, Athena, Fargate, and AzCopy.", "The diagram below represents the high-level architecture of the pipeline solution:", "We use CDK to build our infrastructure on AWS. First, let\u2019s create a source Bucket to receive files from external providers or on-premise and set up daily inventory reports that provide a flat-file list of your objects and metadata.", "Next, create a destination bucket as temporary storage with lifecycle policy expiration rule configured on prefix /tmp_transition. All files with the prefix (eg. /tmp_transition/file1.csv) will copy to Azure and will be removed by lifecycle policy after 24hours.", "Use the following code to create S3 buckets.", "Next, we need to create VPC with both public and private subnets, NAT Gateway, an S3 endpoint, and attach an endpoint policy that allows access to the Fargate container to which S3 bucket we are copying data to Azure.", "Now define your VPC and related resources using the following code.", "While creating NAT Gateway, an Elastic IP Address will create in AWS. We will need the IP address to set up the Azure Storage Firewall rule in step3.", "To simplify managing resources, we can use the Azure Resource Manager template (ARM template) to deploy resources at our Azure subscription level.", "I will assume you already have an Azure Subscription setup. We will use Cloud shell to deploy a Resource Group, Azure Storage account, a container, and Firewall rule to allow traffic from a specific IP address.", "Click on the Cloud Shell icon in the Azure Portal's header bar, and it will open the Cloud Shell.", "Run the following command to deploy:", "Once the template has been deployed, we can verify the deployment by exploring the Azure portal's resource group. All resources deployed will be displayed in the Overview section of the Resource group.", "Let\u2019s create a Firewall rule for our Storage Account:", "We will then generate Shared Access Signatures (SAS) to grant limited access to Azure Storage resources.", "We will get the required SAS and URLs that grant (a)dd (d)elete (r)ead (w)rite access to a blob container democontainer.", "Let\u2019s move back to AWS and put SAS to AWS SSM Parameter Store.", "Run following command in local terminator.", "Now, let\u2019s move up to lambda functions. We will create three lambda functions and one lambda layer:", "This lambda function uses AWS Data Wrangler\u2019s Athena module to filter new files in the past UTC date and save files list to a CSV manifest file.", "Copy the following code to CDK stack.py. download awswranger-layerzip file from here.", "In the above coding, we use Athena query to create Glue Database, Table and add a partition to that table every day. Then lambda executes except query to return the difference between the two date partitions.", "Note that start_query_execution is asynchronous, hence no need to wait for the result in Lambda. Once the query is executed, the result will save to s3_output=f\"s3://{os.getenv('DESTINATION_BUCKET_NAME')}/csv_manifest/dt={partition_dt}\" as a CSV file.", "In this section, we will create a lambda function fn_create_batch_job and enable Amazon S3 to send a notification to trigger fn_create_batch_job when a CSV file is added to an Amazon S3 Bucket /csv_manifest prefix. Put following code to CDK stack.py:", "Create ./src/lambda_create_batch_job.py with the following code:", "Lambda fn_create_batch_job function create S3 Batch Operation Job, copy all the files listed in CSV manifest to S3 Destination Bucket /tmp_transition prefix .", "S3 Batch Operations is an Amazon S3 data management feature that lets you manage billions of objects at scale. To start an S3 Batch Operation Job, we also need to set up an IAM role S3BatchRole with the corresponding policies:", "We will create an Eventbridge custom rule that tracks an S3 Batch Operations job in Amazon EventBridge through AWS CloudTrail and send events in Completed status to the target notification resource fn_process_transfer_task .", "Lambda fn_process_transfer_task will then start a Fargate Task programmatically to copy files in /tmp_transition prefix to Azure Storage Container democontainer .", "Now, We have set up the Serverless part. Let\u2019s move up to the Fargate task and process the data replication.", "2. Build a Docker image and install Azcopy there.", "Note that to use AzCopy transfer files from AWS, we will need to set up AWS Credentials in the container. We can retrieve AWS credentials using:", "3. Push Docker image to ECR", "Great! We have what we need! You can find the full solution CDK project in my Github Repo. Clone the repo and deploy the stack:", "Once the stack has been successfully created, navigate to the AWS CloudFormation console, locate the stack we just created, and go to the Resources tab to find the deployed resources.", "Now it\u2019s time to test our workflow; go to the S3 source bucket demo-databucket-source . Upload as many files in different folders (prefix). Wait 24 hours for the next inventory report generated; then, you will see the whole pipeline start running, and files will eventually be copied to Azure democontainer .", "We should see the logs of the Fargate task like the below screenshot.", "We can also monitor, troubleshoot, and set alarms for ECS resources using CloudWatch Container Insights.", "In this article, I introduced the approach to automate data replication from AWS S3 to Microsoft Azure Storage. I walked you through how to use CDK to deploy VPC, AWS S3, Lambda, Cloudtrail, Fargte resources, showing you how to use the ARM template deploy Azure services. I showed you how to use the AWS Wrangler library and Athena query to create a table and querying the table.", "I hope you have found this article useful. You can find the complete project in my GitHub repo.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AWS Community Builder | AWS AZURE GCP Certified Engineer | A Cloud Technology Enthusiast | AWS Certified Security/Machine Learning/Database Analytics Specialty"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2a1b8d1ac6eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://yia333.medium.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": ""}, {"url": "https://yia333.medium.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Yi Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22047a0ec61b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&user=Yi+Ai&userId=22047a0ec61b&source=post_page-22047a0ec61b----2a1b8d1ac6eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://unsplash.com/@jsweissphoto", "anchor_text": "@jsweissphoto"}, {"url": "https://unsplash.com/", "anchor_text": "Unsplash"}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/installing.html", "anchor_text": "AWS-CLI"}, {"url": "https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html", "anchor_text": "CDK-CLI"}, {"url": "https://aws.amazon.com/cdk/", "anchor_text": "AWS CDK"}, {"url": "https://raw.githubusercontent.com/yai333/DataPipelineS32Blob/master/Azure-Template-DemoRG/template.json", "anchor_text": "https://raw.githubusercontent.com/yai333/DataPipelineS32Blob/master/Azure-Template-DemoRG/template.json"}, {"url": "https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob/layers", "anchor_text": "here"}, {"url": "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?toc=/azure/storage/blobs/toc.json", "anchor_text": "AzCopy"}, {"url": "http://169.254.170.2/$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI", "anchor_text": "http://169.254.170.2/$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"}, {"url": "https://github.com/yai333/DataPipelineS32Blob", "anchor_text": "Github Repo"}, {"url": "https://console.aws.amazon.com/cloudformation", "anchor_text": "console"}, {"url": "https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob", "anchor_text": "GitHub repo"}, {"url": "https://medium.com/tag/aws?source=post_page-----2a1b8d1ac6eb---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/azure?source=post_page-----2a1b8d1ac6eb---------------azure-----------------", "anchor_text": "Azure"}, {"url": "https://medium.com/tag/data-wranger?source=post_page-----2a1b8d1ac6eb---------------data_wranger-----------------", "anchor_text": "Data Wranger"}, {"url": "https://medium.com/tag/data-pipeline?source=post_page-----2a1b8d1ac6eb---------------data_pipeline-----------------", "anchor_text": "Data Pipeline"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----2a1b8d1ac6eb---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&user=Yi+Ai&userId=22047a0ec61b&source=-----2a1b8d1ac6eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&user=Yi+Ai&userId=22047a0ec61b&source=-----2a1b8d1ac6eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2a1b8d1ac6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2a1b8d1ac6eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2a1b8d1ac6eb--------------------------------", "anchor_text": ""}, {"url": "https://yia333.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://yia333.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yi Ai"}, {"url": "https://yia333.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "318 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22047a0ec61b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&user=Yi+Ai&userId=22047a0ec61b&source=post_page-22047a0ec61b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Feb2bd512dbe0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb&newsletterV3=22047a0ec61b&newsletterV3Id=eb2bd512dbe0&user=Yi+Ai&userId=22047a0ec61b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}