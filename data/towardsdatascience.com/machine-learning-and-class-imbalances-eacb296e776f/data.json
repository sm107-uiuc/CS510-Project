{"url": "https://towardsdatascience.com/machine-learning-and-class-imbalances-eacb296e776f", "time": 1683013188.178563, "path": "towardsdatascience.com/machine-learning-and-class-imbalances-eacb296e776f/", "webpage": {"metadata": {"title": "Strategies for Imbalanced Data Daniel Foley | Towards Data Science", "h1": "Dealing with Class Imbalances in Machine Learning", "description": "This article discusses 4 proven strategies for dealing with Class Imbalances in Machine Learning prediction using Python"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1106.1813.pdf", "anchor_text": "paper", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1106.1813.pdf", "anchor_text": "https://arxiv.org/pdf/1106.1813.pdf", "paragraph_index": 13}], "all_paragraphs": ["It's pretty rare to apply machine learning to a real-world problem and you have something as straightforward as you might find in tutorials or in data science courses. A lot of the time you come across classification problems where you are trying to predict something that does not occur very often. This is common when trying to predict things like conversion, churn, fraud. In this post, I am going to talk about class imbalances in machine learning. In particular, I want to go over some of the implications of having imbalanced data as well as highlight and explain some ways to address it. My hope is that by the end of this post you will have the tools you need to handle this issue in any prediction problems you may come across.", "Like I said above, imbalanced data comes about when we have significant differences in the proportion of the class we are trying to predict. For example, in a lot of industries, customer churn over a certain period of time would be relatively rare (hopefully). If we try and formulate a machine learning model to predict whether a customer will churn or not we might only have 5 per cent of customers churning and 95 per cent not churning. For something like conversion or fraud, this is probably going to be even lower.", "Ok so the problem is hopefully a little bit clearer, but why do we care? Let's just train the model and make some predictions. Let's take the churn example in the above paragraph and estimate a logistic regression. After estimating the model we get 95 per cent of the predictions correct. That was easy. If we dig a bit deeper into the results, however, and look at the confusion matrix, we might find something like Figure 1 below. Our model predicted that basically nobody churned. That isn't very helpful, we could have made that prediction ourselves and saved ourselves the trouble of writing any code. Obviously, this model is pretty useless and not really adding any value. What's going on here? Why is our model so bad? The training set is so unbalanced that the model has a hard time finding useful patterns in the minority class. This is more pronounced with some algorithms such as logistic regression as it assumes the class distributions are equal. Lucky for us, there are several ways to deal with this and this is what I will be going through in this post.", "The first potential solution we could try is to resample our data to balance the classes. There are a few different sampling methods we could try all of which can be easily implemented in the imblearn python library.", "One key point to mention is that these re-sampling techniques should only be performed on the training data. The test set should remain unbalanced.", "To start off our analysis lets first use a handy function from sklearn to create a dataset. The make_classification function is really useful and allows us to create a dataset to demonstrate these techniques and algorithms. Notice, we have added weights=[0.99,0.01] to the arguments to generate an unbalanced dataset. This level of unbalance is very similar to a machine learning project I worked on recently and is not that uncommon to see datasets like these in practice.", "First off, let's take a look at undersampling. This technique is pretty simple and involves randomly selecting samples from the majority class and using these along with our minority class for training data. The result is we end up with an equal number of positive and negative examples which can avoid the problems outlined above. The advantage of this technique is that is pretty easy to implement and it can be quite effective. The downside is that you are essentially throwing away data. I would probably not favour this approach unless my training data was quite large and I could afford to throw away data. In Figure 2 below we can see the code to implement this as well as our training set after undersampling. Notice how much data we have removed.", "Oversampling is the opposite of undersampling. With this approach, we randomly sample data from the minority class and duplicate it to create more samples. We do this until the class distribution is equal. Although this can be useful for addressing imbalances, it has some disadvantages as well. The major one being that we are duplicating already existing data. This means we are not really adding any additional information to the classification problem and it can potentially lead to overfitting which is definitely something we want to avoid.", "As the name implies, SMOTE is an oversampling method used to generate additional data from the data we already have. It is similar to random oversampling but rather than duplicate existing data we are generating new synthetic data. The original paper describes the technique as follows:", "\u201cThe minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbours\u201d.", "So what does this mean? well, the algorithm first uses k-nearest neighbours to decide which data points we are going to use to produce new data. For simplicity, let's say we choose k = 1. The algorithm will create a line segment between our data point at its nearest neighbour and produce a new data point somewhere along this line. Let's say the feature we are generating data for is the number of days since a user last visited our website. To generate new synthetic data, the algorithm computes two new variables. The gap which is a random number between 0 and 1 let's say we get 0.8, and the diff, which is the difference between the number of days since a user last visited for our first data point (20) and its nearest neighbour (15). The value of our synthetic data for this feature is then: 24= 20+ (20\u201315)*.8.", "Ok, let's see how we can implement this in python and also visualise what the algorithm produces. You might think the graph looks a bit strange and I would probably agree but this is just an artefact of the algorithm and reinforces that the data is \u2018synthetic\u2019 rather than completely new as it directly depends on the other data points (or at least its nearest data points).", "There are pros and cons to each of these techniques but in practice, I would recommend trying all of them to see which works best and let your model metrics guide you.", "Here is a link to the full paper for those of you more interested in the details: https://arxiv.org/pdf/1106.1813.pdf", "As I mentioned above, many machine learning algorithms assume that the class distribution is equal so they may not work very well out of the box. This is the case for something like logistic regression. To get around that there is a nice argument called class_weights in sklearn which lets us tell the model what the class distribution is. It is a simple and effective solution and I have found good success with it in the past. To be more specific, it will penalise the model more for making mistakes on the minority class. These mistakes will make it harder to minimise the cost function so the model learns to put more emphasis on getting the minority class correct.", "The code below shows how we can implement this in python. We can either choose class_weight = \u2018balanced\u2019 or pass a dictionary with specific class weights, e.g. weights = {0:0.01, 1:1.0}", "With a variety of techniques presented and explained to you, I thought I would also share an interesting conversation I had with a colleague when discussing imbalanced data. He made the point that:", "Imbalanced data does not really matter when using a model such as logistic regression in so far as it does not affect the coefficient estimates (only the intercept).", "The intercept will adjust to the imbalance and the coefficients will remain the same. After thinking about it, I realised that he was right. There is one problem with this, however. If you plot the distribution of the prediction probabilities you will likely see a very skewed distribution. For example, when I did this my distribution ranged from 0 to ~0.2. This for me is a problem particularly when the outputs of the model need to be interpreted and explained to the business teams in your company. For this reason alone I think it is much better to re-balance the data even though in principle a logistic regression can handle imbalances.", "As I alluded to earlier, the choice of algorithm can also greatly impact your results when dealing with imbalanced data. It is also very important to choose appropriate metrics to assess the performance of your model. Again, accuracy would not be a particularly useful metric to consider in this case. Using metrics such as ROC AUC, Precision and Recall and the confusion matrix will give us a much better idea of how the model is performing.", "I won't dive too deep into how these metrics work here but here are some resources which go into detail for each of these metrics.", "Alright, that is enough of the background stuff, let\u2019s get to the interesting part and look at an example of this with some code. I recently had to apply these techniques in a project and they worked reasonably well for me. Unfortunately, as the data is sensitive I can\u2019t show it but we can re-create a similar scenario by generating some data. In any case, the techniques and the code I show here should be readily transferable to other classification problem. Below we will estimate a few different models and see how they perform on the unbalanced dataset. We will then try out the techniques above and see how they improve our results using some key classification performance metrics.", "Again we can use the make_classification function in sklearn to create our imbalanced dataset. For this example, we will have 1 per cent positive examples and 99 per cent negative examples.", "To estimate these models using different sampling techniques we will make use of imblearn pipelines. I'm sure some of you have heard of sklearn pipelines, and these are essentially the same but designed to specifically work with sampling techniques. As far as I know, it is not possible to use these sampling techniques with sklearn pipelines currently. This is my first time using pipelines and I have to say they make your code much cleaner and easier to read. They are pretty handy so I will definitely incorporate these into my workflow going forward.", "To use these pipelines, all we need to do is create an instance of our sampling class and our algorithm and then pass these as tuples into the pipeline class. I originally tried to see if I could pass multiple algorithms into the pipeline, i.e. logistic regression and random forest but this did not work. However, if you pass algorithms with a fit a transform method such as PCA it should work fine. From the imblearn docs:", "Intermediate steps of the pipeline must be transformers or resamplers, that is, they must implement fit, transform and sample methods.", "We can see that the results from our unbalance dataset are not that great. The model with a threshold of 0.5 performs very badly identifying nearly none of our positive examples. We could try and alter the threshold appropriately but again this will give us very skewed probabilities as outputs. We can also see from our PR-curve that the model has a hard time identifying the positive labels without making a lot of mistakes. Interestingly, the ROC actually seems to look fine. This highlights that ROC can be a misleading metric when dealing with unbalanced data. To see what's happening here, think about what the ROC is plotting, the true positive rate (TPR) vs the false positive rate (FPR). Below are the definitions of TPR and FPR.", "Because our classes are imbalanced we have far more negative examples. In essence, our model is biased towards predicting negative examples so we have a lot of True negatives (TN). This keeps our false positive rate (FPR) from increasing too much since our TN will be increasing in the denominator. This is why we can get a high AUC even if we are doing a bad job at classifying the positive examples (the ones we care about most). If we use precision and recall, however, this problem is avoided since we are not using the True negatives (TN) at all.", "In this case, my preference is to use the confusion matrix and to a lesser extent the PR-curve to gauge the performance of the model. Note I make use of a few helper functions that I created to plot these metrics. The code is located at the bottom of this post. All of the figures presented use the default thresholds of 0.5. This can easily be altered to adjust the predictions as required, depending on your use case.", "Overall, the results are pretty much the exact same across different sampling methods. So what should we do in this case? In general these results arent great so at this point, I would suggest experimenting with other models. But if we were determined to estimate a logistic regression and we had a large data set we could favour undersampling since it is the least complex of the three options and try and optimise our decision threshold to improve our TP rate.", "So logistic regression didn't perform too well at this problem, how about we try a random forest. These models tend to perform well in practice and they are usually my go-to model for classification problems. As for the results, this model performs slightly better than the logistic regression on the unbalanced data at the default threshold identifying 9 per cent of the TP\u2019s. Obviously we want something a bit better than this so let's try our sampling techniques again.", "Comparing the three sampling methods suggests undersampling performs the best classifying 65 per cent of the true positives. It also does a much better job identifying the true negatives than the logistic regression. Our best results, however, come from balancing the class weights. Since this is very simple to do in sklearn (class_weights = \u2018balanced\u2019) and we do not have to throw any data away this would be my preference. Doing this gives us a 70 per cent TP rate.", "What else could we do to improve the model score?", "There are a few other things we could try here to improve our model performance as well as make sure our predictions are robust.", "So what can we take away from these examples?", "Ok, guys that is it for this post, as always thanks for reading and I hope you found this useful.", "Below are some useful functions to help us assess our model performance.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feacb296e776f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eacb296e776f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dannyf16?source=post_page-----eacb296e776f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dannyf16?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Daniel Foley"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa823d37636a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&user=Daniel+Foley&userId=a823d37636a4&source=post_page-a823d37636a4----eacb296e776f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@scottkelley?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Scott Kelley"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1106.1813.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1106.1813.pdf", "anchor_text": "https://arxiv.org/pdf/1106.1813.pdf"}, {"url": "https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152", "anchor_text": "ROC AUC"}, {"url": "https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c", "anchor_text": "Precision and Recall"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html", "anchor_text": "cross_val_score"}, {"url": "https://scikit-learn.org/stable/modules/grid_search.html", "anchor_text": "Hyperparameter Optimisation"}, {"url": "https://www.linkedin.com/in/daniel-foley-1ab904a2/", "anchor_text": "Daniel Foley - Data Scientist - Product Madness | LinkedInView Daniel Foley's profile on LinkedIn, the world's largest professional community. Daniel has 5 jobs listed on their\u2026www.linkedin.com"}, {"url": "https://towardsdatascience.com/lets-build-a-streaming-data-pipeline-e873d671fc57", "anchor_text": "Let\u2019s Build a Streaming Data PipelineApache Beam and DataFlow for real-time data pipelinestowardsdatascience.com"}, {"url": "https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f", "anchor_text": "Gaussian Mixture Modelling (GMM)Making Sense of Text Data using Unsupervised Learningtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/k-means-clustering-8e1e64c1561c", "anchor_text": "K-Means ClusteringMaking Sense of Text Data using Unsupervised Learningtowardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----eacb296e776f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----eacb296e776f---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eacb296e776f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----eacb296e776f---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----eacb296e776f---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&user=Daniel+Foley&userId=a823d37636a4&source=-----eacb296e776f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&user=Daniel+Foley&userId=a823d37636a4&source=-----eacb296e776f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feacb296e776f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eacb296e776f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eacb296e776f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eacb296e776f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eacb296e776f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eacb296e776f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eacb296e776f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dannyf16?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dannyf16?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Foley"}, {"url": "https://medium.com/@dannyf16/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://www.linkedin.com/in/daniel-foley-1ab904a2/", "anchor_text": "https://www.linkedin.com/in/daniel-foley-1ab904a2/"}, {"url": "https://www.datascientistguide.com/", "anchor_text": "https://www.datascientistguide.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa823d37636a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&user=Daniel+Foley&userId=a823d37636a4&source=post_page-a823d37636a4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fec905917d8b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-class-imbalances-eacb296e776f&newsletterV3=a823d37636a4&newsletterV3Id=ec905917d8b1&user=Daniel+Foley&userId=a823d37636a4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}