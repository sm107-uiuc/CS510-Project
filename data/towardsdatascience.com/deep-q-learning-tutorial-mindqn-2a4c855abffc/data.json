{"url": "https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc", "time": 1683016653.24649, "path": "towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc/", "webpage": {"metadata": {"title": "Deep Q-Learning Tutorial: minDQN. A Practical Guide to Deep Q-Networks | by Mike Wang | Towards Data Science", "h1": "Deep Q-Learning Tutorial: minDQN", "description": "Reinforcement Learning is an exciting field of Machine Learning that\u2019s attracting a lot of attention and popularity. An important reason for this popularity is due to breakthroughs in Reinforcement\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/agents/tutorials/0_intro_rl", "anchor_text": "Deep Q-Learning algorithm", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756", "anchor_text": "Here", "paragraph_index": 27}, {"url": "https://github.com/mswang12/minDQN/blob/main/minDQN.py", "anchor_text": "here", "paragraph_index": 28}], "all_paragraphs": ["Reinforcement Learning is an exciting field of Machine Learning that\u2019s attracting a lot of attention and popularity. An important reason for this popularity is due to breakthroughs in Reinforcement Learning where computer algorithms such as Alpha Go and OpenAI Five have been able to achieve human level performance on games such as Go and Dota 2. One of the core concepts in Reinforcement Learning is the Deep Q-Learning algorithm. Naturally, a lot of us want to learn more about the algorithms behind these impressive accomplishments. In this tutorial, we\u2019ll be sharing a minimal Deep Q-Network implementation (minDQN) meant as a practical guide to help new learners code their own Deep Q-Networks.", "Reinforcement Learning can broadly be separated into two groups: model free and model based RL algorithms. Model free RL algorithms don\u2019t learn a model of their environment\u2019s transition function to make predictions of future states and rewards. Q-Learning, Deep Q-Networks, and Policy Gradient methods are model-free algorithms because they don\u2019t create a model of the environment\u2019s transition function.", "The CartPole environment is a simple environment where the objective is to move a cart left or right in order to balance an upright pole for as long as possible. The state space is described with 4 values representing Cart Position, Cart Velocity, Pole Angle, and Pole Velocity at the Tip. The action space is described with 2 values (0 or 1) allowing the car to either move left or right at each time step.", "2. Choose an action using the Epsilon-Greedy Exploration Strategy", "3. Update the Q-table using the Bellman Equation", "The Q-table is a simple data structure that we use to keep track of the states, actions, and their expected rewards. More specifically, the Q-table maps a state-action pair to a Q-value (the estimated optimal future value) which the agent will learn. At the start of the Q-Learning algorithm, the Q-table is initialized to all zeros indicating that the agent doesn\u2019t know anything about the world. As the agent tries out different actions at different states through trial and error, the agent learns each state-action pair\u2019s expected reward and updates the Q-table with the new Q-value. Using trial and error to learn about the world is called Exploration.", "One of the goals of the Q-Learning algorithm is to learn the Q-Value for a new environment. The Q-Value is the maximum expected reward an agent can reach by taking a given action A from the state S. After an agent has learned the Q-value of each state-action pair, the agent at state S maximizes its expected reward by choosing the action A with the highest expected reward. Explicitly choosing the best known action at a state is called Exploitation.", "A common strategy for tackling the exploration-exploitation tradeoff is the Epsilon Greedy Exploration Strategy.", "Note that at the beginning of the algorithm, every step the agent takes will be random which is useful to help the agent learn about the environment it\u2019s in. As the agent takes more and more steps, the value of epsilon decreases and the agent starts to try existing known good actions more and more. Note that epsilon is initialized to 1 meaning every step is random at the start. Near the end of the training process, the agent will be exploring much less and exploiting much more.", "The Bellman Equation tells us how to update our Q-table after each step we take. To summarize this equation, the agent updates the current perceived value with the estimated optimal future reward which assumes that the agent takes the best current known action. In an implementation, the agent will search through all the actions for a particular state and choose the state-action pair with the highest corresponding Q-value.", "S = the State or Observation", "A = the Action the agent takes", "R = the Reward from taking an Action", "\u019b = the discount factor which causes rewards to lose their value over time so more immediate rewards are valued more highly", "Vanilla Q-Learning: A table maps each state-action pair to its corresponding Q-value", "Deep Q-Learning: A Neural Network maps input states to (action, Q-value) pairs", "A core difference between Deep Q-Learning and Vanilla Q-Learning is the implementation of the Q-table. Critically, Deep Q-Learning replaces the regular Q-table with a neural network. Rather than mapping a state-action pair to a q-value, a neural network maps input states to (action, Q-value) pairs.", "One of the interesting things about Deep Q-Learning is that the learning process uses 2 neural networks. These networks have the same architecture but different weights. Every N steps, the weights from the main network are copied to the target network. Using both of these networks leads to more stability in the learning process and helps the algorithm to learn more effectively. In our implementation, the main network weights replace the target network weights every 100 steps.", "The main and target neural networks map input states to an (action, q-value) pair. In this case, each output node (representing an action) contains the action\u2019s q-value as a floating point number. Note that the output nodes do not represent a probability distribution so they will not add up to 1. For the example above, one action has a Q-value of 8 and the other action has a Q-value of 5.", "In our implementation, the main and target networks are quite simple consisting of 3 densely connected layers with Relu activation functions. The most notable features are that we use He uniform initialization as well as the Huber loss function to achieve better performance.", "In the Epsilon-Greedy Exploration strategy, the agent chooses a random action with probability epsilon and exploits the best known action with probability 1 \u2014 epsilon.", "Both the Main model and the Target model map input states to output actions. These output actions actually represent the model\u2019s predicted Q-value. In this case, the action that has the largest predicted Q-value is the best known action at that state.", "After choosing an action, it\u2019s time for the agent to perform the action and update the Main and Target networks according to the Bellman equation. Deep Q-Learning agents use Experience Replay to learn about their environment and update the Main and Target networks.", "To summarize, the main network samples and trains on a batch of past experiences every 4 steps. The main network weights are then copied to the target network weights every 100 steps.", "Experience Replay is the act of storing and replaying game states (the state, action, reward, next_state) that the RL algorithm is able to learn from. Experience Replay can be used in Off-Policy algorithms to learn in an offline fashion. Off-policy methods are able to update the algorithm\u2019s parameters using saved and stored information from previously taken actions. Deep Q-Learning uses Experience Replay to learn in small batches in order to avoid skewing the dataset distribution of different states, actions, rewards, and next_states that the neural network will see. Importantly, the agent doesn\u2019t need to train after each step. In our implementation, we use Experience Replay to train on small batches once every 4 steps rather than every single step. We found this trick to really help speed up our Deep Q-Learning implementation.", "Just like with vanilla Q-Learning, the agent still needs to update our model weights according to the Bellman Equation.", "From the original Bellman equation in Figure 3, we want to replicate the Temporal Difference target operation using our neural network rather than using a Q-table. Note that the target network and not the main network is used to calculate the Temporal Difference target. Assuming that the temporal difference target operation produces a value of 9 in the example above, we can update the main network weights by assigning 9 to the target q-value and fitting our main network weights to the new target values.", "Our Deep Q-Network implementation needed a few tricks before the agent started to learn to solve the CartPole problem effectively. Here are some of the tips and tricks that really helped.", "Putting it all together, you can find our minimal Deep Q-Network implementation solving the CartPole problem here. This implementation uses Tensorflow and Keras and should generally run in less than 15 minutes.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hi there, I write and teach about cool and interesting Engineering topics"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2a4c855abffc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mike-12.medium.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": ""}, {"url": "https://mike-12.medium.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Mike Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6daa16423c79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&user=Mike+Wang&userId=6daa16423c79&source=post_page-6daa16423c79----2a4c855abffc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://www.tensorflow.org/agents/tutorials/0_intro_rl", "anchor_text": "Deep Q-Learning algorithm"}, {"url": "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756", "anchor_text": "Here"}, {"url": "https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52", "anchor_text": "He Initialization is a good initialization strategy"}, {"url": "https://github.com/mswang12/minDQN/blob/main/minDQN.py", "anchor_text": "here"}, {"url": "https://www.mlq.ai/deep-reinforcement-learning-q-learning/", "anchor_text": "Deep Reinforcement Learning: Guide to Deep Q-Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/", "anchor_text": "A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python"}, {"url": "https://www.tensorflow.org/agents/tutorials/0_intro_rl", "anchor_text": "Introduction to RL and Deep Q Networks"}, {"url": "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756", "anchor_text": "How to match DeepMind\u2019s Deep Q-Learning score in Breakout"}, {"url": "https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/?completed=/deep-q-learning-dqn-reinforcement-learning-python-tutorial/", "anchor_text": "Training Deep Q Learning and Deep Q Networks (DQN) Intro and Agent \u2014 Reinforcement Learning w/ Python Tutorial p.6"}, {"url": "https://medium.com/swlh/introduction-to-q-learning-with-openai-gym-2d794da10f3d", "anchor_text": "Introduction to Q-learning with OpenAI Gym"}, {"url": "https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52", "anchor_text": "All the ways to initialize your neural network"}, {"url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)", "anchor_text": "Model-free (reinforcement learning)"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----2a4c855abffc---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2a4c855abffc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2a4c855abffc---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----2a4c855abffc---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----2a4c855abffc---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&user=Mike+Wang&userId=6daa16423c79&source=-----2a4c855abffc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&user=Mike+Wang&userId=6daa16423c79&source=-----2a4c855abffc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2a4c855abffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2a4c855abffc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2a4c855abffc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2a4c855abffc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2a4c855abffc--------------------------------", "anchor_text": ""}, {"url": "https://mike-12.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mike-12.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mike Wang"}, {"url": "https://mike-12.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "140 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6daa16423c79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&user=Mike+Wang&userId=6daa16423c79&source=post_page-6daa16423c79--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F59e5e77747e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-learning-tutorial-mindqn-2a4c855abffc&newsletterV3=6daa16423c79&newsletterV3Id=59e5e77747e6&user=Mike+Wang&userId=6daa16423c79&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}