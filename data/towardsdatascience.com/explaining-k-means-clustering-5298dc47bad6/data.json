{"url": "https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6", "time": 1683010811.7622159, "path": "towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6/", "webpage": {"metadata": {"title": "Explaining K-Means Clustering. Comparing PCA and t-SNE dimensionality\u2026 | by Kamil Mysiak | Towards Data Science", "h1": "Explaining K-Means Clustering", "description": "Today\u2019s data comes in all shapes and sizes. NLP data encompasses the written word, time-series data tracks sequential data movement over time (ie. stocks), structured data which allows computers to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "Wiki", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "Curse of Dimensionality", "paragraph_index": 18}, {"url": "http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf", "anchor_text": "link", "paragraph_index": 32}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "paragraph_index": 83}], "all_paragraphs": ["Today\u2019s data comes in all shapes and sizes. NLP data encompasses the written word, time-series data tracks sequential data movement over time (ie. stocks), structured data which allows computers to learn by example, and unclassified data allows the computer to apply structure. Whichever dataset you possess, you can be sure there is an algorithm ready to decipher its secrets. In this article, we want to cover a clustering algorithm named KMeans which attempts to uncover hidden subgroups hiding in your dataset. Furthermore, we will examine what effects dimension reduction has on the quality of the clusters obtained from KMeans.", "In our example, we will be examining a human resources dataset consisting of 15,000 individual employees. The dataset contains employee job characteristics such as job satisfaction, performance score, workload, years of tenure, accidents, number of promotions. We will apply KMeans in order to uncover similar groups of employees.", "Classification problems will have target labels which we are trying to predict. Famous datasets such as Titanic and Iris are both prime for classification as they both have targets we are trying to predict (ie. survived and species). Furthermore, classification tasks require us to split our data into training and test, where the classifier is trained on the training data and then its performance is measured via the test dataset.", "When dealing with a clustering problem, we want to use an algorithm to uncover meaningful groups in our data. Perhaps we are trying to uncover customer segments or identify anomalies in our data. Either way, the algorithm uncovers the groups with little human intervention as we don\u2019t have target labels to predict.", "That said, we can partner unsupervised clustering algorithms with supervised algorithms by first identifying groups/clusters and then building supervised models to predict the cluster membership.", "At its core, KMeans attempts to organize the data into a specified number of clusters. Unfortunately, it is up to us to determine the number of clusters we wish to find but fear not we have tools at our disposal that assist with this process. The goal of KMeans is to identify similar data points and cluster them together while trying to distance each cluster as far as possible. Its \u201csimilarity\u201d calculation is determined via Euclidean distance or an ordinary straight line between two points (Wiki). The shorter the Euclidean distance the more similar the points are.", "First, the user (ie. you or I) determines the number of clusters KMeans needs to find. The number of clusters cannot exceed the number of features in the dataset. Next, KMeans will select a random point for each centroid. The number of centroids is equal to the number of clusters selected. The centroid is the point around which each cluster is built around.", "Second, the Euclidean distance is calculated between each point and each centroid. Each point will be initially assigned to the closest centroid/cluster based on the Euclidean distance. Each data point can belong to one cluster or centroid. The algorithm then averages Euclidean distance (between each point and centroid) for each cluster and this point becomes the new centroid. This process of averaging the Euclidean distances within clusters and assigning new centroids repeats until cluster centroids no longer move. The animation below shows the process, refresh the page if needed.", "We need to be aware of how KMeans selects the initial centroid/s and what problems might this produce. Without our intervention, KMeans will randomly select the initial centroid/s which ultimately can cause different resulting clusters on the same data. From the plots below we can see how running the KMeans algorithm on two different occasions resulted in different initial centroids. The same data points were assigned to different clusters between the first and second time running KMeans.", "Have no fear Sklearn has our backs! The Sklearn KMeans library has certain parameters such as \u201cn_int\u201d and \u201cmax_iter\u201d to mitigate this issue. The \u201cn_int\u201d parameter determines the number of times KMeans will randomly select different centroids. \u201cMax_iter\u201d determines how many iterations will run. An iteration is the process of finding the distance, taking the average distance, and moving the centroid.", "If we set our parameters at n_int=25 and max_iter=200 KMeans will randomly select 25 initial centroids and run each centroid up to 200 iterations. The best out of those 25 centroids will be the final cluster.", "The Sklearn also has an \u201cint\u201d parameter which will select the first centroid at random and locate all the data points which are furthest away from the first centroid. Then, the second centroid is assigned nearby those far points as they are less likely to belong to the first centroid. Sklearn selects \u201cint=kmeans++\u201d by default which applies the above logic.", "\u201cYou mentioned something about needing to select the number of clusters\u2026.? Just how do we do that?\u201d", "Domain Knowledge: Very often we have a certain level of knowledge and experience in the domain from which our dataset was gathered. This expertise can allow us to set the number of clusters we believe exists in the general population.", "Hypothesis Testing: Setting a specific number of clusters can also act as a test of a certain hypothesis we might have. For example, when analyzing marketing data we have a hunch there are 3 subgroups of customers who very likely, likely, and not likely to purchase our product.", "Data Comes Pre-Labeled: There are times when the data we are analyzing comes with pre-labeled targets. These datasets are typically used for supervised ML problems but that doesn\u2019t mean we cannot cluster the data. Pre-labeled data is unique as you need to remove the targets from your initial analysis and then use them to validate how well the model clustered the data.", "Elbow Method: This is a very popular iterative statistical technique for determining the optimal number of clusters by actually running the K-Means algorithm for a range of cluster values. The elbow method calculates the sum of squared distances from each point to its assigned centroid for each iteration of KMeans. Each iteration runs through a different number of clusters. The result is a line chart that displays the sum of squared distances at each cluster. We want to select the number of clusters at the elbow of the line chart or the lowest sum of squared distances (ie. Inertia) at the lowest number of clusters. The lower the sum of squares distances means the data inside each cluster are more tightly grouped.", "KMeans is very sensitive to scale and requires all features to be on the same scale. KMeans will put more weight or emphasis on features with larger variances and those features will impose more influence on the final cluster shape. For example, let\u2019s consider a dataset of car information such as weight (lbs) and horsepower (hp). If there is a larger variation in weight between all the cars the average Euclidean distance will be more affected by weight. Ultimately, the membership of each cluster will be more affected by weight than horsepower.", "Clustering algorithms such as KMeans have a difficult time accurately clustering data of high dimensionality (ie. too many features). Our dataset is not necessarily highly dimensional as it contains 7 features but even this amount will create issues for KMeans. I would suggest you explore the Curse of Dimensionality for more details. As we saw earlier, many clustering algorithms use a distance formula (ie. Euclidean distance) to determine cluster membership. When our clustering algorithm has too many dimensions, pairs of points will begin to have very similar distances and we wouldn\u2019t be able to obtain meaningful clusters.", "In this example, we are going to compare PCA and t-SNE data reduction techniques prior to running our K-Means clustering algorithm. Let\u2019s take a few mins to explain PCA and t-SNE.", "Principal component analysis or (PCA) is a classic method we can use to reduce high-dimensional data to a low-dimensional space. In other words, we simply cannot accurately visualize high-dimensional datasets because we cannot visualize anything above 3 features (1 feature=1D, 2 features = 2D, 3 features=3D plots). The main purpose behind PCA is to transform datasets with more than 3 features (high-dimensional) into typically a 2D or 3D plots for us feeble humans. That\u2019s what is meant by low-dimensional space. The beauty behind PCA is the fact that despite the reduction into a lower-dimensional space we still retain most (+90%) of the variance or information from our original high-dimensional dataset. The information or variance from our original features is \u201csqueezed\u201d into what PCA calls principal components (PC). The first PC will contain the majority of the information from the original features. The second PC will contain the next largest amount of information, the 3rd PC the third largest amount of info and so on and so on. The PC are not correlated (ie. orthogonal) which means they all contain unique pieces of information. We can typically \u201csqueeze\u201d most (ie. 80\u201390%) of the information or variance contained in the original features into a few principal components. We use these principal components in our analyses instead of using the original features. This way we can perform an analysis with only 2 or 3 principal components instead of 50 features while still maintaining 80\u201390% of the information from our original features.", "Let\u2019s take a look at some of the details behind how PCA does its magic. To make the explanation a bit easier let\u2019s see how we can reduce a dataset with 2 features (ie. 2D) in one principal component (1D). That said, reducing 50 features into 3 or 4 principal components utilizes the same methodology.", "We have a 2D plot of weight and height. In other words, we have a dataset of 7 people and have plotted their height in relation to their weight.", "First, PCA needs to center the data by measuring the distances from each point to the y-axis (height) and x-axis (weight). It then calculates the average distance for both axes (ie. height and weight) and centers the data using these averages.", "Then PCA will plot the first principal component (PC1) or the best fitting line which maximizes the variance or the amount of information between weight and height. It determines the best fitting line by maximizing the distance from the point projected onto the best fitting line and the origin (ie. blue light below). It does it for each green point and then it squares each distance, to remove negative values, and sums everything up. The best-fitting line or PC1 will have the largest sum of squared distances from the origin to the projected points for all points.", "Now, we simply rotate the axes to where PC1 is now our x-axis and we are finished.", "If we wanted to reduce 3 features down to two principal components we would simply place a perpendicular (y-axis) to our best-fitting line in a 2D space. Why a perpendicular line? Because each principal component is orthogonal or uncorrelated with all other features. If we wanted to find a third principal component we would simply find another orthogonal line to PC1 and PC2 but this time in a 3D space.", "As you can see from the bar plot below, we initially began with a dataset of 5 features. Remember the number of principal components will always equal the number of features. However, we can see that the first 2 principal components account for 90% of the variance or information contained in the original 5 features. This is how we determine the optimal number of principal components. It is important to remember PCA is very often used for visualizing very high dimensional data (ie. thousands of features), therefore, you will most often see PCA of 2 or 3 principal components.", "Just like PCA, t-SNE takes high-dimensional data and reduces it to a low-dimensional graph (2-D typically). It is also a great dimensionality reduction technique. Unlike PCA, t-SNE can reduce dimensions with non-linear relationships. In other words, if our data had this \u201cSwiss Roll\u201d non-linear distribution where the change in X or Y does not correspond with a constant change in the other variable. PCA would not be able to accurately distill this data into principal components. This is because PCA would attempt to draw the best fitting line through the distribution. T-SNE would be a better solution in this case because it calculates a similarity measure based on the distance between points instead of trying to maximize variance.", "Let\u2019s examine how t-SNE converts high dimensional data space into lower dimensions. It looks at the similarity between local or nearby points by observing the distance (think Euclidean distance). Points that are nearby each other are considered similar. t-SNE then converts this similarity distance for each pair of points into a probability for each pair of points. If two points are close to each other in the high-dimensional space they will have a high probability value and vice versa. This way the probability of picking a set of points is proportional to their similarity.", "Then each point gets randomly projected into a low dimensional space. For this example, we are plotting in a 1-D space but we can plot this in a 2-D or 3-D space as well. Why 2-D or 3-D? Because those are the only dimensions we (humans) can visualize. Remember t-SNE is a visualization tool first and a dimensionality reduction tool second.", "Finally, t-SNE calculates the similarity probability score in a low dimensional space in order to cluster the points together. The result is a 1-D plot we see below.", "One last thing we need to discuss about t-SNE is \u201cPerplexity\u201d, which is a required parameter when running the algorithm. \u201cPerplexity\u201d determines how broad or how tight of a space t-SNE captures similarities between points. If your perplexity is low (perhaps 2), t-SNE will only use two similar points and produce a plot with many scattered clusters. However, when we increase the perplexity to 10, t-SNE will consider 10 neighbor points as similar and cluster them together resulting in larger clusters of points. There is a point of diminishing returns where perplexity can become too large and we achieve a plot with one or two scattered clusters. At this point, t-SNE incorrectly considers points not necessarily related as belonging to a cluster. We typically set perplexity anywhere between 5 and 50, according to the original published paper (link).", "One of the main limitations of t-SNE is its high computational costs. If you have a very large feature set it might be good to first use PCA to reduce the number of features to a few principal components and then use t-SNE to further reduce the data to 2 or 3 clusters.", "When using unsupervised ML algorithms we often cannot compare our results against a known true label. In other words, we do not have a test set to gauge the performance of our model. That said, we still need to understand how well K-Means managed to cluster our data. We already know how tightly the data is contained within our clusters by looking at the Elbow graph and the number of clusters we selected.", "Silhouette Method: This technique measures the separability between clusters. First, an average distance is found between each point and all other points in a cluster. Then it measures the distance between each point and each point in other clusters. We subtract the two average measures and divide by whichever average is larger.", "We ultimately want a high (ie. closest to 1) score which would indicate that there is a small intra-cluster average distance (tight clusters) and a large inter-cluster average distance (clusters well separated).", "Visual Cluster Interpretation: Once you have obtained your clusters it is very important to interpret each cluster. This is typically done by merging the original dataset with the clusters and visualizing each cluster. The more clear and distinct each cluster is the better. We will review this process below.", "Here\u2019s the plan for the analysis below:", "This is a relatively clean dataset without any missing values or outliers. We do not see any mixed-type features, odd values or rare labels which need encoding. Features have low multicollinearity as well. Let\u2019s move on to scaling our dataset.", "As aforementioned, the standardization of data will ultimately bring all features to the same scale and bringing the mean to zero and the standard deviation to 1.", "Let\u2019s utilize the Elbow Method to determine the optimal number of clusters KMeans should obtain. It seem 4 or 5 clusters would be best and for the sake of simplicity we\u2019ll select 4.", "Let\u2019s apply KMeans on the original dataset requesting 4 clusters. We achieved a silhouette score of 0.25 which is on the low end.", "Using PCA to reduce the dataset into 3 principal components we can plot the KMeans derived clusters into 2D and 3D visuals. PCA visualizations tend to aggregate clusters around a central point which makes interpretation difficult but we can see clusters 1 and 3 to have some distinct structure compared to clusters 0 and 2. However, when we plot the clusters into a 3D space we can clearly distinct all 4 clusters.", "First, let\u2019s determine what is the optimal number of principal components we need. By examining the amount of variance each principal component encompasses we can see that the first 3 principal components explain roughly 70% of the variance. Finally, we apply PCA again and reduce our dataset to 3 principal components.", "Now that we have reduced the original dataset of 7 features to just 3 principal components let\u2019s apply the KMeans algorithm. We once again needed to determine what is the optimal number of clusters and again it seems 4 is the right choice. It is important to remember we are now using the 3 principal components instead of the original 7 features to determine the optimal number of clusters.", "Now we are ready to apply KMeans on the PCA principal components. We can see that we were able to increase our silhouette score from 0.25 to 0.36 by passing KMeans a lower dimensional dataset. Looking at the 2D and 3D scatter plots we can see a significant improvement in the distinction between clusters.", "We can see a definite improvement in KMeans ability to cluster our data when we reduce the number of dimensions to 3 principal components. In this section we will reduce our data once again using t-SNE and compare KMeans results to that of PCA KMeans. We will reduce down to 3 t-SNE components. Please keep in mind t-SNE is a computationally heavy algorithm. Computational time can be reduced using the \u2018n_iter\u2019 parameter. Furthermore, the code you see below is a result of dozens iterations of the \u2018Perplexity\u2019 parameter. Anything above a perplexity of 80 tended to aggregate our data into one large disperse cluster.", "Below is the result of reducing our original dataset into 3 t-SNE components plotted into a 2D space.", "Once again it seems 4 is the magic number of clusters for our KMeans analysis.", "Applying KMeans to our 3 t-SNE derived components we were able to obtain a Silhouette score of 0.39. If you recall the Silhouette score obtained from KMeans on PCA\u2019s 3 principal components was 0.36. A relatively small improvement but an improvement nonetheless.", "The interpretation of t-SNE can be slightly counterintuitive as the density of t-SNE clusters (ie. low dimensional space) is not proportionally related to the data relationships in the original (high dimensional space) dataset. In other words, we can have quality dense clusters as derived from KMeans but t-SNE might display them as very broad or even as multiple clusters, especially when perplexity is too low. Density, cluster size, the number of clusters (under the same KMeans cluster) and shape really have little meaning when it comes to reading t-SNE plots. We can have very broad, dense or even multiple clusters (especially when perplexity is too low) for the same KMeans cluster but that does not relate to the quality of the cluster. Rather, t-SNE\u2019s major advantage is the distance and location of each KMeans cluster. Clusters which are close to each other will be more related to each other. However, that does not mean clusters which are far away from each are dissimilar proportionally. Finally, we want to see a certain degree of separation between KMeans clusters as displayed using t-SNE.", "Staring at PCA/t-SNE plots and comparing evaluation metrics such as the Silhouette score will give us a technical perspective on the clusters derived from KMeans. If we want to understand the clusters from a business perspective we need to examine the clusters against the original features. In other words, what types of employees make up the clusters we obtained from KMeans. Not only will this analysis help guide the business and potentially continued analysis but also give us insights into the quality of the clusters.", "Let\u2019s first merge the KMeans clusters with the original unscaled features. We\u2019ll create two separate data frames. One for the PCA derives KMeans clusters and one for the t-SNE KMeans clusters.", "Let\u2019s begin with a univariate review of the clusters by comparing the clusters based on each individual feature.", "Where the real fun begins when we examine the clusters from a bivariate perspective. When we examine employee satisfaction and last evaluation scores, we can see that KMeans clusters derived from our principal components are much more defined. We will focus our examination of this bivariate relationship between satisfaction and performance on the clusters derived from PCA.", "Cluster 0 represents the largest amount of employees (7488) who on average are relatively satisfied at work with a very wide range of performance scores. Since this cluster represents almost 50% of the population it is not surprising we are seeing this wide range of satisfaction and performance scores. There is also a smaller but visible cluster of very satisfied and high performing employees.", "Cluster 1 represents a critical part of the workforce as these are very high performing employees but unfortunately, have extremely low employee satisfaction. The fact cluster 1 represents 20% of the workforce only adds to the severity of the situation. Unsatisfied employees are at a much greater risk of voluntary turnover. Their high-performance scores indicate not only their proficiency but potential institutional knowledge. Losing these employees would create a significant reduction in organizational knowledge and performance. It would be interesting to see the organizational level (ie. mgr, director, executive) of these employees. Losing a large portion of senior management is a significant problem.", "Cluster 2 seems to represent low performing employees who are on the lower end of the satisfaction continuum. Once again dissatisfied employees are at a higher risk of voluntary turnover and due to their low-performance evaluation we might consider these employees as \u2018constructive turnover\u2019. In other words, the voluntary turnover of these employees might actually be a good thing for the organization. We have to look at these results from two different perspectives. First, these employees make up over 25% of the population. If a significant portion of these employees quit the organization might be hard-pressed to have enough employees to successfully perform its function. Secondly, such a large portion of the population being dissatisfied and poorly performing speaks volumes about the recruiting, management, and training function of the organization. These employees might be the result of poorly constructed organizational initiatives. The company would be doing itself an enormous service if they delved deeper into what makes these employees dissatisfied and poorly performing.", "Finally, cluster 3 contains only 2% of the population and has no discernible distribution. We need a larger dataset to fully explore these employees.", "Very interesting, the bivariate relationship between satisfaction and performance scores is almost identical to satisfaction and average monthly hours. Once again the KMeans clusters derived from PCA are significantly more distinct, let\u2019s focus our attention on the PCA features.", "It seems the employees in PCA cluster 0 not only have a wide range of performance but also average monthly hours. Overall, these results are promising as the majority of the workforce is overall happy, performing admirably, and working a significant amount of hours.", "Cluster 1 employees are once again dissatisfied and working the largest average monthly hours. If you recall these were also the very high performing employees. These long hours could very well have an impact on their overall job satisfaction. Keep in mind there is a smaller yet emerging group of cluster 1 employees who are overall satisfied and very high performing. There could be more than just 4 clusters in our dataset.", "The cluster 2 employees are not only low performing but also working the lowest average monthly hours. Keep in mind this dataset might be a little skewed as working 160 hours in a month is considered full-time.", "Finally, cluster 3 again does not present itself in the plots.", "As we continue with our bivariate plots we compare satisfaction and time spent in company or tenure. To no surprise, KMeans clusters from PCA principal components are much more distinct. The t-SNE plot has a similar shape to the PCA plot but its clusters are much more scattered. Looking at the PCA plots we have made an important discovery regarding cluster 0 or the vast majority (50%) of the employees. The employees in cluster 0 have primarily been with the company between 2 and 4 years. This is a fairly common statistic as the number of employees with extended tenure (ie 5 years plus) will decrease as tenure increases. Furthermore, their overall high satisfaction points to a potential increase in average tenure as satisfied employees are less likely to quit.", "Cluster 1 is a little hard to describe as its data points are scattered around two distinct clusters. Overall, their tenure doesn\u2019t vary much between 4 and 6 years of tenure, however, their satisfaction is the opposite of each other. There is a cluster is high job satisfaction and a larger cluster with very low job satisfaction. It would seem cluster 1 might encompass two distinct groups of employees. Those who are very happy and very dissatisfied with their jobs. That said, despite their differences in satisfaction, their job performance, average monthly hours and tenure are on the higher end. It seems cluster 1 are either happy or unhappy very hardworking and committed employees.", "Cluster 2 follows the same pattern as the plots above. Not only are they dissatisfied at work, but they have one of the lowest average tenures as well. However, it is important to recall these employees account for 26% of the organization. Losing a significant portion of these employees would create issues. Lastly, dissatisfied employees are less likely to engage in organizational citizenship behaviors such as altruism, conscientiousness, helpfulness, and more likely to engage in counterproductive work behavior which can create toxic cultures. Dissecting who are these employees in terms of tenure, company level, turnover rates, location, and job types would be very helpful in diagnosing the issue and developing an action plan.", "Finally, cluster 3 is yet again too scattered to provide us with any useful information. There simply aren\u2019t enough employees which encompass this cluster to discern any useful patterns.", "PCA derived clusters once again outperformed t-SNE, however, only marginally. Cluster 0 has continued to follow a similar trend, overall happy employees with an average number of projects. Nothing to be overly excited or worried about. This might point to the ideal number of projects for the organization\u2019s employees.", "Cluster 1 employees are also following their trend we have seen from previous plots. Very dissatisfied employees who are being worked extremely hard when looking at the number of projects. If you recall, this group was also working very long hours during the month. We are once again seeing a smaller but visible group of satisfied employees working on an average number of projects. This was the same trend we saw with satisfaction X tenure, average monthly hours, and last evaluation score. There is definitely yet another group of very satisfied and hard-working employees. Gaining a more in-depth understanding of these employees in terms of recruiting (ie. sources, recruiters), management practices, compensation, might lead to insights into successful recruiting and hiring practices. These insights might also help the organization understand how to convert lower-performing employees.", "Cluster 2 is following its trend as well. Less satisfied employees working on a very small number of projects. This trend has been seen across all bivariate relationships with satisfaction.", "Cluster 3 is simply too small to make out any discernible trends.", "We can certainly continue examining the bivariate relationship between each cluster and our features but we are seeing a definitive trend. Below are the remaining relationship plots we urge you to examine.", "The purpose of this article was to examine the effects of different dimension reduction techniques (ie. PCA and t-SNE) will have on the quality of clusters produced by KMeans. We examined a relatively clean human resources dataset consisting of over 15,000 employees along with their job characteristics. From the original 7 features, we managed to reduce the dataset into 3 principal components and 3 t-SNE components. Our silhouette scores ranged from 0.25 (unreduced dataset) to 0.36 (PCA) and 0.39 (t-SNE). Upon visual inspection, both PCA and t-SNE derived KMeans clusters did a much better job clustering the data compared to the unreduced original 7 features. It wasn\u2019t until we began to compare the PCA and t-SNE KMeans derived clusters in terms of employee job characteristics that we saw significant differences. The PCA KMeans extracted clusters seemed to produce much clearer and defined clusters.", "From a business perspective we would be doing the reader a disservice if the PCA KMeans clusters were not summarized as well.", "Cluster 0: This cluster encompassed the majority of the employees (50%) in the dataset. This cluster contains mostly the average employees. They maintain an average level of job satisfaction, their number of monthly worked hours has a very wide range but not extending to the extreme. The same is true regarding their performance as they mostly maintain satisfactory performance scores. The number of projects this cluster of employees is involved in also average between 3 and 5. The only outlier we saw for these employees is their relatively young tenure (2\u20134 yrs) with the company. These results are not uncommon as even organizations with enough employees will follow a Gaussian distribution on many factors. As we increase the sample size we will statistically have a higher probability of selecting employees that fall towards the middle of the distribution and the outliers will have less and less pull on the distribution (ie. regression to the mean). It is easy to see why KMeans created this cluster of employees.", "Cluster 1: There was a certain duality or juxtaposition to this group of employees at times. On one hand, we saw a very dissatisfied group of employees who achieved absolutely stunning performance scores, worked very long hours, managed an extreme number of projects, had an above-average tenure, and committing zero accidents. In other words, very hard working and valuable employees who were dissatisfied with the organization. The company would suffer a significant loss of productivity and knowledge if these employees began to turnover.", "On the other hand, we were seeing a smaller but visible cluster of very satisfied employees with amazing performance scores, above-average number of projects, monthly hours, and tenure. By all means, the model employee the organization was lucky to have. Satisfaction was definitely the splitting factor as the two unique groups tended to merge when we compare the group\u2019s performance with tenure or even number of projects.", "It would seem there are additional clusters of employees KMeans did not identify. Perhaps 5 or even 6 clusters might have been better criteria for the KMeans algorithm.", "Cluster 2: Although this cluster was not as well defined as cluster 0, we were able to see a definite trend among the data. On average these employees were not overly satisfied with the organization. Their performance typically ranked towards the lower end of the scale. They worked the lowest number of monthly hours on the lowest number of projects. Their tenure hovered around 3 years which was average.", "Cluster 3: Finally, cluster 3 employees made up roughly 2% of the dataset which made it virtually impossible to identify any discernible trend in the data.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | I/O Psychologist | Motorcycle Enthusiast | On a Search for my Personal Legend/ https://www.linkedin.com/in/kamil-mysiak-b789a614/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5298dc47bad6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558----5298dc47bad6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ericmuhr?utm_source=medium&utm_medium=referral", "anchor_text": "Eric Muhr"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "Wiki"}, {"url": "http://shabal.in/visuals/kmeans/6.html", "anchor_text": "http://shabal.in/visuals/kmeans/6.html"}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "Curse of Dimensionality"}, {"url": "http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf", "anchor_text": "link"}, {"url": "https://medium.com/tag/k-means?source=post_page-----5298dc47bad6---------------k_means-----------------", "anchor_text": "K Means"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----5298dc47bad6---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/t-sne?source=post_page-----5298dc47bad6---------------t_sne-----------------", "anchor_text": "T Sne"}, {"url": "https://medium.com/tag/plotly?source=post_page-----5298dc47bad6---------------plotly-----------------", "anchor_text": "Plotly"}, {"url": "https://medium.com/tag/employees?source=post_page-----5298dc47bad6---------------employees-----------------", "anchor_text": "Employees"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----5298dc47bad6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----5298dc47bad6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5298dc47bad6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5298dc47bad6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5298dc47bad6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5298dc47bad6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5298dc47bad6--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://kamilmysiak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "348 Followers"}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F63448b4832be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-k-means-clustering-5298dc47bad6&newsletterV3=98fe4fecb558&newsletterV3Id=63448b4832be&user=Kamil+Mysiak&userId=98fe4fecb558&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}