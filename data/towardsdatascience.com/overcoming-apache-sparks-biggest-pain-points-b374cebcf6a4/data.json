{"url": "https://towardsdatascience.com/overcoming-apache-sparks-biggest-pain-points-b374cebcf6a4", "time": 1683014939.235153, "path": "towardsdatascience.com/overcoming-apache-sparks-biggest-pain-points-b374cebcf6a4/", "webpage": {"metadata": {"title": "Overcoming Apache Spark\u2019s biggest pain points | by Edson Hiroshi Aoki | Towards Data Science", "h1": "Overcoming Apache Spark\u2019s biggest pain points", "description": "It was about 6 years ago that I first used Apache Spark, which at that time, it was proof of being initiated in the world of \u201cBig Data\u201d analytics. There was no question that mastering Spark was a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.freepik.com/photos/computer", "anchor_text": "www.freepik.com", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/how-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579", "anchor_text": "check my another article", "paragraph_index": 28}], "all_paragraphs": ["Computer photo created by KamranAydinov \u2014 www.freepik.com", "It was about 6 years ago that I first used Apache Spark, which at that time, it was proof of being initiated in the world of \u201cBig Data\u201d analytics. There was no question that mastering Spark was a duty of any wannabe-data scientist or data engineer; after all, how else to leverage the massive amounts of data and distributed CPU computing to create the best possible Machine Learning models?", "If at that time I could see the future in 2020, I would be perhaps a bit surprised that a large percentage of ML/AI practitioners still do not use Spark or only use it for data engineering, not machine learning. Part of it naturally is because of the partial shift of interest to GPU-oriented, rather than CPU-oriented Machine Learning techniques, especially deep learning. But for most applications outside image and natural language processing, where the usefulness of CPU-oriented techniques is unquestionable, it is surprising that many data scientists still heavily rely on single-machine ML tools such as Scikit-learn and the non-distributed versions of XGBoost and LightGBM.", "Personally, I feel this is a pity because when used properly, Spark is an incredibly powerful tool for anyone who works with data, helping us to avoid wasting time figuring out how to fit large datasets into memory and processors, and allowing us to have full control of the data analytics workflow including extracting data, generating models, and deploying models into production and testing.", "Having conducted workshops and coached dozens of data scientists and engineers on Apache Spark, I was able to make sense of the biggest struggles users typically faces with the tool, why they happen and how to overcome them. This two-part guide is meant for those who do not only want to be able to use Spark, but to truly understand the internals of the Spark in order to solve complex problems and generate high performance, stable code.", "Note that I assume that the reader already has a basic understanding of Spark, for example, what are Spark drivers and executors, that datasets are divided into partitions, what is lazy evaluation and Spark\u2019s basic data structures.", "Unlike single-processor, vanilla Python (e.g. Pandas), where the details of the internal processing is a \u201cblack box\u201d, performing distributed processing using Spark requires the user to make a potentially overwhelming amount of decisions:", "The implications of each decision regarding partitioning and resource management can be summarized as follows:", "To handle the variation on the need of partitions and resources across the multiple steps of the pipeline, and the difficulty on keeping track on the number of partitions of each intermediary dataset, there are two strategies which can be used separately or be combined:", "Two of the first things that a Spark user learns are the concepts of immutability and lazy evaluation. Immutability means that Spark datasets cannot be modified; every operation on a dataset will create a new dataset. Lazy evaluation is based on the fact that there are 2 types of operations on datasets:", "When the process calls a transformation operation, this results in the creation of an immutable, in-memory \u201cexecution plan\u201d that describes how to generate the dataset based on other datasets. However, no actual data is generated at this point. Only when an action operation is called, Spark evaluates the execution plans of the inputs in order to generate the data necessary to calculate the outputs. Since the inputs themselves may have execution plans that depend on other datasets, these are recursively evaluated, and the process continues.", "If at some point another action is triggered that requires the same intermediate dataset to be re-created again, the process repeats itself; the execution plan is re-created and all steps the execution plan need to be executed again. To prevent this from happening, the user can \u201cpersist\u201d the intermediate dataset. When a \u201cpersisted\u201d dataset is created via triggering an execution plan, the dataset will be saved into the distributed memory (or some configured distributed storage, if there is not enough space in memory), where it will stay until manually \u201cun-persisted\u201d or until the Spark\u2019s garbage collector identifies that the dataset is \u201cout of scope\u201d (not accessible by the running code).", "Many Spark users do not deeply think about the concepts of immutability and lazy evaluation and their practical consequences, assuming that it\u2019s sufficient to know that we should \u201cpersist\u201d an intermediate dataset when it is going to be used more than once, preventing duplication of computation more than one action requires the same intermediate dataset.", "The truth, however, is that making good use of immutability and lazy evaluation goes far beyond that. As a matter of fact, they are closely related to a less known aspect, the execution plan optimization done via the Spark Catalyst. Not understanding these concepts in depth can easily lead to creeping slow, unstable Spark processing, as well as a lot of time wasted by the user on debugging and deciphering cryptic error messages.", "We may have the false impression that calling an action leads the series of transformations that precede the action to be executed one by one, each step generating one intermediate dataset. If that was the case, if DataFrame2 is generated by transforming DataFrame1, Spark would first create DataFrame1, and then create DataFrame2, as it would be the case if we were using Pandas.", "But it\u2019s not as simple as that. When an action is called, all the necessary execution plans are actually merged into a single execution plan, and except in the case of shuffling, the processing of each partition is done independently. This means that transformations are being done at partition level, not dataset level. So for one partition, Spark might still be generating DataFrame1, whereas, for another partition, Spark might already be generating DataFrame2 from DataFrame1. For example, we could have a situation as follows:", "From a performance point of view, this is a powerful mechanism as it prevents processor and memory wastage, i.e. waiting for an entire intermediate dataset to be generated before moving to the next intermediate dataset. Another advantage of partition-level transformations is that as we will see in the next section, it allows Spark to better optimise processing.", "The Spark Catalyst is undoubtedly one of Spark\u2019s most valuable features., as efficient distributed processing is far more complex to achieve than efficient single-core or single-memory processing. Essentially, the Catalyst will optimise execution plans to maximise distributed performance. For example, in Spark, performing multiple operations in the same row at time, and then moving to the next row and so on, is much a faster than performing multiple operations in the same column and then moving to the next column, so the execution plan is optimised accordingly.", "An important factor to be considered is that a single, large execution plan has far more optimization potential then multiple smaller ones. Similarly, by leveraging on the fact that transformations are done at partition level, not dataset level, Spark can combine multiple transformations for the same partitions and optimise them for even better performance. Taking our previous example, the result after optimisation would be like this:", "However, from a development / debugging point of view, if something wrong happens or if there is a performance bottleneck, lazy evaluation, combined with partition-level transformations, make it quite hard for the user to figure out exactly what processing step is causing the error or bottleneck. After all, the Spark UI will only tell the user which triggering action is causing the error or bottleneck, not the actual transformation.", "The solution is to \u201cforce\u201d Spark to generate one entire intermediate dataset at each time by inserting multiple persist() statements followed by actions, such as count() or take(). However, this code would be highly inefficient due to the aforementioned processor/memory wastage and lack of optimization, so it\u2019s better to use employ such sort of solution only for development purposes (not production) and on a temporary basis.", "One thing that I have observed is that execution plans do not always scale linearly with the number of operations but sometimes polynomially or exponentially. Since execution plans are stored in the Spark driver\u2019s memory (unlike persisted objects that are stored in the Spark executors\u2019 memory), this may cause Spark to run out of driver memory or become extremely slow due to the Spark Catalyst\u2019s optimisations. This is somewhat ironic as the Catalyst is supposed to make the code faster to run when deployed to the Spark executors; but the Catalyst, that runs in the Spark driver, can become the bottleneck of processing itself.", "This is particularly true for DataFrames with thousands of columns where we need to perform multiple operations across all the columns. Spark\u2019s immutability property aggravates the situation by constantly having new instances of executor plans created and evaluated by the Catalyst.", "Persisting can be an effective strategy to deal with this problem. It would eliminate the need of creating new instances of the executor plan up to generating the persisted intermediate dataset, as well as preventing the Catalyst from trying to optimise the execution plan before the creation of the dataset.", "Yet for complex, long pipelines, executions plans become exceedingly huge and not even frequent persisting can prevent slowness caused by optimization or the driver from running out of memory, as persisting can prevent multiple instantiations of large executions plans but not their creation in the first place. In addition, too much persisting can become a problem itself, as it takes up distributed memory and storage space. While attempting to unpersist often and/or trying to structure the code to maximise garbage collection certainly helps, this is a formidable engineering task and cannot always prevent depletion of resources.", "One solution for that is a technique named checkpointing, which consists of saving and loading intermediate results from an appropriate storage system defined by the user. When used efficiently, checkpointing prevents both the huge growth on the size of execution plans and compromising too many Spark\u2019s distributed resources with persisted objects.", "Another solution, already mentioned, is to split a large processing job into smaller jobs. In this way, execution plans cannot become too large and there is no need to worry about leftover objects in either the driver or executors\u2019 memory.", "There is certainly far more on Spark that can be covered on this article, e.g. how to structure your code to improve the quality of Catalyst\u2019s optimisations. But I hope that this article can help several people to overcome their difficulties with Spark and leverage on its extraordinary power and versatility for both data engineering and machine learning. While Spark remains a challenging tool to work with, I can give an honest testimony on how Databricks and the open-source community dramatically improved it in terms of user-friendliness and flexibility in the 6 years I have been using it.", "If you want to learn how to use Spark and Apache Hadoop YARN to run multiple distributed jobs in parallel, such as multiple instances of model training, check my another article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist at DBS Bank. All views are solely my own and not from my organization."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb374cebcf6a4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://edsonaoki.medium.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Edson Hiroshi Aoki"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4db29e9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=post_page-35b4db29e9b----b374cebcf6a4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "https://www.freepik.com/photos/computer", "anchor_text": "www.freepik.com"}, {"url": "https://towardsdatascience.com/how-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579", "anchor_text": "check my another article"}, {"url": "https://medium.com/tag/spark?source=post_page-----b374cebcf6a4---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b374cebcf6a4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----b374cebcf6a4---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----b374cebcf6a4---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----b374cebcf6a4---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=-----b374cebcf6a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=-----b374cebcf6a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb374cebcf6a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b374cebcf6a4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b374cebcf6a4--------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Edson Hiroshi Aoki"}, {"url": "https://edsonaoki.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4db29e9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=post_page-35b4db29e9b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8b9561c02d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-apache-sparks-biggest-pain-points-b374cebcf6a4&newsletterV3=35b4db29e9b&newsletterV3Id=8b9561c02d4e&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}