{"url": "https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a", "time": 1682994642.107136, "path": "towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a/", "webpage": {"metadata": {"title": "Everything you need to know about Neural Networks and Backpropagation \u2014 Machine Learning Easy and Fun | by Gavril Ognjanovski | Towards Data Science", "h1": "Everything you need to know about Neural Networks and Backpropagation \u2014 Machine Learning Easy and Fun", "description": "I find it hard to get step by step and detailed explanations about Neural Networks in one place. Always some part of the explanation was missing in courses or in the videos. So I tried to gather all\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Artificial Neural Network", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Function_composition", "anchor_text": "function composition", "paragraph_index": 19}], "all_paragraphs": ["I find it hard to get step by step and detailed explanations about Neural Networks in one place. Always some part of the explanation was missing in courses or in the videos. So I tried to gather all the information and explanations in one blog post (step by step). I would separate this blog in 8 sections as I find it most relevant.", "Artificial Neural Network is computing system inspired by biological neural network that constitute animal brain. Such systems \u201clearn\u201d to perform tasks by considering examples, generally without being programmed with any task-specific rules.", "The Neural Network is constructed from 3 type of layers:", "There are 3 yellow circles on the image above. They represent the input layer and usually are noted as vector X. There are 4 blue and 4 green circles that represent the hidden layers. These circles represent the \u201cactivation\u201d nodes and usually are noted as W or \u03b8. The red circle is the output layer or the predicted value (or values in case of multiple output classes/types).", "Each node is connected with each node from the next layer and each connection (black arrow) has particular weight. Weight can be seen as impact that that node has on the node from the next layer. So if we take a look on one node it would look like this", "Let\u2019s look at the top blue node (\u201cImage 1\u201d). All the nodes from the previous layer (yellow) are connected with it. All these connections represent the weights (impact). When all the node values from the yellow layer are multiplied with their weight and all this is summarized it gives some value for the top blue node. The blue node has predefined \u201cactivation\u201d function (unit step function on \u201cImage 2\u201d) which defines if this node will be \u201cactivated\u201d or how \u201cactive\u201d it will be, based on the summarized value. The additional node with value 1 is called \u201cbias\u201d node.", "In order to understand the mathematical equations I will use a simpler Neural Network model. This model will have 4 input nodes (3 + 1 \u201cbias\u201d). One hidden layer with 4 nodes (3 + 1 \u201cbias\u201d) and one output node.", "We are going to mark the \u201cbias\u201d nodes as x\u2080 and a\u2080 respectively. So, the input nodes can be placed in one vector X and the nodes from the hidden layer in vector A.", "The weights (arrows) are usually noted as \u03b8 or W. In this case I will note them as \u03b8. The weights between the input and hidden layer will represent 3x4 matrix. And the weights between the hidden layer and the output layer will represent 1x4 matrix.", "If network has a units in layer j and b\u200b units in layer j+1, then \u03b8\u2c7c will be of dimension b\u200b\u00d7(a+1).", "Next, what we want is to compute the \u201cactivation\u201d nodes for the hidden layer. In order to do that we need to multiply the input vector X and weights matrix \u03b8\u00b9 for the first layer (X*\u03b8\u00b9)and then apply the activation function g. What we get is :", "And by multiplying hidden layer vector with weights matrix \u03b8 for the second layer(A*\u03b8) we get output for the hypothesis function:", "This example is with only one hidden layer and 4 nodes there. If we try to generalize for Neural Network with multiple hidden layers and multiple nodes in each of the layers we would get next formula.", "Where we have L layers with n nodes and L-1 layer with m nodes.", "In Neural Network the activation function defines if given node should be \u201cactivated\u201d or not based on the weighted sum. Let\u2019s define this weighted sum value as z. In this section I would explain why \u201cStep Function\u201d and \u201cLinear Function\u201d won\u2019t work and talk about \u201cSigmoid Function\u201d one of the most popular activation functions. There are also other functions which I will leave aside for now.", "One of the first ideas would be to use so called \u201cStep Function\u201d (discrete output values) where we define threshold value and:", "This looks nice but it has drawback since the node can only have value 1 or 0 as output. In case when we would want to map multiple output classes (nodes) we got a problem. The problem is that it is possible multiple output classes/nodes to be activated (to have the value 1). So we are not able to properly classify/decide.", "Another possibility would be to define \u201cLinear Function\u201d and get a range of output values.", "However using only linear function in the Neural Network would cause the output layer to be linear function, so we are not able to map any non-linear data. The proof for this is given by:", "then by function composition we get", "which is also a linear function.", "It is one of the most widely used activation function today. It equation is given with the formula below.", "It has multiple properties which makes it so popular:", "Because of this properties it allows the nodes to take any values between 0 and 1. In the end, in case of multiple output classes, this would result with different probabilities of \u201cactivation\u201d for each output class. And we will choose the one with the highest \u201cactivation\u201d(probability) value.", "Using \u201cbias\u201d node is usually critical for creating successful learning model. In short, a bias value allows to shift the activation function to the left or right and it helps getting better fit for the data (better prediction function as output).", "Below there are 3 Sigmoid functions that I draw where you can notice how multiplication/add/subtract the variable x by some value can influence the function.", "Let\u2019s start with defining the general equation for the cost function. This function represent the sum of the error, difference between the predicted value and the real (labeled) value.", "Since this is type of a classification problem y can only take discrete values {0,1}. It can only be in one type of class. For example if we classify images of dogs (class 1), cats (class 2) and birds (class 3). If the input image is dog. The output classes will be value 1 for dog class and value 0 for the other classes.", "This means that we want our hypothesis to satisfy", "So that\u2019s why we will define our hypothesis as", "Where g in this case will be Sigmoid function, since this function has range values between (0,1).", "Our goal is to optimize the cost function so we need to find min J(\u03b8). But Sigmoid function is a \u201cnon-convex\u201d function (\u201cImage 15\u201d) which means that there are multiple local minimums. So it\u2019s not guaranteed to converge (find) to the global minimum. What we need is \u201cconvex\u201d function in order gradient descent algorithm to be able to find the global minimum (minimize J(\u03b8)). In order to do that we use log function.", "So that\u2019s why we use following cost function for neural networks", "In case where labeled value y is equal to 1 the hypothesis is -log(h(x)) or -log(1-h(x)) otherwise.", "The intuition is pretty simple if we look at the function graphs. Let first look at the case where y=1. Then -log(h(x)) would look like the graph below. And we are only interested in the (0,1) x-axis interval since hypothesis can only take values in that range (\u201cImage 13\u201d)", "What we can see from the graph is that if y=1 and h(x) approaches value of 1 (x-axis) the cost approaches the value 0 (h(x)-y would be 0) since it\u2019s the right prediction. Otherwise if h(x) approaches 0 the cost function goes to infinity (very large cost).", "In the other case where y=0, the cost function is -log(1-h(x))", "From the graph here we can see that if h(x) approaches value of 0 the cost would approach 0 since it\u2019s also the right prediction in this case.", "Since y (labeled value) is always equal to 0 or 1 we can write cost function in one equation.", "If we fully write our cost function with the summation we would get:", "And this is for the case where there is only one node in the output layer of Neural Network. If we generalize this for multiple output nodes (multiclass classification) what we get is:", "The right parts of the equations represent cost function \u201cregularization\u201d. This regularization prevent the data from \u201coverfitting\u201d, by reducing the magnitude/values of \u03b8.", "This process of Forward propagation is actually getting the Neural Network output value based on a given input. This algorithm is used to calculate the cost value. What it does is the same mathematical process as the one described in section 2 \u201cModel Representation Mathematics\u201d. Where in the end we get our hypothesis value \u201cImage 7\u201d.", "After we got the h(x) value (hypothesis) we use the Cost function equation (\u201cImage 21\u201d) to calculate the cost for the given set of inputs.", "Here we can notice how forward propagation works and how a Neural Network generates the predictions.", "What we want to do is minimize the cost function J(\u03b8) using the optimal set of values for \u03b8 (weights). Backpropagation is a method we use in order to compute the partial derivative of J(\u03b8).", "This partial derivative value is then used in Gradient descent algorithm (\u201cImage 23\u201d) for calculating the \u03b8 values for the Neural Network that minimize the cost function J(\u03b8).", "Backpropagation is about determining how changing the weights impact the overall cost in the neural network.", "What it does is propagating the \u201cerror\u201d backwards in the neural network. On the way back it is finding how much each weight is contributing in the overall \u201cerror\u201d. The weights that contribute more to the overall \u201cerror\u201d will have larger derivation values, which means that they will change more (when computing Gradient descent).", "Now that we have sense of what Backpropagation algorithm is doing we can dive deeper in the concepts and math behind.", "The derivative of a function (in our case J(\u03b8)) on each variable (in our case weight \u03b8) tells us the sensitivity of the function with respect to that variable or how changing the variable impacts the function value.", "Let\u2019s look at a simple example neural network", "There are two input nodes x and y. The output function is calculating the product x and y. We can now compute the partial derivatives for both nodes", "The partial derivative with respect to x is saying that if x value increase for some value \u03f5 then it would increase the function (product xy) by 7\u03f5 and the partial derivative with respect to y is saying that if y value increase for some value \u03f5 then it would increase the function by 3\u03f5.", "As we defined, Backpropagation algorithm is calculating the derivative of cost function with respect to each \u03b8 weight parameter. By doing this we determine how sensitive is the cost function J(\u03b8) to each of these \u03b8 weight parameters. It also help us determine how much we should change each \u03b8 weight parameter when computing the Gradient descent. So at the end we get model that best fits our data.", "We will by using the neural network model below as starting point to derive the equations.", "In this model we got 3 output nodes (K) and 2 hidden layers. As previously defined, the cost function for the neural network is:", "What we need is to compute the partial derivative of J(\u03b8) with respect to each \u03b8 parameters. We are going to leave out the summarization since we are using vectorized implementation (matrix multiplication). Also we can leave out the regularization (right part of the equation above) and we will compute it separately at the end. Since it is addition the derivative can be computed independently.", "NOTE: Vectorized implementation will be used so we calculate for all training examples at once.", "We start with defining the derivative rules that we will use.", "Now we define the basic equation for our neural network model where l is layer notation and L is for the last layer.", "In our case L has value 4, since we got 4 layers in our model. So let\u2019s start by computing the partial derivative with respect to weights between 3rd and 4th layer.", "Step (6) \u2014 Sigmoid derivativeTo explain the step (6) we need to calculate the partial derivative of sigmoid function.", "In case of the last layer L we got,", "Step (11) \u2014 Get rid of the summarization (\u03a3)Also in the last step (11) it\u2019s important to note that we need to multiply \u03b4 by a transpose in order to get rid of the summarization (1\u2026m for training examples). \u03b4 \u2014 matrix with dimensions [number_of_training_examples, output_layer_size] so this also means that we will get rid from the second summarization (1\u2026K for number of output nodes).a \u2014 matrix with dimensions [hidden_layer_size, number_of_training_examples]", "Now we continue with the next derivative for the \u03b8 parameters between 2nd and 3rd layer. For this derivation we can start from step (9) (\u201cImage 30\u201d). Since \u03b8(2) is inside a(3) function we need to apply the \u201cChain Rule\u201d when calculating the derivative (step(6) from derivative rules on \u201cImage 28\u201d).", "Now we got the derivative for \u03b8 parameter between 2nd and 3rd layer. What we left to do is compute the derivative for \u03b8 parameter between input layer and 2nd layer. By doing this we will see that the same process (equations) will be repeated so we can derive general \u03b4 and derivative equations. Again we continue from step (3) (\u201cImage 34\u201d).", "From the equation above we can derive equations for \u03b4 parameter and derivative with respect to \u03b8 parameter.", "At the end we get is three matrices (same as \u03b8 weight matrices) with same dimensions as the \u03b8 weight matrices and calculated derivatives for each \u03b8 parameter.", "Add the regularizationAs already mentioned regularization is needed for preventing the model from overfitting the data. We have already defined regularization for our cost function which is the right part of the equation defined on \u201cImage 21\u201c.", "In order to add the regularization for the gradient (partial derivative) we need to compute the partial derivative for the regularization above.", "Which means just adding the sum of all theta values from each layer to the partial derivatives with respect to \u03b8.", "We can now implement all the equations in code where we will calculate the Cost and derivatives (using Backpropagation) so we can use them in Gradient descent algorithm later to optimize \u03b8 parameters for our model.", "Hopefully this was clear and easy to understand. If you think that some part needs better explanation please feel free to add a comment or suggestion. For any questions feel free to contact me.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer, Startup Enthusiast, Tech Lover, Seeker"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe5285bc2be3a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ognjanovski.gavril?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ognjanovski.gavril?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Gavril Ognjanovski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e9aa8156fe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&user=Gavril+Ognjanovski&userId=6e9aa8156fe7&source=post_page-6e9aa8156fe7----e5285bc2be3a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Artificial Neural Network"}, {"url": "https://en.wikipedia.org/wiki/Function_composition", "anchor_text": "function composition"}, {"url": "https://www.desmos.com/calculator", "anchor_text": "desmos.com"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.researchgate.net/publication/226717592_Design_for_Optimizability_Traffic_Management_of_a_Future_Internet", "anchor_text": "researchgate.com"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.desmos.com/calculator", "anchor_text": "desmos.com"}, {"url": "https://www.desmos.com/calculator", "anchor_text": "desmos.com"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.coursera.org/learn/machine-learning/home", "anchor_text": "coursera.org"}, {"url": "https://www.mathsisfun.com/calculus/derivatives-introduction.html", "anchor_text": "Introduction to DerivativesMath explained in easy language, plus puzzles, games, quizzes, worksheets and a forum. For K-12 kids, teachers and\u2026www.mathsisfun.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e5285bc2be3a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e5285bc2be3a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e5285bc2be3a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e5285bc2be3a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----e5285bc2be3a---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&user=Gavril+Ognjanovski&userId=6e9aa8156fe7&source=-----e5285bc2be3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&user=Gavril+Ognjanovski&userId=6e9aa8156fe7&source=-----e5285bc2be3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe5285bc2be3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e5285bc2be3a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e5285bc2be3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ognjanovski.gavril?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ognjanovski.gavril?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gavril Ognjanovski"}, {"url": "https://medium.com/@ognjanovski.gavril/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "634 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e9aa8156fe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&user=Gavril+Ognjanovski&userId=6e9aa8156fe7&source=post_page-6e9aa8156fe7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8d1666956212&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a&newsletterV3=6e9aa8156fe7&newsletterV3Id=8d1666956212&user=Gavril+Ognjanovski&userId=6e9aa8156fe7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}