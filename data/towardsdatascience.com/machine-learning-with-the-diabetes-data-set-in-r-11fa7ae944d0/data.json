{"url": "https://towardsdatascience.com/machine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0", "time": 1682994698.008281, "path": "towardsdatascience.com/machine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0/", "webpage": {"metadata": {"title": "Machine learning with the \u201cdiabetes\u201d data set in R | by William Butler | Towards Data Science", "h1": "Machine learning with the \u201cdiabetes\u201d data set in R", "description": "Inspired by Susan Li\u2019s article on applying basic machine learning techniques in Python, I decided to implement the same techniques in R. In addition, I hope to expand somewhat the explanations for\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/machine-learning-for-diabetes-562dd7df4d42", "anchor_text": "Susan Li\u2019s article", "paragraph_index": 0}], "all_paragraphs": ["Inspired by Susan Li\u2019s article on applying basic machine learning techniques in Python, I decided to implement the same techniques in R. In addition, I hope to expand somewhat the explanations for why each method is useful and how they compare to one another.", "All of the analyses below use the Pima Indians diabetes data set, which can be accessed within R by:", "Overall, this data set consists of 768 observations of 9 variables: 8 variables which will be used as model predictors (number of times pregnant, plasma glucose concentration, diastolic blood pressure (mm Hg), triceps skin fold thickness (in mm), 2-hr serum insulin measure, body mass index, a diabetes pedigree function, and age) and 1 outcome variable (whether or not the patient has diabetes).", "We\u2019ll begin by applying the k-nearest neighbors method of classifying patients by their similarity to other patients. For this method (and all subsequent methods), we\u2019ll start by separating the data set into \u201ctraining\u201d and \u201ctest\u201d sets. We\u2019ll build our model based on the relationship between the predictors and the outcome on the training set, then use the model\u2019s specifications to predict the outcome on the test set. We can then compare our predicted outcomes to the test set\u2019s actual diabetes status to give us a measure of model accuracy. For my exercises, I\u2019ll use the sample.split function from the caTools package.", "For k-nearest neighbors, we compute the outcome for each test case by comparing that case to the \u201cnearest neighbors\u201d in the training set. The assigned outcome depends on how many of these neighbors you decide to look at; the majority class of the three closest neighbors may be different than the majority class of the five closest neighbors (see figure at left).", "To ensure we use a number for k that gives better model performance, I performed a two-part cross-validation. First, I varied the possible values for k from 2 to 10; second, I repeated the splitting of the data into training and test sets 100 times to ensure a robust estimate of model performance for each k. I used the knn function within the class package and computed model accuracy on the test set for each fold.", "From this analysis, we can see that k-nearest neighbors performs better for somewhat larger values of k, with performance reaching a maximum of about 73% classification accuracy. Though there is still some variance depending on the exact data split, using 9 or 10 neighbors seems to yield fairly stable model estimates on the test set.", "Next, we\u2019ll apply another of the basic workhorses of the machine learning toolset: regression. For this data set, where we\u2019re predicting a binary outcome (diabetes diagnosis), we\u2019re using logistic regression rather than linear regression (to predict a continuous variable). Again, I\u2019ll cross-validate the logistic regression model by repeatedly splitting the data into different training and test sets.", "Across all folds, we achieve a mean model accuracy of 77%, with performance ranging from 67\u201384% depending on the exact training-test split. Logistic regression appears to be somewhat more accurate on this data set than k-nearest neighbors, even with the optimal choices of k (compare the filled distribution to the vertical lines showing the approximate optimal distribution for KNN).", "Following the same logic as used for choosing logistic regression over linear regression, we\u2019ll be building a classification tree rather than a regression tree. Decision trees construct \u201cnodes\u201d at which data is separated, eventually terminating in \u201cleaves\u201d which give you the model\u2019s assigned class. Here again, I implemented 100 folds of training-test splits, and then assigned the value of each predictor to an output matrix to compare variable importance across folds.", "An example tree for this data set is shown below:", "Across decision trees, blood glucose was consistently the most important variable in the tree (>40% of the model), with BMI and age coming in next at 10\u201315% and the remaining variables contributing <10%. Overall, the accuracy of the decision tree model was around 74\u201375%.", "Random forests, in contrast to the simple decision tree used above, aggregate multiple decorrelated decision trees in order to yield a prediction. When building each set of decision trees, at each split only a sample of predictors is chosen as split candidates, rather than allowing the tree to choose from all possible predictors. Therefore, each individual tree is less likely to select identical splits to the other trees. In the case of the randomForest function that I use here, the default number of predictors that can be selected from at each split is floor(ncol(x)/3), or 2.", "In contrast to the simple tree classification method above, in the random forest model the importance of each variable is more evenly distributed. While glucose is still the most importance factor in the model, in the random forest it accounts for only ~25% of the model. This is intuitive, given that one of the ways in which random forests improve upon decision trees is by decorrelating the bagged trees from one another by making it less likely that every tree will use the same (strongest) predictor variable (in this case, glucose levels) as the first split in the tree.", "Overall, this method also offers only a slight (but perhaps still meaningful) improvement over cross-validated decision trees. While the simple decision trees had a mean accuracy of 74% and ranged from 65\u201382%, the random forest yielded models with a mean accuracy of 76%, ranging from 68\u201382%. Therefore, as expected, the random forest method yielded somewhat more robust, invariant results.", "Finally, we\u2019ll fit another ensemble learning method to the data: gradient boosting. Generally, gradient boosting refers to iteratively fitting a model to the residual of the previous model, thereby improving the overall model fit. Gradient boosting \u201cboosts\u201d the decision tree model of classification by consecutively fitting gradually more complex trees to the data, and by using the residuals of the previous tree as a guide to the subsequent tree. The gradient here refers to solving the problem of minimization through gradient descent, that is, finding the gradient at your current value and following the gradient in a decreasing direction.", "Once again, we see that glucose levels are the overwhelming primary factor in determining the diagnosis of diabetes. And in this case, the least important variables (insulin, blood pressure, and skin thickness) are minimized even more greatly than in previous models. Overall, the gradient boosted model performed slightly better than the random forest, with a mean classification accuracy of 76% ranging from 68\u201383%, an overall increase of ~0.6% relative to random forest.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Systems neuroscientist with an interest in analytics and data visualization"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F11fa7ae944d0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wbutler17?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wbutler17?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "William Butler"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6ff7ffa4bd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&user=William+Butler&userId=b6ff7ffa4bd9&source=post_page-b6ff7ffa4bd9----11fa7ae944d0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/machine-learning-for-diabetes-562dd7df4d42", "anchor_text": "Susan Li\u2019s article"}, {"url": "http://By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282", "anchor_text": "https://commons.wikimedia.org/w/index.php?curid=2170282"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----11fa7ae944d0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----11fa7ae944d0---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----11fa7ae944d0---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/diabetes?source=post_page-----11fa7ae944d0---------------diabetes-----------------", "anchor_text": "Diabetes"}, {"url": "https://medium.com/tag/data-science?source=post_page-----11fa7ae944d0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&user=William+Butler&userId=b6ff7ffa4bd9&source=-----11fa7ae944d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&user=William+Butler&userId=b6ff7ffa4bd9&source=-----11fa7ae944d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F11fa7ae944d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----11fa7ae944d0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----11fa7ae944d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wbutler17?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wbutler17?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "William Butler"}, {"url": "https://medium.com/@wbutler17/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6ff7ffa4bd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&user=William+Butler&userId=b6ff7ffa4bd9&source=post_page-b6ff7ffa4bd9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fb6ff7ffa4bd9%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-the-diabetes-data-set-in-r-11fa7ae944d0&user=William+Butler&userId=b6ff7ffa4bd9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}