{"url": "https://towardsdatascience.com/what-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c", "time": 1683005172.410182, "path": "towardsdatascience.com/what-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c/", "webpage": {"metadata": {"title": "What If Only Batch Normalization Layers Were Trained? | by Ygor Serpa | Towards Data Science", "h1": "What If Only Batch Normalization Layers Were Trained?", "description": "Recently, I read the paper \u201cTraining BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs\u201d, by Jonathan Frankle, David J. Schwab, and Ari S. Morcos, recently made\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2003.00152", "anchor_text": "arXiv platform", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization", "paragraph_index": 1}, {"url": "https://blog.deeplearning.ai/blog/the-batch-antiviral-resources-robot-superstars-ai-for-scientists-3d-data-augmentation-batchnorm-demystified", "anchor_text": "deeplearning.ai\u2019s the Batch", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "self-normalizing networks", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2102.06171", "anchor_text": "normalizer-free networks", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "original explanation", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "contradicting evidence", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1805.11604.pdf", "anchor_text": "smoothing the optimization landscape", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1805.10694", "anchor_text": "improved formulation", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "SELU", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1807.10117", "anchor_text": "SERLU", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me", "paragraph_index": 29}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing", "paragraph_index": 29}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow", "paragraph_index": 29}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up.", "paragraph_index": 29}], "all_paragraphs": ["I, for one, would never bet my money on it.", "Recently, I read the paper \u201cTraining BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs\u201d, by Jonathan Frankle, David J. Schwab, and Ari S. Morcos, recently made available at the arXiv platform. The idea immediately caught my attention. Up to now, I had never considered Batch Normalization (BN) layers as part of the learning process itself, only as an aid to the convergence and stability of deep networks. A couple of experiments later, I couldn\u2019t be more wrong. In the following, I present my take at reproducing this paper\u2019s results and what I learned from it.", "In more detail, I successfully reproduced the paper\u2019s main experiment using the Tensorflow 2 Keras API, reaching similar conclusions. Namely, that ResNets can achieve decent results in the CIFAR-10 dataset by training only the gamma (\u03b3) and beta (\u03b2) parameters of Batch Normalization layers. Numerically, I got 45, 52 and 50% Top-1 accuracy using the ResNet-50, 101 and 152 architectures, which is far from great, but also far from random.", "In the following, I overview the Batch Normalization concept, the common explanations behind its benefits, the code I used, and the results obtained. Finally, I present a discussion of the results and their relevance.", "Recently, this work was featured on the deeplearning.ai\u2019s the Batch newsletter.", "In simple terms, Batch Normalization layers estimate the mean (\u03bc) and variance (\u03c3\u00b2) of its inputs and produce standardized outputs, i.e., outputs with zero mean and unit variance. In practice, this technique meaningfully improves the convergence and stability of deep networks. Additionally, it shifts and scales its outputs using two parameters (\u03b3 and \u03b2).", "Being x the layer\u2019s inputs and z its outputs, z is given by the following formula:", "While \u03bc and \u03c3\u00b2 parameters are estimated from the input data, \u03b3 and \u03b2 are trainable. Thus, they can be leveraged by the back-propagation algorithm to optimize the network.", "As said, this operation has been found to significantly improve how fast networks train and also their performance on held-out data. Moreover, it has no contraindications. For this reason, most models use it heavily, often in-between all Conv-ReLU operations, forming the \u201cConv-BN-ReLU\u201d trio (and its variants). However, although this is one of the most frequently seen layers, the reasons behind its benefits are much debated in the literature. Here is a summary of the three mostly gave explanations.", "Edit: In some cases, batch norm does have contraindications. Recent works have raised awareness that, despite improving convergence, batch norm severely impacts training speed. In special, when training across multiple GPUs and devices, computing batch statistics without compromising parallel performance is a significant challenge. For that reason, several authors have proposed alternatives, such as self-normalizing networks or even normalizer-free networks. These works aim to retain batch norm\u2019s benefits while avoiding any kind of dependence between data points during training.", "Internal Convariance Shift: Simply put, if outputs have zero mean and unit variance, the next layer trains upon a stable input. In other words, it keeps outputs from varying too wildly. While this is the original explanation, later works found contradicting evidence, rejecting this hypothesis. In short, if you train the VGG network (1) without BN, (2) with BN and (3) with BN plus artificial covariance shifts, approaches (2) and (3) still outperform (1), despite the artificial covariance shift added to the network.", "Output Smoothing: BN is also credited for smoothing the optimization landscape, reducing how much the loss function varies and bounding its gradients. A smoother objective trains more predictably and is less prone to numerical issues.", "Length-Direction Decoupling: Some authors argue that BN is an improved formulation for optimization problems and, therefore, can be extended to more traditional optimization settings. In more detail, the BN framework allows the length and direction of parameters to be optimized independently, improving convergence.", "In sum, all three explanations focus on the normalization aspect of Batch Normalization. Contrastingly, we shall look at the shift-and-scale point of BN, realized by the \u03b3, and \u03b2 parameters.", "If an idea is any good, it should be resilient to the implementation and the choice of hyperparameters. In my code, I recreated the main experiment as minimally as possible, using Tensorflow 2 and my own choice of hyperparameters. In more detail, I tested the following proposition:", "ResNet models can achieve decent results on CIFAR-10 with all weighs locked, except for the batch normalization parameters.", "Thus, I will be using Keras\u2019 CIFAR-10 and ResNet modules and the overall recommendation for the CIFAR-10 dataset, which is the Categorical Cross-Entropy loss and the Softmax activation. My code downloads the dataset and the randomly-initialized ResNet model, freezes the unwanted layers, and trains for 50 epochs using a batch size of 1024 images. You can inspect the code below:", "A couple of things should be noted in the above code:", "Here are the results I have obtained with the above code:", "To have a good sense of how well a model is performing, we should always consider the performance of random guessing. The CIFAR-10 dataset has ten classes. Thus, at random, we can be right 10% of the time. The above methods are about five times better than random guessing. Therefore, we can consider them as having decent performance.", "Interestingly, the validation accuracy took ten epochs to start increasing, which is a clear sign that, for the first ten epochs, the network was just overfitting the data as best as it could. Later on, the validation performance rises substantially. However, it varies greatly every five epochs, which shows the model is not very stable.", "In the paper, Figure 2 shows that they achieved a validation accuracy of ~70, ~75, and ~77%. Considering the authors done some tuning, used a custom training schedule, and employed Data Augmentation, this seems pretty reasonable and consistent with my findings, confirming the hypothesis.", "Using an 866-layers ResNet, the authors got to almost ~85% accuracy, which is only a few percentage points below the ~91% achievable by training the whole architecture. Furthermore, they tested different initialization schemes, architectures, and tested unfreezing the last layer and skip connections, which resulted in some additional performance gains.", "Besides accuracy, the authors also investigated the histogram of the \u03b3 and \u03b2 parameters, finding that the network learned to suppress about a third of all activations in each BN layer, by setting \u03b3 to near-zero values.", "At this point, you might ask: why all this? First of all, it\u2019s fun :) Second, BN layers are commonplace, but we still have only a superficial understanding of their role. What we know are their benefits. Thirdly, this kind of investigation is what leads us to a more in-depth understanding of how our models operate.", "I don\u2019t believe that this has practical applications by itself. No one will freeze their layers and leave it all to the BNs. However, this might inspire different training schedules. Maybe training the network for a few epochs like this and then training all weights might lead to superior performance. Inversely, this technique might prove useful for fine-tuning pre-trained models. I can also see this idea being leveraged for pruning weights of big networks.", "What puzzles me the most with this study is how much we have all been ignoring these two parameters. I, at least, never minded both. I recall seeing only one discussion about it, which argued that it is good to initialize \u03b3 with \u2018zeros\u2019 on ResNet blocks to force the back-propagation algorithm to use the skip connections more in early epochs.", "My second questioning regards the SELU and SERLU activation functions, which have the self-normalizing property. Both functions make the Batch Normalization layers obsolete, as they naturally normalize their outputs during training. Now, I question myself if this captures the entirety of the Batch Normalization layer.", "Finally, the hypothesis is still a bit primitive. It only considers the CIFAR-10 dataset and significantly deep networks. It is open if this can scale to other datasets or solve different tasks, such as a Batchnorm-only GAN. Also, I would find it interesting to see a follow-up article on the role of \u03b3 and \u03b2 for fully trained networks.", "Feel free to comment or connect with me if you have any questions on this article. If you are new to Medium, I highly recommend subscribing. Medium articles are the perfect pair to StackOverflow for Data and IT professionals, and even more so for new comers. Please consider using my affiliate link when signing up.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Former game developer turned data scientist after falling in love with AI and all its branches."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d13e2d3f35c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ygorserpa.medium.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----1d13e2d3f35c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@cassi_josh?utm_source=medium&utm_medium=referral", "anchor_text": "Cassi Josh"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/2003.00152", "anchor_text": "arXiv platform"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization"}, {"url": "https://blog.deeplearning.ai/blog/the-batch-antiviral-resources-robot-superstars-ai-for-scientists-3d-data-augmentation-batchnorm-demystified", "anchor_text": "deeplearning.ai\u2019s the Batch"}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "self-normalizing networks"}, {"url": "https://arxiv.org/abs/2102.06171", "anchor_text": "normalizer-free networks"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "original explanation"}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "contradicting evidence"}, {"url": "https://arxiv.org/pdf/1805.11604.pdf", "anchor_text": "smoothing the optimization landscape"}, {"url": "https://arxiv.org/abs/1805.10694", "anchor_text": "improved formulation"}, {"url": "https://keras.io/applications/#resnet", "anchor_text": "Keras API"}, {"url": "https://keras.io/examples/cifar10_resnet/", "anchor_text": "this guide"}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "SELU"}, {"url": "https://arxiv.org/abs/1807.10117", "anchor_text": "SERLU"}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing"}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up."}, {"url": "https://medium.com/tag/data-science?source=post_page-----1d13e2d3f35c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1d13e2d3f35c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1d13e2d3f35c---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/batch-normalization?source=post_page-----1d13e2d3f35c---------------batch_normalization-----------------", "anchor_text": "Batch Normalization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d13e2d3f35c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&user=Ygor+Serpa&userId=857d8734c7da&source=-----1d13e2d3f35c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&user=Ygor+Serpa&userId=857d8734c7da&source=-----1d13e2d3f35c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d13e2d3f35c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d13e2d3f35c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d13e2d3f35c--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://ygorserpa.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3644829c90d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c&newsletterV3=857d8734c7da&newsletterV3Id=3644829c90d9&user=Ygor+Serpa&userId=857d8734c7da&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}