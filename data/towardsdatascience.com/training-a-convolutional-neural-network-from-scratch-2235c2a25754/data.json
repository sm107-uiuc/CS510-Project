{"url": "https://towardsdatascience.com/training-a-convolutional-neural-network-from-scratch-2235c2a25754", "time": 1682996527.937649, "path": "towardsdatascience.com/training-a-convolutional-neural-network-from-scratch-2235c2a25754/", "webpage": {"metadata": {"title": "Training a Convolutional Neural Network from scratch | by Victor Zhou | Towards Data Science", "h1": "Training a Convolutional Neural Network from scratch", "description": "In this post, we\u2019re going to do a deep-dive on something most introductions to Convolutional Neural Networks (CNNs) lack: how to train a CNN, including deriving gradients, implementing backprop from\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.numpy.org/", "anchor_text": "numpy", "paragraph_index": 0}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "introduction to CNNs", "paragraph_index": 1}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "my introduction to CNNs", "paragraph_index": 4}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST", "paragraph_index": 4}, {"url": "https://repl.it/@vzhou842/A-CNN-from-scratch-Part-1", "anchor_text": "run the CNN in your browser", "paragraph_index": 7}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github", "paragraph_index": 7}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss", "anchor_text": "Cross-Entropy Loss", "paragraph_index": 16}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#5-softmax", "anchor_text": "Softmax", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Quotient_rule", "anchor_text": "Quotient Rule", "paragraph_index": 26}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/#training-stochastic-gradient-descent", "anchor_text": "introduction to Neural Networks", "paragraph_index": 34}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "CNNs introduction", "paragraph_index": 37}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#41-implementing-pooling", "anchor_text": "wrote in my introduction to CNNs", "paragraph_index": 46}, {"url": "https://repl.it/@vzhou842/A-CNN-from-scratch-Part-2", "anchor_text": "Run this CNN in your browser", "paragraph_index": 63}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github", "paragraph_index": 63}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 64}, {"url": "https://keras.io/examples/mnist_cnn/", "anchor_text": "official Keras MNIST CNN example", "paragraph_index": 66}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github", "paragraph_index": 67}, {"url": "https://victorzhou.com", "anchor_text": "https://victorzhou.com", "paragraph_index": 70}], "all_paragraphs": ["In this post, we\u2019re going to do a deep-dive on something most introductions to Convolutional Neural Networks (CNNs) lack: how to train a CNN, including deriving gradients, implementing backprop from scratch (using only numpy), and ultimately building a full training pipeline!", "This post assumes a basic knowledge of CNNs. My introduction to CNNs covers everything you need to know, so I\u2019d highly recommend reading that first. If you\u2019re here because you\u2019ve already read that, welcome back!", "Parts of this post also assume a basic knowledge of multivariable calculus. You can skip those sections if you want, but I recommend reading them even if you don\u2019t understand everything. We\u2019ll incrementally write code as we derive results, and even a surface-level understanding can be helpful.", "Buckle up! Time to get into it.", "We\u2019ll pick back up where my introduction to CNNs left off. We were using a CNN to tackle the MNIST handwritten digit classification problem:", "Our (simple) CNN consisted of a Conv layer, a Max Pooling layer, and a Softmax layer. Here\u2019s that diagram of our CNN again:", "We\u2019d written 3 classes, one for each layer: Conv3x3, MaxPool, and Softmax. Each class implemented a forward() method that we used to build the forward pass of the CNN:", "You can view the code or run the CNN in your browser. It\u2019s also available on Github.", "Here\u2019s what the output of our CNN looks like right now:", "Obviously, we\u2019d like to do better than 10% accuracy\u2026 let\u2019s teach this CNN a lesson.", "Training a neural network typically consists of two phases:", "We\u2019ll follow this pattern to train our CNN. There are also two major implementation-specific ideas we\u2019ll use:", "These two ideas will help keep our training implementation clean and organized. The best way to see why is probably by looking at code. Training our CNN will ultimately look something like this:", "See how nice and clean that looks? Now imagine building a network with 50 layers instead of 3 \u2014 it\u2019s even more valuable then to have good systems in place.", "We\u2019ll start our way from the end and work our way towards the beginning, since that\u2019s how backprop works. First, recall the cross-entropy loss:", "where p_c\u200b is the predicted probability for the correct class c (in other words, what digit our current image actually is).", "Want a longer explanation? Read the Cross-Entropy Loss section of my introduction to CNNs.", "The first thing we need to calculate is the input to the Softmax layer\u2019s backward phase, \u2202L / \u2202out_s\u200b, where out_s\u200b is the output from the Softmax layer: a vector of 10 probabilities. This is pretty easy, since only p_i\u200b shows up in the loss equation:", "That\u2019s our initial gradient you saw referenced above:", "We\u2019re almost ready to implement our first backward phase \u2014 we just need to first perform the forward phase caching we discussed earlier:", "We cache 3 things here that will be useful for implementing the backward phase:", "With that out of the way, we can start deriving the gradients for the backprop phase. We\u2019ve already derived the input to the Softmax backward phase: \u2202L / \u2202out_s\u200b. One fact we can use about \u2202L / \u2202out_s\u200b is that it\u2019s only nonzero for c, the correct class. That means that we can ignore everything but out_s(c)!", "First, let\u2019s calculate the gradient of out_s(c) with respect to the totals (the values passed in to the softmax activation). Let t_i\u200b be the total for class i. Then we can write out_s(c) as:", "You should recognize the equation above from the Softmax section of my CNNs tutorial.", "Now, consider some class k such that k is not c. We can rewrite out_s(c) as:", "and use Chain Rule to derive:", "Remember, that was assuming k doesn\u2019t equal c. Now let\u2019s do the derivation for c, this time using Quotient Rule:", "Phew. That was the hardest bit of calculus in this entire post \u2014 it only gets easier from here! Let\u2019s start implementing this:", "Remember how \u2202L / \u2202out_s is only nonzero for the correct class, c? We start by looking for c by looking for a nonzero gradient in d_L_d_out. Once we find that, we calculate the gradient \u2202out_s(i) / \u2202t (d_out_d_totals) using the results we derived above:", "Let\u2019s keep going. We ultimately want the gradients of loss against weights, biases, and input:", "To calculate those 3 loss gradients, we first need to derive 3 more results: the gradients of totals against weights, biases, and input. The relevant equation here is:", "Putting this into code is a little less straightforward:", "First, we pre-calculate d_L_d_t since we'll use it several times. Then, we calculate each gradient:", "Try working through small examples of the calculations above, especially the matrix multiplications for d_L_d_w and d_L_d_inputs. That's the best way to understand why this code correctly computes the gradients.", "With all the gradients computed, all that\u2019s left is to actually train the Softmax layer! We\u2019ll update the weights and bias using Stochastic Gradient Descent (SGD) just like we did in my introduction to Neural Networks and then return d_L_d_inputs:", "Notice that we added a learn_rate parameter that controls how fast we update our weights. Also, we have to reshape() before returning d_L_d_inputs because we flattened the input during our forward pass:", "Reshaping to last_input_shape ensures that this layer returns gradients for its input in the same format that the input was originally given to it.", "We\u2019ve finished our first backprop implementation! Let\u2019s quickly test it to see if it\u2019s any good. We\u2019ll start implementing a train()method from my CNNs introduction:", "Running this gives results similar to:", "The loss is going down and the accuracy is going up \u2014 our CNN is already learning!", "A Max Pooling layer can\u2019t be trained because it doesn\u2019t actually have any weights, but we still need to implement a method for it to calculate gradients. We\u2019ll start by adding forward phase caching again. All we need to cache this time is the input:", "During the forward pass, the Max Pooling layer takes an input volume and halves its width and height dimensions by picking the max values over 2x2 blocks. The backward pass does the opposite: we\u2019ll double the width and height of the loss gradient by assigning each gradient value to where the original max value was in its corresponding 2x2 block.", "Here\u2019s an example. Consider this forward phase for a Max Pooling layer:", "The backward phase of that same layer would look like this:", "Each gradient value is assigned to where the original max value was, and every other value is zero.", "Why does the backward phase for a Max Pooling layer work like this? Think about what \u2202L / \u2202inputs intuitively should be. An input pixel that isn\u2019t the max value in its 2x2 block would have zero marginal effect on the loss, because changing that value slightly wouldn\u2019t change the output at all! In other words, \u2202L / \u2202inputs = 0 for non-max pixels. On the other hand, an input pixel that is the max value would have its value passed through to the output, so \u2202output / \u2202input = 1, meaning \u2202L / \u2202input = \u2202L / \u2202output.", "We can implement this pretty quickly using the helper method we wrote in my introduction to CNNs. I\u2019ll include it again as a reminder:", "For each pixel in each 2x2 image region in each filter, we copy the gradient from d_L_d_out to d_L_d_input if it was the max value during the forward pass.", "That\u2019s it! On to our final layer.", "We\u2019re finally here: backpropagating through a Conv layer is the core of training a CNN. The forward phase caching is simple:", "Reminder about our implementation: for simplicity, we assume the input to our conv layer is a 2d array. This only works for us because we use it as the first layer in our network. If we were building a bigger network that needed to use Conv3x3 multiple times, we'd have to make the input be a 3d array.", "We\u2019re primarily interested in the loss gradient for the filters in our conv layer, since we need that to update our filter weights. We already have \u2202L / \u2202out\u200b for the conv layer, so we just need \u2202out / \u2202filters\u200b. To calculate that, we ask ourselves this: how would changing a filter\u2019s weight affect the conv layer\u2019s output?", "The reality is that changing any filter weights would affect the entire output image for that filter, since every output pixel uses every pixel weight during convolution. To make this even easier to think about, let\u2019s just think about one output pixel at a time: how would modifying a filter change the output of one specific output pixel?", "Here\u2019s a super simple example to help think about this question:", "We have a 3x3 image convolved with a 3x3 filter of all zeros to produce a 1x1 output. What if we increased the center filter weight by 1? The output would increase by the center image value, 80:", "Similarly, increasing any of the other filter weights by 1 would increase the output by the value of the corresponding image pixel! This suggests that the derivative of a specific output pixel with respect to a specific filter weight is just the corresponding image pixel value. Doing the math confirms this:", "We can put it all together to find the loss gradient for specific filter weights:", "We\u2019re ready to implement backprop for our conv layer!", "We apply our derived equation by iterating over every image region / filter and incrementally building the loss gradients. Once we\u2019ve covered everything, we update self.filters using SGD just as before. Note the comment explaining why we're returning - the derivation for the loss gradient of the inputs is very similar to what we just did and is left as an exercise to the reader :).", "With that, we\u2019re done! We\u2019ve implemented a full backward pass through our CNN. Time to test it out\u2026", "We\u2019ll train our CNN for a few epochs, track its progress during training, and then test it on a separate test set. Here\u2019s the full code:", "Example output from running the code:", "Our code works! In only 3000 training steps, we went from a model with 2.3 loss and 10% accuracy to 0.6 loss and 78% accuracy.", "Want to try or tinker with this code yourself? Run this CNN in your browser. It\u2019s also available on Github.", "We only used a subset of the entire MNIST dataset for this example in the interest of time \u2014 our CNN implementation isn\u2019t particularly fast. If we wanted to train a MNIST CNN for real, we\u2019d use an ML library like Keras. To illustrate the power of our CNN, I used Keras to implement and train the exact same CNN we just built from scratch:", "Running that code on the full MNIST dataset (60k training images) gives us results like this:", "We achieve 97.4% test accuracy with this simple CNN! With a better CNN architecture, we could improve that even more \u2014 in this official Keras MNIST CNN example, they achieve 99.25% test accuracy after 12 epochs. That\u2019s a really good accuracy.", "All code from this post is available on Github.", "We\u2019re done! In this post, we did a full walkthrough of how to train a Convolutional Neural Network. This is just the beginning, though. There\u2019s a lot more you could do:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS @ Princeton University. I write about web development, machine learning, and more at https://victorzhou.com."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2235c2a25754&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://victorczhou.medium.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Victor Zhou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd190d205cab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&user=Victor+Zhou&userId=dd190d205cab&source=post_page-dd190d205cab----2235c2a25754---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.numpy.org/", "anchor_text": "numpy"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "introduction to CNNs"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "my introduction to CNNs"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST"}, {"url": "https://repl.it/@vzhou842/A-CNN-from-scratch-Part-1", "anchor_text": "run the CNN in your browser"}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss", "anchor_text": "Cross-Entropy Loss"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#5-softmax", "anchor_text": "Softmax"}, {"url": "https://en.wikipedia.org/wiki/Quotient_rule", "anchor_text": "Quotient Rule"}, {"url": "https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#numpy.newaxis", "anchor_text": "np.newaxis"}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/#training-stochastic-gradient-descent", "anchor_text": "introduction to Neural Networks"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "CNNs introduction"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#41-implementing-pooling", "anchor_text": "wrote in my introduction to CNNs"}, {"url": "https://repl.it/@vzhou842/A-CNN-from-scratch-Part-2", "anchor_text": "Run this CNN in your browser"}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://keras.io/examples/mnist_cnn/", "anchor_text": "official Keras MNIST CNN example"}, {"url": "https://github.com/vzhou842/cnn-from-scratch", "anchor_text": "Github"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://en.wikipedia.org/wiki/Batch_normalization", "anchor_text": "Batch Normalization"}, {"url": "https://en.wikipedia.org/wiki/ImageNet", "anchor_text": "ImageNet"}, {"url": "http://image-net.org/challenges/LSVRC/", "anchor_text": "ILSVRC"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-2/", "anchor_text": "https://victorzhou.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2235c2a25754---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2235c2a25754---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----2235c2a25754---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/python?source=post_page-----2235c2a25754---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----2235c2a25754---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&user=Victor+Zhou&userId=dd190d205cab&source=-----2235c2a25754---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&user=Victor+Zhou&userId=dd190d205cab&source=-----2235c2a25754---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2235c2a25754&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2235c2a25754---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2235c2a25754--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2235c2a25754--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2235c2a25754--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2235c2a25754--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2235c2a25754--------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Victor Zhou"}, {"url": "https://victorczhou.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "925 Followers"}, {"url": "https://victorzhou.com", "anchor_text": "https://victorzhou.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd190d205cab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&user=Victor+Zhou&userId=dd190d205cab&source=post_page-dd190d205cab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8d9c8575861&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-convolutional-neural-network-from-scratch-2235c2a25754&newsletterV3=dd190d205cab&newsletterV3Id=b8d9c8575861&user=Victor+Zhou&userId=dd190d205cab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}