{"url": "https://towardsdatascience.com/bayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06", "time": 1683008004.080102, "path": "towardsdatascience.com/bayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06/", "webpage": {"metadata": {"title": "Bayesian Thinking for Linear Regression @ Kaggle Days Meetup | by Souradip Chakraborty | Towards Data Science", "h1": "Bayesian Thinking for Linear Regression @ Kaggle Days Meetup", "description": "One of the major motivations of this research is the fact that there has been an increasing focus on Deep model interpretability with the advent of more and more complex models. More is the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=6rWvmwucgZM&t=1850s", "anchor_text": "talks", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation", "anchor_text": "Maximum a posteriori", "paragraph_index": 6}, {"url": "https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html", "anchor_text": "Kieran R Campbell\u2019s", "paragraph_index": 21}], "all_paragraphs": ["One of the major motivations of this research is the fact that there has been an increasing focus on Deep model interpretability with the advent of more and more complex models. More is the complexity of the model, difficult it gets to have interpretability with respect to the outputs and a lot of research is going in the field of Bayesian thinking and learning.", "But before understanding and being able to appreciate Bayesian in deep neural models, we should be well versed and adept with Bayesian thinking in linear models for example- Bayesian Linear regression. But there are very few good materials available online in a combined fashion which can give a clear motivation and understanding of the Bayesian Linear regression.", "This was one of the major motivations for this blog and here I will try to give an understanding of how to approach the Linear regression from a Bayesian analysis standpoint.", "Just before starting let's understand that we are very cleary with Maximum Likelihood estimates of Linear regression models. I have discussed this in one of my previous talks, please refer to the link.", "Just to introduce very briefly the concept of the Maximum Likelihood Estimate in a Linear regression scenario.", "The most important point to understand from this is that MLE gives you a point estimate of the parameter by maximizing the Likelihood P(D|\u03b8).", "Even, MAP which is Maximum a posteriori estimation maximizes the posterior probability P(\u03b8|D), which also gives point estimation.", "So, these methods don\u2019t give you enough information about the distribution of the coefficients, whereas in Bayesian we estimate the posterior distribution of the parameters. Hence, in this case, the output is not a single value but a probability density/mass function.", "Just to understand the intuition behind it.", "Just one more step to go !!!", "Before delving deep into Bayesian Regression, we need to understand one more thing which is Markov Chain Monte Carlo Simulations and why it is needed?", "MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space. But why approximating the distribution and not calculating the exact distribution might be one question that you must be intrigued by.", "So, it's almost impossible to compute and intractable due to the denominator and hence we go for approximating the posterior distribution using MCMC.", "Monte Carlo methods help us in generating random variables following a given distribution. For example: -\u03b8 ~ N(mean, sigma**2), will help in generating multiple \u03b8 from the Normal distribution and there are numerous methods to do so.", "Markov chain is a sequence of numbers where each number is dependent on the previous number in the sequence. Markov Chain Monte Carlo helps in generating random variables from a distribution where each value of \u03b8 is drawn from a known distribution with a mean equal to the previous value.", "As you can understand, this is basically a random walk and there can be a lot of values generated which might not be necessary and relevant. Hence, we need to do an acceptance/rejection sampling of the values to generate the distribution of interest.", "In this case the Metropolis-Hastings algorithm is used to refine the values by rejection and acceptance sampling.", "We can also use Gibb\u2019s sampling, where the goal is to find the posterior distribution P(\u03b81,\u03b82|y,x) which is done by obtaining the posterior conditional distributions P(\u03b81|\u03b82, y,x) and P(\u03b82|\u03b81, y,x). So, we generate", "\u03b81 ~P(\u03b81|\u03b82, y,x), replace the value of generated \u03b81 in the second equation and generate \u03b82 ~ P(\u03b82|\u03b81, y,x) and we continue the process for several iterations to get the posterior.", "Now, let\u2019s illustrate the same with an example.", "In the below example, I will be illustrating the Bayesian Linear Regression methodology firstly with Gibbs sampling. Here, I have assumed certain distributions for the parameters.", "In this section, I will show you an illustration of using Gibbs sampling for Bayesian Linear regression. This section has been taken from the brilliantly illustrated Kieran R Campbell\u2019s blog on Bayesian Linear regression.", "So, to begin with, let's understand how the data is for our case. Let D be the dataset for the following experiment and D is defined as,", "Y ~ N( b*x + c, 1/t), where Y is a random variable normally distributed with mean b*x + c and variance of 1/t, where t represents the precision.", "In this case, we will be considering a univariate feature space (X) with two parameters, slope(c) and intercept (b). So, in this experiment, we will learn the posterior distributions of the above parameters c & b and the precision t.", "Let's write down the assumed prior distributions of the above parameters", "So, lets first write down the density function of a normal distribution and the log_pdf to have a generalizable form, which will be of use multiple times later.", "Now it will be very easy for us to pen down the likelihood for our case which also follows Normal with mean bx+c and precision t.", "Taking the logarithm of the same gives the below expression", "Now comes the little complex part which is to derive the conditional posterior distribution for all the three parameters b,c,t.", "Conditional Posterior Distribution for the intercept_c :", "So, as we can see that this is the process to find out the conditional posterior distribution for the intercept c.", "The code snippet of the above equation", "Similarly, we can find the same for the slope b.", "Conditional Posterior Distribution for the slope_b :", "The code-snippet of the update for the slope is", "Conditional Posterior Distribution for the precision_t :", "The conditional posterior distribution gives b,c will not be like the above two as the prior follows Gamma distribution.", "Now, since we could get the closed distributional form for the parameters, now we can generate from the posterior distribution using MCMC simulations.", "So, first, to run the experiment, I generated a sample data with known slope and intercept coefficients which we can validate from our Bayesian Linear regression. We have assumed for the experiment a=6, b = 2", "Now, after updating the parameters based on the equations and snippets shown above, we run the MCMC simulations for getting the true posterior distribution of the parameters.", "On running the experiment for 15000 iterations, we see the trace plot to validate our hypothesis. There is a concept of a Burn-in period in MCMC simulations where we ignore the first few iterations as it doesn\u2019t replicate samples from the true posterior rather random samples.", "So as you can see, the sample posterior distribution replicated our assumptions and we have much more information about the coefficients and the parameters.", "But it is not always possible to have a closed distributional form of the conditional posterior and hence we have to opt for a proposal distribution with acceptance & rejection sampling using the Metropolis-Hastings algorithm discussed above briefly.", "Note: There are a lot of advantages of Bayesian thinking and reasoning and I will be coming up with subsequent materials on Variational Inference and Bayesian thinking in my upcoming blogs and materials. I will be delving into Bayesian Deep Learning and its advantages. So, keep reading and sharing knowledge.", "P.S \u2014 This talk was part of the Kaggle Days Meetup Delhi-NCR session, please follow them for such materials and sessions. You will also get my video explaining the topic briefly.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Statistical Analyst @WalmartLabs. Masters in Data Science from Indian Statistical Institute. Youngest Speaker@Data Hack Summit Analytics Vidhya\u20192018"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2f50a597eb06&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@souradip_chak?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souradip_chak?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Souradip Chakraborty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F629a094c91b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&user=Souradip+Chakraborty&userId=629a094c91b8&source=post_page-629a094c91b8----2f50a597eb06---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.nature.com/articles/s41598-017-17876-z/figures/5", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=6rWvmwucgZM&t=1850s", "anchor_text": "talks"}, {"url": "https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=ulZW99jsMXY&t=655s", "anchor_text": "Link"}, {"url": "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation", "anchor_text": "Maximum a posteriori"}, {"url": "https://www.researchgate.net/figure/Likelihood-prior-and-posterior-probability-distribution-for-a-parameter-x_fig9_258158633", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=yvWlpwnT1nw", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=8FbqSVFzmoY&t=390s", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=OTO1DygELpY&t=198s", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=OTO1DygELpY&t=198s", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=7QX-yVboLhk", "anchor_text": "Link"}, {"url": "https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html", "anchor_text": "Kieran R Campbell\u2019s"}, {"url": "https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html", "anchor_text": "https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html"}, {"url": "https://www.youtube.com/watch?v=7QX-yVboLhk", "anchor_text": "https://www.youtube.com/watch?v=7QX-yVboLhk"}, {"url": "https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d", "anchor_text": "https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d"}, {"url": "https://www.youtube.com/watch?v=ulZW99jsMXY&t=655s", "anchor_text": "https://www.youtube.com/watch?v=ulZW99jsMXY&t=655s"}, {"url": "https://www.youtube.com/watch?v=OTO1DygELpY&t=198s", "anchor_text": "https://www.youtube.com/watch?v=OTO1DygELpY&t=198s"}, {"url": "https://www.linkedin.com/in/souradip-chakraborty/", "anchor_text": "Souradip Chakraborty - Data Scientist - Walmart Labs India | LinkedInI'm an aspiring Statistician and Machine Learning Scientist. I explore areas in Machine Learning, Deep Learning and\u2026www.linkedin.com"}, {"url": "https://developers.google.com/community/experts/directory/profile/profile-souradip_chakraborty", "anchor_text": "Experts | Google DevelopersMachine Learning I am Souradip Chakraborty, currently employed as a Data Scientist (Research) at Walmart Labs, India\u2026developers.google.com"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----2f50a597eb06---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----2f50a597eb06---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2f50a597eb06---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2f50a597eb06---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2f50a597eb06---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&user=Souradip+Chakraborty&userId=629a094c91b8&source=-----2f50a597eb06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&user=Souradip+Chakraborty&userId=629a094c91b8&source=-----2f50a597eb06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2f50a597eb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2f50a597eb06---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2f50a597eb06--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2f50a597eb06--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2f50a597eb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souradip_chak?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souradip_chak?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Souradip Chakraborty"}, {"url": "https://medium.com/@souradip_chak/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "134 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F629a094c91b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&user=Souradip+Chakraborty&userId=629a094c91b8&source=post_page-629a094c91b8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F629a094c91b8%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06&user=Souradip+Chakraborty&userId=629a094c91b8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}