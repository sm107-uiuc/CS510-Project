{"url": "https://towardsdatascience.com/do-not-use-decision-tree-like-this-369769d6104d", "time": 1683013474.480505, "path": "towardsdatascience.com/do-not-use-decision-tree-like-this-369769d6104d/", "webpage": {"metadata": {"title": "Do Not Use Decision Tree Like This | Towards Data Science", "h1": "Do Not Use Decision Tree Like This", "description": "Information Gain used by ID3 has its limitation. C4.5 with Information Gain Ratio will fix this for constructing better-preformed Decision Tree ML model."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["As one of the most popular classic machine learning algorithm, the Decision Tree is much more intuitive than the others for its explainability. In one of my previous article, I have introduced the basic idea and mechanism of a Decision Tree model. It demonstrated this machine learning model using an algorithm called ID3, which is one of the most classic ones for training a Decision Tree classification model.", "If you are not that familiar with Decision Tree, it is highly recommended to check out the above article before reading into this one.", "To intuitively understand Decision Trees, it is indeed good to start with ID3. However, it is probably not a good idea to use it in practice. In this article, I\u2019ll introduce a commonly used algorithm to build Decision Tree models \u2014 C4.5.", "Before we can demonstrate the major drawbacks of the ID3 algorithm, let\u2019s have a look at what are the major building blocks of it. Basically, the important is the Entropy and Information Gain.", "Here is the formula of Entropy:", "The set \u201cX\u201d is everything in the set of the node, and \u201cx\u1d62\u201d refers to the specific decision of each sample. Therefore, \u201cP(x\u1d62)\u201d is the probability of the set to be made with a certain decision.", "Let\u2019s use the same training dataset as an example. Suppose that we have an internal node in our decision tree with \u201cweather = rainy\u201d. It is can be seen that the final decisions are both \u201cNo\u201d. Then, we can easily calculate the entropy of this node as follows:", "Basically, the probability of being \u201cNo\u201d is 2/2 = 1, whereas the probability of being \u201cYes\u201d is 0/2 = 0.", "On top of the concept of Entropy, we can calculate the Information Gain, which is the basic criterion to decide whether a feature should be used as a node to be split.", "For example, we have three features: \u201cWeather\u201d, \u201cTemperature\u201d and \u201cWind Level\u201d. When we start to build our Decision Tree using ID3, how can we decide which one of them should be used as the root node?", "ID3 makes use Information Gain as the criterion. The rule is that, select the feature with the maximum Information Gain among all of them. Here is the formula of calculating Information Gain:", "Using the same example, when we calculating the Information Gain for \u201cWeather = Rainy\u201d, we also need to take its child nodes\u2019 Entropy into account. Specific derivation and calculating progress can be found in the article that was shared in the introduction.", "The major drawbacks of using Information Gain as the criterion for determining which feature to be used as the root/next node is that it tends to use the feature that has more unique values.", "But why? Let me demonstrate it using an extreme scenario. Let\u2019s say, we have got the training set with one more feature: \u201cDate\u201d.", "You might say that the feature \u201cDate\u201d should not be considered in this case because it intuitively will not be helpful to decide whether we should go out for running or not. Yes, you\u2019re right. However, practically, we may have much more complicated dataset to be classified, and we may not be able to understand all the features. So, we may not always be able to determine whether a feature does make sense or not. In here, I will just use \u201cDate\u201d as an example.", "Now, let\u2019s calculate the Information Gain for \u201cDate\u201d. We can start to calculate the entropy for one of the dates, such as \u201c2020\u201301\u201301\u201d.", "Since there is only 1 row for each date, the final decision must be either \u201cYes\u201d or \u201cNo\u201d. So, the entropy must be 0! In terms of the information theory, it is equivalent to say:", "The date tells us nothing, because the result is just one, which is certain. So, there is no \u201cuncertainty\u201d at all.", "Similarly, for all the other dates, their entropies are 0, too.", "Now, let\u2019s calculate the entropy for the date itself.", "WoW, that is a pretty large number compared to the other features. So, we can calculate the Information Gain of \u201cDate\u201d now.", "Unsurprisingly, the Information Gain of \u201cDate\u201d is the entropy of itself because all its attribute having entropies that are 0.", "If we calculate the Information Gain for the other three features (you can find details in the article that is linked in the introduction), they are:", "Obviously, the Information Gain of Date is overwhelmingly larger than the others. Also, it can be seen that it will be even larger if the training dataset is larger. After that, don\u2019t forget that the feature \u201cDate\u201d actually does not make sense in deciding whether we should go out for running or not, but it is decided as the \u201cBest\u201d one to be the root node.", "Even funnier, after we decided to use \u201cDate\u201d as our root node, we\u2019re done :)", "We end up with a Decision Tree as shown above. This is because the feature \u201cDate\u201d is too good. If we use it as the root node, all its attributes will simply tell us whether we should go out for running or not. It is not necessary to have the other features.", "Yes, you may have a face like this fish at the moment, so do I.", "The easiest fix of the Information Gain limitation that exists in ID3 Algorithm is from another Decision Tree algorithm called C4.5. The basic idea of reducing this issue is to use Information Gain Ratio rather than Information Gain.", "Specifically, Information Gain Ratio is simply adding a penalty on the Information Gain by dividing with the entropy of the parent node.", "Therefore, if we\u2019re using C4.5 rather than ID3, the Information Gain Ratio of the feature \u201cDate\u201d will be as follows.", "Well, it is indeed still the largest one compared to the other features, but don\u2019t forget that we are really using an extreme example where each attribute value of the feature \u201cDate\u201d will have only one row. In practice, Information Gain Ratio will be quite enough to avoid most of the scenarios that Information Gain will cause bias.", "In my opinion, using Information Gain Ratio is the most significant improvement from ID3 to C4.5. Nevertheless, there are more improvements in C4.5 that you should know.", "If you are not familiar with the concept \u201cPruning\u201d of Decision Tree, again, you may need to check out my previous article that is attached in the introduction of this article.", "PEP is another significant improvement in C4.5. Specifically, it will prune the tree in a top-down manner. For every internal node, the algorithm will calculate its error rate. Then, try to prune this branch to compare the error rate before and after the pruning. So, it is decided whether we should reserve this branch.", "C4.5 supports continuous values. So, we are not limited to have \u201cLow\u201d, \u201cMedium\u201d and \u201cHigh\u201d such categorical values. Instead, C4.5 will automatically detect the thresholds of the continuous value that can generate the maximum Information Gain Ratio and then split the node using this threshold.", "In this article, I have illustrated why ID3 is not ideal. The major reason is that the criterion it uses \u2014 Information Gain \u2014 might be significantly bias to those features that have larger numbers of distinct values.", "The solution has been given in another Decision Tree algorithm called C4.5. It evolves the Information Gain to Information Gain Ratio that will reduce the impact of large numbers of distinct values of the attributes.", "Again, if you feel that you need more context and basic knowledge about Decision Trees, please check out my previous article.", "If you feel my articles are helpful, please consider joining Medium Membership to support me and thousands of other writers! (Click the link above)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F369769d6104d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://qiuyujx.medium.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": ""}, {"url": "https://qiuyujx.medium.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Christopher Tao"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb8176fabf308&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&user=Christopher+Tao&userId=b8176fabf308&source=post_page-b8176fabf308----369769d6104d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/geralt-9301/", "anchor_text": "geralt"}, {"url": "https://pixabay.com/photos/book-read-student-students-board-4126483/", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/go-out-for-exercise-or-not-let-data-science-decide-34f8f28ce7b4", "anchor_text": "Go Out For Exercise Or Not? Let Data Science DecideAn introduction to the Decision Tree Machine Learning Algorithmtowardsdatascience.com"}, {"url": "https://pixabay.com/users/aitoff-388338/", "anchor_text": "aitoff"}, {"url": "https://pixabay.com/photos/railway-platform-mind-gap-1758208/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/users/Clker-Free-Vector-Images-3736/", "anchor_text": "Clker-Free-Vector-Images"}, {"url": "https://pixabay.com/vectors/fish-funny-cartoon-odd-surprised-33712/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/users/jarmoluk-143740/", "anchor_text": "jarmoluk"}, {"url": "https://pixabay.com/photos/electrician-electric-electricity-1080554/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/users/silviarita-3142410/", "anchor_text": "silviarita"}, {"url": "https://pixabay.com/photos/bake-motto-world-improve-sweetness-1838364/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/users/Bessi-909086/", "anchor_text": "Bessi"}, {"url": "https://pixabay.com/photos/tree-lake-reflection-water-calm-838667/", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/go-out-for-exercise-or-not-let-data-science-decide-34f8f28ce7b4", "anchor_text": "Go Out For Exercise Or Not? Let Data Science DecideAn introduction to the Decision Tree Machine Learning Algorithmtowardsdatascience.com"}, {"url": "https://medium.com/@qiuyujx/membership", "anchor_text": "Join Medium with my referral link \u2014 Christopher TaoAs a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story\u2026medium.com"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----369769d6104d---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/data-science?source=post_page-----369769d6104d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----369769d6104d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----369769d6104d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----369769d6104d---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&user=Christopher+Tao&userId=b8176fabf308&source=-----369769d6104d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&user=Christopher+Tao&userId=b8176fabf308&source=-----369769d6104d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F369769d6104d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----369769d6104d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----369769d6104d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----369769d6104d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----369769d6104d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----369769d6104d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----369769d6104d--------------------------------", "anchor_text": ""}, {"url": "https://qiuyujx.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://qiuyujx.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christopher Tao"}, {"url": "https://qiuyujx.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8.7K Followers"}, {"url": "https://www.linkedin.com/in/christopher-tao-5717a274/", "anchor_text": "https://www.linkedin.com/in/christopher-tao-5717a274/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb8176fabf308&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&user=Christopher+Tao&userId=b8176fabf308&source=post_page-b8176fabf308--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d2af94a683f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-not-use-decision-tree-like-this-369769d6104d&newsletterV3=b8176fabf308&newsletterV3Id=1d2af94a683f&user=Christopher+Tao&userId=b8176fabf308&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}