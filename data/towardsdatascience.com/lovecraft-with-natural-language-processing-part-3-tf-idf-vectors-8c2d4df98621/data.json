{"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621", "time": 1683009346.541861, "path": "towardsdatascience.com/lovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621/", "webpage": {"metadata": {"title": "Lovecraft with NLP: TF-IDF and K-Means Clustering | Towards Data Science", "h1": "Lovecraft with Natural Language Processing \u2014 Part 3: TF-IDF Vectors", "description": "This is the third part of my Lovecraft NLP series. In the previous posts, I was discussing rule-based sentiment analysis and word counts with tokenisation. Our approach has been quite simplistic so\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524", "anchor_text": "rule-based sentiment analysis", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690", "anchor_text": "word counts with tokenisation", "paragraph_index": 0}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidfVectorizer", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690", "anchor_text": "previous post", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "documentation", "paragraph_index": 10}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction", "anchor_text": "smoothing effect", "paragraph_index": 17}, {"url": "https://stats.stackexchange.com/questions/299013/cosine-distance-as-similarity-measure-in-kmeans", "anchor_text": "discussion", "paragraph_index": 28}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html", "anchor_text": "k-means clusters from sklearn", "paragraph_index": 37}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/tfidf_clustering.csv", "anchor_text": "CSV file", "paragraph_index": 40}], "all_paragraphs": ["This is the third part of my Lovecraft NLP series. In the previous posts, I was discussing rule-based sentiment analysis and word counts with tokenisation.", "Our approach has been quite simplistic so far in the series, we were basically breaking down the text into words, and counted them in some way. The next step in the world of NLP is going to be looking at TF-IDF vectors, which stands for Term Frequency \u2014 Inverse Document Frequency. This is a less binary approach, used in search engines and spam filters, with which we will decode the words into continuous numbers rather than integers.", "First, we are going to have a look at the theory by defining the relevant terms. Then we will see how one can use the spaCy and scikit-learn libraries to calculate the TF-IDF vectors for the 63 Lovecraft stories we have in our corpus. Finally, we will try to create meaningful groups by applying k-means clustering on the vector representations.", "Before we start, I would like to manage the expectations of the outcome: the groups that we will get in the end are not going to be that meaningful. In the previous two posts, I was quite satisfied with the results of the simple NLP analyses, and it was very easy to draw meaningful conclusions regarding Lovecraft\u2019s work. That is not the case here, there are some patterns, but the stories are not really assigned to nice homogenous groups. At least I couldn\u2019t find a good overall pattern, but maybe you will, the end results are available in a CSV file.", "Let\u2019s start with a quick summary of the important terminology.", "In this section, we are going to have a look at how to create a TF-IDF vector representation of a document. We are going to build on the TfidfVectorizer class from sklearn, and use a tokenizer that we define using spaCy. (If you are new to the topic, I recommend you have a look at my previous post, I am going to assume that you know how text tokenisation works in general.)", "Let\u2019s have a look at the tokenizer first. We basically need a function that takes a string as input and returns a list of tokens as output. You are free to shape this function however you want, I followed a quite straightforward approach, and this is what my spacy_tokenizer function will do:", "We set up the nlp object that is going to do the heavy lifting (you need a spaCy language model installed for this, once again, refer to my previous post if you are unsure how to do that):", "And then you can define the tokenizer function:", "This is the corpus I am going to use as an example:", "Now that we have our tokenizer and corpus to work on, we can set up a TfidfVectorizer. There are a number of parameters you can play with, check out the documentation for more, we are only using these two parameters:", "The tfidf_vectorizer object works similarly to a standard sklearn machine learning model. If you call the fit method, it will learn the vocabulary and the IDF part of the formula, and the transform method will transform the corpus into a sparse matrix format containing the TF-IDF values. These two steps are combined in the fit_transform method:", "If we call result now, this is what we get:", "It\u2019s a sparse matrix with 3 documents and 5 terms, out of those 3*5 = 15 possible numbers there are 8 non-zero TF-IDF values. We can check which terms are actually considered from the sentences with the get_feature_names method:", "The sparse matrix format is an efficient way to store this information, but you might want to convert it to a more readable, dense matrix format using the todense method. To create a pandas DataFrame from the results, you can use the following code:", "And finally we can have a look at the TF-IDF vectors of the three sentences:", "The word \u201cmonster\u201d appears in all three sentences, so the values in the \u201cmonster\u201d column are the lowest in all the rows, meaning it\u2019s not an important term in differentiating between the sentences.", "Let\u2019s check how the numbers are calculated in the first sentence. The TF of both \u201cbad\u201d and \u201cmonster\u201d is going to be 0.5, considering there are only two words that are not ignored in the sentence. To calculate the IDF values, we need to count how many documents contain the specific words. \u201cbad\u201d appears in 2 out of 3, \u201cmonster\u201d appears in all 3. The IDF score of \u201cbad\u201d (with sklearn\u2019s smoothing effect) is going to be:", "So the unadjusted TF-IDF scores are:", "However, the length of this vector is not yet 1:", "is equal to 0.8151878801407859, so we need to divide both scores with the length, and that is how we will get the values in the DataFrame above:", "All right, so we have the TF-IDF vectors, all that\u2019s left is to calculate the differences! As we mentioned above, because we have normalised vectors, the cosine similarity is equal to the dot product, also known as the linear kernel. We import the method from sklearn, then apply on the first sentence (index 0):", "The result will be the following array, which contains the cosine similarities:", "The first number, 1, is expected, after all, we want a sentence to be perfectly similar to itself. Let\u2019s see how the last number is calculated. You simply take the words that appear in both sentences, these are \u201cbad\u201d and \u201cmonster\u201d, take the pairwise product, and sum it up:", "We can see that the last sentence is closer to the first one than the second.", "We can calculate all the cosine similarities and collect them in a pandas DataFrame:", "Once again, we can see that sentences 0 and 2 are close to each other.", "The final application we are going to check out on the small example corpus is k-means clustering. We are going to use sklearn\u2019s KMeans class, the great thing about which is that it enables you to put in a sparse matrix as an input, so we don\u2019t have to convert it to a traditional dense matrix format.", "Now, the k-means algorithm has no cosine distance option, we will use the common Euclidean distance between vectors. However, once again, because we normalised the vectors, this should result in the same order of similarities. See this discussion for more background.", "Fitting a k-means model on our original result TF-IDF matrix:", "And if we call the labels_ attribute:", "we can see the groups it created:", "So once again, sentences 0 and 2 are put in one group.", "That concludes the practical background we will need in order to analyse the Lovecraft stories in the next section.", "In this section, we are going to repeat the steps from the previous one, using the Lovecraft stories corpus. As mentioned in the introduction, sadly, the results are not as great as one would hope for, but here we go!", "Setting up the tfidf_vectorizer is really similar to what we did before, but now we are using the \u2018filename\u2019 option. Assuming you have a list of file paths leading to TXT files containing the individual stories:", "Just like in the previous section, result is going to be a sparse matrix, the dimensions are 63 x 17,813. That means we have 63 stories, and 17,813 unique lemmas.", "Next, we want to create clusters. I ran k-means clusters from sklearn between k = 2 and 10, and then collected the results into a pandas DataFrame. In the DataFrame, each story will be assigned to a row, and the columns will contain the label assigned to that story in each clustering structure. For example, in the 4means_label, we will be able to see which group the stories would be assigned to if we wanted to separate them into 4 different groups, and the labels are going to run between 0 and 3.", "To run the different k-means clustering:", "and then we organise the labels into a cluster_df DataFrame:", "You can download the results in a CSV file in my GitHub repository.", "If you are familiar with Lovecraft\u2019s writings, see if you can find any nice patterns in any of the groupings. The best I could come up was partial results, for example, in the k = 5 case, the dream sequence stories are very nicely separated in the group labelled as 2, but I could not come up with a comprehensive reason to the clustering as a whole.", "I think this lack of nice results is most likely caused by the lack of underlying structure between the stories. Sure, there are some clearly distinct stories, like the aforementioned dream sequence series, but you can\u2019t really divide the whole corpus by such clear lines.", "Even though we did not manage to artificially create story groups that we could justify afterwards, the structure and the code is there, and I hope you found it useful. One of the frequent usages of TF-IDF is search engines, so if you include a query as the 64th text input, you can calculate which documents are the closest to it, and which documents will contain an answer to your query with the highest probability.", "There are a couple of further analysis one can do with TF-IDF and this corpus. I haven\u2019t considered n-grams when building the vectors, and used the default option of considering each word individually. With n-grams, you can try to find frequently used expressions between the documents.", "Another potential enhancement is to list the documents closest to each individual document, rather than arranging them in clusters. We might be able to see better connections that way.", "In the next post of the series, we are going to have a look at how to reduce the dimensions of TF-IDF vectors and convert them into latent factor vectors.", "Hobson, L. & Cole, H. & Hannes, H. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications, 2019.", "The Complete Works of H. P. Lovecraft:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c2d4df98621&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://matepocs.medium.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Mate Pocs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F686b78ddcf4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&user=Mate+Pocs&userId=686b78ddcf4b&source=post_page-686b78ddcf4b----8c2d4df98621---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1208296", "anchor_text": "F-P"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1208296", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524", "anchor_text": "rule-based sentiment analysis"}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690", "anchor_text": "word counts with tokenisation"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency", "anchor_text": "Term Frequency (TF)"}, {"url": "https://en.wikipedia.org/wiki/Zipf's_law", "anchor_text": "Zipf\u2019s law"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2", "anchor_text": "Inverse Document Frequency (IDF)"}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction", "anchor_text": "documentation"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency%E2%80%93Inverse_document_frequency", "anchor_text": "TF-IDF value"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "cosine similarity"}, {"url": "https://en.wikipedia.org/wiki/Dot_product", "anchor_text": "dot product"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Wikipedia article"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidfVectorizer"}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690", "anchor_text": "previous post"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "documentation"}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction", "anchor_text": "smoothing effect"}, {"url": "https://stats.stackexchange.com/questions/299013/cosine-distance-as-similarity-measure-in-kmeans", "anchor_text": "discussion"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html", "anchor_text": "k-means clusters from sklearn"}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/tfidf_clustering.csv", "anchor_text": "CSV file"}, {"url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "anchor_text": "6.2. Feature extraction - scikit-learn 0.23.1 documentationThe class is a high-speed, low-memory vectorizer that uses a technique known as feature hashing, or the \"hashing\u2026scikit-learn.org"}, {"url": "https://spacy.io/usage/linguistic-features", "anchor_text": "Linguistic Features \u00b7 spaCy Usage DocumentationProcessing raw text intelligently is difficult: most words are rare, and it's common for words that look completely\u2026spacy.io"}, {"url": "https://arkhamarchivist.com/free-complete-lovecraft-ebook-nook-kindle/", "anchor_text": "https://arkhamarchivist.com/free-complete-lovecraft-ebook-nook-kindle/"}, {"url": "https://medium.com/tag/python?source=post_page-----8c2d4df98621---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8c2d4df98621---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/tfidf-vectorizer?source=post_page-----8c2d4df98621---------------tfidf_vectorizer-----------------", "anchor_text": "Tfidf Vectorizer"}, {"url": "https://medium.com/tag/lovecraft?source=post_page-----8c2d4df98621---------------lovecraft-----------------", "anchor_text": "Lovecraft"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8c2d4df98621---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&user=Mate+Pocs&userId=686b78ddcf4b&source=-----8c2d4df98621---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&user=Mate+Pocs&userId=686b78ddcf4b&source=-----8c2d4df98621---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c2d4df98621&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c2d4df98621---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c2d4df98621--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c2d4df98621--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c2d4df98621--------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mate Pocs"}, {"url": "https://matepocs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "460 Followers"}, {"url": "http://linkedin.com/in/matepocs", "anchor_text": "linkedin.com/in/matepocs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F686b78ddcf4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&user=Mate+Pocs&userId=686b78ddcf4b&source=post_page-686b78ddcf4b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fea5ecc8b4148&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621&newsletterV3=686b78ddcf4b&newsletterV3Id=ea5ecc8b4148&user=Mate+Pocs&userId=686b78ddcf4b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}