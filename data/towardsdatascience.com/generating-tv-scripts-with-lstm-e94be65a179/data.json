{"url": "https://towardsdatascience.com/generating-tv-scripts-with-lstm-e94be65a179", "time": 1683012882.449763, "path": "towardsdatascience.com/generating-tv-scripts-with-lstm-e94be65a179/", "webpage": {"metadata": {"title": "Generating TV Scripts with LSTM. A beginner project with LSTMs | by Ankita Das | Towards Data Science", "h1": "Generating TV Scripts with LSTM", "description": "I\u2019ll be explaining the major and the very basic concepts that will be required while doing the project. The sections which I\u2019ve covered include: How the model architecture works with the batched\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html", "anchor_text": "This", "paragraph_index": 12}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM", "anchor_text": "torch.nn.LSTM", "paragraph_index": 35}, {"url": "https://twitter.com/Ano_myself", "anchor_text": "https://twitter.com/Ano_myself", "paragraph_index": 48}, {"url": "http://www.linkedin.com/in/ankita-das-2010/", "anchor_text": "www.linkedin.com/in/ankita-das-2010/", "paragraph_index": 48}], "all_paragraphs": ["Hello friends, I just completed this project as a part of the Deep Learning Nanodegree at Udacity.", "The project is about predicting the next word in the script, given the previous context using LSTM.", "I\u2019ll be explaining the major and the very basic concepts that will be required while doing the project. The sections which I\u2019ve covered include:", "How our data can be transformed into a form that the model can work with?", "How the model architecture works with the batched data, passing it from one layer to the other and how it finally leads to predicting the next word in the sequence.", "This is a fundamental project done with LSTM, anyone having basic knowledge about neural networks and RNNs can easily understand this. I have tried breaking the project down to the very basic details as simply as possible.", "I have put a good amount of focus on the model architecture, the input shapes, output shapes, and parameters that each layer considers. I have compiled and explained what each of these input/output shapes mean and why they are shaped in a particular way, for you to visualize better.", "Note: The project has been coded in Pytorch. However, there is not much difference in the parameters that the LSTM model layers consider. So, anyone trying to do it in Keras too can refer to my explanation.", "The theoretical details of what an LSTM is, or why it works though I\u2019ve touched a part of why it works better than Neural Networks for this case.I have attached a link to the main project, but I haven\u2019t explained the whole of it line by line.", "However, I have covered the parts that I felt would be sufficient for understanding the main working so that you can generalize the concept to use it for other similar projects.", "We have the text, we first batch the data in a form that it can be fed into the LSTM.", "An important part of which is Text preprocessing, we have applied a few very basic necessary but not sufficient preprocessing to the text:", "Some other text processing techniques might be Stemming, lemmatization. This is a good read on text processing.", "After having preprocessed the text and converted it to integers, the text data is batched in a way that LSTM can consider it as an input.", "Now before moving further, a very basic yet important question:", "The two most important terms in RNN/ LSTMs are recurrent units and hidden units. Both of which play a part in considering the context of the text. For example, if you join a discussion in between it is difficult for you to converse, you need to know the context of the discussion before you say something. That is made possible with the help of hidden units, the hidden units carry forward the information from the previous context, so the LSTM/RNN unit considers 2 inputs: the input word and the hidden unit.", "The normal neural network could have worked as well, but it will work in a way without considering the previous context, just giving the probability of the occurrence of a word as the next word, given the previous word.", "In the image above \u201ch\u201d is the hidden unit, \u201co\u201d denotes output from each LSTM unit, the final output is matched for similarity with the target.", "Let\u2019s consider a piece of text, this is an extract from quora:", "\u201c \u201cWhat are you thinking?\u201d asked the boss. \u201cWe didn\u2019t ask anyone any question here. By asking a few questions we won\u2019t be able to assess the skills of anyone. So our test was to assess the attitude of the person. We kept certain tests based on the behavior of the candidates and we observed everyone through CCTV.\u201d", "After selecting a suitable sequence length and batch size, 4 & 3 here respectively, an input to the LSTM cells looks like as follows (I am ignoring punctuations here):", "If instead, we would have used a normal Neural Network, the batches would have looked something similar to this (batch size: 3):", "So that it\u2019s quite clear that the Neural Network does not consider the previous context.", "Coming on to how these batches are processed by the LSTMs.", "Below is a picture of the architecture of our model with the parameters", "Please refer to it for better understanding of the architecture, I have used smaller numbers for the ease of representation.", "So if these integers (words mapped to integers in the vocab_to_int dictionary) are passed into the LSTM unit it might uncover a biased meaning thinking of these integers as some weights, in order to avoid this problem we sometimes one-hot encode these integers. But, when the vocabulary dictionary is large, the encodings take up a lot of unnecessary space.", "The vectors occupy a space of 3*3 cell size i.e. 9", "Now imagine this for a vocabulary size of 10000. That would mean a 10\u00b9\u2070 cell size of space.", "Therefore comes the concept of Embedding Layer.", "Instead of replacing these words by one-hot encodings, this time we replace it with some fixed-length vectors filled with a random choice of numbers. This matrix of values gets trained along with the model over time and learns a close vector representation of the words, which can help add meaning to them and help in increasing the overall accuracy.", "The torch.nn.Embedding layer takes in the parameters:", "The input to this layer is shaped as (batch_size,seq_length) and output from this layer is of shape (batch_size,seq_length,embedding_dim).", "For a vocab size of 25000, the embedding vector of 300 works well which is considerably smaller than each vector size of 25000 in case of one-hot encoded vectors.", "Some pre-trained word representations models like fastText, GloVe are available and can be directly used instead of generating these representations with the Embedding Layer.", "The torch.nn.LSTM expects the parameters input_size,hidden_dim, num_layers. Input_size & hidden_dim are the number of features in the input and hidden state respectively.", "Note: For someone who is familiar with CNN's can relate to it as input_size as the in_channels and hidden_dim as out_channels of the CNN layer.", "Since we have used an embedding layer, the embedding vector length is the number of input features.Therefore, input_size = embedding_dim.The parameters hidden_dim and num_layers(number of layers of the LSTM, generally has a value 1 or 2) can be chosen arbitrarily.", "The nn.LSTM layer returns an output and a hidden state, the output has the shape (batch_size,sequence_length,hidden_dim).", "This output is reshaped to (batch_size*sequence_length , hidden_dim) and passed on to the Linear Layer with in_features = hidden_dim and out_features = output_size .", "In our case, output_size = vocabulary size, i.e. the number of output nodes is equal to the total number of words present in the vocabulary, so that the network can choose the most probable next word from the whole vocabulary of words.", "The output from the Linear layer is shaped as (batch_size*sequence_length,output_size) which is further reshaped to have a shape (batch_size,sequence_length,output_size) and though we train the model on the whole corpus for predicting the next word for every word of the sequence, we need only the next word after the last word of the sequence, so the returned output from our model considers just that.", "Thus the final output layer is shaped as (batch_size,output_size).", "The output from our model gives us the most probable next word for each sequence for the given batch. \ud83d\udc81", "The full code for this project can be found here:", "I have tried to cover all the important and necessary concepts for doing this project, yet if you face difficulty anywhere and if you feel I have missed a point, do let me know in the comments below.", "I\u2019ll be eagerly waiting to see you\u2019ll complete this project. \ud83d\ude4c", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write to prove that everything can be simplified. :) Twitter : https://twitter.com/Ano_myself | LinkedIN: www.linkedin.com/in/ankita-das-2010/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe94be65a179&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e94be65a179--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ad2013?source=post_page-----e94be65a179--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ad2013?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Ankita Das"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F909b99cc1cf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&user=Ankita+Das&userId=909b99cc1cf3&source=post_page-909b99cc1cf3----e94be65a179---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@wilhelmgunkel?utm_source=medium&utm_medium=referral", "anchor_text": "Wilhelm Gunkel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html", "anchor_text": "This"}, {"url": "https://unsplash.com/@jannerboy62?utm_source=medium&utm_medium=referral", "anchor_text": "Nick Fewings"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM", "anchor_text": "torch.nn.LSTM"}, {"url": "https://github.com/Ankita-Das/Deep-Learning-Nanodegree/blob/master/TV_script_generation/dlnd_tv_script_generation_copy.ipynb", "anchor_text": "Ankita-Das/Deep-Learning-NanodegreePermalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage\u2026github.com"}, {"url": "https://unsplash.com/@jupp?utm_source=medium&utm_medium=referral", "anchor_text": "Jonathan Kemper"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/lstm?source=post_page-----e94be65a179---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/nlp?source=post_page-----e94be65a179---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/basics?source=post_page-----e94be65a179---------------basics-----------------", "anchor_text": "Basics"}, {"url": "https://medium.com/tag/projects?source=post_page-----e94be65a179---------------projects-----------------", "anchor_text": "Projects"}, {"url": "https://medium.com/tag/udacity-nanodegree?source=post_page-----e94be65a179---------------udacity_nanodegree-----------------", "anchor_text": "Udacity Nanodegree"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&user=Ankita+Das&userId=909b99cc1cf3&source=-----e94be65a179---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&user=Ankita+Das&userId=909b99cc1cf3&source=-----e94be65a179---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e94be65a179--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe94be65a179&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e94be65a179---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e94be65a179--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e94be65a179--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e94be65a179--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e94be65a179--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e94be65a179--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ad2013?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ad2013?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ankita Das"}, {"url": "https://medium.com/@ad2013/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "16 Followers"}, {"url": "https://twitter.com/Ano_myself", "anchor_text": "https://twitter.com/Ano_myself"}, {"url": "http://www.linkedin.com/in/ankita-das-2010/", "anchor_text": "www.linkedin.com/in/ankita-das-2010/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F909b99cc1cf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&user=Ankita+Das&userId=909b99cc1cf3&source=post_page-909b99cc1cf3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F909b99cc1cf3%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-tv-scripts-with-lstm-e94be65a179&user=Ankita+Das&userId=909b99cc1cf3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}