{"url": "https://towardsdatascience.com/tcav-interpretability-beyond-feature-attribution-79b4d3610b4d", "time": 1682996132.395869, "path": "towardsdatascience.com/tcav-interpretability-beyond-feature-attribution-79b4d3610b4d/", "webpage": {"metadata": {"title": "TCAV: Interpretability Beyond Feature Attribution | by Parul Pandey | Towards Data Science", "h1": "TCAV: Interpretability Beyond Feature Attribution", "description": "The emphasis today is slowly moving towards model interpretability rather than model predictions alone. However, the real essence of Interpretability should be to make Machine learning models more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=lyRPyRKHO8M&t=3408s", "anchor_text": "keynote address", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "original research paper", "paragraph_index": 2}, {"url": "https://github.com/tensorflow/tcav", "anchor_text": "Github Repository", "paragraph_index": 2}], "all_paragraphs": ["It\u2019s not enough to know if a model works, we need to know how it works: Sundar Pichai", "The emphasis today is slowly moving towards model interpretability rather than model predictions alone. However, the real essence of Interpretability should be to make Machine learning models more understandable for humans and especially for those who don\u2019t know much about machine learning. Machine Learning is a potent tool, and with such power comes a responsibility to ensure that values like fairness are well reflected within the models. It is also essential to ensure that the AI models do not reinforce the bias that exists in the real world. To tackle such issues, Google AI Researchers are working on a solution called TCAV (Testing with Concept Activation Vectors) to understand what signals the neural network models use for their prediction.", "In his keynote address at Google I/O 2019, Sundar Pichai talked about how they are trying to build a more helpful Google for everyone, including building AI for everyone. He reiterated that Bias in Machine Learning is a matter of concern, and the stakes are even high when it comes to AI. To make AI more responsible and transparent, he discussed the TCAV methodology, and through this article, I\u2019ll like to give an overview of the same and how it intends to address the issue of Bias and Fairness. The article will be light on math, so if you want a deeper look, you can read the original research paper or visit TCAV\u2019s Github Repository.", "In the ML realm, there are mainly three kinds of Interpretability techniques:", "Mostly, you are given a model created by years of engineering and expertise, and you cannot change its architecture, nor can you retrain it. So how do you go about interpreting a model about which you have no clue? TCAV is a technique that aims to handle such scenarios.", "Most Machine Learning models are designed to operate on low-level features like edges and lines in a picture or, say, the color of a single pixel. This is very different from the high-level concepts more familiar to humans, like stripes in a zebra. For instance, if you have an image, every pixel of that image is an input feature. Although it is possible to look at every single pixel and infer their numerical values, they make no sense to humans. We won\u2019t say that the 5th pixel of this image has a value of 28; as humans, we always say that there is a blue river in the picture. TCAV tries to overcome this issue.", "Also, typical interpretability methods require you to have one particular image that you are interested in understanding. TCAV explains that is generally true for a class of interest beyond one image (global explanation).", "Let\u2019s say we have a model that is trained to detect zebras from images. We would want to know which variables have played a role in deciding whether the image was a zebra or not. TCAV can help us understand if the concept of stripes was essential to the model\u2019s prediction, which is actually yes in this case.", "Similarly, consider a classifier trained on images of doctors. If the training data consisted mainly of males wearing white coats and stethoscopes, the model would assume that being male with a white coat was an important factor in being a doctor. How would this help us? This would bring out the bias in the training data, which has fewer images of females, and we could easily rectify that.", "Testing with Concept Activation Vectors (TCAV) is a new interpretability initiative from the Google AI Team. The Concept Activation Vectors (CAVs) provide an interpretation of a neural net\u2019s internal state in terms of human-friendly concepts. TCAV uses directional derivatives to quantify the degree to which a user-defined idea is vital to a classification result\u2013for example, how sensitive a prediction of \u201czebra\u201d is to the presence of stripes.", "The Team pioneered by Been Kim and Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres aims to make humans empowered by machine learning overwhelmed by it. Here is what Been thinks about interpretability.", "TCAV essentially learns \u2018concepts\u2019 from examples. For instance, TCAV needs a couple of examples of \u2018female\u2019, and something \u2018not female\u2019 to learn a \u201cgender\u201d concept. The goal of TCAV is to determine how much a concept (e.g., gender, race) was necessary for a prediction in a trained model even if the concept was not part of the training.", "Continuing with the \u2018Zebra Classifier\u2019, consider that the neural network consists of inputs x \u2208 R^ n and a feedforward layer l with m neurons, such that input inference and its layer l activations can be seen as a function :", "For a given set of examples that represent this concept(e.g., stripes)(a) or an independent data set with the concept labeled (b) and a trained network(c), TCAV can quantify the model\u2019s sensitivity to the concept for that class.", "We need to find a vector in the space of activations of layer l that represents this concept. CAVs are learned by training a linear classifier to distinguish between the activations produced by a concept\u2019s examples and examples in any layer (d). We then define a \u201cconcept activation vector\u201d (or CAV) as the normal to a hyperplane separating examples without a concept and examples with a concept in the model\u2019s activations", "For the class of interest (zebras), TCAV uses the directional derivative SC,k,l(x) to quantify conceptual sensitivity(e). This SC,k,l(x) can quantitatively measure the sensitivity of model predictions concerning concepts at any model layer", "Here is a step by step guide on using TCAV in your workflow:", "TCAV was used in two widely used image prediction models, i.e., InceptionV3 and GoogleNet.", "While the results show the importance of the red concept for fire engines, some results also confirmed the inherent bias in the models towards gender and race, despite not being explicitly trained with these categories. For example:", "TCAV is a step toward creating a human-friendly linear interpretation of the internal state of a deep learning model so that questions about model decisions may be answered in terms of natural high-level concepts.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Data Scientist @H2O.ai | Author of Machine Learning for High-Risk Applications"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F79b4d3610b4d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pandeyparul.medium.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28----79b4d3610b4d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html", "anchor_text": "How convolutional neural networks see the world"}, {"url": "https://www.youtube.com/watch?v=lyRPyRKHO8M&t=3408s", "anchor_text": "Google Keynote (Google I/O\u201919)"}, {"url": "https://www.youtube.com/watch?v=lyRPyRKHO8M&t=3408s", "anchor_text": "keynote address"}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "original research paper"}, {"url": "https://github.com/tensorflow/tcav", "anchor_text": "Github Repository"}, {"url": "https://www.youtube.com/watch?v=lyRPyRKHO8M&t=3408s", "anchor_text": "TCAV shows that stripes are a critical \u2018concept\u2019 when deciding if an image contains a zebra or not"}, {"url": "https://www.youtube.com/watch?v=lyRPyRKHO8M&t=3408s", "anchor_text": "TCAV shows that being male is an important \u2018concept\u2019 when deciding if an image belongs to a doctor or no"}, {"url": "https://youtu.be/8Bi-EhFPSLk", "anchor_text": "Source: Quanta Magazine"}, {"url": "https://arxiv.org/pdf/1711.11279.pdf", "anchor_text": "Testing with Concept Activation Vectors"}, {"url": "https://github.com/tensorflow/tcav/blob/master/Run%20TCAV.ipynb", "anchor_text": "TensorFlow/tcavContribute to TensorFlow/tcav development by creating an account on GitHub.github.com"}, {"url": "https://beenkim.github.io/slides/TCAV_ICML_pdf.pdf", "anchor_text": "Source"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----79b4d3610b4d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----79b4d3610b4d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/bias?source=post_page-----79b4d3610b4d---------------bias-----------------", "anchor_text": "Bias"}, {"url": "https://medium.com/tag/google-ai?source=post_page-----79b4d3610b4d---------------google_ai-----------------", "anchor_text": "Google Ai"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----79b4d3610b4d---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&user=Parul+Pandey&userId=7053de462a28&source=-----79b4d3610b4d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&user=Parul+Pandey&userId=7053de462a28&source=-----79b4d3610b4d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F79b4d3610b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----79b4d3610b4d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----79b4d3610b4d--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://pandeyparul.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5be6ccf82bc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftcav-interpretability-beyond-feature-attribution-79b4d3610b4d&newsletterV3=7053de462a28&newsletterV3Id=5be6ccf82bc8&user=Parul+Pandey&userId=7053de462a28&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}