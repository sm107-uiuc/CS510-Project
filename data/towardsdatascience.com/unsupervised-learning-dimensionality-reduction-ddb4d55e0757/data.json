{"url": "https://towardsdatascience.com/unsupervised-learning-dimensionality-reduction-ddb4d55e0757", "time": 1682995830.128139, "path": "towardsdatascience.com/unsupervised-learning-dimensionality-reduction-ddb4d55e0757/", "webpage": {"metadata": {"title": "Unsupervised Learning: Dimensionality Reduction | by Victor Roman | Towards Data Science", "h1": "Unsupervised Learning: Dimensionality Reduction", "description": "As stated in previous articles, unsupervised learning refers to a kind of machine learning algorithms and techniques that are trained and fed with unlabeled data. In other words, we do not know the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e", "anchor_text": "previous articles", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-classification-and-main-algorithms-c16b06806cd3", "anchor_text": "previous article", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/classification-project-finding-donors-853db66fbb8c", "anchor_text": "in a previous article", "paragraph_index": 14}, {"url": "https://medium.com/@rromanss23", "anchor_text": "here", "paragraph_index": 48}], "all_paragraphs": ["As stated in previous articles, unsupervised learning refers to a kind of machine learning algorithms and techniques that are trained and fed with unlabeled data. In other words, we do not know the correct solutions or the values of the target variable beforehand.", "The main goal of these types of algorithms is to study the intrinsic and hidden structure of the data in order to get meaningful insights, segment the datasets in similar groups or to simplify them.", "Throughout this article, we are going to explore some of the algorithms and techniques most commonly used to reduce the dimensionality of datasets.", "Dimensionality is the number of variables, characteristics or features present in the dataset. This dimensions are represented as columns, and the goal is to reduce the number of them.", "In most cases, those columns are correlated and, therefore, there is some information that is redundant which increase the dataset\u2019s noise. This redundant information impacts negatively in Machine Learning model\u2019s training and performance and that is why using dimensionality reduction methods becomes of paramount importance. It is a very useful way to reduce model\u2019s complexity and avoid overfitting.", "There are two main categories of dimensionality reduction:", "Feature Selection stands for a family of greedy algorithms used to reduce the dimensional feature space of a given dataset. The aim is to obtain a model capable of automatically selecting the subset of features most relevant to problem faced.", "Greedy algorithms make locally optimal choices at each stage of a combinational search and normally yield suboptimal solutions. This is where they differenciate to exhaustive search algorithms, which evaluate the whole set of combinations and yield the overall optimal solution. The benefits of greedy algorithms is that they are computationally much more efficient, in expenses of precision, but, most of the times they yield sufficiently good solutions.", "In that way, we will improve the computation efficiency of the training process, reduce dataset\u2019s noise, avoid overfitting and reduce model\u2019s complexity.", "We will study two of the main techniques of feature selection:", "The idea behind the SBS algorithm is the following:", "We have studied Random Forests algorithms in a previous article. These are a kind of ensemble algorithms that combine a set of weak Decision Tree models in order to build a more robust and precise one.", "Using Random Forest, we can assess the importance of each feature, how much they are contributing to the model\u2019s information, by calculating the averaged impurity decrease computed from all decision trees present in the forest. This will be done without making any assumptions of whether the data is linearly separable or not.", "This is a quite simple method of obtaining feature importance, as we are going to use the random forest implementation of the scikit-learn library, which already collects feature importance by using the feature_importance_ attribute after fitting a RandomForesClassifier.", "We already have studied an example of this in a previous article where we wanted to identify those features that most strongly help to predict wheter a specific individual made at most or more than $50.000.", "Feature extraction is also used to reduce the number of features of a certain dataset, but in contrast to feature selection, the output features will not be the same as the originals.", "When using feature extraction, we project the data into a new feature space, so the new features will be combinations of the original features, compressed in a way that they will retain the most relevant information.", "Some of the most used algorithms for unsupervised feature extraction are:", "In order to understand how the PCA algorithm works, let\u2019s consider the following distribution of data:", "PCA finds a new quadrant system (y\u2019 and x\u2019 axis) that it is obtained from the old one by translation and rotation only.", "Basically, PCA finds the directions of maximum variance in high-dimensional data and projects this data into a new subspace with the same or fewer dimensions than the original one.", "These new directions that contain the maximum variance are called Principal Components, they have the constraint of being orthogonall to each other.", "Data points will be projected in the direction of maximal variance to form a new axis. The further the points are to the axis, the biggest the information loss.", "It is a mathematical fact that when we project points onto a direction of maximal variance, it minimized the distance from old and higher-dimensional data points to its new transformed value. In other words, it minimizes the information loss.", "Core ideas of PCA for Feature Transformation", "As a summary, what PCA do is to combine every feature and to extract the top more relevant ones automatically. It is a systemized way to transform input features into principal components, and uses them as the new features.", "Principal components are directions in data that maximize variance (minimize information loss) when projecting or compressing them down.", "The more the variance of data along a Principal Component, the more information that direction contains and the higher that principal component is ranked.", "The number of Principal Components will be less or equall the number of input features.", "Let\u2019s see an example of how this algorithm is implemented in the Scikit-Learn library.", "Random projection is a powerful dimensionality reduction method that is computationally more efficient tha PCA. It is commonly used in datasets that have too many dimensionsfor PCA to be directly computed.", "Like PCA, it takes a dataset with d dimensions and n samples and produces a transformation of the dataset with k dimensions, being k much smaller than d (k << d).", "The basic premise is to reduce the number of dimension of our dataset by multiplying it to a random matrix. Which will project the dataset into a new subspace of features.", "Theoreticall Approach: Johnson \u2014 Lindenstrauss Lemma", "A datset with N samples in high-dimensional Euclidean space can be mapped down to a space in much lower dimension in a way that preserves the distance to the points to a large degree.", "In other words, the distance squared between two points in the dataset is calculated and the distance of those two points in the new dataset must be either:", "Being u and v the considered datapoints.", "The epsilon value \u03b5, is the level of error we are allowing between the points of the dataset. The default value of epsilon is 0.1.", "We can use random projection by either setting a number of components or by specifying a value for epsilon and having the algorithm automatically calculating a conservative value for the number of dimensions.", "ICA is a method for dimensionality reduction similar to PCA or Random Projection in the sense that it takes a set of features and produces a different set that is useful in some way.", "But while PCA tries to maximize variance, ICA assumes that the features are mixtures of independent sources and it tries to isolate these independent sources that are mixed in the dataset.", "The motivation behind ICA would be to take the original set of features and try to identify those of them that contribute independently to the dataset, in other words, those with the leat correlation to the other features. So it will isolate those most important components. This problem is called Blind Source Isolation.", "These variables are related as follows:", "So the goal is to calculate W, to be able to obtain S, the source matrix of the independant features.", "To do so, the algorithm will perform the following steps:", "ICA assumes that the components are statistical independent. They must have non Gaussian distributions, as we would not be able to restore the original signals if they were Gaussian.", "From this point, the central limit theorem says that the distribution of a sum of independent variables tends towards a Gaussian distribution.", "One interesting applicaiton of ICA is the analysis of Electroenphalographic data. The following is an example of the readings of 14 channels from an EEG scan that lasted 4.5 seconds and the independent components extracted from the dataset.", "If you liked this post then you can take a look at my other posts on Data Science and Machine Learning here.", "If you want to learn more about Machine Learning, Data Science and Artificial Intelligence follow me on Medium, and stay tuned for my next posts!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Industrial Engineer and passionate about 4.0 Industry. My goal is to encourage people to learn and explore its technologies and their infinite posibilites."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fddb4d55e0757&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rromanss23.medium.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Victor Roman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77fd145c8783&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&user=Victor+Roman&userId=77fd145c8783&source=post_page-77fd145c8783----ddb4d55e0757---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/K2RZYdZTNSc", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e", "anchor_text": "previous articles"}, {"url": "https://towardsdatascience.com/supervised-learning-basics-of-classification-and-main-algorithms-c16b06806cd3", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/classification-project-finding-donors-853db66fbb8c", "anchor_text": "in a previous article"}, {"url": "https://www.semanticscholar.org/paper/Applying-Independent-Component-Analysis-to-Factor-Cha-Chan/a34be08a20eba7523600203a32abb026a8dd85a3", "anchor_text": "https://www.semanticscholar.org/paper/Applying-Independent-Component-Analysis-to-Factor-Cha-Chan/a34be08a20eba7523600203a32abb026a8dd85a3"}, {"url": "https://medium.com/@rromanss23", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ddb4d55e0757---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ddb4d55e0757---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----ddb4d55e0757---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----ddb4d55e0757---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&user=Victor+Roman&userId=77fd145c8783&source=-----ddb4d55e0757---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&user=Victor+Roman&userId=77fd145c8783&source=-----ddb4d55e0757---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fddb4d55e0757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ddb4d55e0757---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ddb4d55e0757--------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rromanss23.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Victor Roman"}, {"url": "https://rromanss23.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77fd145c8783&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&user=Victor+Roman&userId=77fd145c8783&source=post_page-77fd145c8783--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e807bd3a047&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-dimensionality-reduction-ddb4d55e0757&newsletterV3=77fd145c8783&newsletterV3Id=7e807bd3a047&user=Victor+Roman&userId=77fd145c8783&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}