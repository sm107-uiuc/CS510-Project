{"url": "https://towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f", "time": 1683014460.072205, "path": "towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f/", "webpage": {"metadata": {"title": "Building the Ultimate AI Agent for Doom using Duelling Double Deep Q-Learning | by Adrian Yijie Xu | Towards Data Science", "h1": "Building the Ultimate AI Agent for Doom using Duelling Double Deep Q-Learning", "description": "Over the last few articles, we\u2019ve discussed and implemented various value-learning architectures for the VizDoom environment, and examined their performance in maximizing reward. To summarize, these\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "discussed", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "implemented", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "our original DQN implementation", "paragraph_index": 5}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 7}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "DDQN implementation", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f", "anchor_text": "DuelDQN implementation", "paragraph_index": 16}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent", "paragraph_index": 24}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 24}, {"url": "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/", "anchor_text": "\u201cImprovements in Deep Q Learning*", "paragraph_index": 25}], "all_paragraphs": ["Over the last few articles, we\u2019ve discussed and implemented various value-learning architectures for the VizDoom environment, and examined their performance in maximizing reward. To summarize, these include:", "Overall, vanilla Deep Q-learning is a highly flexible and responsive online reinforcement learning approach that utilizes rapid intra-episodic updates to it\u2019s estimations of state-action (Q) values in an environment in order to maximize reward. Q-learning can be thought of as an off-policy approach to TD, where the algorithm aims to select state-action pairs of highest value independent of the current policy being followed, and has been associated with many of the original breakthroughs for the OpenAI Atari gym environments.", "In contrast, Double Deep Q-learning improves addresses the overestimation of state-action values observed in DQN by decoupling the action selection from the Q-value target calculation through the use of a dual-network setup, a prevalent problem observed earlier on during training. Similarly, Dueling Deep Q-learning improves on the model\u2019s ability to generalize during training by splitting the single output state-action stream into separate value and advantage streams, allowing the agent to learn to focus on separate objectives for improved performance.", "In this finale to our DQN-series of articles, we\u2019ll combine all that we\u2019ve learned thus far into a single composite approach\u2014 Duelling Double Deep Q-Learning (DuelDDQN) . By combining all of the perceived advantages of our previous models, we\u2019ll aim to further improve the convergence of our agent in the VizDoom Environment.", "We\u2019ll be implementing our approach in the same VizDoomgym scenario as in our last article, Defend The Line, with the same multi-objective conditions. Some characteristics of the environment include:", "Recall that in our original DQN implementation, we already utilized two concurrent networks networks- an evaluation network for action selection, and a periodically updated target network to ensure that the generated TD-targets are stationary. We can leverage this existing setup to build our DuelDDQN architecture without initializing more networks.", "Note that as the two networks are updated with one another\u2019s weights periodically, the two models are still partially coupled, but what matters is that the action selection and Q-value evaluation are done by separate networks not sharing the same set of a weights at a particular timestep.", "Our Google Colaboratory implementation is written in Python utilizing Pytorch, and can be found on the GradientCrescent Github. Our approach is based on the approach detailed in Tabor\u2019s excellent Reinforcement Learning course. As our DDQN implementation is similar to our previous vanilla DQN implementation, the overall high-level workflow is shared, and won\u2019t be repeated here.", "Let\u2019s start by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments. We\u2019ll also install the AV package necessary for Torchvision, which we\u2019ll use for visualization. Note that the runtime must be restarted after installation is complete.", "Next, we initialize our environment scenario, inspect the observation space and action space, and visualize our environment.", "Next, we\u2019ll define our preprocessing wrappers. These are classes that inherit from the OpenAI gym base class, overriding their methods and variables in order to implicitly provide all of our necessary preprocessing. We\u2019ll start defining a wrapper to repeat every action for a number of frames, and perform an element-wise maxima in order to increase the intensity of any actions. You\u2019ll notice a few tertiary arguments such as fire_first and no_ops \u2014 these are environment-specific, and of no consequence to us in Vizdoomgym.", "Next, we define the preprocessing function for our observations. We\u2019ll make our environment symmetrical by converting it into the standardized Box space, swapping the channel integer to the front of our tensor, and resizing it to an area of (84,84) from its original (320,480) resolution. We\u2019ll also greyscale our environment, and normalize the entire image by dividing by a constant.", "Next, we create a wrapper to handle frame-stacking. The objective here is to help capture motion and direction from stacking frames, by stacking several frames together as a single batch. In this way, we can capture position, translation, velocity, and acceleration of the elements in the environment. With stacking, our input adopts a shape of (4,84,84,1).", "Finally, we tie all of our wrappers together into a single make_env() method, before returning the final environment for use.", "Next, let\u2019s define the dueling part of our model, a deep Q-network featuring two output streams. This is essentially a three layer convolutional network that takes preprocessed input observations, with the generated flattened output fed to a fully-connected layer, after which the output is then split into the value stream (with a single node output), and the advantage stream (with a node output corresponding to the number of actions in the environment).", "Note there are no activation layers here, as the presence of one would result in a binary output distribution. Our loss is the squared difference of our estimated Q-value of our current state-action and our predicted state-action value. We then attach the RMSProp optimizer to minimize our loss during training.", "Next, we\u2019ll define our agent, which follows our previous DDQN implementation and DuelDQN implementation. Our agent be using an epsilon greedy policy with a decaying exploration rate, in order to maximize exploitation over time. To learn to predict state-action-values that maximize our cumulative reward, our agent will be using the discounted future rewards obtained by sampling the stored memory.", "You\u2019\u2019ll notice that we initialize two copies of our DQN as part of our agent, with methods to copy weight parameters of our original network into a target network. While our vanilla approach utilized this setup to generate stationary TD-targets, our DuelDDQN approach will expand beyond this:", "With all of supporting code defined, let\u2019s run our main training loop. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "We\u2019ve graphed the average score of our agents together with our episodic epsilon value, across 500 and 1000 episodes below. It can be observed that convergence of the DuelDDQN model is significantly superior to that of the baseline DQN and DDQN models, even improving slightly on the performance of the DuelDQN model.", "We can visualize the performance of our agent at 500 episodes below.", "Recall, that our previous agents suffered from the peculiar problem of falling into local minima, relying on inter-monster friendly fire as the primary scoring tactic rather than agent initiative. Our DuelDDQN agent is no different, reaching this minima faster than previous models. Solving this problem would require either a modification of the environment (replacing turning with strafing may be one option), accelerating learning to avoid mode collapse (such as through momentum), or through reward engineering \u2014 adding a reward corresponding to the time duration of survival or to the raw damage taken by the monsters from the agent\u2019s weapon, for example.", "So what have we learned through the course of our series?", "That wraps up this implementation on Dueling Double Deep Q-learning, and our series on DQN in generl. In our next article, we\u2019ll be taking a break from Reinforcement Learning, and dabble with selected generative image processing techniques.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository", "Simonini, \u201cImprovements in Deep Q Learning*", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fea2d5b8cdd9f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----ea2d5b8cdd9f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "discussed"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "implemented"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "Deep Q-learning"}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "Double Deep Q-learning"}, {"url": "https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f", "anchor_text": "Duelling Deep Q-learning"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "our original DQN implementation"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "here"}, {"url": "https://github.com/shakenes/vizdoomgym.git", "anchor_text": "https://github.com/shakenes/vizdoomgym.git"}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "DDQN implementation"}, {"url": "https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f", "anchor_text": "DuelDQN implementation"}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/", "anchor_text": "\u201cImprovements in Deep Q Learning*"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ea2d5b8cdd9f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ea2d5b8cdd9f---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/openai?source=post_page-----ea2d5b8cdd9f---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/games?source=post_page-----ea2d5b8cdd9f---------------games-----------------", "anchor_text": "Games"}, {"url": "https://medium.com/tag/ai?source=post_page-----ea2d5b8cdd9f---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ea2d5b8cdd9f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ea2d5b8cdd9f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fea2d5b8cdd9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ea2d5b8cdd9f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ea2d5b8cdd9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}