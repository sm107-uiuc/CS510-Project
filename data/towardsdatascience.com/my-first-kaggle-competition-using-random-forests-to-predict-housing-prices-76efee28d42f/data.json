{"url": "https://towardsdatascience.com/my-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f", "time": 1682993778.488582, "path": "towardsdatascience.com/my-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f/", "webpage": {"metadata": {"title": "My First Kaggle Competition | by Utkarsh Chawla | Towards Data Science", "h1": "My First Kaggle Competition", "description": "I recently stumbled upon this article by Rachel Thomas, depicting the various advantages of blogging and, lo and behold, here I am with my first article. In this article, I will share my experience\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045", "anchor_text": "article", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/rachel-thomas-942a7923/", "anchor_text": "Rachel Thomas", "paragraph_index": 0}, {"url": "http://course.fast.ai/ml.html", "anchor_text": "Machine Learning for coders MOOC", "paragraph_index": 1}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "Housing Price Prediction", "paragraph_index": 2}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data", "anchor_text": "data section", "paragraph_index": 6}, {"url": "https://stats.stackexchange.com/a/56659", "anchor_text": "RMSLE", "paragraph_index": 7}, {"url": "https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621", "anchor_text": "article", "paragraph_index": 15}, {"url": "https://github.com/fastai/fastai", "anchor_text": "fastai repo", "paragraph_index": 16}, {"url": "https://github.com/fastai/fastai/blob/master/old/fastai/structured.py", "anchor_text": "source code and documentation", "paragraph_index": 16}, {"url": "https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e", "anchor_text": "Parfit", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "pearman\u2019s rank correlation coefficient", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Dendrogram", "anchor_text": "dendrogram", "paragraph_index": 34}, {"url": "https://medium.com/@jeremyphoward", "anchor_text": "Jeremy Howard", "paragraph_index": 47}, {"url": "https://medium.com/@racheltho", "anchor_text": "Rachel Thomas", "paragraph_index": 47}, {"url": "http://www.fast.ai/", "anchor_text": "fast.ai", "paragraph_index": 47}], "all_paragraphs": ["I recently stumbled upon this article by Rachel Thomas, depicting the various advantages of blogging and, lo and behold, here I am with my first article.", "In this article, I will share my experience of participating in my first ever kaggle competition. I completed fast.ai\u2019s Machine Learning for coders MOOC, and I hoped to apply the knowledge gained from this course in this kaggle competition.", "We will be working on the Housing Price Prediction competition.The description says :", "This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.", "The problem is simple, we have to predict the prices(Dependent variable) of residential homes in Ames, Iowa. We are provided with 79 explanatory features(Independent variables) describing (almost) every aspect of the house.", "We will be using Random Forest for this problem, which is an ensemble of Decision Trees. Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.", "Most of these columns and their fields don\u2019t make much sense to us, but kaggle has provided us with some brief description in the data section of the competition. You can further look for data_description.txt resource to get the detailed description of every column and its data fields.", "The evaluation metric is the RMSLE(Root Mean Square Logarithmic Error), so it makes sense to take the log of the dependent variable(that is SalePrice) in this case.", "You might have noticed that the first six rows of the \u201cAlley\u201d column are all NaN values(which depicts missing data for the respective row). So let\u2019s find out the percentage of missing values in the whole dataset for all the features.", "We have 5 features with Nan% greater than 50. It is highly probable that these features won\u2019t provide us with any useful insight on the dataset so we can consider removing them right now, but we are going to look at feature importance soon enough anyway, so let\u2019s keep them for now and let the random forest make this decision for us.", "In order to feed this data-set to the random forest, we will have to convert all the non-numerical data into numerical data. All the non-numerical data columns represent categorical data which is of two types:", "Ordinal: Where the order of the categories matter.Nominal: Where the order doesn\u2019t matter.", "There are two ways to convert these into numerical:", "Label encoding:- Replace the categories with integer labels, ranging from 0 to (number of categories -1). It makes the categorical data ordinal in nature.", "One-Hot encoding:- Splitting one column with categorical data to multiple columns(equal to the number of categories). The numbers are replaced by 1s and 0s, depending on which column has what value.", "For more information on Label vs One-hot encoding, you can visit this article.", "Most of the columns are ordinal anyway, so we will go with Label encoding for this data-set (I tried One-Hot encoding as well later on, and it increased my RMSLE by 0.01). The \u201ctrain_cats\u201d and \u201cproc_df\u201d are taken from the fastai repo on Github. You can view the source code and documentation if you want to.", "We also replaced the missing values for numerical data columns with the median of the respective column, and added a {name}_na column which specifies if the data was missing.", "Looking at the data, I found out that there is no feature that provides us with the age of the house, so I added that in along with another feature that gives total living area.", "One last thing before building our random forest is to divide our data-set into two parts: the training set and the validation set. Possibly the most important idea in machine learning is having separate training & validation data sets. As motivation, suppose you don\u2019t divide up your data, but instead use all of it. And suppose you end up having lots of parameters:", "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it\u2019s not the best choice. Why is that? If you were to gather some new data points, they most likely would be closer to the curve in the middle graph.", "This illustrates how using all our data can lead to overfitting. A validation set helps diagnose this problem.The score of your model on the validation set is going to represent how well your model will do in the real world, on the data that it has never seen before. So, validation set is just a subset of your dataset which tells you how generalizable your model is.", "We are finally ready to build the random forest. We will be using Parfit for optimizing our hyper-parameters.", "The best hyper-parameters are:min_samples_leaf : 1,", "Using the optimized parameters for the random forest, we got an RMSLE of 0.1258 on the validation set.", "Let\u2019s look at one tree(Estimator) from our bag of trees in the Random forest.", "This tree has been visualized up to depth = 2. Every node of the tree splits our data into two halves, such that the weighted average of the MSE is the best possible. The MSE is decreasing as we go further down because the tree is basically finding the best possible split point for every node.The tree first divides the data on the basis of TotalLivingSf and then its children are divided on the basis of AgeSold and TotalLivingSf respectively and so on.We have 300 estimators in our forest, and we take the average of all these estimators for making a prediction.", "Next, we are going to dive into feature importance to remove redundant features, and to find out which features are responsible for giving the most insight into our data.", "The function \u2018rf_feat_importance\u2019 is going to return a pandas dataframe with column names and their respective feature importance. We used the \u2018plot_fi\u2019 function to plot a horizontal bar graph depicting the feature importances of all the columns.", "The graph shows that the importance is decreasing in an exponential fashion, so there is no need to consider features that provide almost no insight into our data-set. We will remove all the features with imp < 0.005 and then re-train our model on the new data and check the RMSLE.", "The RMSLE after this was 0.12506 which is a slight improvement over the previous 0.1258. The number of features has reduced from 79 to 25. Generally speaking, removing redundant columns should not make it worse. If it makes the RMSLE worse, they were not redundant after all.", "The removal of redundant columns might make our model a little bit better. If you think about how these trees are built, when the algorithm is deciding what to split on, it will have fewer things to worry about trying.", "There is a good probability of creating a better tree with less data, but it is not going to change the outcome by much. This will let us focus on the features that matter the most.", "Now we are going to find the correlation between the features using Spearman\u2019s rank correlation coefficient. If two features are providing us with the same insight, it is wise to get rid of one of them, because it is basically redundant. This technique is helpful in finding a link between multiple features.", "We will plot something called dendrogram, which is a kind of hierarchical clustering.", "The sooner two features collide into each other, the more correlated they are. Based on this, we can see five pairs of features that are highly correlated with each other.", "These are:1.) GrLivArea and TotalLivingSF.2.) GarageArea and GarageCars.3.) 1stFlrSF and TotalBsmtSF.4.) GarageYrBlt and YearBuilt.5.) FireplaceQu and Fireplaces.", "Both of the features in each pair are providing us with similar insight, so it is wise to remove one feature from every pair. But which one of the two is to be removed? Let\u2019s find out.", "Now in order to find out which one of the two features(in each pair) is to be removed, first of all, I calculated a baseline score (including all the 25 features) and then started removing these ten features one by one. After removing each feature we will again calculate the score(after retraining the model) and compare it with the baseline to see how the removal of that particular feature affects our score.", "Now we will start removing those 10 features and recalculate the score.Each output of the following code tells us the RMSLE after retraining the model on the new data. The first token tells us the feature(or column) that was removed and the second token tells us the RMSLE after removing that particular feature from the data.", "We can see that getting rid of GrLivArea, TotalBsmtSF, GarageYrBlt, and GarageArea reduces our error. So, we can get rid of all these redundant features.", "Our RMSLE reduced from 0.1258 to 0.12341 after getting rid of all the redundant features.", "We are now left with only 21 features starting from 79. This is our final model.We should now merge our training and validation set and retrain our model on this merged data-set.", "Now we will generate our predictions for the test set but before that we need to do similar transformations on the test set.", "This model gave us a score of 0.12480 on the Kaggle test set, which corresponds to a rank of 1338 out of 4052 on the LeaderBoard, which puts us in the top 34 %.", "I am currently learning about the second type of decision tree ensembles, namely boosting. I will follow up this article with an implementation revolving around Gradient Boosting Regressor instead of a Random Forest and see how that affects our feature importances and the final score.", "Any advice and suggestions will be greatly appreciated.", "I\u2019d like to thank Jeremy Howard and Rachel Thomas; for making these extraordinary MOOCs. I would recommend for everyone to check out fast.ai, they also have courses for Deep learning and computational linear algebra along with machine learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76efee28d42f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----76efee28d42f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@utkarshchawla691?source=post_page-----76efee28d42f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@utkarshchawla691?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Utkarsh Chawla"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fceca127ffa33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&user=Utkarsh+Chawla&userId=ceca127ffa33&source=post_page-ceca127ffa33----76efee28d42f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045", "anchor_text": "article"}, {"url": "https://www.linkedin.com/in/rachel-thomas-942a7923/", "anchor_text": "Rachel Thomas"}, {"url": "http://course.fast.ai/ml.html", "anchor_text": "Machine Learning for coders MOOC"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "Housing Price Prediction"}, {"url": "https://i.imgur.com/BEtQD00.png", "anchor_text": "https://i.imgur.com/BEtQD00.png"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data", "anchor_text": "data section"}, {"url": "https://stats.stackexchange.com/a/56659", "anchor_text": "RMSLE"}, {"url": "https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621", "anchor_text": "article"}, {"url": "https://github.com/fastai/fastai", "anchor_text": "fastai repo"}, {"url": "https://github.com/fastai/fastai/blob/master/old/fastai/structured.py", "anchor_text": "source code and documentation"}, {"url": "https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e", "anchor_text": "Parfit"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "pearman\u2019s rank correlation coefficient"}, {"url": "https://en.wikipedia.org/wiki/Dendrogram", "anchor_text": "dendrogram"}, {"url": "https://medium.com/@jeremyphoward", "anchor_text": "Jeremy Howard"}, {"url": "https://medium.com/@racheltho", "anchor_text": "Rachel Thomas"}, {"url": "http://www.fast.ai/", "anchor_text": "fast.ai"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----76efee28d42f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----76efee28d42f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----76efee28d42f---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----76efee28d42f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----76efee28d42f---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&user=Utkarsh+Chawla&userId=ceca127ffa33&source=-----76efee28d42f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&user=Utkarsh+Chawla&userId=ceca127ffa33&source=-----76efee28d42f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F76efee28d42f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----76efee28d42f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----76efee28d42f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----76efee28d42f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----76efee28d42f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----76efee28d42f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----76efee28d42f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@utkarshchawla691?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@utkarshchawla691?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Utkarsh Chawla"}, {"url": "https://medium.com/@utkarshchawla691/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fceca127ffa33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&user=Utkarsh+Chawla&userId=ceca127ffa33&source=post_page-ceca127ffa33--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fceca127ffa33%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f&user=Utkarsh+Chawla&userId=ceca127ffa33&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}