{"url": "https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab", "time": 1682993773.028726, "path": "towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab/", "webpage": {"metadata": {"title": "Web Scraping Using Selenium \u2014 Python | by Atindra Bandi | Towards Data Science", "h1": "Web Scraping Using Selenium \u2014 Python", "description": "Before we delve into the topic of this article let us first understand what is web-scraping and how is it useful. Web scraping is a technique for extracting information from the internet\u2026"}, "outgoing_paragraph_urls": [{"url": "https://chromedriver.storage.googleapis.com/index.html?path=2.42/", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://support.google.com/chrome/answer/95346?co=GENIE.Platform%3DDesktop&hl=en", "anchor_text": "Google Chrome", "paragraph_index": 8}, {"url": "https://www.edmunds.com/", "anchor_text": "Edmunds.com", "paragraph_index": 9}, {"url": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702", "anchor_text": "entry level luxury car brands.", "paragraph_index": 9}, {"url": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702", "anchor_text": "website", "paragraph_index": 13}, {"url": "https://github.com/bandiatindra/Edmunds_WebScraping/blob/master/Web_Scraping_Unstructured_Data_Analysis.ipynb", "anchor_text": "Github", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/@akhilesh.narapareddy", "anchor_text": "Akhilesh Narapareddy", "paragraph_index": 38}], "all_paragraphs": ["Before we delve into the topic of this article let us first understand what is web-scraping and how is it useful.", "Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing.", "Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business.", "Let\u2019s now get our hands dirty!!", "Since we have defined our purpose of scraping, let us delve into the nitty-gritty of how to actually do all the fun stuff! Before that below are some of the housekeeping instructions regarding installations of packages.", "a. Python version: We will be using Python 3.0, however feel free to use Python 2.0 by making slight adjustments. We will be using jupyter notebook, so you don\u2019t need any command line knowledge.", "b. Selenium package: You can install selenium package using the following command", "c. Chrome driver: Please install the latest version of chromedriver from here.", "Please note you need Google Chrome installed on your machines to work through this illustration.", "The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping Edmunds.com, a car forum. This website aids people in their car buying decisions. People can post their reviews about different cars in the discussion forums (very similar to how one posts reviews on Amazon). We will be scraping the discussion about entry level luxury car brands.", "We will scrape ~5000 comments from different users across multiple pages. We will scrape user id, date of comment and comments and export it into a csv file for any further analysis.", "We will first import important packages in our Notebook \u2014", "Let\u2019s now create a new instance of google chrome. This will help our program open an url in google chrome.", "Let\u2019s now access google chrome and open our website. By the way, chrome knows that you are accessing it through an automated software!", "So, how does our web page look like?", "We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them.", "The XML path (XPath)for the userid is shown below. There is an interesting thing to note here that the XML path contains a comment id, which uniquely denotes each comment on the website. This will be very helpful as we try to recursively scrape multiple comments .", "If we see the XPath in the picture, we will observe that it contains the user id \u2018dino001\u2019.", "How do we extract the values inside a XPath?", "Selenium has a function called \u201cfind_elements_by_xpath\u201d. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the \u2018text\u2019 function. In our case the text is basically the user id (\u2018dino001\u2019).", "2. Comment Date: Similar to the user id, we will now inspect the date when the comment was posted.", "Let\u2019s also see the XPath for the comment date. Again note the unique comment id in the XPath.", "So, how do we extract date from the above XPath?", "We will again use the function \u201cfind_elements_by_xpath\u201d to get the selenium element. Now, if we carefully observe the highlighted text in the picture, we will see that the date is stored inside the \u2018title\u2019 attribute. We can access the values inside attributes using the function \u2018get_attribute\u2019. We will pass the tag name in this function to get the value inside the same.", "3. Comments: Lastly, let\u2019s explore how to extract the comments of each user.", "Below is the XPath for the user comment \u2014", "Once again, we have the comment id in our XPath. Similar to the userid we will extract the comment from the above XPath", "We just learnt how to scrape different elements from a web page. Now how to recursively extract these items for 5000 users?", "As discussed above, we will use the comment ids, which are unique for a comment to extract different users data. If we see the XPath for the entire comment block, we will see that it has a comment id associated with it.", "The following code snippet will help us extract all the comment ids on a particular web page. We will again use the function \u2018find_elements_by_xpath\u2019 on the above XPath and extract the ids from the \u2018id\u2019 attribute.", "The above code gives us a list of all the comment ids from a particular web page.", "How to bring all this together?", "Now we will bring all the things we have seen so far into one big code, which will recursively help us extract 5000 comments. We can extract user ids, date and comments for each user on a particular web page by looping through all the comment ids we found in the previous code.", "Below is the code snippet to extract all comments from a particular web page.", "Lastly, if you check our url has page numbers, starting from 702. So, we can recursively go to previous pages by simply changing the page numbers in the url to extract more comments until we get the desired number of comments.", "This process will take some time depending on the computational power of your computer. So, chill, have a coffee, talk to your friends and family and let Selenium do its job!", "Summary: We learnt how to scrape a website using Selenium in Python and get large amounts of data. You can carry out multiple unstructured data analytics and find interesting trends, sentiments, etc. using this data. If anyone is interested in looking at the complete code, here is the link to my Github.", "Let me know if this was helpful. Enjoy Scraping BUT BE CAREFUL!", "If you liked reading this, I would recommend reading another article about scraping Reddit data using Reddit API and Google BigQuery written by a fellow classmate (Akhilesh Narapareddy) at the University of Texas, Austin.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8a60f4cf40ab&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bandiatindra?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bandiatindra?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Atindra Bandi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8eac85bcc326&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&user=Atindra+Bandi&userId=8eac85bcc326&source=post_page-8eac85bcc326----8a60f4cf40ab---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://chromedriver.storage.googleapis.com/index.html?path=2.42/", "anchor_text": "here"}, {"url": "https://support.google.com/chrome/answer/95346?co=GENIE.Platform%3DDesktop&hl=en", "anchor_text": "Google Chrome"}, {"url": "https://www.edmunds.com/", "anchor_text": "Edmunds.com"}, {"url": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702", "anchor_text": "entry level luxury car brands."}, {"url": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702", "anchor_text": "website"}, {"url": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702'", "anchor_text": "https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702'"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "http://twitter.com/id", "anchor_text": "@id"}, {"url": "https://github.com/bandiatindra/Edmunds_WebScraping/blob/master/Web_Scraping_Unstructured_Data_Analysis.ipynb", "anchor_text": "Github"}, {"url": "https://towardsdatascience.com/@akhilesh.narapareddy", "anchor_text": "Akhilesh Narapareddy"}, {"url": "https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892", "anchor_text": "Scrape Reddit data using Python and Google BigQueryAn user friendly approach to access Reddit API and Google Bigquerytowardsdatascience.com"}, {"url": "https://medium.com/tag/python?source=post_page-----8a60f4cf40ab---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----8a60f4cf40ab---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----8a60f4cf40ab---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/tag/selenium?source=post_page-----8a60f4cf40ab---------------selenium-----------------", "anchor_text": "Selenium"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&user=Atindra+Bandi&userId=8eac85bcc326&source=-----8a60f4cf40ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&user=Atindra+Bandi&userId=8eac85bcc326&source=-----8a60f4cf40ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8a60f4cf40ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8a60f4cf40ab---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8a60f4cf40ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bandiatindra?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bandiatindra?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Atindra Bandi"}, {"url": "https://medium.com/@bandiatindra/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "190 Followers"}, {"url": "https://www.linkedin.com/in/atindra-bandi-83a60543/", "anchor_text": "https://www.linkedin.com/in/atindra-bandi-83a60543/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8eac85bcc326&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&user=Atindra+Bandi&userId=8eac85bcc326&source=post_page-8eac85bcc326--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe695e5ab4a37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-using-selenium-python-8a60f4cf40ab&newsletterV3=8eac85bcc326&newsletterV3Id=e695e5ab4a37&user=Atindra+Bandi&userId=8eac85bcc326&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}