{"url": "https://towardsdatascience.com/what-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc", "time": 1683010661.102551, "path": "towardsdatascience.com/what-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc/", "webpage": {"metadata": {"title": "What AlexNet Brought To The World Of Deep Learning | by Richmond Alake | Towards Data Science", "h1": "What AlexNet Brought To The World Of Deep Learning", "description": "The AlexNet convolutional neural network(CNN) was introduced in the year 2012. Since then, the utilization of deep convolutional neural network has skyrocketed to the point where several machine\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "ImageNet Classification With Deep Convolutional Neural Network", "paragraph_index": 3}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST", "paragraph_index": 7}, {"url": "https://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-580/specifications", "anchor_text": "NVIDIA GTX 580 3GB GPU", "paragraph_index": 11}, {"url": "https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf", "anchor_text": "paper", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/batch-normalization-explained-algorithm-breakdown-23d2794511c", "anchor_text": "Batch Normalization (BN)", "paragraph_index": 22}, {"url": "https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization", "anchor_text": "machine learning libraries and frameworks", "paragraph_index": 25}, {"url": "https://pytorch.org/hub/pytorch_vision_alexnet/", "anchor_text": "PyTorch", "paragraph_index": 55}], "all_paragraphs": ["The AlexNet convolutional neural network(CNN) was introduced in the year 2012. Since then, the utilization of deep convolutional neural network has skyrocketed to the point where several machine learning solutions leverage deep CNNs.", "This article will present the essential findings, and talking points of the research paper, in which the AlexNet architecture was introduced.", "Machine learning and Deep learning practitioner of all levels can follow along with the content presented in this article.", "The AlexNet convolutional neural network architecture was presented in the paper \u201cImageNet Classification With Deep Convolutional Neural Network\u201d. The paper was authored by Alex Krizhevsky, Ilya Sutskever and the godfather of deep learning, Geoffery Hinton.", "The authors of the paper aimed to show the trivial task of image classification can be tackled by using deep convolutional neural networks, efficient computation resources and common CNN\u2019s implementation techniques.", "The paper proved that a deep convolutional neural network consisting of 5 convolutional layers and 3 fully connected layers could classify images efficiently and accurately.", "A deep convolutional neural network was called AlexNet, and it was introduced in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012 contest), where it set a precedent for the field of Deep Learning.", "Before the introduction of AlexNet, many traditional neural networks and convolutional neural networks performed well on solving image classification on datasets such as the MNIST handwritten character dataset. But to solve the problem of general image classification of objects in everyday life, a larger dataset is required to account for the considerable diversity of objects that occurred within images.", "The lack of robust datasets was solved by the introduction of large datasets such as ImageNet, which contained 22,000 classes across 15 million high-resolution images.", "Another limitation prior to the introduction of AlexNet was computer resources.", "To increase the capacity of a network meant to increase the number of layers and neurons within the network.", "At the time, the compute resource to train such a network was scarce. But the introduction of optimized GPUs made the possibility of training deep conventional neural network achievable. The particular GPU used to train the AlexNet CNN architecture was the NVIDIA GTX 580 3GB GPU.", "To train neurons within a neural network, it had been standard to utilize either Tanh or sigmoid non-linearity, this was the goto activation function that was leveraged to model the internal neuron activation within CNNs.", "The AlexNet went on to use Rectified Linear Units, ReLU for short. ReLU was introduced in this paper by Vinod Nair and Geoffrey E. Hinton in 2010.", "ReLu can be described as a transfer function operation that is performed on the output of the prior convolution layer. The utilization of ReLu ensures that values within the neurons that are positive their values are maintained, but for negative values, they are clamped down to zero.", "The benefit of using ReLu is that it enables the training process to be accelerated as gradient descent optimization occurs at a faster rate in comparison to other standard non-linearity techniques.", "Another benefit of the ReLu layer is that it introduces non-linearity within the network. It also removes the associativity of successive convolutions.", "In the original research paper that introduced the AlexNet neural network architecture, the training of models was conducted with the utilization of two GTX 580 GPUs with 3GB memory.", "GPU parallelization and distributed training are techniques that are very much in use today.", "From information derived from the research paper, the model was trained on two GPU, where half of the model\u2019s neurons were on one, and the other half held within the memory of a second GPU. The GPUs communicated with each other, without the need of going through the host machine. Communication between the GPU is constrained on a layer basis; therefore, only specific layers can communicate with each other.", "For example, the inputs in the fourth layer of the AlexNet network was obtained from half of the third layer\u2019s feature maps on the current GPU, and the rest of the other half is derived from the second GPU. This will be better illustrated later in this article.", "Normalization is taking a set of data points and placing them on a comparable basis or scale(this is an overly simplistic description).", "Batch Normalization (BN) within CNNs is a technique that standardizes and normalizes inputs by transforming a batch of input data to have a mean of zero and a standard deviation of one.", "Many are familiar with batch normalization, but the AlexNet architecture used a different method of normalization within the network: Local Response Normalization (LRN).", "LRN is a technique that maximizes the activation of neighbouring neurons. Neighbouring neurons describe neurons across several feature maps that share the same spatial position. By normalizing the activations of the neurons, neurons with high activations are highlighted; this essentially mimics the lateral inhibition that happens within neurobiology.", "LRN are not widely utilized in modern CNN architectures, as there are other more effective methods of normalization. Although, LRN implementations can still be found in some standard machine learning libraries and frameworks, so feel free to experiment.", "Pooling layers in CNNs essentially encapsulate information within a set of pixels or values within a feature map and projects them into a lower sized grid, while reflecting the general information from the original set of pixels.", "The illustration below provides an example of a pooling, more specifically max pooling. Max pooling is a variant of sub-sampling where the maximum pixel value of pixels that fall within the receptive field of the pooling window.", "Within the paper that introduces the AlexNet CNN architecture, a different methodology of pooling was introduces and utilizes. Overlapping pooling. In traditional pooling techniques, the stride, from one centre of a pooling window to another is positioned to ensure that values from one pooling window are not within a subsequent pooling window.", "In contrast to the traditional methodology of pooling, overlapping pooling utilizes a stride that is less than the dimension of the pooling window. This means that the outputs of subsequent pooling windows encapsulate information from pixels/values that have been pooled more than once. It\u2019s hard to see the benefits of this, but according to the findings of the paper, overlapping pooling reduces the ability for a model to overfit during training.", "Another standard method of reducing the chances of overfitting a network is through data augmentation. By artificially augmenting the dataset, you increase the number of training data, which in turn increases the amount of data the network is exposed to during the training phase.", "Augmentation of images usually come in the form of transformation, translation, scaling, cropping, flipping etc.", "The images used to train the network in the original AlexNet paper were artificially augmented during the training phase. The augmentation techniques utilized were cropping and alteration of pixel intensities within images.", "Images within the training set were randomly cropped from their 256 by 256 dimensions, to obtain a new cropped image of 224 by 224.", "It turns out that randomly performing augmentation to training set can significantly reduce the potential of a network to overfit during training.", "The augmented images are simply derived from the content of the original training images, so why does augmentation work so well?", "Simply kept, data augmentation increases the invariance in your dataset without the need for sourcing new data. The ability for the network to generalize well to unseen dataset also increases.", "Let\u2019s take a very literal example; the images in the \u2018production\u2019 environment might not be perfect, some might be tilted, blurred or contain only bits of essential features. Therefore, training a network against a dataset that includes a more robust variation of training data will enable the trained network to have more success classifying images in a production environment.", "Dropout is a term many deep learning practitioners are familiar with. Dropout is a technique that is utilized to reduce a model\u2019s potential to overfit.", "Dropout technique works by adding a probability factor to the activation of neurons within the layers of a CNN. This probability factor indicates to the neurons chances of been activated during a current feed-forward step and during involved in the process of backpropagation.", "Dropout is useful as it enables the neurons to reduce dependability on neighbouring neurons; each neuron learns more useful features as a result of this.", "In the AlexNet architecture, the dropout technique was utilized within the first two fully connected layers.", "One of the disadvantages of using dropout technique is that it increases the time it takes for a network to converge.", "Although, the advantage of utilizing dropout far beats its disadvantages.", "In this section, we will get an understanding of the internal composition of the AlexNet network. We will focus on information associated with the layers and breakdown the internal properties of each significant layers.", "The AlexNet CNN architecture consists of 8 layers, which included 5 conv layers and 3 fully connected layers. Some of the conv layers are a composition of convolution, pooling and normalization layers.", "AlexNet was the first architecture to adopt an architecture with consecutive convolutional layers (conv layer 3, 4 and 5).", "The final fully connected layer in the network contains a softmax activation function that provides a vector that represents a probability distribution over 1000 classes.", "Softmax activation is utilized to derive the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector in which its set of values represents the probability of an occurrence of a class or event. The values within the vector all add up to 1.", "Apart from the last fully connected layer, the ReLU activation function is applied to the rest of the layers included in the network.", "The illustration of the AlexNet network above is split into two partitions since the model was trained across two GTX 580 GPUs. Although the network is partitioned across two GPUs, from the illustration, we can see some cross GPU communication within the conv3, FC6, FC7 and FC8 layers.", "The table below is a breakdown of some of the characteristics and properties of the layers in the network.", "In the original paper, the input layer is said to have the dimensions 224 x 224 x 3, but in the table above the input layer has the input dimensions of 227 x 227 x 3, the discrepancies are due to the fact that there is some unmentioned padding that occurs during the actual training of the network that is not included in the published paper.", "The introduction and success of AlexNet changed the landscape of deep learning. After its triumphant performance at the ILSVRC\u201912 contest, the following years winning architectures were all deep convolutional neural networks.", "A variant of the AlexNet won the ILSVRC\u201913 contest with different hyperparameters. The winning architecture in the year 2014, 2015 and 2016 was built with deeper networks and smaller convolutional kernels/filters.", "Understanding the architecture of AlexNet is easy, and it\u2019s even easier to implement, especially with tools such as PyTorch and TensorFlow that include a module of the architecture within their libraries and frameworks.", "In a future article, I\u2019ll be showing how the AlexNet architecture presented in this paper can be implemented and utilized with TensorFlow.", "To connect with me or find more content similar to this article, do the following:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Content Creator with 1M+ views\u2014 Computer Vision Engineer. Interested in gaining and sharing knowledge on Technology and Finance"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46c7974b46fc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://richmondalake.medium.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Richmond Alake"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88797ba3f2f6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&user=Richmond+Alake&userId=88797ba3f2f6&source=post_page-88797ba3f2f6----46c7974b46fc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@pietrozj?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Pietro Jeng"}, {"url": "https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "ImageNet Classification With Deep Convolutional Neural Network"}, {"url": "https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/", "anchor_text": "Alex Krizhevsky"}, {"url": "https://www.bizjournals.com/sanjose/news/2016/11/30/techflash-q-a-reasons-to-be-hopeful-about-ourrobot.html", "anchor_text": "Ilya Sutskever"}, {"url": "https://www.wired.com/story/googles-ai-guru-computers-think-more-like-brains/", "anchor_text": "Geoffery Hinton"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST"}, {"url": "https://www.kaggle.com/shubham2306/cnn-on-mnist-using-keras-accuracy-0-993", "anchor_text": "MNIST Dataset illustration"}, {"url": "https://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-580/specifications", "anchor_text": "NVIDIA GTX 580 3GB GPU"}, {"url": "https://www.techpowerup.com/gpu-specs/geforce-gtx-580.c270", "anchor_text": "Nvidia GTX 580"}, {"url": "https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/batch-normalization-explained-algorithm-breakdown-23d2794511c", "anchor_text": "Batch Normalization (BN)"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization", "anchor_text": "machine learning libraries and frameworks"}, {"url": "https://www.oreilly.com/radar/visualizing-convolutional-neural-networks/", "anchor_text": "Max Pooling Illustration by Justin Francis at Oriely"}, {"url": "https://towardsdatascience.com/you-should-understand-sub-sampling-layers-within-deep-learning-b51016acd551", "anchor_text": "(You Should) Understand Sub-Sampling Layers Within Deep LearningAverage pooling, max-pooling, sub-sampling, downsampling, are all phrases that you\u2019ll come across within Deep Learning\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/understanding-and-implementing-dropout-in-tensorflow-and-keras-a8a3a02c1bfa", "anchor_text": "Understanding And Implementing Dropout In TensorFlow And KerasDropout is a common regularization technique that is leveraged within the state of the art solutions to computer vision\u2026towardsdatascience.com"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "AlexNet Neural Network Multi GPU Architecture"}, {"url": "https://pytorch.org/hub/pytorch_vision_alexnet/", "anchor_text": "PyTorch"}, {"url": "https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA", "anchor_text": "YouTube channel"}, {"url": "https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA", "anchor_text": "here"}, {"url": "https://medium.com/@richmond.alake", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/richmondalake/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3", "anchor_text": "How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures)Instructions on how to approach knowledge acquisition through published research papers by a recognized figure.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/algorithm-bias-in-artificial-intelligence-needs-to-be-discussed-and-addressed-8d369d675a70", "anchor_text": "Algorithm Bias In Artificial Intelligence Needs To Be Discussed (And Addressed)You have a part to play in the matter\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----46c7974b46fc---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----46c7974b46fc---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----46c7974b46fc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/technology?source=post_page-----46c7974b46fc---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----46c7974b46fc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&user=Richmond+Alake&userId=88797ba3f2f6&source=-----46c7974b46fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&user=Richmond+Alake&userId=88797ba3f2f6&source=-----46c7974b46fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F46c7974b46fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----46c7974b46fc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46c7974b46fc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----46c7974b46fc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----46c7974b46fc--------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://richmondalake.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Richmond Alake"}, {"url": "https://richmondalake.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "7.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88797ba3f2f6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&user=Richmond+Alake&userId=88797ba3f2f6&source=post_page-88797ba3f2f6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F366f35b0b39b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc&newsletterV3=88797ba3f2f6&newsletterV3Id=366f35b0b39b&user=Richmond+Alake&userId=88797ba3f2f6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}