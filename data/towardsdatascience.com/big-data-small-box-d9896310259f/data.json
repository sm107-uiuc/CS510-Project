{"url": "https://towardsdatascience.com/big-data-small-box-d9896310259f", "time": 1683014745.291638, "path": "towardsdatascience.com/big-data-small-box-d9896310259f/", "webpage": {"metadata": {"title": "Big data, small box. Building and touring a big data\u2026 | by Kyle Jarvis | Towards Data Science", "h1": "Big data, small box", "description": "TL; DR: See how we can use Vagrant to build and configure a big data development environment that\u2019s small enough to fit into your laptop. Fire up this environment in one line of code. Get to grips\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/downloads.html", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html", "anchor_text": "bulk upload API", "paragraph_index": 15}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html", "anchor_text": "documentation", "paragraph_index": 16}, {"url": "https://www.elastic.co/what-is/elasticsearch-hadoop", "anchor_text": "ElasticSearch Hadoop", "paragraph_index": 29}, {"url": "https://github.com/kjarvis1905/vagrant-elk/blob/238be02e36148a569254d3a1768663ad52eaa50c/bootstrap/install/spark_install.sh#L31-L47", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html#search-search-api-example", "anchor_text": "achieve the same outcome using cURL.", "paragraph_index": 32}], "all_paragraphs": ["TL; DR: See how we can use Vagrant to build and configure a big data development environment that\u2019s small enough to fit into your laptop. Fire up this environment in one line of code. Get to grips with the basics of ElasticSearch, Kibana, and Spark: index a Spark DataFrame to Elasticsearch, then see how to query the index using Python, Kibana or good ol\u2019 cURL. Enjoy the use of your VM to build your proficiency in some key technologies from the big data stack.", "I\u2019ve tried to make this article as accessible as possible, and you should be able to follow along without any specific knowledge of any of the technologies we\u2019ll be using.", "In this article we\u2019ll link together key technologies from the big data stack, exploring how data flows between these components, and what function each component plays. We\u2019ll be able to do this very easily by using Vagrant, a technology that lets us build and configure Virtual Machines, and provision them with software. In fact, if you\u2019ve made it this far, you\u2019re only a few short steps from having a functional \u2018big data\u2019 development environment, albeit without the resources to be processing TB-sized data sets.", "You\u2019ll be able to use this setup to learn how to get productive with a big data technology stack in your own time (and make mistakes without anyone seeing), so that when it comes to working with these technologies for real, you\u2019ll have a solid foundation and some hands on experience upon which to build!", "Time to get vagrant up and running.. Vagrant lets us initialise, provision and configure virtual machines according to instructions written in a Vagrantfile. A Vagrantfile contains the instructions for the VM provisioner, and (potentially) everything else required to download, install and configure software on the VM (although chunks of instructions can be refactored into separate scripts that are executed in the provisioning process). That means, if you\u2019re lucky, someone has already created a Vagrantfile describing a VM that has the ability to do exactly what you want, and you can get straight to the fun part. To get started with the VM that I\u2019ve created for this article, follow the steps below.", "*As part of the process of setting up the VM, Spark is downloaded from here and installed. Actually, before downloading Spark, I\u2019ve included a step to have the installation script check whether the spark-2.4.6-bin-hadoop2.7.tgz file already exists in the same directory as the Vagrantfile, if it does, then the download step is skipped. I recommend manually downloading and placing this file in the Vagrantfile directory, because the default mirror site from which this .tgz is downloaded can be super slow.", "Once you\u2019ve typed vagrant up , that\u2019s it! Treat yourself to a cup of tea, because installation and provisioning will not be instantaneous.", "This simple little command just triggered a chain reaction of events that will ultimately culminate in the creation a fully-big-data-fledged VM for us to use and explore. Here\u2019s what happened:", "Before seeing how we can use our VM to explore these technologies, let\u2019s briefly cover the roles played by each of them, and how they are typically used together in production big data environments.", "Despite ElasticSearch and Kibana being the \u2018destinations\u2019 that data would tend to flow to, we\u2019re going to start the exploration of our shiny new VM by looking at these two tools, for which server processes should already be running!", "You can easily stop and start the VM at your convenience. To stop the VM, type vagrant halt in a shell in the directory where the Vagrantfile lives (if you\u2019ve ssh\u2019ed into your VM, make sure to close the ssh connection before executing this command) . When you want to restart the VM, and pickup where you left off, simply run vagrant up . Thankfully, this will be much quicker than the first time we ran the VM!", "With our virtual machine up and running, let\u2019s perform some basic checks to make sure everything is working. You should have already cloned the git repository and run the command vagrant up to launch, configure and provision the virtual machine. Now run the command vagrant ssh to ssh into the running machine. Check that ElasticSearch and Kibana have been launched successfully using the commands:", "Both these commands should show us some reassuring-looking output that tells us our services are running. If all looks good, we should have no problem running the following:", "We just submitted a HTTP GET request to the ElasticSearch REST API, we should have received a response giving some diagnostic information about the state of the Elastic cluster. Querying and retrieving information from ES indices is just as easy! The next thing that we can try is to switch to a browser on our host machine and navigate to 127.0.0.1:5602 (Note that in the Vagrantfile we have configured port 5601 of the guest VM, which is the port Kibana is listening on, to forward traffic to port 5602 on the host machine). In our browser we should be able to see a splash screen that tells us the Kibana UI is starting. Once loaded, navigate to \u2018dev tools\u2019 (the spanner icon at the bottom of the navigation panel on the left hand side). This gives us access to a console that lets us send requests to the ES server and helps us with syntax, auto-completion and to explore the options available to us. Response are displayed in the adjacent window. Try typing GET /_cat/health?v and send the query to the ES server. Under the hood, a very similar thing happened as when we executed (3).", "It\u2019s good to know our ES cluster is running \u2014 but it would be better to go through the process of indexing documents and searching through them. Navigate to a working directory \u2014 probably /home/vagrant , then download and unzip some sample data using the commands below.", "Now we\u2019ll use the ElasticSearch bulk upload API to index the documents in accounts.json . To do this we need to make a POST request to the ES server. The full command is:", "Note that accounts.json is conveniently formatted according to the requirements outlined in the API documentation. This just created 1000 new documents in the financial index. A successful request will be rewarded with a long response composed of units similar to the below:", "Now let\u2019s retrieve some documents from the index. For a bit of diversity, switch back to the Kibana application open in the browser. Go to the console and type GET /financial/_doc/1 then \u201cclick to send request\u201d. In the adjacent window we will see the response, which should be successful, along with the contents of the document with _id = 1 . The _source attribute will be populated using the contents of the document. Note that we could achieve the same result by using a different tool, like curl, to make the request.", "If you\u2019ve made it this far \u2014 congratulations. You have everything you need to be able to explore and experiment with ElasticSearch and Kibana. Remember, you can easily stop and restart your VM later to continue with the next steps.", "The next technology that we\u2019re going to use is Spark. Spark is practically synonymous with high performance big data processing. The reason I want to introduce it here is that it is possible to directly index the rows of a Spark DataFrame into an Elastic cluster. That makes it incredibly easy to write out steps our outputs directly from a data processing pipeline to Elastic, where the index can be searched and individual documents retrieved via an API, or explored and visualised using a Kibana dashboard. Before I go into any more detail about how to do this, let\u2019s cover how we\u2019ll use Spark in our Vagrant VM. Spark has APIs written in a number of languages, we will use the Python API, aka PySpark.", "From a shell connected to the VM, navigate to /vagrant/examples , and run", "which activates a Python virtual environment that has already been set up using pipenv . PySpark has already been installed into this virtual environment by using pip to build and install the PySpark source code included with the Spark distribution. If you\u2019re interested in understanding these steps in more detail, you can inspect the bootstrap/install/spark_install.sh and bootstrap/install/setup_demo_project.sh scripts in the GitHub repository.", "At this point you have two options available to you: a set of shortcuts, or a long-way-round. To see the shortcuts available to you, look at the short cuts section.", "Read on for a more ground-up approach, where we will break out the intermediate steps one-by-one.", "The first thing we should do is start a Python interpreter and run the following commands, to check we can start a Spark session properly:", "This should give you a SparkSession object, the main entry point to the underlying Spark wizardry, and the next thing we should do is use it to load some data into a DataFrame. For this we will re-use the accounts.json obtained in the previous section. We need to quickly transform this data by dropping the rows that contain the index/id information so that we can read just the data into a Spark DataFrame. We can do that using sed (stream editor), which you could use in a separate shell:", "Check the result with head -n 5 accounts_edited.json , you\u2019ll notice we\u2019ve ditched the intermittent lines with index information. You\u2019ll also notice my regex expression is very lazy and would fall down quickly in more general circumstances. I make no excuses.", "Switch back to our Python interpreter and load the json file into a Spark DataFrame (note that if you downloaded your data to a different location, you will have to set the appropriate path in the code snippet):", "This Spark DataFrame is exactly the same type of object you\u2019re likely to end up with at the end (or in the middle (or at the start\u2026)) of any kind of data processing pipeline that uses Spark. What we\u2019ll do next is see how to index the data contained in this object directly into ElasticSearch.", "Now, in order to be able to index a Spark DataFrame directly into ElasticSearch, we\u2019re required to put in some effort to set things up \u2014 namely, the interface between ElasticSearch and Spark, which is provided by ElasticSearch Hadoop. Again, some of the work has already been done: the required archive has already been downloaded and unzipped during provisioning of the VM, and the relevant snippet of the code can be found here. You\u2019ll also notice that I added an export statement to the .bashrc file defining an environment variable called ES_HDP_JAR . Echo the value of this variable to the console to see the path of the jar file we need to pass to Spark when instantiating a session. It is the inclusion of this jar file that let\u2019s use index data directly from the PySpark DataFrameWriter object. We can make use of the environment variable from within Python to easily configure a new SparkSession:", "We will index the DataFrame created using the accounts_edited.json data covered in earlier steps. The command we need to run is:", "The above command creates a new index sparkindex using the contents of the DataFrame df . To view the contents of this new index, switch back to the Kibana console (or use a command line equivalent) and use the search API with an empty \u201cmatch_all\u201d clause:", "Note that the ElasticSearch documentation includes examples, similar to the one above, with a \u2018copy as cURL\u2019 option, so you can see how to achieve the same outcome using cURL.", "The body of the response will contain some of the matched documents:", "Notice that, because we did not specify a column to use as a unique document identifier, the _id field has been generated on our behalf.", "In /vagrant/examples type pipenv shell then spark-es-demo --help and you\u2019ll see there are a few commands that you can run:", "Running these and inspecting the respective code (found in /vagrant/examples/examples/spark_demo.py ) should give you a few working, documented examples to refer to.", "(7. a) Will download some demo data and put it into a Spark DataFrame.", "(7. b) Will take you through the entire process of getting data into an Elastic cluster via a Spark DataFrame.", "(7. c) Will allow you to compose and submit a basic match query to an ES cluster. By default it will look for an index called sparkindex, running (7. b) will create this, and put some data in it so that this command can be run successfully. Running the command with the --match-all option will submit a match all query and return some results, so that you can see some examples of documents returned from the ES index. You\u2019ll be prompted to select a field to search, and then asked for a value to search for. Some combinations that will return actual results are:", "0. firstname followed by Nanette (Thanks Nanette)", "3. employer followed by Anocha (Thanks Anocha)", "Play around to figure out some others.", "In the preceding sections, we\u2019ve briefly covered how to get up and running with some key big data technologies. We\u2019ve stepped through examples of how to:", "From here you can use our Vagrant development environment to hone your big data skill set: become fluent in Lucene, which is the library ElasticSearch uses to compose searches and return documents that match our queries, use Kibana to build dashboards to visualise and explore your favourite data sets, build ETL pipelines to write transformed datasets into ElasticSearch Indices, and much more.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Experimental Physicist turned full-stack data-science enthusiast. Working with big data and machine learning in a global investment bank."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9896310259f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9896310259f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kyle.jarvis1905?source=post_page-----d9896310259f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Kyle Jarvis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5de09a73c99f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&user=Kyle+Jarvis&userId=5de09a73c99f&source=post_page-5de09a73c99f----d9896310259f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@fabioha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "fabio"}, {"url": "https://unsplash.com/s/photos/big-data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.virtualbox.org/wiki/Downloads", "anchor_text": "VirtualBox"}, {"url": "https://www.vagrantup.com/intro/getting-started", "anchor_text": "https://www.vagrantup.com/intro/getting-started"}, {"url": "https://github.com/kjarvis1905/vagrant-elk", "anchor_text": "here"}, {"url": "https://spark.apache.org/downloads.html", "anchor_text": "here"}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html", "anchor_text": "bulk upload API"}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html", "anchor_text": "documentation"}, {"url": "https://www.elastic.co/what-is/elasticsearch-hadoop", "anchor_text": "ElasticSearch Hadoop"}, {"url": "https://github.com/kjarvis1905/vagrant-elk/blob/238be02e36148a569254d3a1768663ad52eaa50c/bootstrap/install/spark_install.sh#L31-L47", "anchor_text": "here"}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html#search-search-api-example", "anchor_text": "achieve the same outcome using cURL."}, {"url": "https://medium.com/tag/big-data?source=post_page-----d9896310259f---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----d9896310259f---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d9896310259f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/big-data-analytics?source=post_page-----d9896310259f---------------big_data_analytics-----------------", "anchor_text": "Big Data Analytics"}, {"url": "https://medium.com/tag/elasticsearch?source=post_page-----d9896310259f---------------elasticsearch-----------------", "anchor_text": "Elasticsearch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&user=Kyle+Jarvis&userId=5de09a73c99f&source=-----d9896310259f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&user=Kyle+Jarvis&userId=5de09a73c99f&source=-----d9896310259f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9896310259f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd9896310259f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d9896310259f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d9896310259f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d9896310259f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d9896310259f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d9896310259f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d9896310259f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kyle Jarvis"}, {"url": "https://medium.com/@kyle.jarvis1905/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "46 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5de09a73c99f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&user=Kyle+Jarvis&userId=5de09a73c99f&source=post_page-5de09a73c99f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F5de09a73c99f%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-small-box-d9896310259f&user=Kyle+Jarvis&userId=5de09a73c99f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}