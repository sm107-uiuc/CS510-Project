{"url": "https://towardsdatascience.com/santander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060", "time": 1683015049.308532, "path": "towardsdatascience.com/santander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060/", "webpage": {"metadata": {"title": "Santander Customer Satisfaction \u2014 A Self Case Study using Python | by Ashish Thomas Chempolil | Towards Data Science", "h1": "Santander Customer Satisfaction \u2014 A Self Case Study using Python", "description": "This case study is based on Kaggle competition conducted in the year 2016. Customer satisfaction is one of the most important key performance indicators in every company today and is seen as a key\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/santander-customer-satisfaction", "anchor_text": "Kaggle competition", "paragraph_index": 0}, {"url": "https://medium.com/@thewingedwolf.winterfell/response-coding-for-categorical-data-7bb8916c6dc1", "anchor_text": "response encoding", "paragraph_index": 45}, {"url": "https://github.com/ashishthomaschempolil/Santander-Customer-Satisfaction", "anchor_text": "github link", "paragraph_index": 60}, {"url": "https://www.linkedin.com/in/ashishthomas7/", "anchor_text": "Linkedin", "paragraph_index": 60}], "all_paragraphs": ["This case study is based on Kaggle competition conducted in the year 2016. Customer satisfaction is one of the most important key performance indicators in every company today and is seen as a key element of a company\u2019s success. Unhappy customers don\u2019t stick around. What\u2019s more, unhappy customers rarely voice their dissatisfaction before leaving. Santander is a Spanish multinational corporation bank and financial based company which operates in Europe, North and South America, and also Asia. In this Kaggle competition that is conducted by Santander we need to predict whether a customer is dissatisfied with their services early on based on the features provided by the company. This will help them to take proactive steps to improve the customer satisfaction before the customer leaves.", "This problem is a classification modelling task to determine whether a customer (datapoint) is dissatisfied or not.", "We are given two files :a test.csv and train.csv which contains around 370 features which are anonymized and 1 dependent feature that is TARGET. Each datapoint represents a customer and the value in TARGET will be 1 if a customer is dissatisfied with the company\u2019s service and 0 if he is satisfied and also the dataset is heavily imbalanced.", "The metric used here is the area under the ROC (receiver operating characteristic) curve which is the area under the plot between True Positive Rate (Sensitivity or Recall)(TPR) and False Positive Rate(FPR) at different thresholds. AUC helps in determining whether a model is good at distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. AUC of 0.5 means the model is making random guesses(random model) while AUC of 0 means that the model is predicting 0s as 1s and 1s as 0s whereas AUC of 1 is that of a perfect classifier.", "Here is a very good article on AUC.", "We will first delve deep into the dataset and observe all the features through visualizations which will aid us in getting conclusions as well as ideas for feature engineering. There are 370 features in total which are all anonymized (not including the \u2018TARGET\u2019 feature). We were able to get what information an anonymized feature convey from literature review and also by observing distribution of the values.", "Since there were many features, we will check for features having zero variance (does not hold any information ie only 1 unique values present) and will remove them if found.", "34 features were found to have zero variance and these were all removed.", "We will now check for features that were duplicated (ie they shared same values across datapoints need not be the same feature name).", "12 features were found to be duplicated and of this 6 were removed.", "We will now remove all sparse features (features that have little information). A condition is set such that if feature has 99 percentile value to be 0 then it is considered as a sparse feature.", "No missing values were also found.", "The final number of features after removing unnecessary features are 142.", "Now we will explore each feature.", "We can see that the dataset is highly imbalanced with only 3.96% being of unsatisfied customers and 96.04% being satisfied customers.", "From looking at other literature and also at the number of unique values present (=208) this feature may be holding the information of the region where the customer is from. There is also an outlier in this feature which is -999999 which may be missing value. The values ranged from 0 to 208 except for the outlier. The most common unique value is found to be 2 (around 97% for both train and test data). Firstly we will convert all -999999 to -1 values and explore the value counts considering only values which does not include \u20182\u2019.", "Since looking at the value counts we can see that -1 represent only a small fraction of the entire dataset (even after excluding 2). So we will replace the missing values with most frequent value for this feature which is \u20182\u2019.", "The minimum value of var15 is found to be 5 and maximum value is 105 in train data. Since the value of var15 is from 5\u2013105, it is safe to assume that this feature implies age of the customer. This is also evidenced in literature review.", "Both the train and test data had similar distribution, both consisted of younger customers the most.", "The minimum age of unsatisfied customers was found to be 23. It seems that every young customer (below the age of 23) are satisfied always. So we can create a new feature during feature engineering which essentially tells whether a customer is below the age of 23 or not.", "From the various literature in kaggle it was inferred that var38 might be the mortgage values.", "Here we are not able to get any information as one value is having very high frequency. We will print out each percentile values.", "We can see that there is a huge difference between 0 percentile value and 10 percentile value. This is the same case for 90 percentile value and 100 percentile value. Now we will check the distribution plot of var38 for all values that are below 97.5 percentile (above 97.5 percentile value 1 value is dominating, cannot extract usable information)", "We can see that the above plot is right skewed with a outlandish peak between 100,000 and 150,000. We can apply log transformation and check the resulting distribution.", "We can see that it looks similar to a normal distribution. Since machine learning models work better on normal distribution, we can create new datasets with log transformation and without log transformation for this feature and compare the performances during feature engineering phase.", "There were several features which had specific keywords in them which were: \u2018imp\u2019, \u2018ind\u2019, \u2018num\u2019, \u2018saldo\u2019 . These keywords were found to be short-forms for some spanish words which were obtained from several literature.", "\u2018num\u2019 features had the highest representation in the dataset.", "Now what we will do is explore each keyword feature 1 by 1 by randomly selecting 2 features for each keyword and we will draw conclusions.", "\u2018imp\u2019 likey is shortened word for importe which is Spanish for amount(inferred from literature review). The total number of features with \u2018imp\u2019 keyword are 14. By looking at the value counts of the two randomly selected feature that had \u2018imp\u2019 keyword in them, we can see that the most common occurring value is 0 for both of them. 0 accounted for more than 87% for both of these features.", "Here also after log transformation, the distribution(excluding 0) becomes a lot like guassian. So we can put down a strategy for all \u2018imp\u2019 features.", "The number of features with saldo keyword are 26. Now we will approach the data similar to how we approached \u2018imp\u2019 feature. By looking at the value counts for the 2 features we can see 0 is the most frequent value here also (around 95% of the entire train dataset).", "All \u2018saldo\u2019 features were found to be continuous.", "Here also (as similiar to \u2018imp\u2019 feature), we can create new datasets with log transformation applied to saldo features except for 0 values which will be kept as it is.", "\u2018num\u2019 may be short form for numerical. Exploring the value counts of all the \u2018num\u2019 features we can say that every feature with \u2018num\u2019 keyword are categorical in nature.", "The maximum number of unique values found for a feature with \u2018num\u2019 keyword is 172(total datapoints is around 70k) and minimum was 2. We will employ a strategy for \u2018num\u2019 feature:", "\u2018ind\u2019 feature may refer to indicator. After looking at the value counts all the features with \u2018ind\u2019 feature had only 2 unique values.", "We are leaving these features as it is.", "From literature review it was understood that the number of occurrences of zeros across different features for a particular datapoint is an important feature for recognizing whether a customer is unsatisfied or not. So we will create features to the preprocessed data as shown below:", "no_zeros: number of zero values present across different features for a datapoint", "no_nonzeros: number of non-zero values present across different features for a datapoint", "Now we will add a feature which calculates number of zeros and non-zeros across different keyword features (\u2018saldo\u2019 , \u2018ind\u2019, \u2018num\u2019, \u2018imp\u2019):", "Now we will take average values for all \u2018saldo\u2019 and \u2018imp\u2019 features for each unique values of all features that have between 50(exclusive) and 210(inclusive) unique values and add it as feature.", "K-means Clustering: We will add K-means cluster values for k = 2,4,6,8,10 as features after applying standardization for this particular step. Note: The features that we obtain after this step is not standardized since we need to apply log transformation. We are applying standardization for getting K-means clustering features only.", "Removing highly correlated features and low correlated features with TARGET: We will remove all features that are highly correlated with one another keeping one. We will remove all features that have low correlation with \u201cTARGET\u201d feature. For removing high correlated features, the threshold is kept as any feature above 0.95 correlation value while for removing low correlation values with \u201cTARGET\u201d the threshold is any feature below 10**-3 correlation value.", "Now we will create new datasets with log transformations. We will apply log transformation to features \u2018var38\u2019, all \u2018saldo\u2019 and \u2018imp\u2019 features except for zero values which will be kept as it is.", "Now we have two datasets one is log transformed and the other is normal dataset (without any log transformation). Now we will apply one hot encoding and response encoding to these and create new datasets. Here the encoding is applied to those features where the number of unique values is between 3 and 10 (both inclusive). Here is a good article on how response encoding is computed. For response encoding I have used laplace smoothing to smoothen the data ( suppose a category value is there in the test data which has not been seen in train data, we do not want the probability of that unique value to go zero so we apply smoothing). The best alpha was found by manually checking the response encoded values of a randomly selected feature from a set of alphas by how varied the resultant encoded values were.", "Finally 6 datasets are created which are :", "Now we will standardize all the datasets. After standardizing we will apply principal component analysis (here n=2) and add it as features to every dataset.", "Initially we had only 142 features for train and test data. Now we have created 6 datasets and the final number of features is as follows:", "For my approach, I have created 6 datasets which is thoroughly explained in Feature Engineering part and will model on these 6 different datasets.", "I have 6 datasets, so what I am going to do is for each dataset I will model on Logistic Regression, Decision Trees, Random Forest, XGBoost and LightGBM. First the dataset was split randomly with stratification for \u2018TARGET\u2019 (ensures same proportion of unique \u2018TARGET\u2019 values in both train and test) with 85:15 split. After modelling on the dataset, I selected top features based on the random forest model and then created new dataset with top 250 features for each dataset. Then Random Forest, XGBoost and LightGBM models were created based on the dataset with top 250 features. The best hyper-parameters is found by using Random Search CV for Logistic Regression, Decision Trees and Random Forest.", "Here class_weight = \u2018balanced\u2019 takes care of class imbalance. For Decision trees and Random Forest, after fitting on the model with best hyper-parameters, calibrated classifier is fitted on top of these models (to get probability values).", "Here model is either Decision Tree Classifier or Random Forest Classifier.", "For XGBoost and LightGBM, I manually changed each parameters and found the best one based on the below article. Using RandomSearchCV for these two models consumed a lot of time.", "Below is the code for XGBoost, similar is done for LightGBM (except for objective where in LightGBM it becomes \u2018binary\u2019).", "Combined AUC plots for different datasets are shown below:", "We then select the best model from each dataset based on validation AUC for ensembling and stacking. We do not take the best models out of all models because for stacking and ensembling larger the difference between the models (this ensures that we take models from different datasets otherwise models trained on same dataset will appear), the better will be the performance of the final ensemble/stacking classifier(each model will be an expert on different topics ie for eg: if we built a dog classifier model we can built a classifier such that one model will detect tail, another model will detect face etc, so combining these models will give a better performance,this is what I mean by different models). So larger the difference between the datasets, larger the difference between the models.", "The best models obtained are then trained on the entire train dataset provided(we split the data before) and then are used to predict the test \u201cTARGET\u201d probability values. The 6 \u201cTARGET\u201d obtained is then used as features for creating a new dataframe. A classifier is trained on top of this dataframe (stacking). Here the classifier used is Logistic Regression (hyperparameters found through RandomSearch CV) and the test probability values of \u201cTARGET\u201d is submitted in Kaggle. The 6 test probability values obtained from the 6 best models are also submitted and their public AUC scores are recorded. Then based on the public AUC score obtained best 2 models (here it was \u2018log re(top 250) xgb\u2019 and \u2019normal re(top 250)) are selected and then simple average ensembling is done on the probability values. The values obtained are then submitted on Kaggle and the public AUC score is recorded.", "We can see from the result dataframe above that XGB model fitted on Log transformed Response Encoded (top 250 features) dataset and XGB model fitted on Normal response encoded (top 250 features) performed very well. The simple average ensembling of these 2 models were able to achieve a kaggle public AUC score of 0.82746 which was just around 2% below the top score in public leaderboard (0.84532). The xgb model trained on Log transformed (all features) performed poorly as it had the worst public score. Response encoding the categorical features improved the models a lot since models trained on other dataset models did not produce great results compared to models trained on the response encoded datasets.", "And also from the feature importance plot of the two models of the ensemble used, we can see that in both the models, the common feature which had highest importance was \u2018var15\u2019(represents \u2018age\u2019 inferred from literature review) which implies that age is an important factor that determines whether a customer is satisfied or not.", "You can find the complete code for this case study on this github link. You can connect me on Linkedin or at ashishthomas7@gmail.com.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5776d3f8b060&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ashishthomaschempolil.medium.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": ""}, {"url": "https://ashishthomaschempolil.medium.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Ashish Thomas Chempolil"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a080999c1f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&user=Ashish+Thomas+Chempolil&userId=9a080999c1f0&source=post_page-9a080999c1f0----5776d3f8b060---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ashishthomaschempolil/Santander-Customer-Satisfaction", "anchor_text": "github link"}, {"url": "https://www.linkedin.com/in/ashishthomas7/", "anchor_text": "Linkedin"}, {"url": "https://unsplash.com/@garrettpsystems?utm_source=medium&utm_medium=referral", "anchor_text": "garrett parker"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/c/santander-customer-satisfaction", "anchor_text": "Kaggle competition"}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "Understanding AUC - ROC CurveIn Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can\u2026towardsdatascience.com"}, {"url": "https://medium.com/@thewingedwolf.winterfell/response-coding-for-categorical-data-7bb8916c6dc1", "anchor_text": "response encoding"}, {"url": "https://beta.vu.nl/nl/Images/werkstuk-elsen_tcm235-865964.pdf", "anchor_text": "Derek van den Elsen. (2017). Santander Customer Satisfaction"}, {"url": "https://github.com/kweonwooj/kaggle_santander_customer_satisfaction/tree/master/34_wpppj", "anchor_text": "kweonwooj\u2019s re-implementation of pjpan\u2019s 34th place solution(in R code) in Python"}, {"url": "https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e", "anchor_text": "Fine-tuning XGBoost in Python like a bossXGBoost (or eXtreme Gradient Boosting) is not to be introduced anymore, proved relevant in only too many data science\u2026towardsdatascience.com"}, {"url": "https://github.com/ashishthomaschempolil/Santander-Customer-Satisfaction", "anchor_text": "github link"}, {"url": "https://www.linkedin.com/in/ashishthomas7/", "anchor_text": "Linkedin"}, {"url": "https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course", "anchor_text": "https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course"}, {"url": "https://beta.vu.nl/nl/Images/werkstuk-elsen_tcm235-865964.pdf", "anchor_text": "Derek van den Elsen, Santander Customer Satisfaction(2017)"}, {"url": "https://github.com/kweonwooj/kaggle_santander_customer_satisfaction/tree/master/34_wpppj", "anchor_text": "kweonwooj\u2019s re-implementation of pjpan\u2019s 34th place solution(in R code) in Python"}, {"url": "https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e", "anchor_text": "https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e"}, {"url": "https://towardsdatascience.com/mercari-price-recommendation-for-online-retail-sellers-979c4d07f45c?gi=5873f2d314af", "anchor_text": "https://towardsdatascience.com/mercari-price-recommendation-for-online-retail-sellers-979c4d07f45c?gi=5873f2d314af"}, {"url": "https://medium.com/@thewingedwolf.winterfell/response-coding-for-categorical-data-7bb8916c6dc1", "anchor_text": "https://medium.com/@thewingedwolf.winterfell/response-coding-for-categorical-data-7bb8916c6dc1"}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5776d3f8b060---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5776d3f8b060---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----5776d3f8b060---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/customer-satisfaction?source=post_page-----5776d3f8b060---------------customer_satisfaction-----------------", "anchor_text": "Customer Satisfaction"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&user=Ashish+Thomas+Chempolil&userId=9a080999c1f0&source=-----5776d3f8b060---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&user=Ashish+Thomas+Chempolil&userId=9a080999c1f0&source=-----5776d3f8b060---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5776d3f8b060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5776d3f8b060---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5776d3f8b060--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5776d3f8b060--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5776d3f8b060--------------------------------", "anchor_text": ""}, {"url": "https://ashishthomaschempolil.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ashishthomaschempolil.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashish Thomas Chempolil"}, {"url": "https://ashishthomaschempolil.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a080999c1f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&user=Ashish+Thomas+Chempolil&userId=9a080999c1f0&source=post_page-9a080999c1f0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa7abfae88bbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsantander-customer-satisfaction-a-self-case-study-using-python-5776d3f8b060&newsletterV3=9a080999c1f0&newsletterV3Id=a7abfae88bbc&user=Ashish+Thomas+Chempolil&userId=9a080999c1f0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}