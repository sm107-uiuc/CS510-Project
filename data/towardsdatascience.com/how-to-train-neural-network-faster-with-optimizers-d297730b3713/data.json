{"url": "https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713", "time": 1682993917.609104, "path": "towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713/", "webpage": {"metadata": {"title": "How to train Neural Network faster with optimizers? | by Piotr Skalski | Towards Data Science", "h1": "How to train Neural Network faster with optimizers?", "description": "As I worked on the last article, I had the opportunity to create my own neural network using only Numpy. It was a very challenging task, but at the same time it significantly broadened my\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795", "anchor_text": "article", "paragraph_index": 0}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "articles", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "vectorization", "paragraph_index": 10}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter", "paragraph_index": 23}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium", "paragraph_index": 23}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub", "paragraph_index": 23}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "other articles", "paragraph_index": 23}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai", "paragraph_index": 24}], "all_paragraphs": ["As I worked on the last article, I had the opportunity to create my own neural network using only Numpy. It was a very challenging task, but at the same time it significantly broadened my understanding of the processes that take place inside the NN. Among others, this experience made me truly realize how many factors influence neural net\u2019s performance. Selected architecture, proper hyperparameter values or even correct initiation of parameters, are just some of those things... This time however, we will focus on the decision that has a huge impact on learning process speed, as well as the accuracy of obtained predictions \u2014 the choice of the optimization strategy. We will look inside many popular optimizers, investigate how they work and compare them with each other.", "Note: Due to quite a large range of material that I want to cover, I decided not to include any code snippets in this article. Remember however, that as usual, you can find all the code used to create visualizations on GitHub. I have also prepared a few notebooks that will allow you to better understand the discussed issues.", "Optimization is a process of searching for parameters that minimize or maximize our functions. When we train machine learning model, we usually use indirect optimization. We choose a certain metric \u2014 like accuracy, precision or recall \u2014 which indicates how well our model solves a given problem. However, we are optimizing a different cost function J(\u03b8) and hope that minimizing its value will improve metric we care about. Of course, the choice of the cost function is usually related to the specific problem that we aim to tackle. Essentially, it\u2019s deliberately designed to indicate how far from an ideal solution we are. As you can imagine, this topic is quite complicated and is a great topic for a separate article.", "It turns out though, that very often finding a minimum of non-convex cost functions is not easy, and we have to use advanced optimization strategies to locate them. If you have learned differential calculus, you certainly know the concept of local minimum \u2014 these are some of the biggest traps that our optimizer can fall into. For those who have not yet come across this wonderful part of the mathematics, I will only say that these are the points where function takes a minimum value, but only in a given region. An example of such a situation is shown on the left hand side of the figure above. We can clearly see that the point located by the optimizer is not the most optimal solution overall.", "Overcoming the so-called saddle points is often considered to be even more challenging. These are plateaus, where the value of the cost function is almost constant. This situation is illustrated on the right-hand side of the same figure. At these points the gradient is almost zeroed in all directions making it impossible to escape.", "Sometimes, especially in the case of multi-layer networks, we may have to deal with regions where the cost function is very steep. In such places the value of the gradient drastically increases \u2014 the exploding gradient \u2014 what leads to taking huge steps, often ruining the entire previous optimization. However, this problem can be easily avoided by gradient clipping \u2014 defining the maximum allowed gradient value.", "Before we get to know more advanced algorithms, let\u2019s take a look at some of basic strategies. Probably one of the most straightforward approaches is to simply go in direction opposite to the gradient. This strategy can be described by the following equation.", "Where \u03b1 is a hyperparameter called learning rate, which translates into the length of the step that we will take in each iteration. Its selection expresses \u2014 in a certain extent \u2014 a compromise between the speed of learning and the accuracy of the result that can be obtained. Choosing too small step leads us to tedious calculations and the necessity of performing much more iterations. On the other hand, however, choosing too high value can effectively prevent us from finding the minimum. Such a situation is presented in the Figure 2 \u2014 we can see how in subsequent iterations we bounce around, not being able to stabilize. Meanwhile, the model where appropriate step was defined, found at a minimum almost immediately.", "In addition, this algorithm is vulnerable to previously described problems of saddle points. Since the size of the correction performed in subsequent iterations is proportional to the calculated gradient, we will not be able to escape from the plateau.", "Finally, to top it all off, the algorithm is ineffective \u2014 it requires the use of the entire training set in each iteration. This means that in every epoch we have to look at all the examples in order to perform next optimisation step. This may not be a problem when the training set consists of several thousand examples, but as I mentioned in one of my articles \u2014 neural networks work best when they have millions of records at their disposal. In that case, it is hard to imagine that in every iteration we use the whole set. It would be a waste of both time and computer memory. All these factors cause that gradient descent, in its purest form, does not work in most cases.", "Let\u2019s first try to deal with the last of the problems mentioned in the previous chapter \u2014 inefficiency. Although vectorization allows us to accelerate calculations, by handling many training examples at once, when the dataset has millions of records the whole process will still take long time to complete.. Let\u2019s try to use a rather trivial solution \u2014 divide the whole dataset set into smaller batches and use them for training in subsequent iterations. Our new strategy has been visualised on the animation above. Imagine that the contours drawn on the left symbolize the cost function that we want to optimize. We can see that due to the much smaller amount of data that we need to process, our new algorithm makes decisions much faster. Let\u2019s also pay attention to the contrast in the movement of the compared models. The gradient descent takes rare, relatively large steps with little noise. On the other hand, batch-size gradient descent takes steps more often, but due to the possible diversity in the analyzed data sets, we have much more noise. It is even possible that in one of the iterations we will move in the opposite direction to the intended one. On average, however, we move towards the minimum.", "I know what you are thinking about\u2026. How to select a batch size? As is often the case in deep learning, the answer is not definitive and depends on the specific case. If the size of our batch is equal to the whole dataset, then we are essentially dealing with an ordinary gradient descent. On the other hand, if the size is 1, then in each iteration we are using only one example from our dataset, as a consequence losing the benefits of vectorisation. This approach is sometimes justified, and an example of a strategy that uses it is stochastic gradient descent. It is done by selecting random dataset records and using them as a training set in subsequent iterations. However, if we decide to use the mini-batch, we usually choose an intermediate value \u2014 typically selected from the range of 64 to 512 examples.", "This idea is used in many areas such as statistics, economics and even deep learning. Many advanced NN optimization algorithms use this concept, because it allows us to continue optimization even if the gradient calculated at the given point is zero. Let\u2019s take a few moments to understand this algorithm. As an example we will use the share value of the biggest tech company from the last year.", "EWA is essentially about averaging many previous values in order to become independent of local fluctuations and focus on the overall trend. It\u2019s value is calculated using recursive formula shown above, where \u03b2 is the parameter used to control the range of values to be averaged. We can say that in subsequent iterations we taken into account 1/(1 - \u03b2) examples. For large values of \u03b2 the graph we get is much smoother, because we average many records. On the other hand, our chart moves more and more to the right, because when we average over a large period of time, the EWA adapts more slowly to the new trend. This can be seen in Figure 5, where we illustrated the actual value of shares at closing as well as 4 graphs showing the value of exponentially weighted averages calculated for different beta parameters.", "This strategy leverages exponentially weighted averages to avoid troubles with points where gradient of cost functions is close to zero. In simple words, we allow our algorithm to gain momentum, so that even if the local gradient is zero, we still move forward relying on the previously calculated values. For this reason, it is almost always a better choice than a pure gradient descent.", "As usual we use back propagation, to calculate the value of dW and db for each layer of our network. This time however, instead of directly using the calculated gradient to update the values of our NN parameters, we first calculate the intermediate values of VdW and Vdb. These values are in fact EWA of derivatives of the cost function in respect of individual parameters. Finally, we will use VdW and Vdb in gradient descent. The whole process can be described with following equations. It is also worth noting that the implementation of this method requires to store EWA values between iterations. You can see the details in one of the notebooks on my repository.", "Let\u2019s now try to develop a certain intuition about the influence of EWA on the behaviour of our model. Once again, let\u2019s imagine that the contours drawn below symbolize the cost function that we optimize. The visualization above shows a comparison between standard gradient descent and GD with momentum. We can see that the shape of the graph of the cost function forces a very slow way of optimization. As in the example of stock market prices, the use of exponentially weighted averages allows us to focus on the leading trend instead of the noise. Component indicating minimum is amplified and component responsible for the oscillations is slowly eliminated. What\u2019s more, if we obtain gradients pointing a similar direction in subsequent updates, the learning rate will increase. This leads to faster convergence and reduced oscillations. This method, however, has a disadvantage \u2014 as you approach the minimum, the momentum value increases and may become so large that the algorithm will not be able to stop at the right place.", "Another way to improve the performance of gradient descent is to apply the RMSProp strategy \u2014 Root Mean Squared Propagation \u2014 which is one of the most frequently used optimizers. This is another algorithm that uses exponentially weighted averages. Moreover, it is adaptive \u2014 it allows for individual adjustment of the learning rate for each parameter of the model. Subsequent parameter values are based on previous gradient values calculated for particular parameter.", "Using the example from Figure 6. and the equations above, let us consider the logic behind this strategy. According to the name, in each iteration we calculate element-wise squares of derivates of the cost function in respect of the corresponding parameters. In addition, we use EWA to average the values obtained in recent iterations. Finally, before we update the values of our network parameters, the corresponding gradient components are divided by the square root of the sum of squares. This means that the learning rate is reduced faster for parameters where the gradient is large and slower for parameters where the gradient is small. In this way, our algorithm reduces oscillations and prevents noise them from dominating the signal. In order to avoid dividing by zero (numerical stability), we also add to the denominator a very small value, marked in referred formulas as \u025b.", "I must admit that while writing this article I had two wow-moments \u2014 moments when I was really shocked by the quality leap between the optimizers I analysed. The first one was when I saw the difference in training time between standard gradient descent and mini-batch. The second now, when I compared the RMSprop with everything we have seen so far. However, this method has its drawbacks. As the denominator of the above equations increases in each iteration, our learning rate is getting smaller. As a consequence, this may cause our model to stop completely.", "Last but not least, let me tell you about Adaptive Moment Estimation. It is an algorithm which, like RMSProp, works well in a wide range of applications. It takes advantage of the biggest pros of RMSProp, and combine them with ideas known from momentum optimization. The result is a strategy that allows for quick and effective optimization. The above figure shows how well discussed optimizers handle optimization of a difficult part of the function. You can immediately see that Adam is doing very well in this case.", "Unfortunately, as the effectiveness of our method increases, the complexity of calculations grows as well. It\u2019s true\u2026. Above, I have written ten matrix equations that describe single iteration of optimization process . I know from experience that some of you \u2014 especially those less familiar with mathematicians \u2014 will now be in a grave mood. Don\u2019t worry! Note that there is nothing new there. These are the same equations that were previously written for momentum and RMSProp optimizers. This time we will simply use both ideas at once.", "I hope that I have managed to explain these difficult issues in a transparent way. Work on this post allowed me to grasp how important it is to choose the right optimizer. Understanding of these algorithms allows to use them consciously, Understanding how changes in individual hyperparameters affect the performance of the whole model.", "Congratulations if you managed to get here. It was certainly not the easiest reading. If you like this article follow me on Twitter and Medium and see other projects I\u2019m working on, on GitHub and Kaggle. This article is the fourth part of the \u201cMysteries of Neural Networks\u201d series, if you haven\u2019t had the opportunity yet, read the other articles. And if you have interesting ideas for the next posts, write in the comments. Stay curious!", "ML Growth Engineer @ Roboflow / Founder @ makesense.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd297730b3713&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://skalskip.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Piotr Skalski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----d297730b3713---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd297730b3713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=-----d297730b3713---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd297730b3713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=-----d297730b3713---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795", "anchor_text": "article"}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub"}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "articles"}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "vectorization"}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter"}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium"}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle"}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "other articles"}, {"url": "https://github.com/SkalskiP", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795", "anchor_text": ""}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d297730b3713---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d297730b3713---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d297730b3713---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d297730b3713---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d297730b3713---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd297730b3713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=-----d297730b3713---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd297730b3713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=-----d297730b3713---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd297730b3713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----d297730b3713---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd74b5c17627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&newsletterV3=11b65705ec0&newsletterV3Id=fd74b5c17627&user=Piotr+Skalski&userId=11b65705ec0&source=-----d297730b3713---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Written by Piotr Skalski"}, {"url": "https://skalskip.medium.com/followers?source=post_page-----d297730b3713--------------------------------", "anchor_text": "4.7K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----d297730b3713---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd74b5c17627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-neural-network-faster-with-optimizers-d297730b3713&newsletterV3=11b65705ec0&newsletterV3Id=fd74b5c17627&user=Piotr+Skalski&userId=11b65705ec0&source=-----d297730b3713---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Gentle Dive into Math Behind Convolutional Neural NetworksMysteries of Neural Networks Part V"}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "12 min read\u00b7Apr 12, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=-----79a07dd44cf9----0-----------------clap_footer----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----d297730b3713----0---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "22"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=-----d297730b3713----0-----------------bookmark_preview----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d297730b3713----1---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----d297730b3713----1-----------------bookmark_preview----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d297730b3713----2---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d297730b3713----2-----------------bookmark_preview----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "Let\u2019s code a Neural Network in plain NumPyMysteries of Neural Networks Part III"}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": "10 min read\u00b7Oct 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae7e74410795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-code-a-neural-network-in-plain-numpy-ae7e74410795&user=Piotr+Skalski&userId=11b65705ec0&source=-----ae7e74410795----3-----------------clap_footer----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----d297730b3713----3---------------------a276681f_96e3_4089_b988_7abbcd161bf5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "47"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae7e74410795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-code-a-neural-network-in-plain-numpy-ae7e74410795&source=-----d297730b3713----3-----------------bookmark_preview----a276681f_96e3_4089_b988_7abbcd161bf5-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "See all from Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----d297730b3713----0-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----d297730b3713----1-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d297730b3713----0---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d297730b3713----0-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d297730b3713----1---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----d297730b3713----1-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/@iamleonie?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/@iamleonie?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Leonie Monigatti"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "A Visual Guide to Learning Rate Schedulers in PyTorchLR decay and annealing strategies for Deep Learning in Python"}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "\u00b79 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24bbb262c863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----24bbb262c863----2-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863?source=read_next_recirc-----d297730b3713----2---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24bbb262c863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863&source=-----d297730b3713----2-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://matthewmacfarquhar.medium.com/predicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://matthewmacfarquhar.medium.com/?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://matthewmacfarquhar.medium.com/?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Matthew MacFarquhar"}, {"url": "https://matthewmacfarquhar.medium.com/predicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "Predicting airline passengers using LSTM and TensorflowIn this article, we will go over what an RNN is, what an LSTM is, and what a GRU is. We will then use an LSTM to train a model on flight\u2026"}, {"url": "https://matthewmacfarquhar.medium.com/predicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": "6 min read\u00b7Nov 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fab86347cf318&operation=register&redirect=https%3A%2F%2Fmatthewmacfarquhar.medium.com%2Fpredicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318&user=Matthew+MacFarquhar&userId=5b4bcd924433&source=-----ab86347cf318----3-----------------clap_footer----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://matthewmacfarquhar.medium.com/predicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318?source=read_next_recirc-----d297730b3713----3---------------------b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab86347cf318&operation=register&redirect=https%3A%2F%2Fmatthewmacfarquhar.medium.com%2Fpredicting-airline-passengers-using-lstm-and-tensorflow-ab86347cf318&source=-----d297730b3713----3-----------------bookmark_preview----b51f8bf4_92b1_4042_9c59_00b42d0ecfe7-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d297730b3713--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----d297730b3713--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}