{"url": "https://towardsdatascience.com/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25", "time": 1683002122.109613, "path": "towardsdatascience.com/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25/", "webpage": {"metadata": {"title": "Process Wikipedia Using Apache Spark to Create Spicy Hot Datasets | by Abhishek Mungoli | Towards Data Science", "h1": "Process Wikipedia Using Apache Spark to Create Spicy Hot Datasets", "description": "It must have happened to most of us that, whenever we want to know about some country\u2019s history, monuments, TV Series & movies, celebrity\u2019s life & their career, incidents from the past, or current\u2026"}, "outgoing_paragraph_urls": [{"url": "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://github.com/attardi/wikiextractor", "anchor_text": "Python package", "paragraph_index": 5}, {"url": "https://spark-in.me/post/parsing-wikipedia-in-four-commands-for-nlp", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Apache_Spark", "anchor_text": "Apache Spark", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/MapReduce", "anchor_text": "MapReduce", "paragraph_index": 9}, {"url": "https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/", "anchor_text": "link", "paragraph_index": 10}, {"url": "https://github.com/samread81/Wiki-Celebrity-DataSet", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium", "paragraph_index": 23}, {"url": "https://github.com/samread81/Wiki-Celebrity-DataSet", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "paragraph_index": 26}], "all_paragraphs": ["It must have happened to most of us that, whenever we want to know about some country\u2019s history, monuments, TV Series & movies, celebrity\u2019s life & their career, incidents from the past, or current events, our first choice is Wikipedia. Smart people use it to acquire a wide variety of knowledge and sound even smarter. But, has it ever crossed your mind how vast is it? How many documents/articles it has?", "Wikipedia currently has about 5.98 million articles, which keeps on increasing every day. There are a wide variety of articles from all fields. We can use this data to create many interesting applications. What would be your next interesting application if you had access to this data? How do you get this entire dataset? Even if you get it, what would be the amount of computing needed to process it?", "All of these questions will be answered.", "In this article, I want to create a dataset of celebrities. All the popular people from past or present who have Wikipedia pages on them, like Virat Kohli, Sachin Tendulkar, Ricky Ponting from Cricket, Brad Pitt, Leonardo DiCaprio, Amitabh Bachchan from Movies, Physicists like Albert Einstein, Isacc Newton, etc. I will use Apache Spark (PySpark) to process this massive Dataset.", "To get started with the task, first, we need the Wikipedia dump. The Wikipedia dump can be downloaded in XML format from here. It keeps on refreshing and contains the latest Wikipedia data.", "Next, this downloaded XML can be parsed very easily using a freely available Python package. More details on how to use the package can be found here.", "Once the parsing is complete, this is how the parsed Directory structure will look.", "Each directory consists of multiple plain text files. Snapshot of the file content", "Each file contains multiple Wikipedia articles. Each article starts with <doc> tag and ends with </doc>. The next step is to go through all these files and filter out the Articles corresponding to Celebrities. If we use a single machine to do that, it may take even days to complete since it is a compute-extensive task. We will utilize the Distributed Systems framework, Apache Spark (PySpark) to perform the task which will just take a few 10 to 15 minutes, with a reasonable amount of executors.", "Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application.", "This section is optional. This section is for Spark Beginners or for those who want to quickly brush up some Spark Transformation functions before proceeding. To read more about these functions, please go through this link.", "First of all, we will, get all the data transferred in the HDFS and read the data using wholeTextFiles.", "The output will have key-value paired RDD, where the key is the file path and content being the value. We can get rid of the file path and just deal with the content. Each file content contains multiple Wikipedia articles separated by <doc> tag. We need to get all these articles out as a separate record. We will use a flatMap() function for that.", "Once we have all the articles, we need to find what the article is about. Each article consists of the first line being the title and the rest is the content. We can use this information to convert each article into a key-value pair where key being the title and value being the content.", "Next, the step is to filter out only articles corresponding to people (celebrities). Before, writing some logic for it, let\u2019s see how some of the Celebrity pages look.", "Let\u2019s also have a look at some non-celebrities page.", "What makes a celebrity page different from the non-celebrities page? What\u2019s common in all the celebrities pages?", "Most of the celebrity\u2019s pages in the first sentence contains the date of birth. This date of birth is in either of these two formats in the Wikipedia data:", "We can utilize this fact to quickly filter out all celebrity pages. I will use Regex to find this format.", "Finally, we can save the output as a table. The complete code looks like:", "There are a total of around 5.98 million articles in Wikipedia. Our celebrities dataset has 1.38 million articles. The list of all the celebrities and datasets obtained along with the code can be found here.", "The dataset contains articles about Michael Jackson, Amitabh Bachchan, Brad Pitt, Sachin Tendulkar, MS Dhoni and all other celebrities we could think of and verify.", "Wikipedia is one of the best go-to places for all possible information on the Internet. We can use it for creating many interesting applications, your next big and interesting NLP project. With the use of Apache Spark, processing this massive Data becomes an easy task. With hardly 20\u201325 lines of code we can create most of the interesting Datasets out of it.", "It takes a lot of effort to write a good post with clarity and easy understandability for the audience. I will keep trying to do justice with my work. Follow me up at Medium and check out my previous posts. I welcome feedback and constructive criticism. The list of all the celebrities and datasets obtained along with the code can be found here.", "My Youtube channel for more content:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Lead Data Scientist at Meesho | Ex-Walmart | IIIT-Hyderabad | NERIST | Insta: simplyspartanx | Youtube: https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a59720e6e25&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de----1a59720e6e25---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2", "anchor_text": "here"}, {"url": "https://github.com/attardi/wikiextractor", "anchor_text": "Python package"}, {"url": "https://spark-in.me/post/parsing-wikipedia-in-four-commands-for-nlp", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Apache_Spark", "anchor_text": "Apache Spark"}, {"url": "https://en.wikipedia.org/wiki/MapReduce", "anchor_text": "MapReduce"}, {"url": "https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/", "anchor_text": "link"}, {"url": "https://github.com/samread81/Wiki-Celebrity-DataSet", "anchor_text": "here"}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium"}, {"url": "https://github.com/samread81/Wiki-Celebrity-DataSet", "anchor_text": "here"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "Abhishek MungoliHi Guys, Welcome to the channel. The channel aims to cover various topics from Machine Learning, Data Science\u2026www.youtube.com"}, {"url": "https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm", "anchor_text": "https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm"}, {"url": "https://spark.apache.org/", "anchor_text": "https://spark.apache.org/"}, {"url": "https://en.wikipedia.org/wiki/Apache_Spark", "anchor_text": "https://en.wikipedia.org/wiki/Apache_Spark"}, {"url": "https://en.wikipedia.org/wiki/MapReduce", "anchor_text": "https://en.wikipedia.org/wiki/MapReduce"}, {"url": "https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/", "anchor_text": "https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/"}, {"url": "https://data-flair.training/blogs/spark-rdd-tutorial/", "anchor_text": "https://data-flair.training/blogs/spark-rdd-tutorial/"}, {"url": "https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/", "anchor_text": "https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/"}, {"url": "https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/", "anchor_text": "https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/"}, {"url": "https://blog.softhints.com/python-regex-match-date/", "anchor_text": "https://blog.softhints.com/python-regex-match-date/"}, {"url": "https://medium.com/tag/big-data?source=post_page-----1a59720e6e25---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a59720e6e25---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1a59720e6e25---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1a59720e6e25---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----1a59720e6e25---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----1a59720e6e25---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----1a59720e6e25---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a59720e6e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a59720e6e25---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a59720e6e25--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a59720e6e25--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a59720e6e25--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/@mungoliabhishek81/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8d41b74042bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprocess-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25&newsletterV3=5f15354d23de&newsletterV3Id=8d41b74042bf&user=Abhishek+Mungoli&userId=5f15354d23de&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}