{"url": "https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281", "time": 1682994093.915787, "path": "towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281/", "webpage": {"metadata": {"title": "An implementation guide to Word2Vec using NumPy and Google Sheets | by Sudo Chia | Towards Data Science", "h1": "An implementation guide to Word2Vec using NumPy and Google Sheets", "description": "This article is an implementation guide to Word2Vec using NumPy and Google Sheets. If you you have trouble reading this, consider subscribing to Medium Membership here! Word2Vec is touted as one of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://derekchia.medium.com/membership", "anchor_text": "Medium Membership here", "paragraph_index": 0}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "Gensim", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "TensorFlow", "paragraph_index": 1}, {"url": "https://github.com/tmikolov/word2vec", "anchor_text": "Tomas Mikolov", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://github.com/DerekChia/word2vec_numpy", "anchor_text": "code", "paragraph_index": 2}, {"url": "https://docs.google.com/spreadsheets/u/3/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing", "anchor_text": "Google Sheet", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embedding", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis", "anchor_text": "distributional hypothesis", "paragraph_index": 5}, {"url": "https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures", "anchor_text": "this", "paragraph_index": 6}, {"url": "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html", "anchor_text": "an excellent article", "paragraph_index": 14}, {"url": "https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess", "anchor_text": "gensim.utils.simple_preprocess", "paragraph_index": 14}, {"url": "http://www.aclweb.org/anthology/D14-1162", "anchor_text": "have diminishing benefit", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax", "paragraph_index": 27}, {"url": "https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "cosine similarity", "paragraph_index": 35}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 39}, {"url": "https://fasttext.cc", "anchor_text": "fastText", "paragraph_index": 39}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 39}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "Doc2Vec", "paragraph_index": 39}, {"url": "https://code2vec.org/", "anchor_text": "Code2Vec", "paragraph_index": 39}, {"url": "https://medium.com/u/fdf264797c2a?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Ren Jie Tan", "paragraph_index": 40}, {"url": "https://twitter.com/remykarem", "anchor_text": "Raimi", "paragraph_index": 40}, {"url": "http://seowyuxin.com", "anchor_text": "Yuxin", "paragraph_index": 40}], "all_paragraphs": ["This article is an implementation guide to Word2Vec using NumPy and Google Sheets. If you you have trouble reading this, consider subscribing to Medium Membership here!", "Word2Vec is touted as one of the biggest, most recent breakthrough in the field of Natural Language Processing (NLP). The concept is simple, elegant and (relatively) easy to grasp. A quick Google search returns multiple results on how to use them with standard libraries such as Gensim and TensorFlow. Also, for the curious minds, check out the original implementation using C by Tomas Mikolov. The original paper can be found here too.", "The main focus on this article is to present Word2Vec in detail. For that, I implemented Word2Vec on Python using NumPy (with much help from other tutorials) and also prepared a Google Sheet to showcase the calculations. Here are the links to the code and Google Sheet.", "The objective of Word2Vec is to generate vector representations of words that carry semantic meanings for further NLP tasks. Each word vector is typically several hundred dimensions and each unique word in the corpus is assigned a vector in the space. For example, the word \u201chappy\u201d can be represented as a vector of 4 dimensions [0.24, 0.45, 0.11, 0.49] and \u201csad\u201d has a vector of [0.88, 0.78, 0.45, 0.91].", "The transformation from words to vectors is also known as word embedding. The reason for this transformation is so that machine learning algorithm can perform linear algebra operations on numbers (in vectors) instead of words.", "To implement Word2Vec, there are two flavors to choose from \u2014 Continuous Bag-Of-Words (CBOW) or continuous Skip-gram (SG). In short, CBOW attempts to guess the output (target word) from its neighbouring words (context words) whereas continuous Skip-Gram guesses the context words from a target word. Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word.", "According to Mikolov (quoted in this article), here is the difference between Skip-gram and CBOW:", "Skip-gram: works well with small amount of the training data, represents well even rare words or phrases", "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words", "To elaborate further, since Skip-gram learns to predict the context words from a given word, in case where two words (one appearing infrequently and the other more frequently) are placed side-by-side, both will have the same treatment when it comes to minimising loss since each word will be treated as both the target word and context word. Comparing that to CBOW, the infrequent word will only be part of a collection of context words used to predict the target word. Therefore, the model will assign the infrequent word a low probability.", "In this article, we will be implementing the Skip-gram architecture. The content is broken down into the following parts for easy reading:", "To begin, we start with the following corpus:", "natural language processing and machine learning is fun and exciting", "For simplicity, we have chosen a sentence without punctuation and capitalisation. Also, we did not remove stop words \u201cand\u201d and \u201cis\u201d.", "In reality, text data are unstructured and can be \u201cdirty\u201d. Cleaning them will involve steps such as removing stop words, punctuations, convert text to lowercase (actually depends on your use-case), replacing digits, etc. KDnuggets has an excellent article on this process. Alternatively, Gensim also provides a function to perform simple text preprocessing using gensim.utils.simple_preprocess where it converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long.", "After preprocessing, we then move on to tokenising the corpus. Here, we tokenise our corpus on whitespace and the result is a list of words:", "Before we jump into the actual implementation, let us define some of the hyperparameters we need later.", "[window_size]: As mentioned above, context words are words that are neighbouring the target word. But how far or near should these words be in order to be considered neighbour? This is where we define the window_sizeto be 2 which means that words that are 2 to the left and right of the target words are considered context words. Referencing Figure 3 below, notice that each of the word in the corpus will be a target word as the window slides.", "[n]: This is the dimension of the word embedding and it typically ranges from 100 to 300 depending on your vocabulary size. Dimension size beyond 300 tends to have diminishing benefit (see page 1538 Figure 2 (a)). Do note that the dimension is also the size of the hidden layer.", "[epochs]: This is the number of training epochs. In each epoch, we cycle through all training samples.", "[learning_rate]: The learning rate controls the amount of adjustment made to the weights with respect to the loss gradient.", "In this section, our main objective is to turn our corpus into a one-hot encoded representation for the Word2Vec model to train on. From our corpus, Figure 4 zooms into each of the 10 windows (#1 to #10) as shown below. Each window consists of both the target word and its context words, highlighted in orange and green respectively.", "Example of the first and last element in the first and last training window is shown below:", "To generate the one-hot training data, we first initialise the word2vec() object and then using the object w2v to call the function generate_training_data by passing settings and corpus as arguments.", "Inside the function generate_training_data, we performed the following operations:", "With our training_data, we are now ready to train our model. Training starts with w2v.train(training_data) where we pass in the training data and call the function train.", "The Word2Vec model consists of 2 weight matrices (w1 and w2) and for demo purposes, we have initialised the values to a shape of (9x10) and (10x9) respectively. This facilitates the calculation of backpropagation error which will be covered later in the article. In the actual training, you should randomly initialise the weights (e.g. using np.random.uniform()). To do that, comment line 9 and 10 and uncomment line 11 and 12.", "Next, we start training our first epoch using the first training example by passing in w_t which represents the one-hot vector for target word to theforward_pass function. In the forward_pass function, we perform a dot product between w1 and w_t to produce h(Line 24). Then, we perform another dot product using w2 and h to produce the output layer u(Line 26). Lastly, we run u through softmax to force each element to the range of 0 and 1 to give us the probabilities for prediction (Line 28) before returning the vector for predictiony_pred, hidden layer h and output layer u.", "I have attached some screenshots to show the calculation for the first training sample in the first window (#1) where the target word is \u2018natural\u2019 and context words are \u2018language\u2019 and \u2018processing\u2019. Feel free to look into the formula in the Google Sheet here.", "Error \u2014 With y_pred, h and u, we proceed to calculate the error for this particular set of target and context words. This is done by summing up the difference between y_pred and each of the context words inw_c.", "Backpropagation \u2014 Next, we use the backpropagation function, backprop, to calculate the amount of adjustment we need to alter the weights using the function backprop by passing in error EI, hidden layer h and vector for target word w_t.", "To update the weights, we multiply the weights to be adjusted (dl_dw1 and dl_dw2) with learning rate and then subtract it from the current weights (w1 and w2).", "Loss \u2014 Lastly, we compute the overall loss after finishing each training sample according to the loss function. Take note that the loss function comprises of 2 parts. The first part is the negative of the sum for all the elements in the output layer (before softmax). The second part takes the number of the context words and multiplies the log of sum for all elements (after exponential) in the output layer.", "Now that we have completed training for 50 epochs, both weights (w1 and w2) are now ready to perform inference.", "With a trained set of weights, the first thing we can do is to look at the word vector for a word in the vocabulary. We can simply do this by looking up the index of the word against the trained weight (w1). In the following example, we look up the vector for the word \u201cmachine\u201d.", "Another thing we can do is to find similar words. Even though our vocabulary is small, we can still implement the function vec_sim by computing the cosine similarity between words.", "If you are still reading the article, well done and thank you! But this is not the end. As you might have noticed in the backpropagation step above, we are required to adjust the weights for all other words that were not involved in the training sample. This process can take up a long time if the size of your vocabulary is large (e.g. tens of thousands).", "To solve this, below are the two features in Word2Vec you can implement to speed things up:", "Beyond that, why not try tweaking the code to implement the Continuous Bag-of-Words (CBOW) architecture? \ud83d\ude03", "This article is an introduction to Word2Vec and into the world of word embedding. It is also worth noting that there are pre-trained embeddings available such as GloVe, fastText and ELMo where you can download and use directly. There are also extensions of Word2Vec such as Doc2Vec and the most recent Code2Vec where documents and codes are turned into vectors. \ud83d\ude09", "Lastly, I want to thank to Ren Jie Tan, Raimi and Yuxin for taking time to comment and read the drafts of this. \ud83d\udcaa", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write regularly on topics in data engineering, machine learning and generally tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F13445eebd281&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sudochia.medium.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": ""}, {"url": "https://sudochia.medium.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Sudo Chia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F292465db773e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&user=Sudo+Chia&userId=292465db773e&source=post_page-292465db773e----13445eebd281---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://derekchia.medium.com/membership", "anchor_text": "Medium Membership here"}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "Gensim"}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "TensorFlow"}, {"url": "https://github.com/tmikolov/word2vec", "anchor_text": "Tomas Mikolov"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "here"}, {"url": "https://github.com/DerekChia/word2vec_numpy", "anchor_text": "code"}, {"url": "https://docs.google.com/spreadsheets/u/3/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing", "anchor_text": "Google Sheet"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embedding"}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis", "anchor_text": "distributional hypothesis"}, {"url": "https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures", "anchor_text": "this"}, {"url": "http://idli.group/Natural-Language-Processing-using-Vectoriziation.html", "anchor_text": "IDIL"}, {"url": "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html", "anchor_text": "an excellent article"}, {"url": "https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess", "anchor_text": "gensim.utils.simple_preprocess"}, {"url": "http://www.aclweb.org/anthology/D14-1162", "anchor_text": "have diminishing benefit"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax"}, {"url": "https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1411.2738.pdf", "anchor_text": "https://arxiv.org/pdf/1411.2738.pdf"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "cosine similarity"}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "Skip-gram Negative Sampling (SGNS)"}, {"url": "https://becominghuman.ai/hierarchical-softmax-as-output-activation-function-in-neural-network-1d19089c4f49", "anchor_text": "Hierarchical Softmax"}, {"url": "https://en.wikipedia.org/wiki/Huffman_coding", "anchor_text": "Huffman coding tree"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://fasttext.cc", "anchor_text": "fastText"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "Doc2Vec"}, {"url": "https://code2vec.org/", "anchor_text": "Code2Vec"}, {"url": "https://medium.com/u/fdf264797c2a?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Ren Jie Tan"}, {"url": "https://twitter.com/remykarem", "anchor_text": "Raimi"}, {"url": "http://seowyuxin.com", "anchor_text": "Yuxin"}, {"url": "https://derekchia.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets/", "anchor_text": "https://derekchia.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets/"}, {"url": "https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py", "anchor_text": "nathanrooy/word2vec-from-scratch-with-pythonA very simple, bare-bones, inefficient, implementation of skip-gram word2vec from scratch with Python \u2026github.com"}, {"url": "https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/", "anchor_text": "Word2vec from Scratch with Python and NumPyTL;DR - word2vec is awesome, it's also really simple. Learn how it works, and implement your own version. Since joining\u2026nathanrooy.github.io"}, {"url": "https://stats.stackexchange.com/questions/325053/why-word2vec-maximizes-the-cosine-similarity-between-semantically-similar-words", "anchor_text": "Why word2vec maximizes the cosine similarity between semantically similar wordsThanks for contributing an answer to Cross Validated! Some of your past answers have not been well-received, and you're\u2026stats.stackexchange.com"}, {"url": "https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08", "anchor_text": "Hierarchical softmax and negative sampling: short notes worth tellingThanks to unexpected and very pleasant attention the audience has paid to my last (and the only) post here dedicated to\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----13445eebd281---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----13445eebd281---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----13445eebd281---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----13445eebd281---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----13445eebd281---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&user=Sudo+Chia&userId=292465db773e&source=-----13445eebd281---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&user=Sudo+Chia&userId=292465db773e&source=-----13445eebd281---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F13445eebd281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----13445eebd281---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----13445eebd281--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----13445eebd281--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----13445eebd281--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----13445eebd281--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----13445eebd281--------------------------------", "anchor_text": ""}, {"url": "https://sudochia.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sudochia.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sudo Chia"}, {"url": "https://sudochia.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "452 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F292465db773e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&user=Sudo+Chia&userId=292465db773e&source=post_page-292465db773e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Febb4117ebb3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281&newsletterV3=292465db773e&newsletterV3Id=ebb4117ebb3c&user=Sudo+Chia&userId=292465db773e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}