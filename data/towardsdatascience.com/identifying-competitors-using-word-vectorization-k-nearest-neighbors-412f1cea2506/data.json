{"url": "https://towardsdatascience.com/identifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506", "time": 1683002037.415532, "path": "towardsdatascience.com/identifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506/", "webpage": {"metadata": {"title": "Identifying Competitors Using Word Vectorization & K-Means Clustering | by Raymond Willey | Towards Data Science", "h1": "Identifying Competitors Using Word Vectorization & K-Means Clustering", "description": "Yelp provides a wellspring of information for businesses to take advantage of when it comes to analyzing sales and service performance relative to competitors. Taking advantage of that data is a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.yelp.com/biz/chanos-cantina-astoria", "anchor_text": "Chano\u2019s Cantina", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/getting-started-with-the-yelp-api-ac30915a77ae", "anchor_text": "Yelp API", "paragraph_index": 1}, {"url": "https://docs.python.org/3/library/ast.html", "anchor_text": "Abstract Syntax Trees", "paragraph_index": 9}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Global Vectors for Word Representation", "paragraph_index": 11}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "download it here", "paragraph_index": 15}, {"url": "http://www.nltk.org/api/nltk.stem.html", "anchor_text": "stemming and lemmatization", "paragraph_index": 20}, {"url": "https://www.merriam-webster.com/thesaurus/gastropubs", "anchor_text": "syllables", "paragraph_index": 21}, {"url": "https://www.merriam-webster.com/dictionary/gastropubs", "anchor_text": "dictionary definition", "paragraph_index": 21}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "Beautiful Soup", "paragraph_index": 22}, {"url": "https://blogs.oracle.com/datascience/introduction-to-k-means-clustering", "anchor_text": "this introduction", "paragraph_index": 30}], "all_paragraphs": ["Yelp provides a wellspring of information for businesses to take advantage of when it comes to analyzing sales and service performance relative to competitors. Taking advantage of that data is a whole different animal. I want to create an algorithm that allows small businesses to do that with relative ease.", "In this post, we look at how to identify a business\u2019s most relevant competitors using categorical tags and pricing with word vectorization and k-means clustering. For illustrative purposes, we will use one of my favorite local restaurants in New York City: Chano\u2019s Cantina. Using the Yelp API, I obtained the top 50 search results for Mexican restaurants & bars in Astoria and saved them to a dataframe.", "Our objective is to determine how Chano\u2019s compares to its competitors, identifying what the restaurant does well and where there is room for improvement. However, trying to compare one restaurant to 49 potential competitors is likely to introduce so much noise that it becomes difficult to extract any meaningful results. Instead, we want to identify 5\u201310 competitors that are most similar to Chano\u2019s in terms of price and product offering. How do we figure out which restaurants are most similar in this regard? K-means clustering!", "Let\u2019s have a look at the Yelp page for Chano\u2019s and see what information might be best suited for the task at hand.", "From above, we see a price estimate with two dollar signs, and a couple of tags for Cocktail Bars and New Mexican Cuisine. Naturally, we want to compare Chano\u2019s to similar establishments, so we would ideally want to find restaurants with the same tags and price range. The problem is that most restaurants are not going to be an exact match, so we want to get as close as possible.", "If we look back at our API results, we see that the relevant information is in the columns categories and price. Note that we are not using the rating variable because this will be our target later. Let\u2019s clear out some of the clutter to get a better sense of what we are dealing with.", "Here we have 50 records, and we want to whittle it down to less than 10 using the categories and price data. Two problems:", "Price is the more straightforward point to deal with: we can replace the value with an integer representing the number of dollar signs. We can then fill null values with the mean price rounded to zero decimal places.", "Regarding the categorical tags, we need Python to read the string value for each business as code so that we can isolate the information we need. For example, the categories for Chano\u2019s Cantina are saved as:", "We want Python to interpret this string as a list of dictionaries rather than read it as a string. Fortunately, we can do this quite easily by using the Abstract Syntax Trees library and calling the literal_eval method, then saving the result as a variable. From there, it is just a matter of iterating through each record to get the title words, remove punctuation, and convert all words to lowercase. We can combine these results with the price value and save the results to a column called tags. The code, along with the relevant output for Chano\u2019s, is below.", "We can convert these outputs to lists of word tokens using the Natural Language Toolkit (NLTK).", "You may be asking yourself why we are not creating dummy variables for the categories instead. This approach would be a more straightforward way to go; however, it would also lead to an interpretation of categories being uniquely discrete. The problem with this approach is that tags such as \u201cMexican\u201d and \u201cNew Mexican Cuisine\u201d are very similar. By using dummy variables, we would miss this nuance. For this reason, we will use the Global Vectors for Word Representation (GloVe) to interpret the categorical tags.", "If you aren\u2019t familiar, word vectorization is a way of assigning meaning to words with numerical representation in higher-dimensional space. What does that mean? Well, let\u2019s say we wanted to compare animals based on how fluffy and how friendly they are, and we put them on a scale of 1\u201310. A Pomeranian might get a score of 10 for being very fluffy, and a 9 for being very friendly. (My wife informed me that Pomeranians aren\u2019t particularly friendly, but let\u2019s pretend they are!)", "On the other hand, an alligator would probably get a score of 1 for both ratings. We can represent these words visually on a graph:", "Using these dimensions, we can represent each animal as a vector, with the first value representing fluffiness, and the second representing friendliness. What\u2019s more, we can imagine that there might be other animals that exist at different points on this line. Can you think of an animal that is fluffier and friendlier than an alligator, but not as fluffy or friendly as a Pomeranian? How about a baboon? If we calculate the average of the two vectors, we can figure out where to place the baboon on the graph. We can represent this process algorithmically as follows:", "The output represents the coordinates of where to put the baboon on the chart. We can expand on this concept infinitely to describe all sorts of words and understand their relationships to one another. With such limitless possibilities, where would one start? Fortunately, we don\u2019t have to answer that question because Stanford University already built a model that contains word vectors for 6 billion words in the English language: this is GloVe. Each word is represented as a vector of 50\u2013300 dimensions, and it is free to access for use in your own models. You can download it here; keep in mind the file is a whopping 822 megabytes.", "The zip file contains four text files encoded in UTF-8 format. Each file contains the same corpus of words, with each line representing the vector for that word. The difference between files is the number of dimensions: 50, 100, 200, and 300. For our purposes, we\u2019ll use 50 dimensions, so we start by saving the file glove.6B.50d.txt to the same path as our Jupyter Notebook.", "The process of assigning vectors to unique words is quite simple.", "With our dictionary of relevant vectors in place, we now want to apply them to the specific sets of tags for each business, getting the mean vector for each company. We can create a class to do this for us, applying it to the tags column using the map function.", "The class takes in a dictionary of words and vectors and provides the mean vector for any list of words by calling the transform method.", "With 6 billion tokens for over 400 thousand words, it is hard to imagine that a word would not be found in GloVe. However, we did come across one in this example: gastropubs. Interestingly, the singular version of this word is present, so the use of stemming and lemmatization was the first potential solution to this problem. However, the word was not found in the NLTK library, either. Time to get creative!", "My next thought was to look for syllables using Merriam-Webster, but none exist. The simplest solution would be to plug-in zero vectors for the missing word, or to remove it from all tags, but I didn\u2019t want to lose the information. Instead, we can get the dictionary definition from Merriam-Webster, and apply a similar method as above to get the average vector of the word\u2019s definition.", "This task starts with some web scraping using Beautiful Soup. We will go into the topic of web scraping in more detail as part of the next piece, so, for now, I\u2019ll just show you the code to get the definition for any word from Merriam-Webster\u2019s website.", "With this function in place, we can search for words that were not found in GloVe, get the tokenized definitions, then use our W2vVectorizer class to get an approximate vector for the words in question.", "One thing to keep in mind is that the mean of vectors returns values in higher-dimensional space. However, we want to return a value of the same shape as all other words, which is handled by the bolded line of code.", "With all of our tags properly vectorized, we can now move onto clustering to determine the most direct competitors to Chano\u2019s.", "There are plenty of blog posts that go into detail on the mechanics of unsupervised learning with k-means clustering, so I\u2019m not going to rehash all of it here. The main thing to know is that k-means clustering is an unsupervised learning method that groups data points by how similar they are.", "To illustrate this, let\u2019s go back to our alligator and pomeranian comparison. This time, we\u2019ll add a few other animals to the chart. The objective of k-means clustering is to identify the appropriate groupings of a random set of animals.", "Here, we can see that a frog is a bit friendlier than an alligator, but it\u2019s not at all fluffier. A Pug isn\u2019t quite as fluffy as a Pomeranian, but it is is a bit friendlier. And then you have the honey badger and tarantula, which are a little fluffy, but not particularly friendly. Based on their scores, k-means clustering aims to figure out how best to group them.", "The task seems simple enough, but what if I told you it was necessary to break these up into two groups instead of three? Or how about four groups instead? We can imagine that the task becomes more complicated if we don\u2019t know the optimal number of groups.", "The objective for us is to identify the 5\u201310 competitors most similar to Chano\u2019s based on their Yelp tags and price estimate. For the rest of this analysis, we will focus on the implementation of k-means clustering. Still, if you aren\u2019t familiar with the mechanics of the algorithm, I would recommend having a look at this introduction from Oracle.", "There are two conventional approaches to finding the optimal number of clusters for a k-means algorithm:", "With each approach, we build multiple k-means algorithms for a set range of groups, with the number of groups represented by the letter k. In this case, we\u2019ll set a range of 2\u201310 groups, or clusters. Using the Elbow Method, the algorithm estimates the center of each cluster, then measures the average distance each point in the cluster is from the center. Taking the average of these distances gives us the Within-Cluster-Sum of Squared Errors (WCSS). We can plot the WCSS as the number of clusters increases, looking for the point where the WCSS levels off.", "To illustrate this, we generate a random set of points to represent 100 different [theoretical] animals and plot them below.", "We can see that there are three distinct clusters. Let\u2019s build k-means algorithms for 2\u201310 clusters and see what happens to the WCSS.", "We can see that at 3 clusters, the WCSS reaches a valley. Intuitively this makes sense, but the distinction is not always clear, making interpretation somewhat subjective. More importantly, we don\u2019t want to have to make a visual inspection every time we run the algorithm. Instead, we will use the Silhouette Method.", "With the Silhouette Method, we look at the average distance between points in a cluster, which is known as cohesion. This number is compared to the average distance between groups, which is a measure of separation. In very simplified terms, we look for the number of clusters where the difference between separation and cohesion is maximized. As with the Elbow Method, we can plot these points.", "Here again, we can see the optimal number of clusters is three because that is where the Silhouette Score is maximized. Unlike the Elbow Method, however, we get a clear, maximum value that can be easily identified programmatically.", "With our GloVe vectors in place and an approach for finding the optimal number of clusters, we can now identify the competitors most similar to Chano\u2019s. As you may have inferred already from the above, we will need to implement the algorithm multiple times over. Fortunately, this is the simplest part of the process.", "Step 1: Import the K-Means algorithm.", "Step 2: Convert tags to vectors.", "Step 3: Find the optimal number of clusters.", "Step 4: Assign group labels and save to dataframe.", "Step 5: Filter businesses that are not in the same group as Chano\u2019s.", "Step 6: Repeat steps two through five with the new dataframe until the number of records is less than ten.", "Results: Below, we can see the essential data for Chano\u2019s and its main competitors.", "Now that we have the main competitors of Chano\u2019s properly identified, we can start diving into the reviews. Looking at the review_count column, we can see that we have almost 1,000 reviews to parse through, but the Yelp API has strict limits on how many reviews can be accessed. To get around this will require some heavy lifting with web scraping using Beautiful Soup, but that\u2019s a topic for another day.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist in the consulting industry with expertise in Machine Learning, Deep Learning, and Hypothesis Testing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F412f1cea2506&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----412f1cea2506--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rwilleynyc?source=post_page-----412f1cea2506--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rwilleynyc?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Raymond Willey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F75f6c61ec59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&user=Raymond+Willey&userId=75f6c61ec59a&source=post_page-75f6c61ec59a----412f1cea2506---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.yelp.com/biz/chanos-cantina-astoria", "anchor_text": "Chano\u2019s Cantina"}, {"url": "https://towardsdatascience.com/getting-started-with-the-yelp-api-ac30915a77ae", "anchor_text": "Yelp API"}, {"url": "https://www.yelp.com/biz/chanos-cantina-astoria", "anchor_text": "https://www.yelp.com/biz/chanos-cantina-astoria"}, {"url": "https://docs.python.org/3/library/ast.html", "anchor_text": "Abstract Syntax Trees"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Global Vectors for Word Representation"}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "download it here"}, {"url": "http://www.nltk.org/api/nltk.stem.html", "anchor_text": "stemming and lemmatization"}, {"url": "https://www.merriam-webster.com/thesaurus/gastropubs", "anchor_text": "syllables"}, {"url": "https://www.merriam-webster.com/dictionary/gastropubs", "anchor_text": "dictionary definition"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "Beautiful Soup"}, {"url": "https://blogs.oracle.com/datascience/introduction-to-k-means-clustering", "anchor_text": "this introduction"}, {"url": "https://medium.com/tag/nlp?source=post_page-----412f1cea2506---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----412f1cea2506---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----412f1cea2506---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----412f1cea2506---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----412f1cea2506---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&user=Raymond+Willey&userId=75f6c61ec59a&source=-----412f1cea2506---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&user=Raymond+Willey&userId=75f6c61ec59a&source=-----412f1cea2506---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F412f1cea2506&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----412f1cea2506---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----412f1cea2506--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----412f1cea2506--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----412f1cea2506--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----412f1cea2506--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----412f1cea2506--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rwilleynyc?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rwilleynyc?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raymond Willey"}, {"url": "https://medium.com/@rwilleynyc/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "242 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F75f6c61ec59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&user=Raymond+Willey&userId=75f6c61ec59a&source=post_page-75f6c61ec59a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd23d9777b77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentifying-competitors-using-word-vectorization-k-nearest-neighbors-412f1cea2506&newsletterV3=75f6c61ec59a&newsletterV3Id=fd23d9777b77&user=Raymond+Willey&userId=75f6c61ec59a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}