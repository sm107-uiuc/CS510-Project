{"url": "https://towardsdatascience.com/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af", "time": 1683009488.732679, "path": "towardsdatascience.com/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af/", "webpage": {"metadata": {"title": "Overview of Reinforcement Learning Algorithms | Towards Data Science", "h1": "A Structural Overview of Reinforcement Learning Algorithms", "description": "A structural overview of Actor Critic, Policy Gradient, DQN, VFA, SARSA, Q-learning, Model-based and Model-free Monte Carlo, Dynamic Programming"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "markov decision process(MDP)", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998", "anchor_text": "here", "paragraph_index": 55}], "all_paragraphs": ["Reinforcement learning has gained tremendous popularity in the last decade with a series of successful real-world applications in robotics, games and many other fields.", "In this article, I will provide a high-level structural overview of classic reinforcement learning algorithms. The discussion will be based on their similarities and differences in the intricacies of algorithms.", "Let\u2019s start with a quick refresher on some basic concepts. If you are already familiar with all the terms of RL, feel free to skip this section.", "Reinforcement learning models are a type of state-based models that utilize the markov decision process(MDP). The basic elements of RL include:", "Episode(rollout): playing out the whole sequence of state and action until reaching the terminate state;", "Current state s (or st): where the agent is current at;", "Next state s\u2019 (or st+1): next state from the current state;", "Action a: the action to take at state s;", "Transition probability P(s\u2019|s, a): the probability of reaching s\u2019 if taking action at at state st;", "Policy \u03c0(s, a): a mapping from each state to an action that determines how the agent acts at each state. It can be either deterministic or stochastic", "Reward r (or R(s, a)): a reward function that generates rewards for taking action a at state s;", "Return Gt: total future rewards at state st;", "Value V(s): expected return for starting from state s;", "Q value Q(s, a): expected return for starting from state s and taking action a;", "According to the Bellman equation, the current value is equal to current reward plus the discounted(\u03b3) value at the next step, following the policy \u03c0. It can also be expressed using the Q value as:", "This is the theoretical core in most reinforcement learning algorithms.", "There are two fundamental tasks of reinforcement learning: prediction and control.", "In prediction tasks, we are given a policy and our goal is to evaluate it by estimating the value or Q value of taking actions following this policy.", "In control tasks, we don\u2019t know the policy, and the goal is to find the optimal policy that allows us to collect most rewards. In this article, we will only focus on control problems.", "Below is a graph I made to visualize the high-level structure of different types of algorithms. In the next few sections, we will delve into the intricacies of each type.", "In the MDP world, we have a mental model of how the world works, meaning that we know the MDP dynamics (transition P(s\u2019|s,a) and reward function R(s, a)), so we can directly build a model using the Bellman equation.", "Again, in control tasks our goal is to find a policy that gives us maximum rewards. To achieve it, we use dynamic programming.", "Policy iteration essentially performs two steps repeatedly until convergence: policy evaluation and policy improvement.", "In the policy evaluation step, we evaluate the policy \u03c0 at state s by calculating the Q value using the Bellman equation:", "In the policy improvement step, we update the policy by greedily searching for the action that maximizes the Q value at each step.", "Let\u2019s see how policy iteration works.", "Value iteration combines the two steps in policy iteration so we only need to update the Q value. We can interpret value iteration as always following a greedy policy because at each step it always tries to find and take the action that maximizes the value. Once the values converge, the optimal policy can be extracted from the value function.", "In most real-world scenarios, we don\u2019t know the MDP dynamics so the applications of iterative methods are limited. In the next section, we will switch gears and discuss reinforcement learning methods that can deal with the unknown world.", "In most cases, the MDP dynamics are either unknown, or computationally infeasible to use directly, so instead of building a mental model we learn from sampling. In all the following reinforcement learning algorithms, we need to take actions in the environment to collect rewards and estimate our objectives.", "In MDP models, we can explore all potential states before we come up with a good solution using the transition probability function. However, in reinforcement learning where the transition is unknown, if we keep greedily searching for the best action, we might end up stuck in a few states without being able to explore the entire environment. This is the exploration-exploitation dilemma.", "To get out of the suboptimal states, we usually use a strategy called epsilon greedy: when we select the best action, there is a probability of \u03b5 that we might get a random action.", "One way to estimate the MDP dynamics is sampling. Following a random policy, we sample many (s, a, r, s\u2019) pairs and use Monte Carlo(counting the occurrences) to estimate the transition and reward functions explicitly from the data. If the data size is large enough, our estimates should be very close to the true values.", "Once we have the estimates, we can use iterative methods to search for the optimal policy.", "Let\u2019s take a step back. If our goal is to just find good policies, all we need is to get a good estimate of Q. From that perspective, estimating the model (transitions and rewards) was just a means towards an end. Why not just cut to the chase and estimate Q directly?", "Recall that Q(s,a) is the expected utility when the agent takes action a from state s.", "The idea of model-free Monte Carlo is to sample many rollouts, and use the data to estimate Q. Let\u2019s take a look at the algorithm.", "We first randomly initialize everything and use epsilon greedy to sample an action, and then we start to play rollouts. At the end of each rollout, we calculate the return Gt for each state St in the rollouts. To get Q(st,at), the average of returns Gt, we can store all the returns and update Q when we finish sampling. However, a more efficient way is to update Q incrementally at the end of each rollout using a moving average as is shown below.", "SARSA is a Temporal Difference (TD) method, which combines both Monte Carlo and dynamic programming methods. The update equation has the similar form of Monte Carlo\u2019s online update equation, except that SARSA uses rt + \u03b3Q(st+1, at+1) to replace the actual return Gt from the data. N(s, a) is also replaced by a parameter \u03b1.", "Recall that in Monte Carlo, we need to wait for the episode to finish before we can update the Q value. The advantage of TD methods is that they can update the estimate of Q immediately when we move one step and get a state-action pair (st, at, rt, st+1, at+1).", "Q-learning is another type of TD method. The difference between SARSA and Q-learning is that SARSA is an on-policy model while Q-learning is off-policy. In SARSA, our return at state st is rt + \u03b3Q(st+1, at+1), where Q(st+1, at+1) is calculated from the state-action pair (st, at, rt, st+1, at+1) that was obtained by following policy \u03c0. However, in Q-learning, Q(st+1, at+1) is obtained by taking the optimal action, which might not necessarily be the same as our policy.", "In general, on-policy methods are more stable but off-policy methods are more likely to find global optima. We can see from below that except the update equation, the other parts of the algorithm are the same as SARSA.", "So far, we have been assuming we can represent the value function V or state-action value function Q as a tabular representation (vector or matrix). However, many real world problems have enormous state and/or action spaces for which tabular representation is insufficient. Naturally, we might wonder if we can parameterize the value function so we don\u2019t have to store a table.", "VFA is the very method that represents the Q value function with a parameterized function Q hat.", "The state and action are represented as a feature vector x(s, a), and the estimated Q hat is the score of the linear predictor.", "The objective is to minimize the loss between the estimated Q(prediction) and real Q(target), and we can use stochastic gradient descent to solve this optimization problem.", "How do we get our target \u2014 the real Q value in the objective function?", "Recall the Q value is the expected returns (Gt), so one way to get Q value is to use Monte Carlo: playing many episodes and counting the occurrences.", "We have the following objective functions and gradients for parameterized Monte Carlo:", "Another way is to utilize the recursive expression of Q value: Q(st, at) = rt + \u03b3Q(st+1, at+1). As we discussed earlier, Temporal Difference (TD) methods combine both Monte Carlo and dynamic programming and allow real-time update. Hence we can also obtain the target Q value using TD methods: SARSA and Q-learning.", "Notice that in the above TD methods, we are actually using the model prediction to approximate the real target value Q(st+1, at+1), and this type of optimization is called semi-gradient.", "Linear VFA often works well if we have the right set of features, which usually require careful hand designing. An alternative is to use deep neural networks that directly use the states as input without requiring an explicit specification of features.", "For example, in the graph below we have a neural network with the state s as the input layer, 2 hidden layers, and the predicted Q values as the output. The parameters can be learned through backpropagation.", "There are two important concepts in DQN: target net and experience replay.", "As you may have realized, a problem of using semi-gradient is that the model updates could be very unstable since the real target will change each time the model updates itself. The solution is to create a target network that copies the training model at a certain frequency so the target model updates less frequently. In equation below, w- are the weights of the target network.", "We also create an experience replay buffer that stores the (s, a, r, s\u2019, a\u2019) pairs from prior episodes. When we update the weights, instead of using the most recent pair generated from the episode, we randomly select an experience from the experience replay buffer to run stochastic gradient descent. This will help avoid overfitting.", "I have previously implemented DQN with Tensorflow to play the CartPole game. If you are interested in learning more about the implementation, check out my article here.", "Different from the previous algorithms that model the value function, policy gradient methods directly learn the policy by parameterizing it as:", "However, when it comes to optimization we still have to go back to the value function as the policy function can\u2019t be used as an objective function by itself. Our objective can be expressed as the value function V(\u03b8), which is the expected total rewards we get from trajectories \u03c4 following stochastic policy \u03c0. Here \u03b8 are parameters for the policy.Be careful not to misinterpret V(\u03b8) as the parameterized value function.", "Where \u03c4 is a state-action trajectory:", "R(\u03c4) is sum of rewards for a trajectory \u03c4:", "Now, the goal is to find the policy parameters (\u03b8) that maximize the value V(\u03b8). To do so, we search for the maxima in V(\u03b8) by ascending the gradient of the policy, w.r.t parameters \u03b8.", "In the gradients shown as follows, the policy \u03c0\u03b8 is usually modeled using softmax, Gaussian or neural networks to ensure it\u2019s differentiable.", "Let\u2019s see what policy gradient is like.", "Actor-critic methods differ from the policy gradient methods in that actor-critic methods estimate both policy and the value function, and update both. In policy gradient methods, we update \u03b8 using Gt, an estimation of the value function at st from a single rollout. Although this estimation is unbiased, it has high variance.", "To address this issue, Actor-critic methods introduce bias using bootstrapping and function approximation. Instead of using Gt from the roll out, we estimate the value with a parameterized function, and this is where the critic comes in.", "Here is \u201cvanilla\u201d actor-critic policy gradient:", "In the above procedure, two new terms are introduced: advantage(At ) and baseline(b(s)).", "b(t) is the expected total future rewards at state St, equivalent to V(St), and this is the value function the critic estimates.", "After every episode, we update both the value function b(s) and the policy function. Different from that in policy gradient, the policy function here is updated with At instead of Gt, which helps reduce variance of the gradient", "On top of the vanilla actor-critic, there are two popular actor-critic methods A3C and A2C that update the policy and value functions with multiple workers. Their main difference is that A3C performs asynchronous updates while A2C is synchronous.", "In this article, we had an overview of many classic and popular reinforcement learning algorithms, and discussed their similarities and differences in the intricacies.", "It\u2019s worthwhile to mention that there are a lot of variants in each model family that we\u2019ve not covered. For example, in the DQN family, there are dueling DQN and double DQN. In the policy gradient the actor-critic families, there are DDPG, ACER, SAC etc.", "Additionally, there is another type of RL methods: evolution strategies(ES). Inspired by the theory of natural selection, ES solves problems when there isn\u2019t a precise analytic form of an objective function. As they are beyond the scope of MDP, I didn\u2019t include them in this article but I might have a discussion in my future articles. Stay tuned! :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff79c8b87e5af&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://siwei-xu.medium.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": ""}, {"url": "https://siwei-xu.medium.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Siwei Causevic"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F301dc9114da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&user=Siwei+Causevic&userId=301dc9114da0&source=post_page-301dc9114da0----f79c8b87e5af---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "markov decision process(MDP)"}, {"url": "https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998", "anchor_text": "here"}, {"url": "https://web.stanford.edu/class/cs234/slides/", "anchor_text": "https://web.stanford.edu/class/cs234/slides/"}, {"url": "https://lilianweng.github.io/lil-log/", "anchor_text": "https://lilianweng.github.io/lil-log/"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f79c8b87e5af---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/markov-decision-process?source=post_page-----f79c8b87e5af---------------markov_decision_process-----------------", "anchor_text": "Markov Decision Process"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f79c8b87e5af---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f79c8b87e5af---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f79c8b87e5af---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&user=Siwei+Causevic&userId=301dc9114da0&source=-----f79c8b87e5af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&user=Siwei+Causevic&userId=301dc9114da0&source=-----f79c8b87e5af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff79c8b87e5af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f79c8b87e5af---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f79c8b87e5af--------------------------------", "anchor_text": ""}, {"url": "https://siwei-xu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://siwei-xu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Siwei Causevic"}, {"url": "https://siwei-xu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "381 Followers"}, {"url": "https://www.linkedin.com/in/siwei-causevic/", "anchor_text": "https://www.linkedin.com/in/siwei-causevic/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F301dc9114da0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&user=Siwei+Causevic&userId=301dc9114da0&source=post_page-301dc9114da0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3374c04371d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af&newsletterV3=301dc9114da0&newsletterV3Id=3374c04371d5&user=Siwei+Causevic&userId=301dc9114da0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}