{"url": "https://towardsdatascience.com/double-deep-q-networks-905dd8325412", "time": 1682997238.361362, "path": "towardsdatascience.com/double-deep-q-networks-905dd8325412/", "webpage": {"metadata": {"title": "Double Deep Q Networks. Tackling maximization bias in Deep\u2026 | by Chris Yoon | Towards Data Science", "h1": "Double Deep Q Networks", "description": "In this post, we will look into the motivation behind double Q-learning networks, and look at three different ways this has been done: If you aren\u2019t completely familiar with Q-learning, I would\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "anchor_text": "my post about Q-learning", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "\u201cAddressing Function Approximation Error in Actor-Critic Methods\u201d (Fujimoto et al., 2018)", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "anchor_text": "my Q-learning post", "paragraph_index": 16}], "all_paragraphs": ["In this post, we will look into the motivation behind double Q-learning networks, and look at three different ways this has been done:", "If you aren\u2019t completely familiar with Q-learning, I would recommend taking a quick look at my post about Q-learning!", "Taking the maximum overestimated values as such is implicitly taking the estimate of the maximum value. This systematic overestimation introduces a maximization bias in learning. And since Q-learning involves bootstrapping \u2014 learning estimates from estimates \u2014 such overestimation can be problematic.", "Here is an example: consider a single state s where the true Q value for all actions equal 0, but the estimated Q values are distributed some above and below zero. Taking the maximum of these estimates (which is obviously bigger than zero) to update the Q function leads to the overestimation of Q values.", "Hasselt et al. (2015) illustrates this overestimation bias in experiments across different Atari game environments:", "As we can see, traditional DQN tends to significantly overestimate action-values, leading to unstable training and low quality policy:", "The solution involves using two separate Q-value estimators, each of which is used to update the other. Using these independent estimators, we can unbiased Q-value estimates of the actions selected using the opposite estimator [3]. We can thus avoid maximization bias by disentangling our updates from biased estimates.", "Below, we will take a look at 3 different formulations of Double Q learning, and implement the latter two.", "The original Double Q-learning algorithm uses two independent estimates Q^{A}and Q^{B} . With a 0.5 probability, we use estimate Q^{A} to determine the maximizing action, but use it to update Q^{B} . Conversely, we use Q^{B} to determine the maximizing action, but use it to update Q^{A}. By doing so, we obtain an unbiased estimator Q^{A}(state, argmaxQ^{next state, action) for the expected Q value and inhibit bias.", "2. The updated version from the same author in \u201cDeep Reinforcement Learning with Double Q-learning\u201d (Hasselt et al., 2015),", "In the second Double Q-learning algorithm, we have a model Q and a target model Q\u2019 instead of two independent models, as in (Hasselt, 2010). We use the Q\u2019 for action selection and Q for action evaluation. That is:", "We minimize the mean squared error between Q and Q* , but we have Q' slowly copy the parameters of Q . We can do so by periodically hard-copying over the parameters, but also through Polyak averaging:", "where \u03b8\u2019 is the target network parameter, \u03b8 is the primary network parameter, and \u03c4 (rate of averaging) is usually set to 0.01.", "3. Clipped Double Q-learning, in \u201cAddressing Function Approximation Error in Actor-Critic Methods\u201d (Fujimoto et al., 2018).", "In Clipped Double Q-learning, we follow the original formulation of Hasselt 2015. We have two independent estimates of the true Q value. Here, for computing the update targets, we take the minimum of the two next-state action values produced by our two Q networks; When the Q estimate from one is greater than the other, we reduce it to the minimum, avoiding overestimation.", "Fujimoto et al. presents another benefit of this setting: the minimum operator should provide higher value to states with lower variance estimation error. This means that the minimization will lead to a preference for states with low-variance value estimates, leading to safer policy updates with stable learning targets.", "We will begin with the same DQN agent setup as Part 1 of this series. If you would like to see a more complete implementation of the setup, please see my Q-learning post or my Github Repository (link in the bottom).", "We will initialize a model and a target model:", "For computing the loss, we use our target model to compute our next Q values:", "And then we slowly copy/average the model parameters over to the target model parameters:", "For computing the loss, we compute the current-state-Q-values and the next-state-Q-values of both models, but use the minimum of the next-state-Q-values to compute the expected Q value. We then update both models using the expected Q value.", "This concludes our implementations of double Q-learning algorithms. Double Q-learning is frequently used in state-of-the-art Q-learning variants and Actor Critic methods. We will see this technique appear time and again in our later posts.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F905dd8325412&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----905dd8325412--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----905dd8325412--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----905dd8325412---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/Dqn-family", "anchor_text": "DQN Family"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "\u201cDouble Q-learning\u201d (Hasselt, 2010)"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "\u201cDeep Reinforcement Learning with Double Q-learning\u201d (Hasselt et al., 2015)"}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "\u201cAddressing Function Approximation Error in Actor-Critic Methods\u201d (Fujimoto et al., 2018)"}, {"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "anchor_text": "my post about Q-learning"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "\u201cDeep Reinforcement Learning with Double Q-learning\u201d (Hasselt et al., 2015)"}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "\u201cAddressing Function Approximation Error in Actor-Critic Methods\u201d (Fujimoto et al., 2018)"}, {"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "anchor_text": "my Q-learning post"}, {"url": "https://github.com/cyoon1729/deep-Q-networks", "anchor_text": "cyoon1729/deep-Q-networksModular Implementations of algorithms from the Q-learning family (PyTorch). Implementations inlcude: DQN, DDQN, Dueling\u2026github.com"}, {"url": "https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf", "anchor_text": "\u201cIssues in Using Function Approximation for Reinforcement Learning\u201d (Thrun and Schwartz, 1993)"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "\u201cDouble Q-learning\u201d (Hasselt, 2010)"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "\u201cDeep Reinforcement Learning with Double Q-learning\u201d (Hasselt et al., 2015)"}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "\u201cAddressing Function Approximation Error in Actor-Critic Methods\u201d (Fujimoto et al., 2018)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----905dd8325412---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----905dd8325412---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----905dd8325412---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/dqn-family?source=post_page-----905dd8325412---------------dqn_family-----------------", "anchor_text": "Dqn Family"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----905dd8325412---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&user=Chris+Yoon&userId=b24112d01863&source=-----905dd8325412---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&user=Chris+Yoon&userId=b24112d01863&source=-----905dd8325412---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----905dd8325412--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F905dd8325412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----905dd8325412---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----905dd8325412--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----905dd8325412--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----905dd8325412--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----905dd8325412--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----905dd8325412--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdouble-deep-q-networks-905dd8325412&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}