{"url": "https://towardsdatascience.com/simple-tutorial-for-distilling-bert-99883894e90a", "time": 1683001474.5302432, "path": "towardsdatascience.com/simple-tutorial-for-distilling-bert-99883894e90a/", "webpage": {"metadata": {"title": "Simple tutorial for distilling BERT | by Paul Gladkov | Towards Data Science", "h1": "Simple tutorial for distilling BERT", "description": "BERT and transformers, in general, is a completely new step in NLP. It was introduced by Google in 2018 and since then it has shown state-of-the-art results in different language understanding tasks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional", "anchor_text": "https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional", "paragraph_index": 1}, {"url": "https://blog.inten.to/speeding-up-bert-5528e18bb4ea", "anchor_text": "\u201cSpeeding up BERT\u201d", "paragraph_index": 4}, {"url": "https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf", "anchor_text": "described by Rich Caruana and his colleagues", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca", "anchor_text": "this post", "paragraph_index": 7}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "the transformers library", "paragraph_index": 7}, {"url": "http://arxiv.org/abs/1903.12136", "anchor_text": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks", "paragraph_index": 13}], "all_paragraphs": ["2019, December 4th \u2014 Update: Code for the experiment with this approach is available here.", "BERT and transformers, in general, is a completely new step in NLP. It was introduced by Google in 2018 and since then it has shown state-of-the-art results in different language understanding tasks. Check out their paper and results at https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional.", "But there is a little fly in the ointment. It is hard to use BERT in production. BERT-base contains 110M parameters. The larger variant BERT-large contains 340M parameters. Such large neural networks are problematic in practice. Due to the large numbers of parameters, it\u2019s very difficult to deploy BERT in resource-restricted systems such as mobile devices. Additionally, low-inference time makes it useless in real-time systems. That\u2019s why finding ways to make it faster is so important.", "When I faced transformers for the first time it was very tempting to try them for routine tasks. Text classification was one of them. But how to overcome the limitations I wrote above? In this post, I want to show a simple, but effective way to train a task-specific classification model that performs on the same level as the BERT-based model.", "There are several possible ways of speeding up BERT. I highly recommend reading \u201cSpeeding up BERT\u201d which contains a complete overview. Distillation is one of them.", "Knowledge distillation was described by Rich Caruana and his colleagues. The idea is simple: train a small \u201cstudent\u201d model that mimics the behavior of the \u201cteacher\u201d model.", "Let\u2019s imagine we have a regular binary text classification problem. For simplicity X is a list of sentences, y is a list of labels (0 or 1). Additional details are not important for describing the main idea.", "I won\u2019t describe the full process of fine-tuning BERT: refer to this post to learn more about this process. Let\u2019s imagine that we have trained BertForSequenceClassification with num_labels=2 from the transformers library. This model will be the \u201cteacher\u201d whose knowledge we want to transfer to the \u201cstudent\u201d model.", "First of all, we will train a small BiLSTM model as the baseline.", "The input data is words indices from the vocabulary. For building vocabulary, I used built-in functionality from the torchtext package. These functions will help us to translate words from the training dataset into word-indices.", "For simplicity, I didn\u2019t use any pre-trained word embeddings. Our model will learn the projection into inner space during the training process.", "For this particular baseline model, we set output_dim=1 because we have a binary classification, thus loss function is logloss. PyTorch has the BCEWithLogitsLoss class, which combines sigmoid function and binary cross-entropy:", "The full code for training with some helper functions would be:", "This particular idea is originally from the paper \u201cDistilling Task-Specific Knowledge from BERT into Simple Neural Networks\u201d.", "Our \u201cstudent\u201d model has to learn to mimic the behavior of the BERT model. But what is \u201cbehavior\u201d? In our case, it\u2019s the output y_bert on the training dataset. The main idea is to use raw predictions, i.e, predictions before the final activation function, usually softmax or sigmoid. The assumption is that by using the raw values, the model can learn inner representations better than by using \u201chard\u201d predictions. Softmax normalizes the values to 1 while keeping the maximum value high and decreases other values to something very close to zero. There\u2019s little information in zeros, so by using raw predictions, we also learn from the not-predicted classes.", "For learning \u201cbehavior\u201d we modify the loss function by adding an extra term L_distill to the traditional cross-entropy loss (L_ce):", "Where L_distill is the mean-squared-error (MSE) loss between the student network\u2019s logits against the teacher\u2019s logits:", "where z(B) and z(S) are the teacher\u2019s and student\u2019s logits, respectively.", "We reuse almost the code entirely with only one modification\u2014 the loss function. For calculation cross-entropy loss I used CrossEntropyLoss, which combines softmax function and cross-entropy.", "We also changed the number of outputs from 1 to 2 for calculating the loss function correctly. The full code would be:", "That\u2019s all. We transferred knowledge from the BERT model to a simpler model.", "In my work, I have tried this approach for the binary classification problem. It has highly imbalanced classes, thus the key metric was an average precision score. The model\u2019s comparison is in the table below. It is interesting to see that the distilled version has a better average precision score than even the BERT-based model. This fact is a good point for further research, in my opinion.", "This is a very basic approach to knowledge distillation. Concrete shallow network\u2019s architecture and hyperparameters will depend on your particular task. In my opinion, it is quite useful in real-life practice. It has some complexity at training time, but you can get a much better model while maintaining its original simplicity at the execution time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F99883894e90a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----99883894e90a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pgladkov?source=post_page-----99883894e90a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Paul Gladkov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f29aecc8221&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&user=Paul+Gladkov&userId=6f29aecc8221&source=post_page-6f29aecc8221----99883894e90a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@iurte?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Iker Urteaga"}, {"url": "https://unsplash.com/s/photos/tube-chemistry?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2", "anchor_text": "here"}, {"url": "https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional", "anchor_text": "https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional"}, {"url": "https://blog.inten.to/speeding-up-bert-5528e18bb4ea", "anchor_text": "\u201cSpeeding up BERT\u201d"}, {"url": "https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf", "anchor_text": "described by Rich Caruana and his colleagues"}, {"url": "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca", "anchor_text": "this post"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "the transformers library"}, {"url": "http://arxiv.org/abs/1903.12136", "anchor_text": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----99883894e90a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&user=Paul+Gladkov&userId=6f29aecc8221&source=-----99883894e90a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&user=Paul+Gladkov&userId=6f29aecc8221&source=-----99883894e90a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99883894e90a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F99883894e90a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----99883894e90a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----99883894e90a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----99883894e90a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----99883894e90a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----99883894e90a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----99883894e90a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Gladkov"}, {"url": "https://medium.com/@pgladkov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f29aecc8221&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&user=Paul+Gladkov&userId=6f29aecc8221&source=post_page-6f29aecc8221--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5c3f23294075&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-tutorial-for-distilling-bert-99883894e90a&newsletterV3=6f29aecc8221&newsletterV3Id=5c3f23294075&user=Paul+Gladkov&userId=6f29aecc8221&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}