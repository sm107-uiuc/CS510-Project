{"url": "https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc", "time": 1683008761.3389702, "path": "towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc/", "webpage": {"metadata": {"title": "The relationship between Perplexity and Entropy in NLP | by Ravi Charan | Towards Data Science", "h1": "The relationship between Perplexity and Entropy in NLP", "description": "Perplexity is a common metric used in evaluating language models. In this post I will define perplexity and then discuss entropy and their relationship"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html?highlight=perplexity", "anchor_text": "Latent Dirichlet Allocation", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem", "anchor_text": "source coding theorem", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Gibbs%27_inequality", "anchor_text": "Gibbs\u2019 inequality", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35", "anchor_text": "Understanding Logistic Regression Coefficients", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Beam_search", "anchor_text": "beam search", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Cooperative_principle#Maxim_of_relation_(or_relevance)", "anchor_text": "Grice\u2019s maxims", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/wiki/Statistical_manifold", "anchor_text": "statistical manifold", "paragraph_index": 40}, {"url": "https://en.wikipedia.org/wiki/Fisher_information_metric", "anchor_text": "Fisher Information metric", "paragraph_index": 40}, {"url": "https://bookstore.ams.org/mmono-191/", "anchor_text": "Methods of Information Geometry", "paragraph_index": 40}], "all_paragraphs": ["Perplexity is a common metric to use when evaluating language models. For example, scikit-learn\u2019s implementation of Latent Dirichlet Allocation (a topic-modeling algorithm) includes perplexity as a built-in metric.", "In this post, I will define perplexity and then discuss entropy, the relation between the two, and how it arises naturally in natural language processing applications.", "A quite general setup in many Natural Language tasks is that you have a language L and want to build a model M for the language. The \u201clanguage\u201d could be a specific genre/corpus like \u201cEnglish Wikipedia\u201d, \u201cNigerian Twitter\u201d, or \u201cShakespeare\u201d or (conceptually at least) just a generic like \u201cFrench.\u201d", "Specifically by a language L, we mean a process for generating text. For clarity, we will consider the case where we are modeling sentences and the text consists of sequence words ending with an end of sentence \u201cword.\u201d But you can replace \u201cword\u201d with \u201ctoken\u201d and \u201csentence\u201d with \u201cdocument\u201d to generalize to any context.", "What is a \u201cprocess\u201d? For our purposes, we can think of a process as a collection of probability distributions. Given a history h consisting of a series of previous words in a sentence, the language L is the probability that the next word is w:", "For example, I am willing to wager that if L is \u201cEnglish\u201d:", "Similarly, given an entire sentence s, we can evaluate L(s) the probability of the sentence occurring. If we include a special beginning of sentence \u201cword\u201d w\u2092 and let the n-th \u201cword\u201d be the end-of-sentence \u201cword\u201d, we get", "However it is common to leave out the first term in the product as well, or sometimes to work with an even longer starting context.", "It is surprisingly easy to get a perfect replica of L of (say) spoken American English. Just flag down any native English speaker walking down the street. Of course, we are usually interested in teaching a computer the model (hence, Machine Learning). So we will let M be whatever language model we have managed to build on a computer.", "This setup, with a language L and model M is quite general and plays a role in a variety of Natural Language tasks: speech-to-text, autocorrect, autocomplete, machine translation \u2013 the list goes on. Autocomplete is the most obvious example: given the words someone has typed so far, try to guess what they might type next by picking the highest-probability completion.\u00b9", "Given a language model M, we can use a held-out dev (validation) set to compute the perplexity of a sentence. The perplexity on a sentence s is defined as:", "You will notice from the second line that this is the inverse of the geometric mean of the terms in the product\u2019s denominator. Since each word has its probability (conditional on the history) computed once, we can interpret this as being a per-word metric. This means that, all else the same, the perplexity is not affected by sentence length.", "In general, we want our probabilities to be high, which means the perplexity is low. If all the probabilities were 1, then the perplexity would be 1 and the model would perfectly predict the text. Conversely, for poorer language models, the perplexity will be higher.", "It\u2019s hard to provide a benchmark for perplexity because, like most Natural Language tasks, the metric is highly dependent on the vocabulary size. Given a corpus, a smaller vocabulary means that other words will all be replaced with an <oov> (out-of-vocabulary) token, instantly increasing the apparent quality of any language model trained on it", "This last point is very important. There is a lower bound on perplexity fixed by the language itself. We will see this mathematically below. But this points to a general feature of metrics in NLP: an easy-to-evaluate metric like perplexity is not necessarily the best predictor of the true performance of a model. Perplexity is good for development (validation) but not necessarily for evaluation. The gold standard for evaluation remains human evaluation.", "Entropy is a slippery concept in physics, but is quite straightforward in information theory. Suppose you have a process (like a language L that generates words). At each step in the process, there is some probability p that the thing that happened (the event) was going to happen. The amount of surprisal is \u2013log(p) where the logarithm is taken in any base you want (equivalent to changing units). Low probability events have high surprisal. Events that were certain to happen (p=1) have 0 surprisals. Events that are impossible (p=0) have infinity surprisal.", "The entropy is the expected value of the surprisal across all possible events indexed by i:", "So, the entropy is the average amount of surprise when something happens.", "Entropy in base 2 is also optimal number of bits it takes to store the information about what happened, by Claude Shannon\u2019s source coding theorem. For example if I told you that a full-length tweet of 280 characters had an entropy of 1 bit per character, that means that, by the laws of mathematics, no matter what Twitter does, they will always have to have 280 bits (35 bytes) of storage for that tweet in their database. (In practice of course, they have to have quite a bit more).", "In the context of our language model, we\u2019ll have to make one tweak. Given that we are interested in sentences s (sequences of events) of length n, we\u2019ll define the entropy rate per word (event) as:", "where the sum is over all sentences of length n and L(s) is the probability of the sentence. Finally, a technical point: we want to define the entropy of the language L (or language model M) regardless of sentence length n. So finally we define", "Under anodyne assumptions\u00b3 the entropy simplifies even further. The essential insight is that, if we take a long enough string of text, each sentence occurs in proportion to its probability anyways. So there is no need to sum over possible sentences. We get:", "This tells us that we can just take a large (n is big) text instead of trying to sample from diverse texts.", "Suppose we mistakenly think that our language model M is correct. Then we observe text generated by the actual language L without realizing it. The cross-entropy H(L,M) is what we measure the entropy to be", "Where the second line again applies the Shannon-McMillan-Breiman theorem.", "Crucially, this tells us we can estimate the cross-entropy H(L,M) by just measuring log M(s) for a random sample of sentences (the first line) or a sufficiently large chunk of text (the second line).", "The cross-entropy has a nice property that H(L) \u2264 H(L,M). Omitting the limit and the normalization 1/n in the proof:", "In the third line, the first term is just the cross-entropy (remember the limits and 1/n terms are implicit). The second term is the Kullback-Leibler divergence (or KL-divergence). By Gibbs\u2019 inequality the KL-divergence is non-negative and is 0 only if the models L and M are the same. The KL-divergence is sort of like a distance measure (telling you how different L and M are).\u2074 \u20de", "Now all that remains to do is show the relationship between the two. Assuming we took the logarithm in base e:", "If we took the logarithm in base 2, use 2 for the base, etc.", "The perplexity measures the amount of \u201crandomness\u201d in our model. If the perplexity is 3 (per word) then that means the model had a 1-in-3 chance of guessing (on average) the next word in the text. For this reason, it is sometimes called the average branching factor.", "I want to leave you with one interesting note. It is an open question what the true entropy of written English text is (leaving aside other formats, like \u201cTwitter\u201d or \u201cSpoken English\u201d and other languages, like \u201cRussian.\u201d)", "By the inequality H(L) \u2264 H(L,M), one way to get an upper bound on the perplexity or entropy is to create a language model, and we saw some perplexities above.", "In this context, we are usually interested in the entropy per-character (likewise perplexity per-character). When measured using the log base 2, this becomes bits-per-character (BPC).", "Claude Shannon estimated (in a time before computers) that the entropy of written English was between 0.6 and 1.3 bits per character. OpenAI\u2019s GPT-2, mentioned above, achieves about 1 bit per character on (yet another) Wikipedia dataset.", "Keeping in mind that there are about 5 characters per word in written English, this corresponds to about 5 bits, or a perplexity of 2\u2075=32. Note this is substantially higher than the perplexities discussed as state-of-the-art benchmarks! What gives? Remember not to compare perplexities across vocabularies or datasets: the word length may not be the same.", "If you want to read more about information theory, see my previous article Understanding Logistic Regression Coefficients.", "[1] Commonly estimated with a beam search.", "[2] Or 1 possible continuation of any given context. So only one possible sentence for each possible starting word, or, in the context of the paper, only one possible completion of an article given the first 40 tokens. This would violate Grice\u2019s maxims, a general set of rules about language. In particular, why bother writing the rest of the article if it is determined by its beginning? Generally speaking, natural language avoids low-perplexity (entropy) utterances because they are unnecessary.", "[3] The assumptions are that the process is stationary and ergodic. These assumptions do not, in fact, hold for natural language. If this bothers you, you can treat the theorem as a pretty reasonable approximation to make.", "[4] It is not a distance metric because it is not symmetric D(p||q) != D(q||p). However, interpreted on a statistical manifold, its second-order Taylor expansion around D(p||p) gives the Fisher Information metric, which is the unique (up to a scalar constant, by Chentsov\u2019s Theorem) Riemannian metric suitable for statistical manifolds. See Methods of Information Geometry for further reference in the finite dimensional case.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff81888775ccc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f81888775ccc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----f81888775ccc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----f81888775ccc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@isaacquesada?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Isaac Quesada"}, {"url": "https://unsplash.com/t/wallpapers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html?highlight=perplexity", "anchor_text": "Latent Dirichlet Allocation"}, {"url": "https://arxiv.org/pdf/1609.07843.pdf", "anchor_text": "WikiText-103"}, {"url": "https://arxiv.org/abs/1909.08053", "anchor_text": "state-of-the-art"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2"}, {"url": "https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem", "anchor_text": "source coding theorem"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler"}, {"url": "https://en.wikipedia.org/wiki/Gibbs%27_inequality", "anchor_text": "Gibbs\u2019 inequality"}, {"url": "https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35", "anchor_text": "Understanding Logistic Regression Coefficients"}, {"url": "https://web.stanford.edu/~jurafsky/slp3/", "anchor_text": "Speech and Language Processing"}, {"url": "https://thegradient.pub/understanding-evaluation-metrics-for-language-models/", "anchor_text": "Understanding Metrics for Language Models"}, {"url": "https://en.wikipedia.org/wiki/Beam_search", "anchor_text": "beam search"}, {"url": "https://en.wikipedia.org/wiki/Cooperative_principle#Maxim_of_relation_(or_relevance)", "anchor_text": "Grice\u2019s maxims"}, {"url": "https://en.wikipedia.org/wiki/Statistical_manifold", "anchor_text": "statistical manifold"}, {"url": "https://en.wikipedia.org/wiki/Fisher_information_metric", "anchor_text": "Fisher Information metric"}, {"url": "https://bookstore.ams.org/mmono-191/", "anchor_text": "Methods of Information Geometry"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f81888775ccc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f81888775ccc---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f81888775ccc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f81888775ccc---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&user=Ravi+Charan&userId=393ce2bbf82c&source=-----f81888775ccc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&user=Ravi+Charan&userId=393ce2bbf82c&source=-----f81888775ccc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff81888775ccc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f81888775ccc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f81888775ccc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f81888775ccc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f81888775ccc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f81888775ccc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f81888775ccc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}