{"url": "https://towardsdatascience.com/understanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8", "time": 1683012236.0569432, "path": "towardsdatascience.com/understanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8/", "webpage": {"metadata": {"title": "Understand Kafka as if you had designed it \u2014 Part 1 | by Felipe Melo | Towards Data Science", "h1": "Understand Kafka as if you had designed it \u2014 Part 1", "description": "Apache Kafka has become the backbone of IT-driven businesses. From real-time messaging to long-term storage, it has become almost ubiquitous in environments dealing with all the \u201c3 Vs\u201d from Big Data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understand-spark-as-if-you-had-designed-it-c9c13db6ac4b", "anchor_text": "https://towardsdatascience.com/understand-spark-as-if-you-had-designed-it-c9c13db6ac4b", "paragraph_index": 67}], "all_paragraphs": ["Apache Kafka has become the backbone of IT-driven businesses. From real-time messaging to long-term storage, it has become almost ubiquitous in environments dealing with all the \u201c3 Vs\u201d from Big Data, i.e. Volume, Variety and Velocity.", "Although its API is fairly simple to understand and use, it\u2019s internals are not straightforward. As always, you don\u2019t have to understand the engine to become a good pilot, however, going deeper can make the difference between good and excellent.", "This is the first part of a 2-parts series explaining the building blocks behind Kafka. This part will focus on its main motivations and how data is stored and managed internally to each broker. The second part will focus on the elements of scalability and durability.", "We\u2019ll do our exploration from the perspective of two very good and ambitious friends.", "A good friend of yours started working for the meteorological department of the city. As any new joiner, he is full of enthusiasm and drive to make things happen.", "He has an idea about a new real-time weather forecasting method, but realised that the number of thermometers spread across the city was far from sufficient. He went to talk to his superiors, who told him \u2014 while not being knowledgeable on the subject \u2014 that it was not a lack of devices but instead an impossibility to process this amount of data in real-time. But he knew you, and for you a technical impossibility is nothing but an opportunity.", "The requirement was quite simple: receive data from the thermometers every two seconds and then 1) store them for further investigation, and 2) send them to his great new model.", "Everything was clear. You just needed a single process to receive the measurements, store them and then dispatch to the model. However, as you are quite familiar with concepts like Single Responsibility and Interface Segregation, you decided to decouple the system in two parts: 1) a process to receive the measurements from the thermometer, attach a timestamp and store them, and 2) a process to query the database every two seconds, passing the last timestamp consumed as offset and send the result to your friend\u2019s model. Your problem was so easy that the hardest part was to choose which open-source RDBMS to use.", "After choosing the RDBMS you love, you started the project by creating two classes, one to receive the measurements and store them, and one to read the stored measurements and dispatch them to the model. Even though both will be reading and writing to and from somewhere, you decided to name them from the perspective of the storage solution, so the class that receives the measurements and write them into the RDBMS was named Writer and the one reads the measurements and dispatches them to the model was named Reader.", "You also created a class called Measurement to hold the data you\u2019re working on. However, since you like to be idiomatic, you decided to follow the naming usually associated with RDBMS entries, which is Record. This way you also make your design more generic.", "Without much effort, your first take on the solution is ready: a Writer that writes Records into your storage solution and a Reader that consumes from there.", "After one week you came back to your friend with your fully operational solution. You pressed enter and his model started to get records every two or so seconds. He invited you for a beer, on him. During the celebration, he confessed that his model was doing better than expected, and that maybe it would be capable to handle incoming records (he also thought record is a good name) every 1 second.", "That did not surprise you, as you\u2019re used to changing requirements. But this time it was solved already. All you had to do was to change the configuration from 2 seconds to 1 second and it would be done.", "The next morning you just changed the timer from 2 seconds to 1 and it kept working like a charm, and so did his model. He spent the day tweaking it some more and by the end of the day it went wild, and became capable of processing millions of records per second in a streaming fashion, i.e. the records no longer needed to be sent every X seconds, but instead, as soon as they were received from the thermometers.", "To serve that new request, you thought about merging the writer and reader processes so that the writer could store the record and also dispatch the message right after. But then you considered how cumbersome it would be to apply retries and back-pressure in case of failures or overload. You then considered to create some kind of parallel communication between the two processes, so that the writer could store the record and send it to the reader through a separate queue, so that the reader didn\u2019t need to incur the overhead of querying the RDBMS just to get records based on timestamp. But that would mean extra complexity to your solution. So you just removed the timer from the Reader process. This way, it queried the RDBMS, got the latest inserted records and dispatched them as a single batch. As the throughput of the storage solution you chose was quite good, it worked.", "A few days later your friend called you. The solution \u2014 and his reputation \u2014 was taking off and got people so interested that they were now ordering a new type of thermometer, that was able to capture some more data. You were so happy to have your solution powering your friend\u2019s dream that you didn\u2019t notice the issue immediately. Instead, you got a sample of the new thermometer and had it sending some test data, just to see your solution, for the first time, failing.", "After some debugging you got to the root of the problem: the schema used by the new thermometer was different. You felt a shiver. What if every thermometer had a different schema? If that was the case, your solution would be doomed from a maintainability perspective. Were you supposed to add a new table for every new kind of thermometer? You called your friend and he confirmed that it was the case. He also asked if it was a major problem, but you denied. That was not a problem, just an opportunity.", "You found yourself in the following situation:", "1. You have two processes, Writer and Reader, that store and retrieve data from a given storage solution", "2. Your storage solution needs to know the schema beforehand, so that it can apply the schema at the time of writing", "3. You don\u2019t need any special processing to write the records, just to store them as they are", "4. You don\u2019t need any special querying or filtering capability, just to be able to retrieve every record in the order they arrive", "5. You only need to know the schema as you retrieve the record, i.e. at the time of reading it", "After a lot of meditation on the current state of affairs, you remembered having once considered to implement some kind of queue in order to pass the records from the Writer directly to the Reader. If that queue was somehow persisted and searchable by a single field, the timestamp, you would be close to a satisfactory solution, as you could 1) keep the processes you already had, 2) store the records as binary objects so their schemas would not matter at write time, 3) retrieve all the records stored after a given timestamp, and 4) apply the schema just before dispatching the record to the model.", "That sounded promising, but in a sense, you were proposing a new kind of storage solution, which should also bring requirements about scalability, resiliency, fault tolerance and the like. That would be a lot of work, and your friend would have to agree.", "The next morning you sent an invitation to your friend for a beer (this time all on you). In the message you mentioned that a revolutionary idea had struck you, and you had to share it.", "\u201cyou know, in the databases world there are two ideas about how to apply schemas to a record: during the writing and during the reading. I call them schema-on-write and schema-on-read. The former makes it faster to perform queries, as you can take advantage of the formats and data types, but it also kind of enslaves you. Every time you need to change it, some havoc is wrecked on the whole system. The latter, on the other hand, gives you the flexibility to change your data as you wish, and this is paramount to the innovations we are creating here. Querying becomes a bit harder, but we don\u2019t need any special filtering, just the timestamp. You know, we can keep doing as we are right now, but we are not here for small feats, are we? So, think about this. Right now, we receive the record and send it to the RDBMS, which writes it to a transactions log and then into the final storage. Only after that we have access to the data. That would be fine if we needed some extra filtering applied to the records, but we don\u2019t. The only thing we need is to get the records as soon as they are saved to the log, kind of tailing it. We can definitely apply a fancy idea called Change Data Capture and retrieve the data directly from that log, but if we implement our own log, we can optimize it to our own needs, and we can make two breakthroughs at the same time, on your model and on my solution\u201d.", "Your friend is mesmerised, not only by the idea but also by the technical jargons he has no clue about. He takes a good sip and asks the important question: what do we need?. Your answer is prompt: as all big and good things in life, time and money. Your confidence forces your friend to confess that he had been promoted, thanks to your joint work, and he now also has a budget for research. Jackpot!", "With a \u201cgo\u201d from your friend it is now time to get things done.", "You are going to develop a log-like storage that receives records, attaches timestamps and saves them. On the reader side, the most common use case will be to tail the log, which can be done efficiently by leveraging page caches and avoiding disk seeks. So, in a sense, you need a key-value storage where the timestamp will be the key for both, writing and reading.", "At a first glance it looks quite simple, but what about updates and deletions? ACID guarantees? And what should be the granularity of the timestamp, seconds, milliseconds, nanoseconds? And if you need to retrieve one specific record or restart from that record? It starts to feel a bit daunting due to the complexity, but you know the way: divide and conquer.", "After some thought you realize that even though the records are associated to timestamps, this is not the best data type to be used as a key. For your purpose, the timestamps represent order of arrival, but if you just assign a monotonically increasing identifier to each record as it arrives, you achieve exactly the same effect.", "Since you are adding records to a log, this identifier can be just a long value containing the offset from the beginning of the log, which would have offset 0. A long value is much easier to refer to than a timestamp. Now, your Writer and Reader will just keep a reference to the latest offset processed. The Writer will increase it by one and assign it to each new record, and the Reader will also increase it by one to retrieve the next record.", "With this better design for accessibility in mind, it is time to decide how to actually store the records. One way or another, they will have to be stored on the disk. If you could guarantee that whenever a reprocessing is required it would happen for the whole set of records, then you could just store all records in the same file. But what if it is required to start from a specific offset? If all messages were stored on the same file, you would have to scan it from the beginning until that offset was reached, which has a level of inefficiency that is not quite compatible with breakthroughs.", "You know about random access files and their capabilities to support reads from and writes to specific offsets. Since you are already using the concept of offsets to identify your records, you came out with a clever idea: what if you create two files instead, one for your records and another one for the offsets?", "If you do it like that, then your offsets file will contain just the information about record offsets and their start and end positions inside the record file. This offset file could be possibly kept in memory, or if not, could be loaded very quickly. Now, your Writer can just append to both files and if someone needs to retrieve records from a specific offset, your Reader can read the offsets file, (if the record is not already found on the page cache), find the start and end positions in the records file and boom!", "You also consider that offset file is already a distinctive enough name, but records file does not sound as great. After some thought, you realize that your records file is actually the log you\u2019re trying to implement. Clear as daylight!", "As that motivational breath strikes, you decide to go even wilder and also allow the retrieval by time. All you have to do is to follow the same reasoning as the offsets file, i.e. if you add another file containing the timestamp of the arrival time of the record and the offset associated to that record in the offsets file, you can also retrieve records from a specific timestamp in a very efficient way. This file will work as an index of arrival times of records, so you consider that timeindex would be a good name of it.", "You now have a solution that can attach offsets to records and save them to a persistent storage called log file as they arrive, and consume them at almost the same time. You can also get records from arbitrary offsets in a very efficient way from an offsets file. Finally, you can even get records based on arrival time by using the timeindex file.", "But what about updates and deletions?", "This time it seems you\u2019ve reached a wall. If you want to provide update capabilities, you\u2019ll also need the following:", "1. An efficient mechanism to change the records file", "2. An update to the offsets file", "Starting from the last, changing the offsets file is not a big deal. Changing the records file is also doable, however, you end up with one of two problems: updating all other records\u2019 start and end positions, or wasting space. Either way is incompatible with the breakthrough that you are chasing.", "Then you realize something that some people will later refer to as part of some interesting theory of a stream-table duality. The invention you are developing does not require updates because an update to an event is just another event. From that perspective, an update is just the append of a new event to the tail of the log, which your invention already supports. Bingo! Now you can say that you have an append-only log with records ordered by arrival date. You can already feel the hype!", "The excitation is taking over, but what about deletions?", "It is fine to have an append-only system since updates are just new events, but deletions are a must, otherwise the disks would die together with your dreams.", "If you support the deletions by allowing modifications to the log, the same issues you found with updates would kick in, plus you wouldn\u2019t be able to call it append-only anymore, missing a catchy word for further publicity.", "You go home and cannot sleep, just thinking about this conundrum. Eventually your mind shuts down for a quick while, just enough time for you to dream of the Ouroboros. You wake up thrilled. Like the serpent eating itself, piece by piece, your invention should eat up its own log, piece by piece. Only two things are missing: the pieces and the lunch time.", "After thanking the great Hypnos, the God of Sleep, you go back to your computer. You need the pieces of your log, which means it should become many. And you need to decide how big each of these pieces can be and when they must go away. About the latter two, at the end of the day, you realize that it depends more on the context under which the system is being used than on your will, so you decide to give the control back to the user.", "Your new approach will create not one, but many log files, each of them containing its own index files. The user will specify how big each of these log files should be and for how long each record must be kept.", "But how to add names to these files? Maybe some randomness? Nah, does not quite fit breakthroughs. But since 1) the names need to be unique, 2) each offset is unique, and 3) each file will be responsible for a range of offsets, what about naming each of those files with two parts, the number of the first offset in it and the role it plays?", "As an example, imagining that the user wants three offsets per file and you have five records, you\u2019d have 0.index, 0.log and 0.timeindex starting from offset 0 and containing the first three records, and 3.index, 3.log and 3.timeindex starting from offset 3 and containing the other two records. Until a sixth record arrives, the sequence starting with 3 will be active, as it still can hold one more record. But as this new record arrives, a new sequence will be started, 6.index, 6.log and 6.timeindex, starting from offset 6 and ready to receive the next three records.", "And after pausing and contemplating how awesome this all is, you also realize that calling these files sequences is not the best choice, as the term sequence is more associated with the whole thing (the whole Ouroboros). Instead, these files are segments of the sequence. Beautiful!", "Now that you have the pieces, it is time to decide when and how they should be eaten up. The when is easy. You\u2019ve received the expected duration of each record from the user and you know when it was inserted (.timeindex to the rescue). Just do the math and find out which records should be removed at every time.", "The removal, on the other hand, is not actually a removal, but a kind of label assigned to each record. You just keep these labels and skip the offsets associated to them when the Reader is asking for data. And here is the real contribution of Ouroboros: records belonging to the same segment are likely to have been generated closer to each other from a time perspective. So as soon as the last record belonging to a segment is marked for deletion, the whole segment can be deleted, which leads to an extremely efficient removal and zero waste of space!", "Ouroboros, powering innovation since Ancient Egypt.", "It has been a huge lot what you have accomplished. You deployed your new solution and it worked like magic. You\u2019re about to call your friend but he calls you first. You don\u2019t even give him time to speak as you start shooting right away:", "Man, here is what we have:", "1. We can receive records containing any format and store them seamlessly", "2. We can consume these records in order of arrival in almost real time", "3. We can find records from any specific offset or time in a very efficient way", "4. We can delete records in an optimal way", "Your friend\u2019s reaction was as expected. He congratulates you and says that he has already noticed the progress, as his model is being bombarded with measurements, no stop. And he also says that he was calling you to tell some more excellent news. His model and your solution were doing so great that now the meteorological departments of the whole state got interested, and some cities of other states also started to get in contact. They were keen to invest in whatever infrastructure is required.", "As excited as you are about your solution, that kind of took you by surprise. Your solution is great from a design perspective, but is it scalable enough to support the huge increase in demand? And how can you support multiple cities, should you have an infrastructure in place for every new city? How can you support multiple readers and writers? And how can you guarantee durability, i.e. what if disks fail? How to perform backups while writers and readers are on full flight?", "You share those concerns with your friend, but at this point there is no turning back. He trusts you and your nickname is challenge. But you\u2019re tired and before going back to those insane hours you need a short sabbatical. That will help you to consider how to solve those scalability questions. You friend agrees.", "You buy a ticket to the Alps as you can think better when seeing things from above. You promise yourself to come back with a world-changing solution.", "Also, if you like this kind of explanation and you\u2019re interested in Spark, you might like to check https://towardsdatascience.com/understand-spark-as-if-you-had-designed-it-c9c13db6ac4b", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3f9316cb8fd8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://felipesmmelo-1411.medium.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": ""}, {"url": "https://felipesmmelo-1411.medium.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Felipe Melo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35741fa22f10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&user=Felipe+Melo&userId=35741fa22f10&source=post_page-35741fa22f10----3f9316cb8fd8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://kafka.apache.org/documentation/#uses", "anchor_text": "https://kafka.apache.org/documentation/#uses"}, {"url": "https://dzone.com/articles/is-apache-kafka-a-database-the-2020-update", "anchor_text": "https://dzone.com/articles/is-apache-kafka-a-database-the-2020-update"}, {"url": "https://en.wikipedia.org/wiki/Page_cache", "anchor_text": "https://en.wikipedia.org/wiki/Page_cache"}, {"url": "https://towardsdatascience.com/understand-spark-as-if-you-had-designed-it-c9c13db6ac4b", "anchor_text": "https://towardsdatascience.com/understand-spark-as-if-you-had-designed-it-c9c13db6ac4b"}, {"url": "https://medium.com/tag/kafka?source=post_page-----3f9316cb8fd8---------------kafka-----------------", "anchor_text": "Kafka"}, {"url": "https://medium.com/tag/streaming?source=post_page-----3f9316cb8fd8---------------streaming-----------------", "anchor_text": "Streaming"}, {"url": "https://medium.com/tag/database?source=post_page-----3f9316cb8fd8---------------database-----------------", "anchor_text": "Database"}, {"url": "https://medium.com/tag/architecture?source=post_page-----3f9316cb8fd8---------------architecture-----------------", "anchor_text": "Architecture"}, {"url": "https://medium.com/tag/software-engineering?source=post_page-----3f9316cb8fd8---------------software_engineering-----------------", "anchor_text": "Software Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&user=Felipe+Melo&userId=35741fa22f10&source=-----3f9316cb8fd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&user=Felipe+Melo&userId=35741fa22f10&source=-----3f9316cb8fd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3f9316cb8fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3f9316cb8fd8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3f9316cb8fd8--------------------------------", "anchor_text": ""}, {"url": "https://felipesmmelo-1411.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://felipesmmelo-1411.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Felipe Melo"}, {"url": "https://felipesmmelo-1411.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "168 Followers"}, {"url": "https://www.linkedin.com/in/felipemmelo/", "anchor_text": "https://www.linkedin.com/in/felipemmelo/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35741fa22f10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&user=Felipe+Melo&userId=35741fa22f10&source=post_page-35741fa22f10--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdf6346a351a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8&newsletterV3=35741fa22f10&newsletterV3Id=df6346a351a8&user=Felipe+Melo&userId=35741fa22f10&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}