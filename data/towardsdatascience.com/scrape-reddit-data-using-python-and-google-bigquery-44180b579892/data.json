{"url": "https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892", "time": 1682994749.2200608, "path": "towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892/", "webpage": {"metadata": {"title": "Scrape Reddit data using Python and Google BigQuery | by Akhilesh Narapareddy | Towards Data Science", "h1": "Scrape Reddit data using Python and Google BigQuery", "description": "Reddit is one of the oldest social media platforms which is still going strong in terms of its user generated content. In this post, we will learn how to access this treasure trove of information."}, "outgoing_paragraph_urls": [{"url": "https://www.reddit.com/prefs/apps", "anchor_text": "here", "paragraph_index": 8}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080", "paragraph_index": 10}, {"url": "https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application", "anchor_text": "praw documentation", "paragraph_index": 10}, {"url": "https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html", "anchor_text": "praw documentation", "paragraph_index": 17}, {"url": "https://pushshift.io/", "anchor_text": "Pushshift.io", "paragraph_index": 27}, {"url": "https://www.reddit.com/user/Stuck_In_the_Matrix", "anchor_text": "/u/Stuck_In_The_Matrix", "paragraph_index": 27}, {"url": "https://cloud.google.com/solutions/bigquery-data-warehouse", "anchor_text": "enterprise data warehouse", "paragraph_index": 29}, {"url": "https://bigquery.cloud.google.com/", "anchor_text": "Google BigQuery", "paragraph_index": 32}, {"url": "https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_05", "anchor_text": "link", "paragraph_index": 36}, {"url": "https://minimaxir.com/2015/10/reddit-bigquery/", "anchor_text": "article", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/@bandiatindra", "anchor_text": "Atindra Bandi", "paragraph_index": 42}], "all_paragraphs": ["Reddit is one of the oldest social media platforms which is still going strong in terms of its users and content generated every year.Behind that age old user interface, is the treasure trove of information that millions of users are creating on a daily basis in the form of questions and comments.", "In this post, we will see how to get data from Reddit website using python and Google Bigquery in a step by step manner.To illustrate this process, I decided to extract the data about cord-cutters, people who cut their cable connection and purchase streaming site subscriptions, as this phenomenon is of an interest to me.", "As a first step, let us understand the structure of Reddit website.", "The site is divided into various subreddits, with each user choosing which subreddits they would like to subscribe to according to their interest. These include music subreddits where links regarding music may be shared, sports subreddits where people talk in detail about sports or in our case cord-cutter subreddit where people discuss about cable connections or about their new streaming subscriptions.", "Upvote and downvote system is the essence of Reddit as it shows an agreement regarding a particular topic among the community members. The more upvotes a post gets, the more prominently it will be displayed on the site. It is important to note that the comments are as important as posts as they often become extended nested discussions.", "While web scraping is one among the famous(or infamous!) ways of collecting data from websites, a lot of websites offer APIs to access the public data that they host on their website. This is to avoid unnecessary traffic that scraping bots create, often crashing their websites causing inconvenience for the users. Even Reddit also offers such API which is easy to access.", "Following are the things that you will need for this exercise:", "There are a few steps that you need to follow before you get the data for your desired topic.", "First step after signing up is to create an app to get Oauth keys for accessing the data. Click here to get started.", "Click on the create an app as shown in the snap shot. Then a dialog box appears like the one below.", "Enter a name for your app in the dialog box and click on the script as we will be using this for our personal use. Make sure that you enter http://localhost:8080 in redirect URL box. You can refer to praw documentation if you need any clarification. Now click on create app button at the bottom.", "Now your application has been created. Store your 14 char personal use script and 27 char secret key somewhere secure. Now you have all the credentials that are required for OAuth2 authentication to connect to Reddit API.", "It's time to open Jupyter notebook now!", "The packages that we require for this exercise are praw and pandas. PRAW is short for Python Reddit API Wrapper which we will use to make requests to the Reddit API. Make sure that you have both of them installed. First step is to import those packages", "Next step after importing the packages is to establish a connection with Reddit API using the credentials that we have created earlier. Client_id will be your 14 char personal use script key and client_secret is your 27 char secret key. Username and password are your Reddit account credentials. Rest of the code will remain the same.", "By running the above snippet of code, we will be establishing the connection and storing this information in a variable named reddit.", "As we have discussed earlier, we would be focusing on getting the data for 'cordcutter' subreddit.", "title, score, url, id, number of comments, date of creation, body text are the fields that are available when it comes to getting the data from Reddit API. But as i will be not considering any time aspect in our analysis, our main focus will be to get just the bodytext(comments) from the subreddit. Refer to praw documentation for different kinds of implementations. Here I have limited the code to the desired output which is just body text for all comments.", "To have all the comments including the nested replies, I have to come up with a nested code with 3 parts to it.", "Getting the list of comments ids", "In this loop, first we are getting each submission information at the start of the loop and then extracting all the comment ids and storing them in list.", "Here, .hot(limit) can be any number depending on your requirement. I have set it to 2 here to illustrate the output, but setting it to None will fetch you all the top level submissions in cordcutter subreddit. replace_more(limit=None) will help us in considering the comments that have nested replies.", "Output of this loop looks like this:", "We are close to getting our desired data. In this part of the code, we will be getting the body of each comment id that we obtained earlier. If the comment has nested replies, it will enter into the next loop and will extract information in a similar fashion.", "By this point, we downloaded the comments from Reddit and there would required a bit of pre-processing to download it as a csv.", "You can find the final version of the code in my github repository.", "Our final output looks like this:", "We have our data, but there is one challenge here. Generally, it takes more time to get months of historical data using Reddit API. Thanks to Jason Michael Baumgartner of Pushshift.io(a.k.a /u/Stuck_In_The_Matrix on Reddit), we have years of historical Reddit data cleaned and stored in Bigquery which is the second part of this post.", "For those who do not know what Bigquery is,", "Google BigQuery is an enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Google\u2019s infrastructure.", "Best part is querying this data would be free. Google provides first 10GB of storage and first 1 TB of querying memory free as part of free tier and we require less than 1 TB for our task.", "Lets look at how to query this information.", "First click on this Google BigQuery link to get started. Google will automatically log you in with your Google credentials that are stored in your browser. If this is your first time on BigQuery, there will be a dialog box asking you to create a project.", "Hit on create a project button.", "Give a name to the organization and click on create project at the top.", "Give the name of the project and you can leave the location box as it is for now. Then click on create. Now you will have your project created and a dashboard appears on the screen.", "Now after this, click on the link .This will open the reddit datasets under the project that you have created. On the left hand side, you will see datasets updated for each month under the schema name fh-bigquery.", "Let\u2019s run the query to get the data for one month from the table.", "This will get you all the comments for \u2018cordcutter\u2019 subreddit. But make sure that you leave the \u201cUse Legacy SQL\u201d check box in the options unchecked as the above snippet of code is in standard sql. However, you can choose your choice of sql and make changes to the code accordingly.", "This is how the result looks like and you can download the result as a csv by clicking \u201cDownload as CSV\u201d button.", "Here, I have just focused on getting the data as we requried it. If you want to play more with reddit data on bigquery, you can refer this article by Max Woolf which goes into more detail about Reddit data in Bigquery.", "In this post, we have seen how to create OAuth2 credentials for connecting to Reddit, making data requests to Reddit API to get most recent data and query historical data in a very fast way through Google Bigquery.", "In addition to getting data through an API and Bigquery, you might find it interesting to look at web scraping using Selenium and python. Following is an article about that by a fellow classmate(Atindra Bandi) at UT Austin.", "That\u2019s all folks! Stay tuned to get an update on a series of articles on recommendation systems, statistics for data science and data visualizations from me in the coming weeks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F44180b579892&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----44180b579892--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44180b579892--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@akhilesh.narapareddy?source=post_page-----44180b579892--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akhilesh.narapareddy?source=post_page-----44180b579892--------------------------------", "anchor_text": "Akhilesh Narapareddy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae669c9ff90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&user=Akhilesh+Narapareddy&userId=ae669c9ff90f&source=post_page-ae669c9ff90f----44180b579892---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://webdata-scraping.com/web-scraping-trending-technique-in-data-science/", "anchor_text": "source"}, {"url": "https://www.anaconda.com/download/", "anchor_text": "here"}, {"url": "https://www.reddit.com/", "anchor_text": "Reddit"}, {"url": "https://www.reddit.com/prefs/apps", "anchor_text": "here"}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080"}, {"url": "https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application", "anchor_text": "praw documentation"}, {"url": "https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html", "anchor_text": "praw documentation"}, {"url": "https://github.com/akhilesh-reddy/Cable-cord-cutter-lift-and-sentiment-analysis-using-Reddit-data", "anchor_text": "akhilesh-reddy/Cable-cord-cutter-lift-and-sentiment-analysis-using-Reddit-dataScraped data from Reddit and performed Named entity recognition,topic modelling on the comments to understand public\u2026github.com"}, {"url": "https://pushshift.io/", "anchor_text": "Pushshift.io"}, {"url": "https://www.reddit.com/user/Stuck_In_the_Matrix", "anchor_text": "/u/Stuck_In_The_Matrix"}, {"url": "https://cloud.google.com/solutions/bigquery-data-warehouse", "anchor_text": "enterprise data warehouse"}, {"url": "https://bigquery.cloud.google.com/", "anchor_text": "Google BigQuery"}, {"url": "https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_05", "anchor_text": "link"}, {"url": "https://minimaxir.com/2015/10/reddit-bigquery/", "anchor_text": "article"}, {"url": "https://towardsdatascience.com/@bandiatindra", "anchor_text": "Atindra Bandi"}, {"url": "https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab", "anchor_text": "Web Scraping Using Selenium \u2014 PythonIn this article, you\u2019ll learn how to navigate through multiple pages of a website and scrape large amounts of data\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/bigquery?source=post_page-----44180b579892---------------bigquery-----------------", "anchor_text": "Bigquery"}, {"url": "https://medium.com/tag/data-science?source=post_page-----44180b579892---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reddit?source=post_page-----44180b579892---------------reddit-----------------", "anchor_text": "Reddit"}, {"url": "https://medium.com/tag/python?source=post_page-----44180b579892---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----44180b579892---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&user=Akhilesh+Narapareddy&userId=ae669c9ff90f&source=-----44180b579892---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&user=Akhilesh+Narapareddy&userId=ae669c9ff90f&source=-----44180b579892---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44180b579892--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F44180b579892&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----44180b579892---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----44180b579892--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----44180b579892--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----44180b579892--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----44180b579892--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----44180b579892--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----44180b579892--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----44180b579892--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----44180b579892--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akhilesh.narapareddy?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akhilesh.narapareddy?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akhilesh Narapareddy"}, {"url": "https://medium.com/@akhilesh.narapareddy/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "132 Followers"}, {"url": "https://www.linkedin.com/in/akhilesh-reddy/", "anchor_text": "https://www.linkedin.com/in/akhilesh-reddy/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae669c9ff90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&user=Akhilesh+Narapareddy&userId=ae669c9ff90f&source=post_page-ae669c9ff90f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F74c536cd5389&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscrape-reddit-data-using-python-and-google-bigquery-44180b579892&newsletterV3=ae669c9ff90f&newsletterV3Id=74c536cd5389&user=Akhilesh+Narapareddy&userId=ae669c9ff90f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}