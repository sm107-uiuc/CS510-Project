{"url": "https://towardsdatascience.com/how-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7", "time": 1683017559.797848, "path": "towardsdatascience.com/how-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7/", "webpage": {"metadata": {"title": "How I Beat the Slots in Pok\u00e9mon Using Reinforcement Learning | by Daniel Saunders | Towards Data Science", "h1": "How I Beat the Slots in Pok\u00e9mon Using Reinforcement Learning", "description": "Reinforcement learning is providing a huge boost to many applications, in particular in e-commerce for exploring and anticipating customer behaviour, including where I work as a data scientist\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Reinforcement_learning", "anchor_text": "Reinforcement learning", "paragraph_index": 0}, {"url": "https://tech.wayfair.com/data-science/2020/01/bayesian-product-ranking-at-wayfair/", "anchor_text": "including where I work as a data scientist, Wayfair", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Multi-armed_bandit", "anchor_text": "multi-armed bandit", "paragraph_index": 0}, {"url": "https://www.pngkit.com/view/u2w7e6q8o0a9q8q8_12-jul-multi-armed-bandit/", "anchor_text": "is a weird one", "paragraph_index": 0}, {"url": "https://www.alamy.com/stock-photo/loose-slots.html", "anchor_text": "\u201cloose\u201d", "paragraph_index": 1}, {"url": "https://github.com/drsaunders/PokeSlots/blob/main/PokeSlots.ipynb", "anchor_text": "(Full code and more detail available in the Jupyter notebook)", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Thompson_sampling", "anchor_text": "Thompson sampling", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "updated using Bayesian logic", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_trial", "anchor_text": "Bernoulli trials", "paragraph_index": 4}, {"url": "https://github.com/lilianweng/multi-armed-bandit", "anchor_text": "code written by Lilian Weng", "paragraph_index": 4}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "excellent tutorial", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "Bayesian prior", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible intervals", "paragraph_index": 16}, {"url": "https://www.npr.org/2011/11/11/142218444/bil-keanes-dotted-line-an-appreciation", "anchor_text": "Billy wandering around the neighbourhood in old Family Circus comics", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Bayesian_regret", "anchor_text": "regret", "paragraph_index": 23}, {"url": "https://www.theverge.com/2015/5/6/8544303/casino-slot-machine-gambling-addiction-psychology-mobile-games", "anchor_text": "they\u2019re a lot sneakier!", "paragraph_index": 26}, {"url": "https://www.linkedin.com/in/drsaunders/", "anchor_text": "https://www.linkedin.com/in/drsaunders/", "paragraph_index": 29}], "all_paragraphs": ["Reinforcement learning is providing a huge boost to many applications, in particular in e-commerce for exploring and anticipating customer behaviour, including where I work as a data scientist, Wayfair. One popular way to model problems for RL algorithms is as a \u201cmulti-armed bandit\u201d, but I have always thought the term was unnecessarily difficult, considering it's supposed to be a helpful metaphor. First of all \u201cone-armed bandit\u201d is 100-year-old slang, and second, the image of a slot machine with multiple arms to pull is a weird one.", "Modern slot machines probably do have different buttons to press, that at least pretend to give different odds, but a better metaphor would be multiple machines in a casino, with some being \u201cloose\u201d and some being \u201ctight.\u201d When I walked into the Celadon City Game Corner, in the 2004 Gameboy Advance game Pok\u00e9mon FireRed, and saw rows of slot machines all with different odds, I knew I had found the ideal \u201creal-life\u201d version of this metaphor \u2014 and a practical application of reinforcement learning.", "(Full code and more detail available in the Jupyter notebook)", "And I mean practical! How else am I going to win 4000 coins to buy the Ice Beam or Flame Thrower abilities, which I'll need to fight the Elite Four??I built a reinforcement learning agent, using Thompson sampling, to tell me which machine to sample next, and, eventually, which one to play the hell out of. I call him MACHAMP: Multi-Armed Coin Holdings Amplifier Made for Pokemon.", "Given a set of possible actions (\u201carms\u201d of a multi-armed bandit \u2014 in this case different machines to try), Thompson sampling optimally trades off exploration vs exploitation to find the best action, by trying the promising actions more often, and so getting a more detailed estimate of their reward probabilities. At the same time, it\u2019s still randomly suggesting the others from time to time, in case it turns out one of them is the best after all. At each step, the knowledge of the system, in the form of posterior probability distributions, is updated using Bayesian logic. The simplest version of the one-armed bandit problem involves Bernoulli trials, where there are only two possible outcomes, reward or no reward, and we are trying to determine which action has the highest probability of reward. As a demonstration of how Thompson sampling works, imagine that we had 4 slot machines, with a 20%, 30%, 50% and 45% chance of payout. Then we can simulate how the solver finds that slot 3 is the best one. Here and in the rest of the notebook, I started from code written by Lilian Weng for her excellent tutorial (everything in the multi_armed_banditmodule is hers).", "At the start, we don't know anything about the probabilities of the machines, and assume that all values for their true reward probability are equally possible, from 0% to 100% (depending on the problem, this choice of Bayesian prior might be a poor assumption, as I discuss below).", "One step of the solver involves randomly sampling from the posterior probability distributions of each of the machines, and trying the best one (this is the Thompson sampling algorithm), then updating these distributions based on whether there was a reward.", "We can see from the graph of the estimated probabilities that one success for machine 4 has made us more optimistic about that machine \u2014 we now think that higher guesses for the reward probability are more likely.After running it for 100 simulated pulls of the four machines, we can see that it's honed in on better estimates of the probabilities.", "And after 10000 trials we are even more confident that 3 has a high probability of reward, because we sampled 3 much more than the others. We also sampled 4 a lot just to be sure, but 1 and 2 we learned quickly are much worse and so sampled less often \u2014 we got less accurate and less confident estimates of their reward probabilities, but we don't care.", "We can use this same exact logic to make the most possible money at the Celadon slots!", "There are 19 playable slot machines in the Celadon game corner, which pay off in coins that can be used to buy TMs (Pok\u00e9mon abilities) and Pok\u00e9mon that are unavailable anywhere else. Three wheels spin, and you press a button to stop them one at a time, with the goal to line up three of the same picture, or at least a combination that starts with a cherry.", "The best jackpot is triple 7s, for 300 coins. How did I know the machines had different odds? Because a game character told me so.", "Before going to something as ridiculously complicated as a Thompson sampling MAB solver, I looked online for other advice for beating the casino. Maybe because it\u2019s a pretty old game (I get to them when I get to them) the information was sparse and sometimes contradictory:", "Taking these with a grain of salt, I made some simplifying assumptions:", "Therefore I decided I would play by mashing the \u201cstop\u201d button as fast as possible without paying any attention to the visuals, record only whether it was a win (of any size) or loss, and let Thompson sampling, via MACHAMP, guide my choice of which machine to try next.", "To kickstart the solver, though, I decided to try every machine four times, and use the results to initialize the posterior probabilities. With only four pulls it was hard to draw any conclusions about which were good or bad machines, since the probability distributions overlapped greatly \u2014 when they weren't identical.", "Since the overlaps made it very hard to read the estimates for individual machines, I instead plotted the credible intervals for each machine: the range of possible values within a certain probability, in this case 80%. It's easy to pick out which machines probably either got 4/4 or 0/4 rewards, and how it is fairly improbable that a 0/4 machine will turn out to be better than a 4/4 machine. Still, a large amount of uncertainty remains, and it's clearly nowhere near enough to pick out the best machine.", "Then I started using the solver to recommend which machine to play next. It was very interesting to intuitively feel the balance of exploration and exploitation as the algorithm sent me from one machine to another, with rewards or no rewards. After each trial I updated MACHAMP with the reward I got (0 or 1), and then asked for a recommendation for which to try next.", "The trail I made around the casino would have looked pretty wacky, like Billy wandering around the neighbourhood in old Family Circus comics. I was certainly fighting my urges to stick with a seemingly winning machine to the exclusion of all others, and instead randomly leaving the \u201chot\u201d ones to try seemingly unpromising ones that I hadn't thought about in ages. Humans don't think in Thompson-sampling optimal ways!", "I stopped after 1000 pulls of the slot machine levers, and took a look at what I had learned. First, there was an imbalance in which machines I had sampled, and it was towards the most promising machines based on successes.", "The amount of sampling was reflected in the final credible intervals, which were in general wider for the machines that seemed to be worse.", "Again these gave a clearer picture than the raw plot of posterior probabilities.", "I couldn't be confident which was the very best (like no one ever was), but I could see which were among the best, and how they differed from which were probably the worst, e.g. machine 5, that returned exactly 0 rewards in 8 pulls.", "If all I cared about was getting an accurate read on each machine, I could have just spread all 1000 pulls evenly across the 19 machines, 52 pulls each, but this would have led to a lot of lost coins as I kept playing machines that were clearly losers, what is called regret. Although to save time I didn't track my winnings, or even count the jackpots, after 1000 pulls, MACHAMP had amplified my bankroll from 120 to 3977 coins.Based on these results, I decided to focus on machine 9, which was indeed one of the ones above the old man. It had one of the best estimated reward probabilities (42.1%), but, also important, had a narrow credible interval, thanks to all the times I tried it (119): I could be confident it's definitely among the best.I did another 1000 pulls just on machine 9, both to test these estimates in practice, and to make that coin. (Also, it was election day, and it was better than hitting reload on the news\u2026) Across all 1119 pulls I won 37.7% of the time, which is noticeably lower than the MACHAMP estimate \u2014 though just within the 80% credible interval. I think the algorithm is conservatively biased towards guessing 50%, as a result of the uniform prior (starting with the guess that all values between 0 and 100% being equally likely). Knowing what I know now, that these machines probably don't pay out more than 40%, I could have started with a different prior that would let me get more accurate estimates with the same number of trials.", "In any case sticking with this \u201chot\u201d machine made my bankroll grow!", "For this period of exploiting machine 9, I started keeping track of my holdings over time, and learned that only jackpots, 300 coins, mattered \u2014 the other wins just keep you in the game. I made 5666 coins in 1000 pulls, with 21 jackpots, so machine 9 gave me an expected earning of 5.6 coins per pull. Using the Game Boy Advance emulator at high speed, I could probably do at least 1 pull per second (when I'm not entering results in a spreadsheet, let alone switching machines!) So that's about 336 coins per minute, the equivalent of 1 jackpot. I bet that's actually faster than one method recommended online for making money, of saving the emulator state before each pull (also I arbitrarily decided that feels too much like cheating!)", "I walked into the Celadon Game Corner with 120 coins, and walked out with over 10,000. Not only was I able to buy everything I needed to fight the Elite Four, I did it without having to rely on sketchy \u2014 and perhaps even superstitious \u2014 information from internet forums. MACHAMP gave me a strong read on which were the best machines, in the theoretically most efficient way. A MACHAMP-style solver could be used to play any gambling minigame of this type, which has a collection of choices each with different fixed probabilities of success, starting from zero knowledge. And indeed on any problem that could be modeled that way, such as which of several UI options appeals to users of an app. Just don't think you beat the real slot machines\u2014 they\u2019re a lot sneakier!", "Besides the Shadow Balls, Ice Beams, Flame Throwers, and Porygons I earned, by literally pulling the lever myself each time and seeing how the posterior distributions changed, I got what I was looking for: a gut level understanding of Thompson sampling and Bayesian reinforcement learning. To say it a different way: To catch coins was my real test; to train myself was my cause.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Tech Lead at Wayfair | Ph.D. Psychology, Bachelors in Computer Science | https://www.linkedin.com/in/drsaunders/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F81502e9d6be7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@danielrsaunders?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielrsaunders?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Daniel Saunders"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e91a549fc85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&user=Daniel+Saunders&userId=8e91a549fc85&source=post_page-8e91a549fc85----81502e9d6be7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://unsplash.com/@kvnga?utm_source=medium&utm_medium=referral", "anchor_text": "Kvnga"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Reinforcement_learning", "anchor_text": "Reinforcement learning"}, {"url": "https://tech.wayfair.com/data-science/2020/01/bayesian-product-ranking-at-wayfair/", "anchor_text": "including where I work as a data scientist, Wayfair"}, {"url": "https://en.wikipedia.org/wiki/Multi-armed_bandit", "anchor_text": "multi-armed bandit"}, {"url": "https://www.pngkit.com/view/u2w7e6q8o0a9q8q8_12-jul-multi-armed-bandit/", "anchor_text": "is a weird one"}, {"url": "https://www.alamy.com/stock-photo/loose-slots.html", "anchor_text": "\u201cloose\u201d"}, {"url": "https://github.com/drsaunders/PokeSlots/blob/main/PokeSlots.ipynb", "anchor_text": "(Full code and more detail available in the Jupyter notebook)"}, {"url": "http://digra.org/wp-content/uploads/digital-library/ScreenshotsFairUseRecommendations_DiGRA.pdf", "anchor_text": "that is fair use on the basis of teaching, scholarship, and research"}, {"url": "https://en.wikipedia.org/wiki/Thompson_sampling", "anchor_text": "Thompson sampling"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "updated using Bayesian logic"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_trial", "anchor_text": "Bernoulli trials"}, {"url": "https://github.com/lilianweng/multi-armed-bandit", "anchor_text": "code written by Lilian Weng"}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "excellent tutorial"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "Bayesian prior"}, {"url": "http://digra.org/wp-content/uploads/digital-library/ScreenshotsFairUseRecommendations_DiGRA.pdf", "anchor_text": "that is fair use on the basis of teaching, scholarship, and research"}, {"url": "http://digra.org/wp-content/uploads/digital-library/ScreenshotsFairUseRecommendations_DiGRA.pdf", "anchor_text": "that is fair use on the basis of teaching, scholarship, and research"}, {"url": "http://digra.org/wp-content/uploads/digital-library/ScreenshotsFairUseRecommendations_DiGRA.pdf", "anchor_text": "that is fair use on the basis of teaching, scholarship, and research"}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible intervals"}, {"url": "https://www.npr.org/2011/11/11/142218444/bil-keanes-dotted-line-an-appreciation", "anchor_text": "Billy wandering around the neighbourhood in old Family Circus comics"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_regret", "anchor_text": "regret"}, {"url": "https://www.theverge.com/2015/5/6/8544303/casino-slot-machine-gambling-addiction-psychology-mobile-games", "anchor_text": "they\u2019re a lot sneakier!"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----81502e9d6be7---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/videogames?source=post_page-----81502e9d6be7---------------videogames-----------------", "anchor_text": "Videogames"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----81502e9d6be7---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/pokemon?source=post_page-----81502e9d6be7---------------pokemon-----------------", "anchor_text": "Pokemon"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----81502e9d6be7---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%25C3%25A9mon-using-reinforcement-learning-81502e9d6be7&user=Daniel+Saunders&userId=8e91a549fc85&source=-----81502e9d6be7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%25C3%25A9mon-using-reinforcement-learning-81502e9d6be7&user=Daniel+Saunders&userId=8e91a549fc85&source=-----81502e9d6be7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F81502e9d6be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----81502e9d6be7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----81502e9d6be7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----81502e9d6be7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----81502e9d6be7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielrsaunders?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielrsaunders?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Saunders"}, {"url": "https://medium.com/@danielrsaunders/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "42 Followers"}, {"url": "https://www.linkedin.com/in/drsaunders/", "anchor_text": "https://www.linkedin.com/in/drsaunders/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e91a549fc85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&user=Daniel+Saunders&userId=8e91a549fc85&source=post_page-8e91a549fc85--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8e91a549fc85%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-beat-the-slots-in-pok%C3%A9mon-using-reinforcement-learning-81502e9d6be7&user=Daniel+Saunders&userId=8e91a549fc85&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}