{"url": "https://towardsdatascience.com/bewildering-brain-332d5192e95b", "time": 1682995949.7313411, "path": "towardsdatascience.com/bewildering-brain-332d5192e95b/", "webpage": {"metadata": {"title": "Bewildering Brain. Writing songs like Bob Dylan using\u2026 | by Alex Ingberg | Towards Data Science", "h1": "Bewildering Brain", "description": "This verses were not written by a poet or by a musician. They were written by a neural network trained over the complete lyrics of Bob Dylan. Can a robot create art? I leave that question to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/List_of_best-selling_music_artists", "anchor_text": "best-selling music artists of all time", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Bob_Dylan", "anchor_text": "Wikipedia", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow,", "paragraph_index": 4}, {"url": "https://www.python.org/", "anchor_text": "Python", "paragraph_index": 4}, {"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook", "paragraph_index": 5}, {"url": "https://www.jetbrains.com/pycharm/", "anchor_text": "Pycharm", "paragraph_index": 5}, {"url": "https://open.spotify.com/album/5k63xxy9YcKM0H9GS3vP1K?si=WEOvgNFASXGIeSLPYMGwbA", "anchor_text": "the self-titled debut album", "paragraph_index": 6}, {"url": "https://open.spotify.com/album/3LnS0XKSzd2TFoagESGUw3?si=W-CbGl-ZTGeDPvVHdhG3dQ", "anchor_text": "Tempest", "paragraph_index": 6}, {"url": "http://mulhod.github.io/bob_dylan_lyrics/index.html", "anchor_text": "Matt Mulholland\u2019s website", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/predicting-logics-lyrics-with-machine-learning-9e42aff63730", "anchor_text": "the work of Hans Kamin where he predicted Logic\u2019s lyrics", "paragraph_index": 15}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html", "anchor_text": "Pandas series", "paragraph_index": 16}, {"url": "https://open.spotify.com/track/6k9DUKMJpWvu6eFG3O64Lg?si=5KUMztmTQXOxWmuz6vszGw", "anchor_text": "now", "paragraph_index": 36}, {"url": "https://github.com/gsurma/text_predictor", "anchor_text": "text-predictor", "paragraph_index": 43}, {"url": "https://gsurma.github.io/", "anchor_text": "Greg Surma", "paragraph_index": 43}, {"url": "https://github.com/alexing/lyrics_prediciton", "anchor_text": "GitHub account", "paragraph_index": 45}, {"url": "https://github.com/alexing/lyrics_prediciton", "anchor_text": "my public GitHub account", "paragraph_index": 59}, {"url": "https://www.phillymag.com/news/2018/09/19/hologram-concert-roy-orbison-tupac/", "anchor_text": "There has already been holographic tours of late musicians.", "paragraph_index": 60}, {"url": "https://www.linkedin.com/in/alexingberg/", "anchor_text": "https://www.linkedin.com/in/alexingberg/", "paragraph_index": 62}], "all_paragraphs": ["By, in the dead just before the child, Everybody gotta have to come see her hand, But there\u2019s too much to hum, Father, when I could tell you down\u2026", "This verses were not written by a poet or by a musician. They were written by a neural network trained over the complete lyrics of Bob Dylan.Can a robot create art? I leave that question to philosophers. In this little experiment I\u2019ll only try to imitate as accurate as I can the style of Dylan\u2019s songs using machine learning.", "Regarded as an idol to many and an incomparable bastion of American music, Robert Allen Zimmerman, a.k.a. Bob Dylan, is arguably the biggest popular musician to have come out of the States ever. With a career spanning 6 (!!) decades and 38 studio albums, it\u2019s easy to see why. Oh, and don\u2019t forget the fact that he has sold more than 100 million records, making him one of the best-selling music artists of all time.What gathers us here today, though, is his lyrics. According to Wikipedia, his lyrics incorporate a wide range of political, social, philosophical, and literary influences, defied pop-music conventions and appealed to the burgeoning counterculture. Sounds interesting, right?", "This particular background and his unique and characteristic style has brought him worldwide recognition and a myriad of awards. The most prestigious one being the 2016 Nobel Prize in Literature \u201cfor having created new poetic expressions within the great American song tradition\u201d.", "For this little experiment I based myself around Tensorflow, a Python framework to handle neural networks.", "It is worth mentioning also that as environments, I used Jupyter Notebook and Pycharm and that everything was developed using Python 3.6.", "Included in the database are all the songs from the self-titled debut album to 2012\u2019s Tempest. I took the file from Matt Mulholland\u2019s website. This saved me a lot of time, so thanks Matt!", "I used two algorithms/techniques in particular: Markov Chains and RNNs (Recurrent Neural Networks). I propose a comparison and analysis of which one outputs better results and how they both perform.", "Markov Chains are stochastic processes whose main characteristic is the fact that the probability of an event depends pure and exclusively on the previous event. This lack of memory as a property is known as Markov\u2019s Property.It sounds complicated, but it\u2019s super easy to understand.", "If we understood rain as a Markov process, the probability of rain only depends on if it rained yesterday.", "There\u2019s two states: rains or doesn\u2019t rain.", "If we are in a sunny day, the probability that it rains the next day is 40%, so there\u2019s 60% chances the next day will be sunny.", "If it\u2019s raining, there\u2019s a 30% chance that the next day it\u2019ll rain again and 70% it\u2019ll be sunny.", "And just like that, day after day we could calculate the probabilities for rain only basing ourselves in the previous day: no matter if its wet season, no matter the global warming and no matter the geographical region.", "Now back on subject, one can understand text as a Markov process, where every word has the probability of appearing depending only on which was the previous word written. This way, \u201cpredicting text\u201d becomes something possible.", "To apply Markov to the lyrics of Dylan, I took the work of Hans Kamin where he predicted Logic\u2019s lyrics as a big reference. The idea was to create a bigram Markov Chain which represents the English language. More specifically, creating a tuple dictionary of consecutive words. Because it\u2019s bigrams we are utilizing, and not single words (unigrams), we obtain a better precision in our predictions and, above all, better readability in the lyrics created by the model. This means, the next word of a sentence is predicted with the two previous words instead of a single last word.", "Using Pandas series, I iterated over all the lyrics in the database and used special tags for the line start, line end, and newlines (\\n): \"<START>\", \"<END>\", and \"<N>\" respectively.", "To predict, we start in the dictionary with the key (None, \"<START>\")(must be the first link of the chain), and then we sample in a random way \u2014 but respecting the distribution \u2014 a word in the list connected to that key; we move the key towards the word we just sampled after. We continue this process until we get to \"<END>\".", "Here are some of my favorite phrases that the algorithm spat out:", "You angel, you, you\u2019re as f \u2014 got me under your wing, The way you walk into the room where my pencil is at.", "I\u2019m bound to get up in the Great North Woods, working as a parting gift. Summer days, summer nights are gone, I know I just wouldn\u2019t have a temperature?", "I went to tell you, baby, Late last night I dream of bells in the dirt in the wind. I walked out to the higher calling of my confession,", "I went into the cup above a blind man at the mouth, he began to think I could, I can\u2019t let go of my lambs to the heart", "How I wandered again to my knees, Don\u2019t need a woman I meet, Putting her in hope not to me, baby, I\u2019ll do anything in this telephone wire.", "How\u2019m I supposed to get ill. Then they bring them clothes! Woo-hoo! Well, I don\u2019t know by now, It ain\u2019t me you\u2019re looking for, babe.", "Come, Look out your window ledge. How long\u2019s it gonna take away my highway shoes.", "To anyone passing by, There was no more letters, no! Not unless you mail them from Desolation Row.", "Like its obvious, the lyrics, even though they have a clear Dylanesque style, feel like a cutout of reality. Like a copy-paste of his work, a collage of different verses to create a new song. Many phrases are identical to the ones written by Bob.It was to be expected, as when we search for higher readability while using bigrams, we also reduce the variance in the words predicted. As a result, we get 3 or more words that come from the same verse. Using unigrams isn\u2019t the answer either since the meaning would be lost in not respecting the syntactic and morphological order of the words: it\u2019d be a word soup in random order.", "Markov chains bring the advantage of being easy to implement and using less variables to predict results: but it came hand by hand with bad predictions. To counter this, I jumped to a more complex model: a recurrent neural network, or RNN.Another detail worth mentioning: the algorithm will continue predicting, no matter the length, until reaching the end of the chain (the tag <END>). This means that a run of the algorithm can output 2 verses and the next one 50.", "Recurrent networks are a type of artificial neural network that recognizes patterns in data sequences like text, genomes, handwriting, oral language or numerical time series originating from sensors, stock market or governmental agencies. These algorithms take time and sequence into account; they have a temporal dimension.", "In comparison to Markov Chains, recurrent networks have memory.", "To understand recurrent networks, first you need to understand the basics about common networks: feedforward ones. Both types of networks are called neuronal because of the way in which they channel information through a series of mathematical operations which are carried out in the nodes of the network. One carries information until the end without touching the same node more than once, while the other runs cycles over the same network. The latter one is called recurrent.", "In the case of feedforward networks, feeding them with input you get an output. In supervised learning the output would be a label. This means data gets mapped raw to a category recognizing patterns which decide, for example, if an image used as input can be classified as cat or dog.", "Recurrent networks, on the other hand, take as input not only the current example being seen, but they also consider what has been perceived previously in time.", "The decision taken in timet-1is going to affect the one being taken a moment after, at time t. So recurrent networks have two sources of input: the present and the recent past, which combine to determine how to respond to new data \u2014 just like a human brain.", "We need to also analyze the concept of Long Short-Term Memory (LSTM) to finish understanding the complete process.\u00bfWhat would happen if I feed my network all the songs by Dylan and tell it to finish the title of this song?", "We know the next word is Blues (if you don\u2019t know the song, it\u2019s of the utmost importance you listen to it now). The network won\u2019t know, as this information doesn\u2019t repeat many times, or its occurrence is not close enough to be remembered.", "For humans, it\u2019s pretty obvious and intuitive that if a phrase appears in the title of a book, it must be an important part of the plot; even in cases when it\u2019s the only time it appears. Contrary to a RNN, we would definitely remember. Even though they fail to remember, there\u2019s techniques like LSTM networks which handle this kind of situations in a successful way.", "While RNNs remember everything up until a certain limited depth, LSTM networks learn what to remember and what to forget.", "This allows LSTM networks to reach and use memories that are beyond the RNNs range. Memories that because of their perceived importance where remembered in the first place by LSTM networks.", "How do we achieve this? Recurrent networks, in general, have a simple structure where modules are repeated, where data flows through. The simple ones are commonly created with layers which use simply hyperbolic tangents (tanh) as activation functions.", "On the other hand LSTM networks come with a more complicated structure; combining these with several other functions, among them sigmoids. They not only count with input and output gates, but also a third gate which we could call forget gate. It receives if the information is worth remembering. If negative, the information is deleted.", "How does the decision making work? Each gate is associated with a weight. For each iteration, the sigmoid function is applied to the input and as output we receive a value between 0 and 1. 0 means nothing gets through and 1 means everything does.Afterwards, each value of each layer gets updated with a mechanism of back-propagation. This allows the gate to learn, with time, which information is important and which one is not.", "To implement all this, I based myself on the text-predictor from Greg Surma. I implemented some small changes to the model, adapted it to Python 3 and played a little bit with the hyperparameters until I got satisfactory results.", "The model is character-based: all the unique characters were calculated with their respective frequencies. The tensors were created replacing each character with their indicated frequency. The length of the output is predefined by a parameter. In this case it\u2019s fixed.", "For more detail, you can check my code in my GitHub account.", "Enough with technicalities: let\u2019s check the results!", "The interesting point in this is not only seeing the final result, but the learning process the algorithm had in each iteration. How it went from a concoction of characters to formed verses in just a couple cycles.", "We can also appreciate the learning curve, where we can regard at how the loss function gets minimized until it gets established around an asymptotic value near 0.6 in less than 90k iterations.", "You know nothing, RNN model. After a cold start, the model gets initialized with trash and random characters.", "The model learned, in a few iterations, which are the indicated characters to create a Dylan song. Also which is its shape: the sizes of verses and the basic punctuation rules like capital letters before starting sentences and the usage of commas.", "Some words are already real words and the morphological relation between words starts to show: adjectives and article as modifiers to nouns. Circumstantial modifiers, objects and predicates after verbs.", "Errors in word predictions start to be less frequent: less vocabulary mistakes.", "Reaches an excellent morphological understanding of the verses: even if the the words don\u2019t make much sense; it has the form of a poetry or song lyrics.", "The amount of errors has diminished noticeably. It definitely looks like it was written by a human. Maybe that human is over the recommended dose of Xanax, but is human after all.", "OK, alright. I accept it. Still we have some vocabulary errors and the lyrics may not have much sense.Even if lyrics generated by an artificial intelligence still have these little flaws, we can certainly see that the model learned correctly to copy the style of the provided dataset.", "If we consider the fact that the network learned how to do all this from scratch, and that in the beginning it didn\u2019t have any understanding of what a letter or a word (without even mentioning the English Grammar) are, we can agree that the results are surprising. We were capable of detecting logical patterns in a dataset and reproduce them: and in no moment the network had any input on what the language was, or which was the rule set on it or even if what was being processed were clinical images from medical patients or the corpus of Shakespeare\u2019s works.", "In this article I planned on comparing two very different methods to predict texts. On the one hand, Markov Chains bring the advantage of being easy to implement. Big theoretical or technological knowledge is not needed to develop them: but predictions come out pretty basic and fell short of expectations. The future of this field is clearly in the use of RNNs, even when implementing, running and testing them take a long time, processing power, space in disk and memory for the tensors, and specially an advanced technological and theoretical knowledge.", "To further improve precision, we could contrast the output predictions in posterior stage with a dictionary. This could be made of unique words taken from the database or the English Language Dictionary. In this way, if the predicted word is not present, we could eliminate it or exchange it for the word with the greatest similarity (less distance).", "Again, if you wish, you can check my code in my public GitHub account.", "How much longer until artificial lyrics arrive? Who will be the first to squeeze the juice out of this market? There has already been holographic tours of late musicians. Roy Orbison, Michael Jackson y Tupac are just some examples worth mentioning. Will this new era of \u201cmusic after death\u201d be the perfect excuse for artificial lyrics?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist, computer engineer and music fanatic. Here I\u2019m half music recommendator and half data adventurer. https://www.linkedin.com/in/alexingberg/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F332d5192e95b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----332d5192e95b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alexing?source=post_page-----332d5192e95b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexing?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Alex Ingberg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8487882ad14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&user=Alex+Ingberg&userId=e8487882ad14&source=post_page-e8487882ad14----332d5192e95b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/List_of_best-selling_music_artists", "anchor_text": "best-selling music artists of all time"}, {"url": "https://en.wikipedia.org/wiki/Bob_Dylan", "anchor_text": "Wikipedia"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow,"}, {"url": "https://www.python.org/", "anchor_text": "Python"}, {"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook"}, {"url": "https://www.jetbrains.com/pycharm/", "anchor_text": "Pycharm"}, {"url": "https://open.spotify.com/album/5k63xxy9YcKM0H9GS3vP1K?si=WEOvgNFASXGIeSLPYMGwbA", "anchor_text": "the self-titled debut album"}, {"url": "https://open.spotify.com/album/3LnS0XKSzd2TFoagESGUw3?si=W-CbGl-ZTGeDPvVHdhG3dQ", "anchor_text": "Tempest"}, {"url": "http://mulhod.github.io/bob_dylan_lyrics/index.html", "anchor_text": "Matt Mulholland\u2019s website"}, {"url": "https://towardsdatascience.com/predicting-logics-lyrics-with-machine-learning-9e42aff63730", "anchor_text": "the work of Hans Kamin where he predicted Logic\u2019s lyrics"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html", "anchor_text": "Pandas series"}, {"url": "https://open.spotify.com/track/6k9DUKMJpWvu6eFG3O64Lg?si=5KUMztmTQXOxWmuz6vszGw", "anchor_text": "now"}, {"url": "https://github.com/gsurma/text_predictor", "anchor_text": "text-predictor"}, {"url": "https://gsurma.github.io/", "anchor_text": "Greg Surma"}, {"url": "https://github.com/alexing/lyrics_prediciton", "anchor_text": "GitHub account"}, {"url": "https://github.com/alexing/lyrics_prediciton", "anchor_text": "alexing/lyrics_predicitonComparing Markov Chains with RNNs on text prediction \u2014 alexing/lyrics_predicitongithub.com"}, {"url": "https://github.com/alexing/lyrics_prediciton", "anchor_text": "my public GitHub account"}, {"url": "https://www.phillymag.com/news/2018/09/19/hologram-concert-roy-orbison-tupac/", "anchor_text": "There has already been holographic tours of late musicians."}, {"url": "https://towardsdatascience.com/predicting-logics-lyrics-with-machine-learning-9e42aff63730", "anchor_text": "Predicting Logic\u2019s Lyrics with Machine LearningUsing machine-learning concepts, we can analyze Logic\u2019s style of hip-hop and predict lyrics he would actually write.towardsdatascience.com"}, {"url": "https://skymind.ai/wiki/lstm", "anchor_text": "A Beginner\u2019s Guide to LSTMs and Recurrent Neural NetworksData can only be understood backwards; but it must be lived forwards. \u2014 S\u00f8ren Kierkegaard, Journals Contents Actually\u2026skymind.ai"}, {"url": "https://towardsdatascience.com/text-predictor-generating-rap-lyrics-with-recurrent-neural-networks-lstms-c3a1acbbda79", "anchor_text": "Text Predictor \u2014 Generating Rap Lyrics \ud83d\udcc4Language Modeling with Recurrent Neural Networks (LSTMs)towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----332d5192e95b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----332d5192e95b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----332d5192e95b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----332d5192e95b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----332d5192e95b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&user=Alex+Ingberg&userId=e8487882ad14&source=-----332d5192e95b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&user=Alex+Ingberg&userId=e8487882ad14&source=-----332d5192e95b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F332d5192e95b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----332d5192e95b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----332d5192e95b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----332d5192e95b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----332d5192e95b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----332d5192e95b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----332d5192e95b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexing?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexing?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alex Ingberg"}, {"url": "https://medium.com/@alexing/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "233 Followers"}, {"url": "https://www.linkedin.com/in/alexingberg/", "anchor_text": "https://www.linkedin.com/in/alexingberg/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8487882ad14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&user=Alex+Ingberg&userId=e8487882ad14&source=post_page-e8487882ad14--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe8487882ad14%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbewildering-brain-332d5192e95b&user=Alex+Ingberg&userId=e8487882ad14&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}