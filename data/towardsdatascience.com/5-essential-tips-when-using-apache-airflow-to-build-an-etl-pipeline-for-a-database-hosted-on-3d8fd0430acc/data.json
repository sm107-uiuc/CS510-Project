{"url": "https://towardsdatascience.com/5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc", "time": 1683013187.396459, "path": "towardsdatascience.com/5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc/", "webpage": {"metadata": {"title": "5 essential tips to build an ETL pipeline for a database hosted on Redshift using Apache Airflow | by Tran Nguyen | Towards Data Science", "h1": "5 essential tips to build an ETL pipeline for a database hosted on Redshift using Apache Airflow", "description": "Apache Airflow is one of the best workflow management systems (WMS) that provides data engineers with a friendly platform to automate, monitor, and maintain their complex data pipelines. Started at\u2026"}, "outgoing_paragraph_urls": [{"url": "https://airflow.incubator.apache.org/", "anchor_text": "Apache Airflow", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/why-quizlet-chose-apache-airflow-for-executing-data-workflows-3f97d40e9571", "anchor_text": "popular choice", "paragraph_index": 0}, {"url": "https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94", "anchor_text": "this step-by-step guideline", "paragraph_index": 0}, {"url": "https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a", "anchor_text": "this series", "paragraph_index": 0}, {"url": "https://medium.com/@dustinstansbury/beyond-cron-an-introduction-to-workflow-management-systems-19987afcdb5e", "anchor_text": "in general", "paragraph_index": 0}, {"url": "https://github.com/jghoman/awesome-apache-airflow", "anchor_text": "this resource", "paragraph_index": 0}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift/blob/master/Data_pipeline_Apache_Airflow_on_AWSRedshift.ipynb", "anchor_text": "Part of the Jupyter notebook version of this tutorial", "paragraph_index": 4}, {"url": "https://github.com/nhntran", "anchor_text": "github.", "paragraph_index": 4}, {"url": "https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://stackoverflow.com/questions/43631693/how-to-stop-kill-airflow-tasks-from-the-ui", "anchor_text": "some approaches to stop/kill the Airflow task on the UI", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/White_Walker", "anchor_text": "White Walkers", "paragraph_index": 20}, {"url": "https://stackoverflow.com/questions/43606311/refreshing-dags-without-web-server-restart-apache-airflow", "anchor_text": "As someone mentioned", "paragraph_index": 21}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift/blob/master/Data_pipeline_Apache_Airflow_on_AWSRedshift.ipynb", "anchor_text": "my Github repository", "paragraph_index": 29}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift", "anchor_text": "The Jupyter notebook, codes, .cfg file, etc.", "paragraph_index": 35}, {"url": "https://github.com/nhntran", "anchor_text": "my Github", "paragraph_index": 35}], "all_paragraphs": ["Apache Airflow is one of the best workflow management systems (WMS) that provides data engineers with a friendly platform to automate, monitor, and maintain their complex data pipelines. Started at Airbnb in 2014, then became an open-source project with excellent UI, Airflow has become a popular choice among developers. There are many good resources/tutorials for Airflow users at various levels. You can start learning Airflow with many good tutorials such as this step-by-step guideline, or this series on Medium in which you can also learn about workflow management system in general. For users already familiar with Airflow, this resource may help you gain a very deep understanding of many aspects of Airflow.", "In this post, I simply want to share my experience when creating a data warehouse ETL pipeline on AWS with Airflow. I hope it is helpful. And please, correct me if you found something wrong in my post.", "1. This article assumes that you already have some working knowledge of data warehouse, AWS, Amazon Redshift in particular, Apache Airflow, command line environment, and Jupyter notebook.", "2. This is your responsibility for monitoring usage charges on the AWS account you use. Remember to terminate the cluster and other related resources each time you finish working.", "3. This is one of the assessing projects for the Data Engineering nanodegree on Udacity. So to respect the Udacity Honor Code, I would not include the full notebook with the workflow to explore and build the ETL pipeline for the project. Part of the Jupyter notebook version of this tutorial, together with other data science tutorials could be found on my github.", "Sparkify is a startup company working on a music streaming app. Through the app, Sparkify has collected information about user activity and songs, which is stored as a directory of JSON logs (log-data - user activity) and a directory of JSON metadata files (song_data - song information). These data reside in a public S3 bucket on AWS.", "This project would be a workflow to explore and build an ETL (Extract \u2014 Transform \u2014 Load) pipeline that:", "Below is the complete Directed Acyclic Graph \u2014 DAG \u2014 with the operators used for the project. (If you don\u2019t know what DAGs or operators are, please read a quick definition of Airflow concepts here).", "With this post, I will not discuss in detail how the project was done step by step. Rather, I just walk you through some crucial tips and issues that I encountered when working with Airflow. I hope it will save you so much time and effort dealing with many weird states you may have when building the workflow.", "Your DAG, the high-level outline that defines tasks in a particular order, should be as simple as possible. It is obviously the best practice in programming, but easy to be forgotten.", "Why should we start with a simple DAG?", "Below is the final DAG configuration requirement for my project:", "Basically, the DAG does not have dependencies on past runs; the start_date is on Jan 12, 2019; the pipeline would be scheduled to run every hour. On failure, the task is retried 3 times; the retries happen every 5 minutes.I was so naive trying to use that final DAG configuration for my first run, end up getting overwhelmed when triggering my DAG on the Airflow UI. There were so many runs in the queue. And many, so many more indeed, might come after these.", "Where all these runs in the queue come from?", "Then, what does a simple DAG look like?", "In my debugging version, the DAG would run right away when triggered ('start_date': datetime.now()) with only 1 run at a time (max_active_runs=1,)) and run only once (schedule_interval= None) without any retry on failure (as default in DAG). This simple DAG will instantly stop when any task fails, which enable us to debug our DAG easily.", "As mentioned in Tip 1, it is quite tricky to stop/kill Airflow tasks. There are some approaches to stop/kill the Airflow task on the UI. The approach that works perfectly for me is as follows:", "Step 1: Turn off the DAG", "Step 2: Delete all the runs", "On the Airflow menu bar, choose Browse -> DAG Runs -> Checkbox to select all the runs -> With Selected -> Delete", "Note that you have to turn off the DAG first, otherwise, you may see the White Walkers and the Army of the Dead in action: the executor may continue to schedule and start the new runs even though you just deleted all the DAG runs.", "With the Airflow default settings, when you update the associated python file, DAGs should be reloaded. As someone mentioned, when the webserver is running, it refreshes DAGs every 30 seconds by default. We are good to go when seeing our DAGs \u201cis now fresh as a daisy\u201d:", "Generally, the Airflow webserver can smoothly handle DAG loading failures in most cases, but not all the time. I spent a whole day trying to figure out what was wrong with my code. I even reload a dummy \u2014 nothing-could-be-wrong DAG, but still could not fix the bug that led to the broken DAG:", "It turned out that there was nothing wrong with my DAG, I only have to refresh the workspace to overcome the webserver issues due to DAG parsing.", "In summary, when there is a broken DAG issue, and you are sure that it is not because of your code, you may try:", "Refresh Workspace will copy all your files to a new machine, therefore, every problem related to file parsing may be solved. After finishing my Airflow project, I have to admit thatRefresh Workspace is my ultimate solution for many Broken DAG issues.", "The Airflow UI may notify that you have a broken DAG, however, it will not show the problem of your DAG. The detailed issues in the broken DAG could be seen by manually reloading the DAGs using python -c \"from airflow.models import DagBag; d = DagBag();\" or starting the Airflow webserver again using /opt/airflow/start.sh on the command line. Below is a demonstration for this:", "AWS resources could be set up and monitored through either the AWS console or through IaC (Infrastructure-as-code). I prefer the IaC approach since it helps developers automate, maintain, deploy, replicate, and share complex infrastructures easily. There are 3 options for IaC on AWS:", "We could use IaC to build, launch Amazon Redshift cluster and print out all the necessary configuration information for Airflow connection, such as host, schema, login_user, password, etc., without clicking and searching around on the AWS console. I found it extremely convenient and time-saving, especially when we have to terminate the cluster/clean up the resources and then create, launch, configure the cluster again another day to reduce AWS costs.", "The whole end-to-end process for setting up and cleaning Amazon Redshift and other AWS resources, as well as a demo for creating a simple data warehouse ETL pipeline on AWS with Airflow, could be found on my Github repository.", "To run DAGs with AWS service, you may use Airflow\u2019s UI to set up the connection, such as configuring your AWS credentials, connect to Redshift, etc.", "However, these connections might be lost when the server is stopped or when you refresh your workspace. To avoid unnecessary failure, it is recommended to check if these connections are available before running your DAGs.", "During DAG development, manually re-setup these Airflow connections, again and again, is a troublesome and error-prone process, so I prefer to programmatically set up the Airflow Connection as a python script file. You can run the script on the command line when necessary. The simple python script for setting up Airflow connections is as followed:", "As a good practice, instead of putting all AWS credentials and the Redshift information directly to the script, when creating the Redshift cluster (as described in Tip 4) you may export these to a .cfg file, and then use the Python packageconfigparser to get the necessary information.", "Thank you so much for reading this post. Good luck with your project, and I am more than happy for any question and discussion.", "The Jupyter notebook, codes, .cfg file, etc. for this post, together with other data science tutorials could be found on my Github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "My random thoughts through the career development process"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3d8fd0430acc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tranhnnguyenvn?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Tran Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F90ed820b9879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&user=Tran+Nguyen&userId=90ed820b9879&source=post_page-90ed820b9879----3d8fd0430acc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://airflow.incubator.apache.org/", "anchor_text": "Apache Airflow"}, {"url": "https://towardsdatascience.com/why-quizlet-chose-apache-airflow-for-executing-data-workflows-3f97d40e9571", "anchor_text": "popular choice"}, {"url": "https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94", "anchor_text": "this step-by-step guideline"}, {"url": "https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a", "anchor_text": "this series"}, {"url": "https://medium.com/@dustinstansbury/beyond-cron-an-introduction-to-workflow-management-systems-19987afcdb5e", "anchor_text": "in general"}, {"url": "https://github.com/jghoman/awesome-apache-airflow", "anchor_text": "this resource"}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift/blob/master/Data_pipeline_Apache_Airflow_on_AWSRedshift.ipynb", "anchor_text": "Part of the Jupyter notebook version of this tutorial"}, {"url": "https://github.com/nhntran", "anchor_text": "github."}, {"url": "https://www.udacity.com/course/data-engineer-nanodegree--nd027", "anchor_text": "the Data Engineering nano degree program on Udacity."}, {"url": "https://towardsdatascience.com/some-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59", "anchor_text": "my other related project"}, {"url": "https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94", "anchor_text": "here"}, {"url": "https://stackoverflow.com/questions/43631693/how-to-stop-kill-airflow-tasks-from-the-ui", "anchor_text": "It is quite hacky to stop/kill Airflow tasks from the UI"}, {"url": "https://stackoverflow.com/questions/43631693/how-to-stop-kill-airflow-tasks-from-the-ui", "anchor_text": "some approaches to stop/kill the Airflow task on the UI"}, {"url": "https://en.wikipedia.org/wiki/White_Walker", "anchor_text": "White Walkers"}, {"url": "https://stackoverflow.com/questions/43606311/refreshing-dags-without-web-server-restart-apache-airflow", "anchor_text": "As someone mentioned"}, {"url": "https://boto3.amazonaws.com/v1/documentation/api/latest/index.html", "anchor_text": "Python SDK"}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift/blob/master/Data_pipeline_Apache_Airflow_on_AWSRedshift.ipynb", "anchor_text": "my Github repository"}, {"url": "https://github.com/nhntran/Data-pipeline-Airflow-on-AWSRedshift", "anchor_text": "The Jupyter notebook, codes, .cfg file, etc."}, {"url": "https://github.com/nhntran", "anchor_text": "my Github"}, {"url": "https://medium.com/tag/programming?source=post_page-----3d8fd0430acc---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/airflow?source=post_page-----3d8fd0430acc---------------airflow-----------------", "anchor_text": "Airflow"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3d8fd0430acc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/infrastructure-as-code?source=post_page-----3d8fd0430acc---------------infrastructure_as_code-----------------", "anchor_text": "Infrastructure As Code"}, {"url": "https://medium.com/tag/aws?source=post_page-----3d8fd0430acc---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&user=Tran+Nguyen&userId=90ed820b9879&source=-----3d8fd0430acc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&user=Tran+Nguyen&userId=90ed820b9879&source=-----3d8fd0430acc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3d8fd0430acc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3d8fd0430acc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3d8fd0430acc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tran Nguyen"}, {"url": "https://medium.com/@tranhnnguyenvn/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F90ed820b9879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&user=Tran+Nguyen&userId=90ed820b9879&source=post_page-90ed820b9879--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff136f72e8c83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-essential-tips-when-using-apache-airflow-to-build-an-etl-pipeline-for-a-database-hosted-on-3d8fd0430acc&newsletterV3=90ed820b9879&newsletterV3Id=f136f72e8c83&user=Tran+Nguyen&userId=90ed820b9879&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}