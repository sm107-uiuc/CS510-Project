{"url": "https://towardsdatascience.com/machine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be", "time": 1683010855.520465, "path": "towardsdatascience.com/machine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be/", "webpage": {"metadata": {"title": "Deep Dive Into Desision Trees and Random Forest | by Vardaan Bajaj | Towards Data Science", "h1": "Deep Dive Into Desision Trees and Random Forest", "description": "In the previous post, we went through Support Vector Machines in great detail and also solved fraudulent credit card transaction dataset from Kaggle. In this post, we\u2019ll be looking at 2 more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/machine-learning-iv-support-vector-machines-kaggle-dataset-with-svms-57d7c885652a", "anchor_text": "previous", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_pruning", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Ensemble_learning", "anchor_text": "ensemble", "paragraph_index": 77}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "here", "paragraph_index": 88}, {"url": "https://medium.com/analytics-vidhya/machine-learning-ii-logistic-regression-explained-data-pre-processing-hands-on-kaggle-728e6a9d4bbf", "anchor_text": "this", "paragraph_index": 88}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "here", "paragraph_index": 89}, {"url": "https://www.kaggle.com/vardaanbajaj/titanic-random-forest", "anchor_text": "here", "paragraph_index": 104}, {"url": "https://towardsdatascience.com/machine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b", "anchor_text": "next", "paragraph_index": 105}, {"url": "http://linkedin.com/in/vardaan-bajaj-23a279124/", "anchor_text": "linkedin.com/in/vardaan-bajaj-23a279124/", "paragraph_index": 107}], "all_paragraphs": ["In this post, we\u2019ll go through:", "In the previous post, we went through Support Vector Machines in great detail and also solved fraudulent credit card transaction dataset from Kaggle. In this post, we\u2019ll be looking at 2 more supervised learning algorithms: Decision Trees and Random Forest. After completing this post, we would have pretty much covered all the widely used supervised machine learning algorithms in the industry.", "The term decision tree is pretty much self-explanatory and its working is similar to a human\u2019s decision making power. How do humans make decisions? First of all, humans define the objective. In machine learning, the objective of a particular task is determined by the dataset. Once humans have a defined objective, they answer various questions/perform various tasks in order to achieve that objective. The first task that is performed to achieve the objective is analogous to the root node of the decision tree. It is representative of the objective of the decision tree. The root node branches out to a certain set of options and each of these options have their own set of options. This process perpetuates until we arrive at the final decision. Now, each of these options is analogous to a decision node and the final decision is represented by a leaf node. In almost all the cases, the root node also acts as a decision node. For huge decision trees, the output of a decision node is a number of decision nodes, except at the deepest level of the tree, where it is the leaf node.", "Decision Tree modelling is a supervised learning algorithm which can be used for both, continuous and discrete valued datasets, both in regression and classification problems. Most commonly, a decision tree is used as a classifier that recursively partitions data into categories. Decision tree is a directed tree i.e. once we reach particular node in the tree, we cannot backtrack to the previous node i.e. parent nodes are not accessible from child nodes.", "After seeing the above example of a decision tree, we can see that the algorithm decides that the best question to ask first is the color of the car. All other questions follow some order as well. The order of asking questions is extremely important otherwise the decision tree can become very complex for a simple scenario like the one shown above. How does the decision tree determine the order of questions it needs to ask? The order of questions being asked depends on which question leads to a good split of the data at that level. The split of data at each node happens in a way similar to binary search. At the root node, we have the entire dataset. The root node should split the data into fed to it into 2 or more groups, where the data in each group has similar attributes. Suppose the root node splits data into 3 groups, then these 3 groups will be the 3 children of the root node and each of these 3 nodes will perform this operation of splitting the data, creating more children and this process is carried on for each node until that node has no more splits to make (leaf node), which is the point where predictions are made.", "How does a decision tree decide which question leads to a good split and which question doesn\u2019t? This is quantified by measuring impurity.", "The problem of learning an optimal decision tree is NP-Complete, so in order to better mimic the optimal solution, the concept of impurity came into existence. Impurity is a measure of homogeneity of data. Data is said to be pure or homogenous if it only contains a single class. The more the classes in the data, the more impure it is. Each node in the decision tree, except the leaf nodes, contains data which has a potential of splitting into further groups i.e. each node has some sort of impurity related to data. Less impure nodes require less information to describe them and more impure nodes require more information to describe them. Hence, sub-nodes of a node have more purity than their parent nodes.", "There are various methods to measure impurity, but the most commonly used are Information Gain and Gini Index. Let\u2019s look at them one by one.", "Information Gain is used to determine which feature/attribute gives us the maximum information about a class. It is based on the concept of entropy. Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder. Higher the entropy, more the randomness (i.e. less purity) and it becomes difficult to draw conclusions from the given information. For totally pure data sample i.e. when only one class exists, the entropy is the least (0) and if data is distributed between all the classes equally, then the entropy is the highest (1).", "The binary cross-entropy loss function that we defined for Logistic Regression is quite similar to entropy. For a dataset with \u2018c\u2019 different classes, entropy is measured as:", "where pi represents the fraction of examples in a given class i. In the example in the image above, there are 2 classes so c=2. p(yes) = 9/14 (0.64) and p(no) = 5/14 (0.36). Once entropy for a node is computed, then the information gain of a specific output given a particular feature is calculated as:", "where S is the output class, A is a particular feature of the dataset and P has the same meaning defined above (for pi). Due to the high computational time of the log function, Gini Index is preferred over information gain for practical purposes.", "Formally, Gini Index measures the probability of a particular variable being wrongly classified when it is randomly chosen. The probability value of 0 means that the variable couldn\u2019t be wrongly classified and it\u2019s possible only when we have only one output class i.e. the data is 100% pure. As the Gini Index value increases, the chances of misclassification of a particular variable increases since the impurity increases. The information conveyed by these values resonate with the entropy values. For a dataset with \u2018c\u2019 classes, Gini Index is defined as:", "where pi represents the fraction of examples in a given class i, which is similar to the entropy definition.", "There are many more measures of impurity like chi-square, classification error, etc. but the main idea here was to familiarize the readers with impurity measurement methods. One important thing to note here is that these impurity measurement functions play a role analogous to the cost function.", "Constructing a decision tree is actually partitioning of the input data features, which leads to 2 or more sub-nodes and this process is carried on recursively for each sub-node. Once this tree is created, features have been partitioned throughout the nodes.", "Various decision tree algorithms for classification are compared along the following lines:", "Some of the various algorithms for decision tree construction are CART, ID3, C4.5, C5.0, CHAID, MARS, etc. In this post, we\u2019ll be discussing CART in great detail. CART is one of the most widely used and a highly effective decision tree algorithm.", "As the name suggests, CART algorithm is used to generate both, classification and regression decision trees. We\u2019ll be focussing on the classification part here. It is used to solve multi-class classification problem (for binary classification, it generates a binary tree) and uses Gini index as a metric to evaluate the split of a feature node in the decision tree.", "In CART algorithm, the objective is to minimize the cost function (Gini Index) at each node. The selection of the input variables/features that decides the specific split for each node is selected in a greedy way to minimize the cost function. In this greedy way, a number of split points with different set of variables/features are considered and that split is chosen which results in the minimum value for the Gini Index (i.e. more homogenous splits) at that node. This process is carried out recursively for all sub-nodes in the tree.", "This process can continue on forever and can lead to the formation of many unnecessary sub-nodes, so it needs to be stopped. For this, we count the total number of training examples that pass through each node of the tree. This number is a hyperparameter and is tuned according to the dataset and the optimal choice of this number leads to formation of robust decision trees. To give some intuition, if a node of the tree only has one training example that passes through it, this means that the generated decision tree is suffering from overfitting since it has given so much importance to just one example that it called for its separate node. Such a decision tree needs to reduce its complexity which is achieved using pruning.", "Decision tree pruning is a technique to reduce the size of a decision tree by removing sections of the tree that provide little power to the classification objective. 2 popular pruning methods are:", "More about these pruning methods can be found here. Now that we know the underlying logic behind the CART algorithm, let\u2019s see through an example of binary classification, how a decision tree is constructed using CART. Let\u2019s use 10 records of UCI machine learning Zoo Animal Classification dataset to build a decision tree by hand. Given the features \u201ctoothed\u201d, \u201chair\u201d, \u201cbreathes\u201d, \u201clegs\u201d, the decision tree should output the species of the animal (mammal/reptile).", "Let\u2019s start with each feature one by one. We observe in the data that all the input features are represented by Boolean values.", "(i) toothed: Let\u2019s summarize the output (species) wrt the \u2018toothed\u2019 feature values.", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018toothed\u2019, we use the weighted sum of the above calculated values as:", "(ii) hair: Let\u2019s summarize the output (species) wrt the \u2018hair\u2019 feature values.", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018hair\u2019, we use the weighted sum of the above calculated values as:", "(iii) breathes: Let\u2019s summarize the output (species) wrt the \u2018breathes\u2019 feature values.", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018breathes\u2019, we use the weighted sum of the above calculated values as:", "(iv) legs: Let\u2019s summarize the output (species) wrt the \u2018legs\u2019 feature values.", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018legs\u2019, we use the weighted sum of the above calculated values as:", "Now that we have calculated Gini Index for the output variable against all 4 input features, we need to choose the root node. The root node is chosen such that it has minimum Gini Index. The lower the Gini Index, the higher the chances of predicting the answer early (due to more purity of the node). So here, we choose hair as the root node. The decision tree at this point looks like:", "Let\u2019s first consider the case when \u2018hair\u2019 takes the values of TRUE.", "Let\u2019s calculate Gini Index for the output against the input features excluding the hair feature, when hair takes the value of TRUE.", "(i) toothed: Let\u2019s summarize the output (species) wrt the \u2018toothed\u2019 feature values when hair = TRUE", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018toothed\u2019 for \u2018hair=true\u2019, we use the weighted sum of the above calculated values as:", "(ii) breathes: Let\u2019s summarize the output (species) wrt the \u2018breathes\u2019 feature values when hair = TRUE", "Gini Index (hair = true & breathes = true) = 0", "Gini Index (hair = true & breathes = false) = 0", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018breathes\u2019 for \u2018hair=true\u2019, we use the weighted sum of the above.", "Gini Index (breathes & hair = true) = 0", "(iii) legs: Let\u2019s summarize the output (species) wrt the \u2018legs feature values when hair = TRUE", "Gini Index (hair = true & legs = false) = 0", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018legs\u2019 for \u2018hair=true\u2019, we use the weighted sum of the above.", "In all the above cases, the Gini Index is equal to 0. This means that if we predict that the species has hair, it can be classified as a \u2018mammal\u2019 based on the given dataset with high confidence. At this point, the decision tree looks like:", "Now, let\u2019s consider the case when \u2018hair\u2019 takes the values of FALSE.", "Let\u2019s calculate Gini Index for the output against the input features excluding the hair feature, when hair takes the value of FALSE.", "(i) toothed: Let\u2019s summarize the output (species) wrt the \u2018toothed\u2019 feature values when hair = FALSE", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018toothed\u2019 for \u2018hair=false\u2019, we use the weighted sum of the above calculated values as:", "(ii) breathes: Let\u2019s summarize the output (species) wrt the \u2018breathes\u2019 feature values when hair = FALSE", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018breathes\u2019 for \u2018hair=false\u2019, we use the weighted sum of the above calculated values as:", "(iii) legs: Let\u2019s summarize the output (species) wrt the \u2018legs\u2019 feature values when hair = FALSE", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018legs\u2019 for \u2018hair=false\u2019, we use the weighted sum of the above calculated values as:", "In the above 3 cases, we see that the Gini Index is minimum for the \u2018legs\u2019 feature given the condition \u2018hair=FALSE\u2019. So, \u2018legs\u2019 is chosen as the child node when \u2018hair=FALSE\u2019. At this point, the decision tree looks like:", "To proceed further, let us consider the cases when legs = TRUE (hair = FALSE is implied now).", "Let\u2019s calculate Gini Index for the output against the input features excluding the hair and legs feature, when hair takes the value of FALSE and legs takes the value of TRUE.", "(i) toothed: Let\u2019s summarize the output (species) wrt the \u2018toothed\u2019 feature values when hair = FALSE and legs = TRUE.", "Gini Index (hair = false & legs = true & toothed=true) = 0", "Gini Index (hair = false & legs = true & toothed=false) = 0", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018toothed\u2019 for \u2018hair=false\u2019 and legs=\u2019true\u2019, we use the weighted sum of the above.", "Gini Index (toothed & hair = false & legs = true) = 0", "(ii) breathes: Let\u2019s summarize the output (species) wrt the \u2018breathes\u2019 feature values when hair = FALSE and legs = TRUE.", "Gini Index (hair = false & legs = true & breathes=false) = 0", "To obtain the final Gini index of \u2018species\u2019 wrt \u2018toothed\u2019 for \u2018hair=false\u2019 and legs=\u2019true\u2019, we use the weighted sum of the above.", "In the above 2 cases, we see that \u2018toothed\u2019 has lower Gini Index, so it is selected as a child node when \u2018legs = TRUE\u2019. The decision tree now looks like:", "Now, let\u2019s consider the case when \u2018legs\u2019 takes the value of FALSE.", "From the above scenario, we can clearly see that when \u2018hair = TRUE\u2019 and \u2018legs = FALSE\u2019, the output \u2018species\u2019 is \u2018Reptile\u2019 in all the cases. Hence, we without calculating the Gini Index, we can safely conclude that the child node produced for the decision tree will be a leaf node with the value \u2018Reptile\u2019. (Remember that when we just considered \u2018hair = TRUE\u2019 case, we saw that all the output \u2018species\u2019 values were \u2018Mammals\u2019 and by calculating Gini Index, we directly produced a leaf node with the value \u2018Mammal\u2019)", "Proceeding further, let\u2019s consider the case when \u2018toothed = TRUE\u2019. Here, the cases \u2018hair = FALSE\u2019 and \u2018legs = TRUE\u2019 are implied.", "Here, we only have one record which is classified as mammal. Hence, this combination of features results in the output \u2018species\u2019 being a \u2018Mammal\u2019.", "Considering the case \u2018toothed = FALSE\u2019, we get", "In a similar way, we can conclude that the output \u2018species\u2019 for this set of features is a \u2018Reptile\u2019. Now, we have finally constructed the decision tree on which we can throw some test data to make predictions.", "If you\u2019ve followed carefully so far, you may have observed that all the variables in this dataset were categorical variables (i.e. the features had a finite set of distinct values (2) like TRUE and FALSE). In the datasets that you encounter in real life have a mixture of categorical and continuous variables. In order to construct decision trees from a dataset having continuous values, these continuous values are converted to categorical values by defining a certain threshold. Using this, decision trees can be constructed for any type of data.", "Although decision trees are quite intuitive and simple to create, they suffer from the following drawbacks.", "1. They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.", "2. Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.", "3. Decision Trees tend to overfit to the training data very quickly and can become very inaccurate.", "Due to the aforementioned disadvantages of Decision Trees, Random Forest algorithm is used instead of Decision Trees. Random Forest algorithm employs the use of a large number of decision trees that operate as an ensemble. A machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.", "Random Forest algorithm is based on the concept of The Wisdom of Crowds. Wisdom of Crowds is a very effective technique employed in various tasks of computer science. The fact that Wikipedia (fully crowdsourced, editable by everyone) and Britannica (fully written by experts) have been found to have similar levels of quality standards is a testament to the power of the wisdom of crowds. Every individual in the crowd makes their prediction with a certain error. Since all the individuals act independently, their errors are also independent to each other\u2019s. In other words, these errors are uncorrelated i.e. when considered in a bunch, these errors cancel each other out and what we\u2019re left with is a good, accurate prediction. This is the way how Random Forest algorithm operates.", "Random Forest algorithm uses a large number of decision trees (creating a forest), which are independent of/uncorrelated to each other, from a given dataset, which collectively outperform any of its constituent trees. This little to no correlation between the constituent trees in the forest imparts the element of randomness to this ensemble method, which produces a wonderful effect of protecting trees from each other for their individual errors. But we need to make sure that the outputs of the various decision trees that we make randomly for random forest to work should not be completely random. In order to ensure this, we need to make sure that:", "(i) The output should be dependent on all the training features and not only some of them in order to make random decision trees effective.", "(ii) The predictions made by each decision tree should have low correlations with each other in order to make the wisdom of crowds concept to work.", "The first point is the feature of the dataset that we collect. So, as a data scientist, how do we make sure that the constituent decision trees are as diversified as possible? For this, random forest uses the following 2 techniques:", "One of the major drawbacks of Decision Trees was that they were not robust to even small changes in the data which made them prone to overfitting. Let\u2019s understand how bagging solves this problem. Consider a training dataset with M ( >> 50) training examples and for simplicity let\u2019s consider these are M numbers in the range of 1 to 50. If we want to use B decision trees for Random Forest, then through bagging, each of the B decision trees is made up of M training examples, but instead of using all the M training examples, a random subset of these training examples (here numbers in the range from 1 to 50) is used and numbers from this subset are randomly repeated until a total of M training examples are created.", "How does bagging help? By creating a large number of decision trees with a large number of training examples distributed randomly across decision trees, the aggregate result is a random forest model which is robust to even small changes in the data, thereby eliminating one of the major disadvantages of decision trees (by using decision trees).", "The idea of using random training examples can also be applied to the features of the training data. Recall that while constructing decision trees, when deciding the split at each node, we take all the features into account. If the training dataset has N features, then in feature randomness, while creating each of the B decision trees, the decision of splitting a node is made by using a random subset of these N features instead of using all N features.", "When both bagging and feature randomness are used to make a forest of decision trees for random forest, each of the decision trees constitutes of a random subset of training examples as well as a random subset of features. This randomness adds a lot of robustness and stability to a Random Forest classifier by ensuring the formation of uncorrelated decision trees that protect each other from their errors. Once all these constituent decision trees make a prediction, majority voting takes place in order to get the final prediction of the classifier.", "We\u2019ve looked at a lot of concepts so far. From the mathematics behind decision trees to creating a decision tree by hand and then seeing how decision trees are used in random forest, we\u2019ve come a long way. Now is the time that we get hands on practice for a Random Forest classifier and for this post, we won\u2019t be dealing with a new dataset, instead we\u2019ll be improving upon our Logistic Regression model which we trained over the Titanic dataset in this post. This is the way actual machine learning tasks are carried out. Various machine learning models are tried and the one that works the best and justifies the results obtained is used in practice. Remember, persistence is the key. So let\u2019s get started.", "The dataset for the Titanic problem can be found here. We have to predict whether a passenger on the Titanic will survive or not given the data corresponding to them. We have already applied data pre-processing and logistic regression in this post. Now, we will use the same pre-processed data from the previous post, but will apply Random Forest classification this time. Dataset after pre-processing looks like:", "After applying recursive feature elimination(RFE), we obtain the top 8 most important features that will help in training the Random Forest Classifier. More on RFE can be found here.", "Now that we have the list of features we should use for training the classifier, let us apply Random Forest classifier on the dataset. Unlike Logistic Regression, Random Forest has a large number of parameters that need to be fed into the model. The most important parameters are:", "1. n_estimators = number of trees in the foreset", "2. max_features = max number of features considered for splitting a node", "3. max_depth = max number of levels in each decision tree", "4. min_samples_split = min number of data points placed in a node before the node is split", "5. min_samples_leaf = min number of data points allowed in a leaf node", "6. bootstrap (like bagging) = method for sampling data points (with or without replacement)", "These are a lot of parameters and it is difficult to obtain their optimal value together manually. So, we make use of the computation power by passing the model a set of values for each parameter, the parameter applies these values to the dataset and chooses the best ones. For this dataset, this took me about 5 minutes to get optimal values using Random Hyperparameter Grid technique.", "On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.", "Now that we have an optimal set of parameters from the given range supplied to the algorithm, let\u2019s train the random forest classifier, both with default values that sci-kit learn provides and with the values we obtained.", "Now that we have trained these models, let\u2019s evaluate them using the evaluation function defined below.", "We observe that the base model had a test accuracy of 84.33% whereas the model in which we fed the parameters obtained RandomizedSearchCV yielded test accuracy of 81.34%, which should have been higher than the base model\u2019s accuracy. This can be due to the fact that the training dataset was not big enough or the values we supplied for random search to choose from might have included the range of optimal values. We can definitely toy around with it to improve our accuracy though.", "Similarly, I also applied GridSearchCV to check for the accuracy of the classifier which was again around 81%. The relatively low accuracy might be due to the reasons posited above.", "But, the key takeaway from this exercise is that we improved our model\u2019s accuracy on the test set from 77.6% using Logistic Regression to around 84% using Random Forest algorithm, which indeed justified our discussion of Random Forest algorithm above. When I submitted the results of this model on Kaggle, I received a public score of 0.7751, which is better than the score we received for Logistic Regression classifier (0.7703), although not by much, but good enough to take a step forward to improve an already existing robust classifier.", "The entire code for this post can be found here.", "That\u2019s it for this post. In all the posts so far, we have explored the most frequently used supervised machine learning algorithms in great detail. From the next post onwards, we\u2019ll dive deep into the basics of Unsupervised Learning and also learn about various unsupervised learning algorithms, starting with K-means clustering.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Development Engineer at American Express with a keen interest in the field of Data Science and Web3. linkedin.com/in/vardaan-bajaj-23a279124/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3ebfe6d584be&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vardaanbajaj?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Vardaan Bajaj"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F168771086803&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&user=Vardaan+Bajaj&userId=168771086803&source=post_page-168771086803----3ebfe6d584be---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/analytics-vidhya/machine-learning-iv-support-vector-machines-kaggle-dataset-with-svms-57d7c885652a", "anchor_text": "previous"}, {"url": "https://www.researchgate.net/figure/An-example-of-a-simple-decision-tree_fig2_311614501", "anchor_text": "Source"}, {"url": "https://www.saedsayad.com/decision_tree.htm", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_pruning", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Ensemble_learning", "anchor_text": "ensemble"}, {"url": "https://en.wikipedia.org/wiki/Random_forest", "anchor_text": "Source"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "here"}, {"url": "https://medium.com/analytics-vidhya/machine-learning-ii-logistic-regression-explained-data-pre-processing-hands-on-kaggle-728e6a9d4bbf", "anchor_text": "this"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "here"}, {"url": "https://www.kaggle.com/vardaanbajaj/titanic-random-forest", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/machine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b", "anchor_text": "next"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3ebfe6d584be---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3ebfe6d584be---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----3ebfe6d584be---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----3ebfe6d584be---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----3ebfe6d584be---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&user=Vardaan+Bajaj&userId=168771086803&source=-----3ebfe6d584be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&user=Vardaan+Bajaj&userId=168771086803&source=-----3ebfe6d584be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3ebfe6d584be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3ebfe6d584be---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3ebfe6d584be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vardaan Bajaj"}, {"url": "https://medium.com/@vardaanbajaj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "84 Followers"}, {"url": "http://linkedin.com/in/vardaan-bajaj-23a279124/", "anchor_text": "linkedin.com/in/vardaan-bajaj-23a279124/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F168771086803&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&user=Vardaan+Bajaj&userId=168771086803&source=post_page-168771086803--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F342b25109362&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be&newsletterV3=168771086803&newsletterV3Id=342b25109362&user=Vardaan+Bajaj&userId=168771086803&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}