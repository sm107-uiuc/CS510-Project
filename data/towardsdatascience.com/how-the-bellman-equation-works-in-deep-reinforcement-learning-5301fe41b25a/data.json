{"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "time": 1683003833.3132591, "path": "towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a/", "webpage": {"metadata": {"title": "How the Bellman equation works in Deep RL? | Towards Data Science", "h1": "How does the Bellman equation work in Deep RL?", "description": "How the Bellman equation and Neural Networks work together in Deep Q-learning ? We give an implementation of this and some details of tensor calculations using PyTorch."}, "outgoing_paragraph_urls": [{"url": "https://github.com/openai/gym/wiki/Table-of-environments", "anchor_text": "Table of environments", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Inverted_pendulum", "anchor_text": "Inverted Pendulum", "paragraph_index": 4}, {"url": "https://www.youtube.com/watch?time_continue=72&v=XiigTGKZfks", "anchor_text": "can be controlled", "paragraph_index": 4}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "CartPole-v0", "paragraph_index": 4}, {"url": "https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equ", "anchor_text": "Bellman equation", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75", "anchor_text": "\u03b5-greedy mechanism", "paragraph_index": 27}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Project-1_Navigation-DQN", "anchor_text": "navigate and collect", "paragraph_index": 43}, {"url": "https://towardsdatascience.com/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58", "anchor_text": "The Approximation Power of Neural Networks (with Python codes)", "paragraph_index": 59}, {"url": "https://www.linkedin.com/pulse/brief-introduction-reinforcement-learning-xiaofei-zheng", "anchor_text": "Brief Introduction to Reinforcement Learning", "paragraph_index": 60}, {"url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "anchor_text": "Reinforcement Learning: Bellman Equation and Optimality (Part 2)", "paragraph_index": 61}, {"url": "https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go", "anchor_text": "AlphaZero: Shedding new light on chess, shogi, and Go", "paragraph_index": 62}], "all_paragraphs": ["In the Bellman equation, the value function \u03a6(t) depends on the value function \u03a6(t+1). Despite this, the value of \u03a6(t) can be obtained before the state reaches time t+1. We can do this using neural networks, because they can approximate the function \u03a6(t) for any time t. We will see how it looks in Python. In the last two sections, we present an implementation of Deep Q-learning algorithm and some details of tensor calculations using the PyTorch package.", "The Markov decision process (MDP) provides the mathematical framework for Deep Reinforcement Learning (RL or Deep RL). For the real problem, we define in MDP the following parameters: {S, A, R, P, \u03b3}, where S is the state space, A is the action space, R is the set of rewards, P is the set of probabilities, \u03b3 is the discount rate.", "While in Computer Vision, the agent learns from a Big Number of Images, the agent in Deep RL learns from a Big Number of Episodes, where for any state, the agent explores several actions and receives different replies (rewards) from the MDP environment.", "If there is only one action for each state and all rewards are the same, the MDP is reduced to a Markov chain. For a large number of MDP environments, see Table of environments of OpenAI/gym.", "One example of an MDP environment is Cartpole (a.k.a. an Inverted Pendulum), a pendulum with a center of gravity above its pivot point. It\u2019s unstable, but can be controlled by moving the pivot point under the center of mass. For environment CartPole-v0, the states and actions are as follows:", "For this environment, the state space has dimension=4 and is of type Box(4). The action space has dimension=2 and is of type Discrete(2).", "Policy map \ud835\udf0b, deterministic and stochastic policies", "The policy map \ud835\udf0b is defined as \ud835\udf0b(a|s) = Pr{At = a | St = s} meaning that the policy \ud835\udf0b is the probability of action a performed at state s (at time t).", "Example. Consider recycling robot equipped with arms to grab the cans. The state space S =[low, high] , where \u2018low\u2019 and \u2018high\u2019 are the states of the robot charge, the action space A =[search, recharge, wait]. We consider two policy types: deterministic and stochastic.", "Assume that the state space is discrete, which means that the agent interacts with its environment in discrete time steps. At each time t, the agent receives a state St including the reward Rt. The cumulative reward is named return, we denote it as Gt. The future cumulative discounted reward is calculated as follows:", "Here, \u03b3 is the discount factor, 0 < \u03b3 < 1. Thus, the return at the time t can be obtained using the return at the time t+1, namely", "This is the recursive relation for the return value Gt.", "The state-value function for the policy \ud835\udf0b is defined as follows:", "Here, \ud835\udd3c\ud835\udf0b is the expectation for Gt, and \ud835\udd3c\ud835\udf0b is named as expected return. By (1) and (2) we derive the eq. (3). This is the Bellman equation.", "Thus, the state-value v_\ud835\udf0b(s) for the state s at time t can be found using the current reward R_{t+1} and the state-value at the time t+1.", "Now, we would like to define the action-value function associated with the policy \ud835\udf0b :", "We can introduce comparison of two policies as follows:", "In this case, we say that policy \ud835\udf0b\u2019 is better than policy \ud835\udf0b.", "It is not necessary that any two policies are comparable, however, there is always a policy which is better than all other policies. Such a policy is said to be optimal policy, it is denoted by \ud835\udf0b*. An optimal policy is guaranteed to exist, but may be not the only one.", "The goal of the agent is to find the optimal policy. Finding the optimal policy is the main goal of Deep RL.", "Different optimal policies have the same value function, we denote it by v*. In fact, we have v* = v(\ud835\udf0b*). The function v* it is said to be the optimal state-value function. The optimal state-value function can be defined as follows:", "For any deterministic policy \ud835\udf0b, the action a is uniquely determined by the current state s, i.e, a = \ud835\udf0b(s). Then, for the deterministic policy \ud835\udf0b in (4), the action can be dropped, i.e., we get eq. (2). In other words, for the deterministic policy \ud835\udf0b, we have the following relation between state-value function and action-value function:", "This is not so for stochastic policies", "Similarly to optimal action-value function v*(s), see (5), we define the optimal action-value function q*(s,a) as follows:", "Suppose, we have an optimal action-value function q*(s,a). Then the optimal policy can be determined as follows:", "Here, A(s) is the set of actions possible for the state s.", "For the deterministic policy \ud835\udf0b , we find the new action for the current state by relation a = \ud835\udf0b(s). For the stochastic policy \ud835\udf0b, we can find the new action by relation a = \ud835\udf0b*(s), where \ud835\udf0b* is the optimal policy, see (7).", "Here\u2019s what an agent should do: first find the optimal action-value function, and then find the optimal policy using formula (7). The last statement is true with some limitations. Exceptions are possible, for example, due to \u03b5-greedy mechanism.", "A Q-table is the matrix of the shape [state, action]. We initialize all slots of this matrix to zero. The appropriate Python code is as follows:", "We will do updates in Q(s, a) for every pair (s,a) after each step or action.", "How is the Q-table is updated after a single step? The most popular method for updating Q-table is the Temporal Difference Learning or TD-learning. We add updates on each step until the episode ends.", "The return Gt in eq.(8) is called an alternative estimate , see (1). This value a.k.a. TD-target. The value Q(s_t, a_t) in (8) is called a current estimate. Using (1), we can rewrite eq. (8) as follows:", "Sarsa is acronym for the sequence state\u2013action\u2013reward\u2013state\u2013action. These five elements of the sequence are as follows:", "The agent is in the current state s_t, then the agent chooses the action a_t, gets the reward r_t, after that the agent enters the state s_{t+1}, and chooses the following action a_{t+1}. This loop is executed for all episodes until value num_episodes, see pseudo-code of algorithm Sarsa below. At each step, the Q-value Q(s, a) is updated by (9), see the yellow line in of the Sarsa pseudo-code.", "The learning rate \u03b1 determines the behavior of the algorithm Sarsa. Too large values \u03b1 will keep our algorithm far from convergence to optimal policy. If \u03b1=1 then Q(s_t, a_t) \u2190 Gt, i.e. Q-value always will be most recent return, no any learning. The too small values \u03b1 lead to learning too slow. If \u03b1=0 then Q(s_t, a_t) \u2190 Q(s_t, a_t), never updated.", "Algorithm Q-learning (a.k.a Sarsamax) differ from Sarsa in eq.(9) as follows: Instead of the value of Q(s,a) at time t, we use the maximum value of Q(s,a), where a runs through all possible actions in the moment t, see (10) and the yellow line in the Q-learning pseudo-code.", "At any time step t, for state s_t, there exists at least one action a, whose estimated value Q(s_t, a) is maximal. This action a is called greedy action. The associated policy \ud835\udf0b*(s) is called greedy policy, see eq. (7). When we select one of greedy actions, we are exploiting our current knowledge of the actions. If instead, we choose one of the non-greedy actions, then we are exploring, because this enables to improve our estimate of the non-greedy action\u2019s value.", "An off-policy agent learns the optimal policy independently of the agent\u2019s actions. An on-policy agent learns the policy being carried out by the agent. Q-learning is an off-policy algorithm because the optimal policy is learning by greedy action a_gr in the formula for maximum, see (10), however, the next action a_t can be another one. Sarsa is an on-policy algorithm because in (9) the agent learns optimal policy and behaves using the same policy Q(s_t,a_t).", "Q-learning may have worse performance in each episode than Sarsa, however, Q-learning learns the optimal policy.", "Answer: by a neural network. We will see how it looks in Python.", "The reason that so much attention is paid to neural networks is because they can approximate the output of any continuous mathematical function. This is possible due to the Kolmogorov theorem stating that multivariate functions can be expressed via a combination of sums and compositions of (a finite number of) univariate functions.", "Our Q-value function, the function of two vector parameters Q(s, a) can be represented as a certain artificial neural network (nn) as exact as we want.", "The realization of Q-learning algorithm with the Deep Learning technology, i.e., with neural networks is called Deep Q-Network or DQN.", "Python package PyTorch is an open source deep learning library developed by Facebook\u2019s AI Research lab. We provide several DQN snippets implemented with using PyTorch. This code is taken from my implementation of training an agent with the \u2018Banana\u2019 environment. The agent is trained to navigate and collect bananas in a certain square world. However, this code is fairly general and can be used for many environments with discrete state space.", "We present several fragments that help to understand how, using neural networks, we can elegantly implement the DQN algorithm.", "Function Agent.__init__: two neural networks (q_local and q_target) are constructed by the model Qnetwork. Each model Qnetworkcontains two hidden layers.", "The difference between Q_expected and Q_targets should be minimized using PyTorch methods, see method learn().", "In class ReplayBuffer: values of s_t (state) and s_{t+1} (next_state) are sampled by function sample(), the data is stored by function add().", "In method dqn(): double loop by episodes and time steps; here, the values \u2018state\u2019, \u2018next_ state\u2019, \u2018action\u2019, \u2018reward\u2019 and \u2018done\u2019 are generated.", "Here are some tips related to PyTorch methods, see figure \u201cThree tensors\u201d below.", "Method detach() indicates that no backpropagation for gradient of tensor loss will be executed for Q_targets_next. This is possible since tensor loss depends only on Q_targets and Q_expected, see method learn() .", "The shape of each network here is [64, 4] where 64 is the number of states in the batch (BATCH_SIZE=64), and 4 is the number of possible actions( move forward, move backward, turn left, turn right) . The expression max(1) means getting the maximum for each of the 64 states. The maximum is taken by running through all 4 actions. Recall that Q-Learning finds max value running over all actions, see (10). In the figure below, we give a numerical example of 64 x 4 tensor self.q_target(next_states).detach().", "The fact is that max(1) returns the list of two tensors: max(1)[0], the tensor containing maximum values; max(1)[1], the tensor containing values \u2018column number\u2019 at which maximum was found. We need only max(1)[0], see figure above.", "Now we want to place this row vector of the shape [64] in the column with the form [64,1]. This is done using the unsqueeze(1) method, see tensor Q_targets_next in the figure above.", "How we calculate Q_targets? For any \u2018state\u2019 in the batch, the value \u2018done\u2019 is 1 if the episode is finished, otherwise \u2018done\u2019 is 0. Then the line of Q_targets is calculated by eq.(10) if and only if the associated episode is not finished.", "This method gathers values along the axis specified by dim = 1. Note that dim=0 is associated with rows, dim=1 means columns. Each row in self.q_local(states) consists of four Q-values associated with four actions. Thus, for each row, along the columns, the method gather takes Q-value associated with the action number in the tensor actions, see figure below.", "Combining the Bellman equation, Neural Networks, Kolmogorov\u2019s theorem, we get an amazing technology, Deep RL. This technology provides new approaches and new algorithms that can solve previously unsolvable problems.", "What \u201cunsolvable problems\u201d do Bellman equations and Deep RL actually allow? Let us point, for example, to the project AlphaZero, a computer program which is master the games of Chess, Shogi and Go. AlphaZero within 24 hours of training achieved a superhuman level of play in Chess by defeating world-champion program Stockfish.", "We examined one particular case of Deep RL, the Deep Q-learning algorithm. In the last two sections, we presented an implementation of this algorithm and some details of tensor calculations using the PyTorch package.", "[1] M. Tavora, The Approximation Power of Neural Networks (with Python codes) (2019), TowardsDataScience", "[2] X. Zheng, Brief Introduction to Reinforcement Learning (2019), LinkedIn", "[4] A. Singh, Reinforcement Learning: Bellman Equation and Optimality (Part 2) (2019), TowardsDataScience", "[6] D. Silver, T. Hubert, J. Schrittwieser, D. Hassabis, AlphaZero: Shedding new light on chess, shogi, and Go (2018), DeepMind", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. in Math, Algorithm and SW developer, Researcher. Fan of Deep Learning and Neural Networks. @r.stekol"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5301fe41b25a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://r-stekol.medium.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5----5301fe41b25a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/openai/gym/wiki/Table-of-environments", "anchor_text": "Table of environments"}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "CartPole"}, {"url": "https://en.wikipedia.org/wiki/Inverted_pendulum", "anchor_text": "Inverted Pendulum"}, {"url": "https://www.youtube.com/watch?time_continue=72&v=XiigTGKZfks", "anchor_text": "can be controlled"}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "CartPole-v0"}, {"url": "https://gym.openai.com/docs/#spaces", "anchor_text": "spaces"}, {"url": "https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equ", "anchor_text": "Bellman equation"}, {"url": "https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75", "anchor_text": "\u03b5-greedy mechanism"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Project-1_Navigation-DQN", "anchor_text": "navigate and collect"}, {"url": "https://towardsdatascience.com/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58", "anchor_text": "The Approximation Power of Neural Networks (with Python codes)"}, {"url": "https://www.linkedin.com/pulse/brief-introduction-reinforcement-learning-xiaofei-zheng", "anchor_text": "Brief Introduction to Reinforcement Learning"}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Deep Reinforcement Learning with Double Q-learning"}, {"url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "anchor_text": "Reinforcement Learning: Bellman Equation and Optimality (Part 2)"}, {"url": "https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4", "anchor_text": "RL \u2014 DQN Deep Q-network"}, {"url": "https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go", "anchor_text": "AlphaZero: Shedding new light on chess, shogi, and Go"}, {"url": "https://medium.com/tag/bellman-equation?source=post_page-----5301fe41b25a---------------bellman_equation-----------------", "anchor_text": "Bellman Equation"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----5301fe41b25a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----5301fe41b25a---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----5301fe41b25a---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/dqn?source=post_page-----5301fe41b25a---------------dqn-----------------", "anchor_text": "Dqn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----5301fe41b25a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----5301fe41b25a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5301fe41b25a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5301fe41b25a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5301fe41b25a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5301fe41b25a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5301fe41b25a--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://r-stekol.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbc96182db31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a&newsletterV3=57ce87a178e5&newsletterV3Id=bc96182db31e&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}