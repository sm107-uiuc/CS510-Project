{"url": "https://towardsdatascience.com/zen-and-the-art-of-model-optimization-5fcf7c405b28", "time": 1683001837.8309171, "path": "towardsdatascience.com/zen-and-the-art-of-model-optimization-5fcf7c405b28/", "webpage": {"metadata": {"title": "Zen and the Art of Model Optimization: An NLP Challenge | Towards Data Science", "h1": "Zen and the Art of Model Optimization", "description": "Comparing models in a social media NLP challenge. Simple algorithms can yield surprisingly powerful results. If a machine learning model is worth running, then it is worth running optimized."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Zen and the Art of Motorcycle Maintenance was one of my favorite books in college. Set amidst a father-son motorcycle journey across the United States, the book considers how to lead a meaningful life. Arguably, the key message expounded by the author, Robert Pirsig, is that we achieve excellence only when we are fully engaged, heart and mind, with the task at hand. If something is worth doing, then it is worth doing well.", "At about the same time, research design and statistical inference courses drilled the importance of interpretability and parsimony into me. Communication is indeed widely stated (hoped?) as an essential skill for a data scientist. It is undoubtedly easier to explain to lay audiences the findings and workings of certain models than other models. For many of us in our day-to-day work, optimized interpretable models can do almost as well, and sometimes even outperform more sophisticated models. If a model is worth running, then it is worth running optimized.", "Methodical data cleaning, careful feature selection/engineering, judicious choice of scoring metrics, and model hyper-parameter fine-tuning can boost model performance powerfully. In this article, I pit Logistic Regression (logit) and Multinominal Naive Bayes (MNB) against Random Forest (RF) classifier and Support Vector Classification (SVC) in a natural language processing challenge.", "Logistic regression has long been used for statistical inference and is the workhorse of machine learning classification tasks, while MNB is considered one of the basic models for NLP. Although random forest algorithms are not \u201cblack boxes\u201d, they are less interpretable than either logit or MNB. Support vector machines, however, are definitely black boxes. I take no position in the great debate between prediction and inference and pick NLP because this is one field where prediction is the sole goal.", "The dataset of choice is from Crowdflower\u2019s Data For Everyone Library that contains 5,000 messages from US politicians\u2019 social media accounts. The messages have been classified along several facets, such as partisan bias, target audience and subject matter. I will seek to predict the political bias \u2014 \u201cpartisan\u201d or \u201cneutral\u201d \u2014 of the messages.", "There are equal numbers of Facebook posts and Twitter tweets in the dataset, but only 26% of the messages are classed as \u201cpartisan\u201d. We thus have an imbalanced dataset, with almost three times as many \u201cneutral\u201d messages as \u201cpartisan\u201d ones. In other words, if this were a bag with a combination of four black and white balls in it, there would be only (approximately) one black ball against three white ones. So I would have long odds against me if my goal was to pick a black ball, namely predict a \u201cpartisan\u201d message.", "The \u201cpartisan\u201d class is encoded as 1 and \u201cneutral\u201d as 0 in the target variable. Given my goal of predicting \u201cpartisan\u201d messages, I will eschew the overall model accuracy score and instead focus on the F1 score on the \u201cpartisan\u201d (minority) class, along with Cohen\u2019s kappa statistic.", "Say we have a binary variable that is classed as either \u201cpositive\u201d or \u201cnegative\u201d, which are terms used purely to differentiate between the two categories. In terms of the stylized confusion matrix depicted below, we have the predicted class of a binary variable along the horizontal axis and the actual class along the vertical axis.", "Naturally, predictions are likely not 100% accurate, so we could group those \u201cpositive\u201d predictions as being either correct (true positives) or wrong (false positives). We could do the same for the \u201cnegative\u201d predictions. The precision rate for the \u201cpositive\u201d class would be the sum of the true positives divided by the total positive class predictions (or true positives + false positives). So precision is the probability of the model picking a true positive from among its predicted positive observations.", "The recall rate, on the other hand, would be the ratio of the number of true positives divided by the sum of the true positives and false negatives. Note that the false negatives are those observations that the model had incorrectly labeled as \u201cnegative\u201d, but were in fact \u201cpositive\u201d. So recall describes the probability of the model picking a true positive from among the actual positive observations.", "The precision and recall rates are more useful measures of a model\u2019s predictive power when the positive class is a minority in imbalanced datasets, and the chief goal is to correctly identify those positive observations. The F1 score is the harmonic mean of the precision and recall rates.", "Cohen\u2019s kappa is a broader statistic that tells us how much better the model performed in predicting both \u201cpositive\u201d and \u201cnegative\u201d classes than random chance alone. A kappa statistic above 0 means that the model is better than chance, and the higher it gets towards the maximum ceiling of 1 means the better the model is at classifying the data.", "The \u201cbag of words\u201d (BoW) is the basic machine learning technique in NLP. There are countless Internet posts describing BoW in detail, so it is unnecessary for me to do so at length. It is sufficient to say that the BoW approach plucks individual words from pre-classified texts, disregarding grammar and order. It assumes that meaning and similarity between texts are encoded in the abstracted vocabulary. While this may sound rather simplistic, it does work surprisingly well.", "One major problem with BoW is that extracting individual words as features quickly result in an embarrassment of riches, otherwise known as the \u201ccurse of dimensionality\u201d. Apart from the potential problem of having more features than observations, word frequency typically follows a long-tail Zipfian distribution, which can affect the performance of generalized linear models adversely. If these weren\u2019t problematic enough, multicollinearity is another issue.", "In our sample of 5,000 social media posts, for example, there will be tens of thousands of individual words and symbols extracted, many of which are irrelevant (i.e. https, www, @, today etc.). The dimensionality curse tends to be an issue for logistic regression (exacerbated by the long-tail distribution), plus others such as kNN and decision trees. MNB, however, tends to do well in this context. RF could do well too given that it is based on repeated sampling with random combinations of regressors.", "We thus need to be ruthless in winnowing down the potential number of variables. I started by cleaning the text, scrubbing out irrelevant symbols, and augmenting the stop-words\u2019 list to pare down the feature dimensionality. A comparison of two messages from the dataset are shown below, juxtaposing the raw and cleaned versions. Clearly, more work is required on the cleaned messages, and the next step is to impose a stop-words list.", "Next should follow stemming or lemmatization, and typically just one or the other is adequate. Stemming merges words together by discarding plurality, gender, tense, case, cardinality etc., and is basically a form of NLP feature engineering to reduce the feature space. I inserted the Porter stemming algorithm into CountVectorizer to stem the text data. Other feature engineering options I could use are the n_grams and min_df parameters in CountVectorizer.", "I ran GridsearchCV on the following four models: logit, MNB, RF Classifier and SVC. The sklearn library in Python allows you to alter the class-weight parameter for logit, RF and SVC, and it is advisable to indicate class_weight=\u201cbalanced\u201d when dealing with imbalanced data.", "The Gridsearch was set to optimize the F1 score on the \u201cpartisan\u201d class vis-a-vis a pipeline that included CountVectorizer (with Porter stemmer, n_gram and min_df), TfidfTransformer, and the model. The data were subjected to an 80\u201320 train-test split, and the training set to a further cv=5 cross-validation procedure.", "SVC took the top mean CV score (F1) on the training set but had the lowest F1 score on the test set. Both RF and SVC achieved notably higher overall accuracy scores than logit or MNB on the test set (green bars in the chart below). However, both logit and MNB bested the more sophisticated models when it came down to the F1 score on the test set (red bars in the chart below).", "All the results were based on the individually optimized algorithms, with the details listed below. Interestingly, MNB emerged the winner in terms of the test set\u2019s F1 score despite eschewing the use of both the Porter Stemmer and TfidfTransformer. All the other models resorted to using the stemmer, though SVC did not partake of the TfidfTransformer either.", "Examining the details of the respective confusion matrices would help provide insights into how each of the models fared. The matrices from the test set results for the logit and SVC models are shown below for illustration.", "According to logit\u2019s confusion matrix, it correctly picked 203 true positive (\u201cpartisan\u201d) messages out of its total positive predictions of 435 (or 203 + 232), which gave it a precision rate of 46.7%. The 203 true positives may also be compared against the total actual positive messages of 262 (or 203 + 59), implying a recall rate of 77.5%. Thus, the logit model achieved an F1 score of 58.2%.", "The SVC model, by contrast, fared poorly in this regard. It managed to obtain 99 true positive messages out of its total positive predictions of 175 (or 99 + 76). While the precision rate might not look too bad, the 99 true positive predictions need to be compared to the total actual positive messages of 262 (or 99 + 163). The weak recall rate dragged down SVC\u2019s F1 score to only 45.3%. It appears that the SVC\u2019s predictive power was focused more on the negative class, which garnered it a higher overall accuracy score, but that was not the aim here.", "One possibility from here is to use word embeddings, which are a more advanced approach than BOW. Word embeddings, however, tend to perform best with a very large set of training documents. It is arguably not the best approach when dealing with short and domain-specific social media posts in a relatively small dataset, like in this case.", "I thought it would be more fruitful to take another path, particularly considering the imbalanced data issue, and that was to resample the training set so that the models are trained on more balanced data. The algorithms of choice are the popular SMOTE (Synthetic Minority Oversampling TEchnique) and the combination SMOTE-Tomek.", "SMOTE helps to balance the classes by oversampling the minority-class, and it does so by generating synthetic minority-class observations through linear interpolation of the space between them (using k-nearest neighbors methodology). The interpolation procedure means that it will only generate synthetic observations from among the available examples.", "Tomek links, on the other hand, is an undersampling procedure that removes majority class observations so as to create a clearer demarcation of the space between the majority and minority classes. The SMOTE-Tomek algorithm basically combines both the SMOTE and Tomek links\u2019 procedures.", "I repeated the 80\u201320 train-test split, with a further fivefold cross-validation procedure on the training set. Resampling procedures should only be undertaken on the training set, after which the trained model is then scored on the unaltered test set.", "For brevity, I will only report the results from the SMOTE and SMOTE-Tomek resamplings with the optimized model parameters discussed above. SMOTE helped boost the logit model\u2019s overall accuracy score and Cohen\u2019s kappa on the test set but elicited only a marginal improvement on the F1 score. SMOTE-Tomek was better for the MNB model, though the improvements were not significant.", "On the flipside, the resampling techniques weakened the scores of the RF and SVC models. This appeared to confirm that these more sophisticated models were focusing their predictive power on the majority \u201cnegative\u201d class in this dataset. In any event, both logit and MNB still exhibited lower accuracy scores than RF and SVC, but their superior F1 scores edged higher still.", "MNB took the crown in the end in terms of both the F1 score on the \u201cpartisan\u201d class and Cohen\u2019s kappa statistic, achieving its highest scores with the help of SMOTE-Tomek resampling. Logit was a close second. Let\u2019s compare the test set\u2019s confusion matrices from the MNB and RF models following the SMOTE-Tomek procedure.", "We can observe that MNB achieved a 50% precision rate and even better, a 72.1% recall rate. The RF model, on the other hand, obtained a 51.9% precision rate and a 52.7% recall rate. Similar to the SVC model, the random forest classifier appeared to have paid more attention to the majority negative class.", "Thus, logit and MNB outperformed RF and SVC in this NLP challenge with the goal of predicting \u201cpartisan\u201d political messages. Remember that the baseline probability of predicting a \u201cpartisan\u201d message by chance was just 26%. The MNB and logit models achieved F1 scores of 0.58\u20130.59, and Cohen\u2019s kappa of 0.39-0.40. The RF and SVC algorithms returned higher accuracy scores, but weaker F1 scores on the target \u201cpartisan\u201d class.", "This article is not about which machine learning model is the absolute best one. It is about why one should strive to optimize the performance of whichever model or models one chooses to run. Optimizing model performance runs the gamut from methodical data cleaning, careful feature selection/engineering, judicious choice of scoring metrics, model hyper-parameter fine-tuning, and through to other relevant measures to improve model performance, such as resampling in cases of imbalanced datasets.", "And in so doing, simple and interpretable models can often be nudged into yielding surprising performance results. And if that helps with the communication aspects of our data science work, then so much the better. I will close with a quote from Robert Pirsig, who in turn paraphrased Plato:", "\u201cAnd what is good, Phaedrus, And what is not good \u2014 Need we ask anyone to tell us these things?\u201d", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Financier by profession. Economist by training. Data scientist & essayist by inclination."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5fcf7c405b28&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://at-tan.medium.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": ""}, {"url": "https://at-tan.medium.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Alvin T. Tan, Ph.D."}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84012ba036d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&user=Alvin+T.+Tan%2C+Ph.D.&userId=84012ba036d8&source=post_page-84012ba036d8----5fcf7c405b28---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5fcf7c405b28---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5fcf7c405b28---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5fcf7c405b28---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/statistics?source=post_page-----5fcf7c405b28---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/dataland?source=post_page-----5fcf7c405b28---------------dataland-----------------", "anchor_text": "Dataland"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&user=Alvin+T.+Tan%2C+Ph.D.&userId=84012ba036d8&source=-----5fcf7c405b28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&user=Alvin+T.+Tan%2C+Ph.D.&userId=84012ba036d8&source=-----5fcf7c405b28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5fcf7c405b28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5fcf7c405b28---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5fcf7c405b28--------------------------------", "anchor_text": ""}, {"url": "https://at-tan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://at-tan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alvin T. Tan, Ph.D."}, {"url": "https://at-tan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "381 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84012ba036d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&user=Alvin+T.+Tan%2C+Ph.D.&userId=84012ba036d8&source=post_page-84012ba036d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb13b09498d77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzen-and-the-art-of-model-optimization-5fcf7c405b28&newsletterV3=84012ba036d8&newsletterV3Id=b13b09498d77&user=Alvin+T.+Tan%2C+Ph.D.&userId=84012ba036d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}