{"url": "https://towardsdatascience.com/what-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b", "time": 1683013318.1308851, "path": "towardsdatascience.com/what-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b/", "webpage": {"metadata": {"title": "What I learned about human learning from machine learning. | by Jeffrey Scholz | Towards Data Science", "h1": "What I learned about human learning from machine learning.", "description": "Anyone who has dabbled in multiple domains notices how principles and meta-narratives in one area carry over to others with surprising frequency. For example, Satya Nadella (Microsoft\u2019s CEO) often\u2026"}, "outgoing_paragraph_urls": [{"url": "https://amzn.to/2YM6ibC", "anchor_text": "autoethnography", "paragraph_index": 1}, {"url": "https://amzn.to/2Gg7Pk9", "anchor_text": "Conscious Business", "paragraph_index": 1}, {"url": "https://amzn.to/2ELldvQ", "anchor_text": "Seven Habits of Highly Effective People", "paragraph_index": 1}, {"url": "https://amzn.to/3beVnfU", "anchor_text": "Fooled By Randomness", "paragraph_index": 1}, {"url": "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt", "anchor_text": "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt", "paragraph_index": 9}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "https://www.tensorflow.org/tutorials/text/text_generation", "paragraph_index": 9}, {"url": "https://www.coursera.org/lecture/language-processing/perplexity-is-our-model-surprised-with-a-real-text-hw9ZI", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Language_game_(philosophy)", "anchor_text": "https://en.wikipedia.org/wiki/Language_game_(philosophy)", "paragraph_index": 63}, {"url": "https://www.linkedin.com/in/jeffreyscholz", "anchor_text": "https://www.linkedin.com/in/jeffreyscholz", "paragraph_index": 65}, {"url": "http://rareskills.io", "anchor_text": "rareskills.io", "paragraph_index": 65}], "all_paragraphs": ["(External links to books are affiliate links, thank you for your support!)", "Anyone who has dabbled in multiple domains notices how principles and meta-narratives in one area carry over to others with surprising frequency. For example, Satya Nadella (Microsoft\u2019s CEO) often compared strategies from the sport of cricket to business strategies in his autoethnography about turning Microsoft around [1]. In Conscious Business, Fred Kaufman cross-applies a number of Bhuddist principles into corporate leadership. Steve Covey does something similar in Seven Habits of Highly Effective People with occasional reflections to his Mormon faith. In Fooled By Randomness, Nassim Taleb draws parallels between principles of randomness he observed in financial markets to other aspects of life, including business.", "This disquisition is of that genre. I\u2019ve been an autodidact most of my life. But after I started working in the field of machine learning, I came to understand some amusing but useful parallels between human learning and machine learning. Now granted, despite all the hype about machine learning (better known by the moniker \u201cartificial intelligence\u201d in popular media), machines don\u2019t learn like humans do. Most of the impressive feats of \u201cartificial intelligence\u201d that made headlines recently are, when one pulls back the curtain Wizard of Oz style, little more than statistical parlor tricks in controlled environments \u2014 a far cry from anything we would consider \u201cintelligent.\u201d No, machines cannot \u201cthink\u201d or \u201clearn\u201d the way humans do. Despite being an A.I. practitioner, I remain an A.I. skeptic.", "Nonetheless, some of the principles I learned from machine learning aided me in quests quite different from that field and I\u2019d like to share a ternion of those here. This essay is targeted to a broad audience but sprinkled with some technical expositions. Non-technical readers are encouraged to rapidly scroll through those parts; the overall message will hopefully remain intact.", "At the beginning of 2018 I decided that I would become a real estate investor. As a total neophyte, I knew that ignorantly forking out large sums of cash made for a precarious venture. For this to succeed, I needed to intelligently fork out large sums of cash, as opposed to ignorantly.", "A friend of mine inquired \u201chow do you know, that you know, enough?\u201d Reading one book on a topic surely isn\u2019t sufficient \u2014 without any contextual knowledge, I couldn\u2019t judge if the book was instructional or oversimplified. The same could be said about reading just two books. But what about three? How many books need to be read?", "That was a reasonable question. There aren\u2019t any good ways for small time real estate investors to objectively self-measure their competency. And given that humans are prone to overestimate their competence (e.g. the Illusory Superiority or Above-average effect [2]) in a subject, these innate psychological traps compound the danger.", "This wasn\u2019t the first time I had nerded out about a topic, read deeply about it, and afterwards at least appeared to gain some mastery with it. I wasn\u2019t worried about my ability to acquire knowledge. I was worried about fooling myself into thinking I was more knowledgeable than I really was, but still unwittingly investing with dangerous blind-spots.", "Thankfully, I was able to make one of those cross-domain insights that turned out to be extremely useful, and that is what I want to share in this section.", "My first foray into modern Natural Language Processing (NLP) was a toy recurrent neural network that would generate Shakespeare-sounding text. It accomplished this by processing a lot of Shakespeare text (https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt) and eventually learning to mimic it. By now, this toy example is widely known enough that it appears in the Tensorflow documentation, so I won\u2019t regurgitate the code. The interested programmer can find it here: https://www.tensorflow.org/tutorials/text/text_generation", "Here is some synthetic Shakespeare from that page:", "GREMIO: Y she had much. God as \u2018twere an ill. I\u2019ll pings thee. And cet your slave, direcliful there! O, thy dear keavens and to\u2019 no more surse!", "Nurse: Happy Mangel, a pruse is again; One hooth against me; That they lives swear having but a fence, I have fish to", "It\u2019s easy to laugh at that as having old-English characteristics but being obviously fake to those familiar with the author. However, the model clearly has some \u201cunderstanding\u201d of what Shakespeare plays tend to look like. How is it able to do that? It\u2019s using a similar technique behind autocompletion: given a string of words, what word is most likely to follow? If I give you a Shakespeare string as \u201cI beseech ___\u201d you can probably guess that \u201cthee\u201d or some other proper noun like \u201cHoratio\u201d or \u201cLucento\u201d goes next. Being able to \u201cfill in the blank\u201d with a sensible word indicates some level of proficiency in the subject.", "On the other hand, if we drop into a totally unfamiliar domain like hematology oncology and see a sentence like", "The postoperative phase of wound healing is prolonged in immunocompromised patients. Complete healing can often not be waited for, as the intensity of chemotherapy necessitates the cytotoxic ______ \u2026.", "I haven\u2019t got any clue what goes in the blank next. (I copied blurb from a random paper on Google scholar that I have no domain knowledge of [3]). This serves as a tangible indicator that I lack proficiency in hematology oncology.", "A machine learning algorithm would not be able to fill in the blank convincingly without some \u201cunderstanding\u201d of the domain.", "As such, I can say a machine has some level of \u201cunderstanding\u201d of a subject if it can predict what is coming next in a sequence of words \u2014 and not just single words but entire sentences and descriptions of thought. This \u201cgrasp\u201d of text has a quantifiable measure called \u201cperplexity\u201d that the interested reader can investigate more on here.", "That was my approach to studying real estate. Keep reading books until I could predict what the author was going to say next.", "Needless to say, that was a fairly expensive and time consuming process. I read at least 20 books before I bought my first investment property. Was it worth it? Bragging about investment success is uncouth, so I\u2019ll spare the reader. But suffice to say, I\u2019m still enthusiastic, active, and solvent in that endeavor today.", "Could I have been more efficient with this process? Frankly, I don\u2019t think so as I will discuss in the last section of this essay. In the meantime, let\u2019s talk about word embeddings.", "Let\u2019s fast forward to 2020 and tell another story. A few months ago, I decided to learn Indonesian. Predictably, I downloaded Duo Lingo and started memorizing the simple phrases the app fed me.", "Saya minum air (I drink water)", "Selamat malam, sampai jumpa lagi (good evening, see you again!)", "Kamu mau nasi dengan ayam goreng? (Do you want rice with fried chicken?)", "And so forth. I felt I was learning pretty quickly until I hit a snag. At the beginning, the app effectively teaches you phrases, not individual words.", "Somewhere around the 10th lesson, it decided to teach me a bunch of verbs:", "The app would teach the word by providing a sentence like I ________ with him, except in Indonesian, so it would be saya ______ dengan dia.", "But there\u2019s hardly anything in the context to suggest that \u201cvisit\u201d, \u201cwork\u201d, or \u201ccompete\u201d are better alternatives. This is hardly better than learning a word via flash card. And since all those words look and sound similar to my linguistically untrained eye and ear, it was a challenging way to learn.", "When you learn a word in isolation, you can\u2019t do better than rote memorization and you aren\u2019t studying for the task you actually care about. When using a language, you generally utter phrases, not individual words. You don\u2019t converse with your foreign interlocutor by flipping flashcards over. So it makes a lot of sense to just skip memorizing words and go straight to phrases, which Duo Lingo for Indonesian did well in the beginning. Learning phrases helps you learn words. But if the phrase is excessively generic you are essentially learning words in isolation again. Phrases give meaning to words just as much as words give meaning to phrases. [4]", "In fact, that\u2019s what linguist John Firth pointed out:", "You shall know a word by the company it keeps \u2014 (John Rupert Firth)", "Ah, but I didn\u2019t learn that concept because I studied linguistics (I have not), I learned it because I had studied word embeddings from machine learning.", "Humans understand words by their relationship to real world objects or phenomena \u2014 as well as how those words are used in relation to other words. Machine learning, at least at this point in time, only understands words in relation to other words. Despite this handicap, computers can still do useful and interesting things with words, as anyone who has interacted with a chatbot or digital assistant knows.", "To use an analogy, imagine you were born without sight. You could still come to understand that \u201cThe balloon is red\u201d is a valid sentence and that \u201cred is the balloon\u201d is gibberish. Similarly, after listening to enough literature, you could associate \u201cblue\u201d with \u201cocean\u201d despite lacking a visual concept of blue. To further illustrate, we know humans are able to understand that phrases like \u201cnotwithstanding the forgoing, provision shall be made for\u2026\u201d tend to appear in legal documents although we cannot sensorially experience that phrase. Words do not need a physical reification to be understood.", "So how can a machine understand words? Let\u2019s start with the end result and work backward. Consider the following image.", "Looking at this plot makes sense \u2014 the machine seems to have some understanding of these words (code provided later). Words that appear in the same context appear closer to each other. How did the machine gain an understanding of words such that it could cluster them sensibly?", "The relationship between words in machine learning is done with embeddings, the most famous algorithm being word2vec.", "In word2vec, each word is represented as a point in 300 dimensional space (although 300 is somewhat arbitrarily chosen). The plot above is the 300 dimension space \u201cprojected\u201d down to two dimensions. In isolation, a certain point in 300 dimensional space has almost no interpretation and is basically useless.", "There are no 300 dimensional landmarks to put the word in context. However, if the computer learns how words relate with each other, it will be able to put the words into clusters that correspond with their mutual contexts. As the linguist said, the machine will know the word by the company it keeps. That words are circularly defined is no more of a hinderance to the machine than it is to a human.", "If the machine learns the words properly, similar context words will appear close to each other as they do in the plot above. Before the machine starts learning, each word is essentially in a random location, and the machine must iteratively rearrange the words in 300 dimensional space as it reads large volumes of text. After enough rearranging, the points will be positioned relative to each other in a meaningful way, even though their absolute position remains meaningless.", "The exact implementation details can be found elsewhere but roughly, the algorithm takes in a lot of text and randomly blacks out a word in a sentence. Then, the algorithm tries to guess the missing word. It keeps doing this over and over, and with each successive guess, it updates the 300-dimensional position of the hidden word. It does this for all words in the corpus. As the words move around the 300 dimensional space, eventually they converge to clusters and structures that mimic the contexts they appear in.", "A machine (or human) can\u2019t make sense out of an isolated 300-dimensional point representation of a word. It only knows its relationship to other words. Therefore, when learning a language as a human, you shouldn\u2019t care as much about where exactly that word floats in thought-space, but how it relates to other words in that space.", "Here is the code to produce the figure above:", "In case you are wondering how I circumvented the flash-card education the app had devolved into, an Indonesian friend of mine intervened and gave me Indonesian folk songs and nursery rhymes to memorize. That music can aid memorization is a separate topic, but this method introduced words to me in the context that they keep.", "Thankfully, my flatmate doesn\u2019t understand Indonesian, so he doesn\u2019t know I\u2019m singing about a parrot sitting in a window or green balloons popping or other childish silliness. As a matter of fact, I\u2019m sure I sound sophisticated to him. Sure enough, I find it much easier to recall words from memorable sentences rather than from contextless phrases like I ______ with him.", "Taking this methodology to its logical conclusion, I try to see Indonesian words in as many different contexts as possible so that I can learn words by the company that they keep \u2014 or to be very nerdy, iteratively formulate the 300-dimensional representations of Indonesian words in my mind. Practically, this means interacting with tutors, listening to music in that language, reading different didactic books on the subject, and so forth. Of course, a polyglot won\u2019t find this conclusion remarkable \u2014 of course one should immerse oneself in a language to learn effectively. Ah, but I did not unearth phenomenon as an experienced linguaphile, I rediscovered this principle by importing its analogue from machine learning.", "Going back to my journey to learning real estate \u2014 was there a less strenuous way to get started in the endeavor besides reading all those books? And speaking of painful learning, it would be nice to learn a language without so much repetition, mental strain, and social embarrassment.", "But when we consider how machines learn, it\u2019s equally barbaric, if not more so. Machine learning in practice consists of buying a bunch of expensive hardware, collecting a ton of data, burning a lot of electricity, and boom \u2014 the algorithm learns the pattern in the data. Sure, the impressive results make for a good headline sometimes, but having seen how the sausage is made, the results hardly seem mystical. As I mentioned in the intro, I consider a lot of \u201cArtificial Intelligence\u201d to be statistical parlor tricks in controlled environments.", "At various points I have mused if there was some way to just feed the massive dataset into a clever algorithm and just jump to the parameters of the model that enable the model to make useful predictions.", "For example, in linear regression the parameters needed to \u201cunderstand\u201d a dataset can often be solved \u201cin one go\u201d with the following formula: [5]", "Here, X is your dataset where each row is a datapoint, and y is the label for that entry. Inverting a matrix is reasonably quick, it can be done in O(n^2.376) time. [6] However, the \u201cimpressive\u201d neural networks (GPT3, Megatron, etc) that can generate impressive essays were trained on datasets that are about 40 Gigabytes large, so X multiplied by it\u2019s transpose would go well past gigabytes squared \u2014 that\u2019s certainly not going to fit into any computer\u2019s memory. Besides, we are discussing linear regression here, and neural networks are not linear. Even though the algorithm for solving linear regression analytically is polynomial in time and space, the sheer size of the datasets used in A.I. make analytical, or \u201cnice\u201d solutions infeasible.", "Very technical discussion: It isn\u2019t too relevant if a problem is \u201cnp-hard\u201d or \u201cpolytime\u201d. One can find reasonably good approximate solutions to np-hard problems \u2014 the USPS can deliver packages efficiently despite the traveling salesman problem being np-hard. Likewise, a problem requiring O(n\u00b3)time to solve exactly isn\u2019t tractable when n is large. Even in the relatively innocent case of O(n\u00b2) space requirements, the above problem shows the data won\u2019t even fit in a computer if the data is large. In both of these cases, an approximate solution that respects real world resource constraints is needed \u2014 but this necessarily creates a computational bottleneck.", "This example with linear regression illustrates the larger matter at hand.", "I won\u2019t dive into computational learning theory here as I think it will unnecessarily distract from the point I\u2019m trying to make. But just looking at mathematics, computational complexity, and statistics, we can show that many classes of problems for which there are no \u201cnice\u201d ways to learn statistical patterns. That is, given a dataset we want a machine to understand, we can show mathematically that there does not exist an elegant or efficient way for a machine to discover the patterns. The only way to discover the pattern is to start with a wild guess and then very slowly improve that guess by painstakingly iterating over the data again and again. This becomes a very slow and expensive process when there is a lot of data.", "If we can show that there is no mathematically elegant and efficient way for a computer to learn from a large dataset, then we shouldn\u2019t get frustrated with ourselves as humans when we take a long time to learn a subject of interest. The pain of reading a lot of real estate books or trying to understand a foreign language come to mind.", "There seems to be encoded into the universe (as defined by mathematics as we currently understand it), a fundamental law that distilling large bodies of information into succinct describable patterns requires expending a minimum amount of energy proportional to the size of that data \u2014 whether it be compute cycles for a machine or study-hours for a human. For fairly large \u201cdatasets\u201d like real estate or language learning, that minimum energy could be quite large. There is just no way to get around it. Just as it is not possible to sort an array of unbounded integers in less than O(n log n) time, it isn\u2019t possible to \u201cunderstand\u201d a dataset without expending a certain amount of energy.", "This of course poses some philosophical questions about the limits of how much knowledge humans can acquire. But in a practical sense, I find it comforting. When I encounter a topic is taking me a long time to learn, I don\u2019t need to entirely attribute the difficulty to my dull wit, but also to this mysterious metaphysical property of the universe: that learning is often provably difficult in a mathematical sense.", "Again, none of this is to say that machine learning is anything like human learning. The legendary computer scientist Edsger Dijkstra pointed out that \u201cThe question of whether a computer can think is no more interesting that the question of whether a submarine can swim.\u201d As I pointed out in the introduction, authors have written extensively about the parallels between such disparate fields as sports, religion, probability, and business. I believe the parallels between machine learning and human learning is like that. Quite disparate, but sometimes interesting and illuminating.", "Have you seen any other parallels between machine learning and human learning? Please share in the comments!", "P.S. I plan to follow up with an essay on knowledge distillation. That was originally going to be a section inside of this one, but it made the entire piece too long. Stay tuned!", "[1] Links are affiliate links, thank you for your support!", "[4] I\u2019m resisting the urge to discuss Wittgenstein\u2019s language games here, but the interested can read further here: https://en.wikipedia.org/wiki/Language_game_(philosophy)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ex-Senior Software Dev Manager at Yahoo. Currently building web3. Connect with me here: https://www.linkedin.com/in/jeffreyscholz or email jeff(at)rareskills.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F40ae4fcb747b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jeffrey-scholz.medium.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": ""}, {"url": "https://jeffrey-scholz.medium.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Jeffrey Scholz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff83fac554ff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&user=Jeffrey+Scholz&userId=f83fac554ff4&source=post_page-f83fac554ff4----40ae4fcb747b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2175285", "anchor_text": "Gerd Altmann"}, {"url": "https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2175285", "anchor_text": "Pixabay"}, {"url": "https://amzn.to/2YM6ibC", "anchor_text": "autoethnography"}, {"url": "https://amzn.to/2Gg7Pk9", "anchor_text": "Conscious Business"}, {"url": "https://amzn.to/2ELldvQ", "anchor_text": "Seven Habits of Highly Effective People"}, {"url": "https://amzn.to/3beVnfU", "anchor_text": "Fooled By Randomness"}, {"url": "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt", "anchor_text": "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "https://www.tensorflow.org/tutorials/text/text_generation"}, {"url": "https://pixabay.com/users/WikiImages-1897/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67764", "anchor_text": "WikiImages"}, {"url": "https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=67764", "anchor_text": "Pixabay"}, {"url": "https://www.coursera.org/lecture/language-processing/perplexity-is-our-model-surprised-with-a-real-text-hw9ZI", "anchor_text": "here"}, {"url": "https://github.com/jeffreyscholz/Blog-word2vec/blob/master/Blog-Word2Vec.ipynb", "anchor_text": "https://github.com/jeffreyscholz/Blog-word2vec/blob/master/Blog-Word2Vec.ipynb"}, {"url": "https://en.wikipedia.org/wiki/Complexity_class#/media/File:Complexity_subsets_pspace.svg", "anchor_text": "Public Domain"}, {"url": "https://en.wikipedia.org/wiki/Illusory_superiority#:~:text=In%20the%20field%20of%20social,and%20abilities%20of%20other%20people.&text=The%20term%20illusory%20superiority%20was,Yperen%20and%20Buunk%2C%20in%201991.", "anchor_text": "https://en.wikipedia.org/wiki/Illusory_superiority"}, {"url": "https://www.researchgate.net/profile/Kai_Santos2/publication/7684399_Wound_care_with_antibacterial_honey_Medihoney_in_pediatric_hematology-oncology/links/564c74d008aeab8ed5e990b6.pdf", "anchor_text": "https://www.researchgate.net/profile/Kai_Santos2/publication/7684399_Wound_care_with_antibacterial_honey_Medihoney_in_pediatric_hematology-oncology/links/564c74d008aeab8ed5e990b6.pdf"}, {"url": "https://en.wikipedia.org/wiki/Language_game_(philosophy)", "anchor_text": "https://en.wikipedia.org/wiki/Language_game_(philosophy)"}, {"url": "https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent", "anchor_text": "https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent"}, {"url": "http://mathforum.org/library/drmath/view/51908.html", "anchor_text": "http://mathforum.org/library/drmath/view/51908.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----40ae4fcb747b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----40ae4fcb747b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/software-engineering?source=post_page-----40ae4fcb747b---------------software_engineering-----------------", "anchor_text": "Software Engineering"}, {"url": "https://medium.com/tag/learning?source=post_page-----40ae4fcb747b---------------learning-----------------", "anchor_text": "Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----40ae4fcb747b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&user=Jeffrey+Scholz&userId=f83fac554ff4&source=-----40ae4fcb747b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&user=Jeffrey+Scholz&userId=f83fac554ff4&source=-----40ae4fcb747b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F40ae4fcb747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----40ae4fcb747b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----40ae4fcb747b--------------------------------", "anchor_text": ""}, {"url": "https://jeffrey-scholz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jeffrey-scholz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeffrey Scholz"}, {"url": "https://jeffrey-scholz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "836 Followers"}, {"url": "https://www.linkedin.com/in/jeffreyscholz", "anchor_text": "https://www.linkedin.com/in/jeffreyscholz"}, {"url": "http://rareskills.io", "anchor_text": "rareskills.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff83fac554ff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&user=Jeffrey+Scholz&userId=f83fac554ff4&source=post_page-f83fac554ff4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe53a1f2913c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-i-learned-about-human-learning-from-machine-learning-40ae4fcb747b&newsletterV3=f83fac554ff4&newsletterV3Id=e53a1f2913c2&user=Jeffrey+Scholz&userId=f83fac554ff4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}