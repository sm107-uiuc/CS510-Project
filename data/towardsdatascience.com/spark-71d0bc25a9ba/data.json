{"url": "https://towardsdatascience.com/spark-71d0bc25a9ba", "time": 1683016859.830227, "path": "towardsdatascience.com/spark-71d0bc25a9ba/", "webpage": {"metadata": {"title": "Spark. From understanding core concepts to\u2026 | by Sanjay Singh | Towards Data Science", "h1": "Spark", "description": "Shilpa, a rookie data scientist, was in love with her first job with a budding startup: an AI-based Fintech innovation hub. While the startup started with the traditional single machine, vertical\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Shilpa, a rookie data scientist, was in love with her first job with a budding startup: an AI-based Fintech innovation hub. While the startup started with the traditional single machine, vertical expansion, infrastructure, it soon realized that the volume and velocity of data it deals with needs a Big Data solution. While HADOOP was a good choice for data storage, Spark, with its in-memory computation capability and real-time data streaming became the obvious choice as a data execution engine.", "Shilpa had a steep learning curve to conquer. While she understands data and is a Python expert, Spark is new to her and she has little time to master the jungle of knowledge available in bits and pieces on the internet: Big data, Hadoop Distributed File System (HDFS), Spark, Spark Context, Driver, Executor, Python, Java Virtual Machine (JVM), Compute Cluster, Yet Another Resource Negotiator (YARN).", "If you are in a similar situation or just wondering what is Spark and what all the fuss about it; you reached a suitable article. In this article, I endeavor to explain:", "I have noticed that whenever I talk about Spark the first thing that comes to listeners' minds how similar or different it is from Big Data and Hadoop. So, let\u2019s first understand how Spark is different from Hadoop.", "A common misconception is that Apache Spark is just a component of Hadoop.", "Hadoop is an open-source software framework for efficiently storing large datasets in the Hadoop Distributed File System (HDFS) on a computer cluster and processing it through big data processors like YARN.", "Apache Hadoop processes datasets in batch mode only and it lacks stream processing in real-time. To fill this gap, Apache has introduced Spark (actually, Spark was developed by UC Berkley Amplab): a lightning-fast in-memory real-time data streaming framework.", "Spark is also an open-source, in-memory computation and data processing framework offered by Apache.", "HDFS is a common factor between Hadoop and Spark. Spark can be considered as a powerful alternative to Map Reduce. In addition to faster data processing and real-time streaming, Spark has several other rich functionality features like support to different languages i.e. Python, R, SQL, with rich framework libraries for Data Analytics and Machine Learning.", "Another common factor between Hadoop and Spark is the Java Virtual Machine (JVM).", "Java Virtual Machine (JVM) can be considered as middleware between Java-based applications like Spark and operating systems where it is running. As Hadoop and Spark are written mostly in Java, it can not work without JVM on the computer where it is running.", "Spark is written in Scala and runs in Java Virtual Machine (JVM). Spark can run in local mode on a single computer or in clustered environments; however, JVM must be installed on every node.", "Spark can run in Local Mode on a single machine or in Cluster-Mode on different machines connected to distributed computing.", "Local Mode is ideal for learning Spark installation and application development. All you need is your laptop or an instance on the computing cloud, like AWS EC2.", "Cluster Mode follows the traditional big data architecture of Master Node, Worke Node, and Resouce Manager.", "Like Hadoop, Spark follows the Master-Worker architecture of one master process (called Driver) and multiple worker processes (called Executors).", "The Driver maintains metadata and other necessary information of Spark application and takes care-of distributing and monitoring work across Executors. The Executor nodes execute the code assigned to it by Driver and report back status to Executor.", "The below picture shows Spark Cluster-Mode architecture", "Spark supports built-in Standalone cluster manager as well as HADOOP YARN, Apache Mesos, and Kubernetes cluster managers.", "In this article, I take you through the steps of installing Spark in Local mode on an AWS EC2 instance.", "I am using a Red Hat Linux instance on Amazon Web Service (AWS) EC2 instance for this.", "Spark shell application UI will start on port 4040. Let\u2019s ensure that the Security group assigned to the AWS EC2 instance provides access to this port from any computer.", "a. Open AWS EC2 instance, go to the Security tab and click on the Security group's name.", "b. Click on Edit inbound rules", "c. Add port 4040 to the rule. Although I am allowing 0.0.0.0/0, which means any computer over the internet, in real life project, you will allow access to your instance through this port only to specific IP addresses.", "So far so good. Access the instance through putty and go through the following steps for installing Spark.", "3. Start the instance through Putty and perform the below steps for installing Spark", "a. First, check if wget is installed in your instance. If not, install it through yum install as shown below.", "b. Check if bzip2 is installed. If not, install it through yum", "c. Download spark files from Apache website through wget", "e. Move the file to /usr/local directory and create a soft link", "f. Assign SPARK_HOME environment variable into the .bashrc file.", "i. If java is not installed, install at least JDK 8. It can be installed the yum", "j. Assign JAVA_HOME environment variable into .bashrc file.", "k. Source environment variables and confirm JAVA_HOME returns the path to jdk folder.", "Congratulations!! Installation is done and it\u2019s time to start Spark in local mode. Note that, complete installation happened on only one computer. There is no cluster.", "Now that the Spark installation is finished, let\u2019s start a Spark session.", "a. To start Spark, enter command spark-shell", "b. Open a browser and enter the below address. Replace <public IP address> with the public IP address of your AWS EC2 instance.", "Jobs tab shows details of jobs running on Executors", "You can also get start Spark Session details with the below Scala command.", "Following are the details from each", "If you have reached so far in the document, please accept my gratitude for your dedication to learn Spark.", "Spark supports Python language. This combination of Spark and Python is also referred to as PySpark. I love Python and will use it for developing applications on Spark.", "I am going to install Anaconda, which has Python as well as Jupyter.", "a. Download the latest version of Anaconda through wget.", "b. You will get a .sh file. Run the below command to install Anaconda 3.", "Click on Enter and go through terms and conditions. Once you get the below message write yes.", "Mention the location where you want to install anaconda3", "c. Once Anaconda installation is finished, check Python and Jupyter version", "For running Jupyter on AWS EC2 instance, you need to create a Jupyter configuration file.", "a. Run the below command to create the Jupyter configuration file.", "This command will create a jupyter_notebook_config.py file.", "b. Open the jupyter_notebook_config.py file and add the below lines at the top of the file.", "c. Make sure to allow connection to port 8888 in the AWS security group inbound list.", "d. Now you are ready to launch the jupyter notebook.", "Take the HTTP URL mentioned and replace localhost with Public DNS or Public IP of your AWS EC2 instance.", "Put this URL on any browser and Jupyter will launch.", "Congratulations! you got Spark running, Python installed and Jupyter configured and running.", "Next, we will connect Jupyter with Spark. If you open the folder where you installed Spark, you will find python as a directory there.", "Connecting Pyspark with Juypter means adding the below lines to your bashrc file. This will setup the pyspark driver.", "Once this is set up, you can start the pyspark jupyter notebook just by entering the command pyspark.", "Congratulations!! You have successfully installed Spark, Python, and Jupyter and now you are ready to create an application and run it on Spark.", "Welcome to this section of the article. Now you are all set to create a Spark application.", "Before we start developing the PySpark application, let\u2019s understand how data is stored in Spark. Spark uses Resilient Distributed Datasets (RDD), which is a logical collection of data partitioned across machines. We are going to use Python to create Spark RDD.", "While Spark provides PySpark Dataframe and Spark SQL as a mode of interaction with Spark RDD and you will hardly directly work on RDD, it\u2019s good to get a feel of it to understand what happens under the hood when you run Spark Data Frame or Spark SQL.", "Launch your Jupyter notebook through the pyspark command and open Jupter notebook in a browser as explained above.", "Enter the following snippet of code to import libraries (pyspark, NumPy). Stop and running Spark context and run a new Spark context. It creates a regular array of words.", "Now, it\u2019s time to create an RDD with this list and understand how it stores the data in different partition.", "Very good, RDD is created. Now, if you want to know how RDD has stored data in different partitions, run the below command", "This was a very small hands-on experience on how Spark RDD works.", "However, in a real-life project, you will mostly use PySpark Dataframe or PySpark SQL for application.", "Please refer to my article on PySpark explaining how to use PySpark Dataframe and PySpark SQL. Article goes through", "Article use the Pima Indian Diabetes Database and develops a Machine Learning model to predict the probability of a patient having diabetes based on variables like BMI, insulin level, age, etc.", "I hope this article helped you understand Spark from scratch and gave you enough insight to start exploring and doing things on your own. Sorry, it was a big article, but I didn\u2019t want to miss any important steps.", "Any questions, I am just a message away.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F71d0bc25a9ba&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://srssingh.medium.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": ""}, {"url": "https://srssingh.medium.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Sanjay Singh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F661a02df07b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&user=Sanjay+Singh&userId=661a02df07b5&source=post_page-661a02df07b5----71d0bc25a9ba---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ajgarciaco?utm_source=medium&utm_medium=referral", "anchor_text": "AJ Garcia"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://apache.claz.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz", "anchor_text": "https://apache.claz.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz"}, {"url": "http://ec2-18-219-126-145.us-east-2.compute.amazonaws.com:4040/jobs/", "anchor_text": "http://<public IP address>:4040/jobs/"}, {"url": "http://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh", "anchor_text": "http://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh"}, {"url": "https://towardsdatascience.com/pyspark-f037256c5e3", "anchor_text": "PySparkRendezvous of Python, SQL, Spark, and Distributed Computing making Machine Learning on Big Data possibletowardsdatascience.com"}, {"url": "https://www.kaggle.com/uciml/pima-indians-diabetes-database", "anchor_text": "Pima Indians Diabetes DatabasePredict the onset of diabetes based on diagnostic measureswww.kaggle.com"}, {"url": "https://www.udemy.com/course/apache-spark-for-data-engineers/?referralCode=CA92888DA98AEA3315AC", "anchor_text": "https://www.udemy.com/course/apache-spark-for-data-engineers/?referralCode=CA92888DA98AEA3315AC"}, {"url": "https://medium.com/tag/spark?source=post_page-----71d0bc25a9ba---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----71d0bc25a9ba---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/aws?source=post_page-----71d0bc25a9ba---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/python?source=post_page-----71d0bc25a9ba---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----71d0bc25a9ba---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&user=Sanjay+Singh&userId=661a02df07b5&source=-----71d0bc25a9ba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&user=Sanjay+Singh&userId=661a02df07b5&source=-----71d0bc25a9ba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F71d0bc25a9ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----71d0bc25a9ba---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----71d0bc25a9ba--------------------------------", "anchor_text": ""}, {"url": "https://srssingh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://srssingh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sanjay Singh"}, {"url": "https://srssingh.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "165 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F661a02df07b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&user=Sanjay+Singh&userId=661a02df07b5&source=post_page-661a02df07b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fee413a0c4e0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-71d0bc25a9ba&newsletterV3=661a02df07b5&newsletterV3Id=ee413a0c4e0e&user=Sanjay+Singh&userId=661a02df07b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}