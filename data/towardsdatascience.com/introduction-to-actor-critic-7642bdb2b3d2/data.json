{"url": "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2", "time": 1682996707.864213, "path": "towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2/", "webpage": {"metadata": {"title": "Introduction to Actor Critic in Reinforcement Learning | by Ziad SALLOUM | Towards Data Science", "h1": "Introduction to Actor Critic in Reinforcement Learning", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com What does it mean to have a policy based reinforcement learning? To put it simply imagine that\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Gradient", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step", "paragraph_index": 10}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "Before delving into the details of the actor critic, let\u2019s remind ourselves of the Policy Gradient .", "What does it mean to have a policy based reinforcement learning?To put it simply imagine that a robot find itself in some situation, but it appears that this situation is similar to something it had experienced before.So the policy based method says: since I have taken action (a) in this particular situation in the past, let\u2019s try the same action this time too.", "PS. Don\u2019t mix similar situation with same states, in similar situation the robot or agent is in some new states that resemble what it has experienced before, and not necessarily the exact same state in which it was in.", "In the image above a robot faces a rampart, but it seems that it has seen this before so as it has done previously, it used its rockets to jump over it.", "However, this might not be the perfect action this time!It might happen that there is a trigger to simply open the gate thus sparing valuable energy, that might be used later on.", "Based on this concept, the robot has to make some retrospective review to check how useful that action (using his rockets) was.", "Now we need to translate that into a mathematical equation.", "Keep in mind that the objective is to always get the highest rewards.To be able to do so we must define a function that collects these rewards and work to optimize it in order to maximize those rewards.Another equally important point, is that we are going to do this using a neural network.So what we will be doing is to find the set of weights \ud835\udf3d of the neural network that helps us maximize the objective function.", "Important! Be aware of the following pitfall: if you are familiar with deep learning and neural network, you are more accustomed to find the weights that minimize the error. We are not doing this in this case, quite the opposite we are looking to maximize an objective function. Failing to realize that will lead you into confusion.", "From the policy based gradient we have found an objective function and computed its gradient as follows (for details on how this equation came into being please check Policy Gradient Step by Step):", "J(\ud835\udf3d) is the objective function that depends on \ud835\udf3dm is the number of episodes (here called trajectories) executed\ud835\uded1 is a policy parametrized by \ud835\udf3d, which means when \ud835\udf3d varies the policy will be affected. Remember that the policy gives the probability of taking a certain action when the agent is at in a certain state.\ud835\udfbd\u2071 is the ith episode or trajectory executed.R(\ud835\udfbd\u2071) is the return (total rewards) of the trajectory \ud835\udfbd\u2071.T is the number of steps in the trajectory \ud835\udfbd\u2071.", "What this equation tells us is that the gradient of J(\ud835\udf3d) is the average of all m trajectories, where each trajectory is the sum of the steps that compose it. At each of this step we compute the derivative of the log of the policy \ud835\uded1 and multiply it by the return R(\ud835\udfbd\u2071).", "In other words we are trying to find how the policy varies following \ud835\udf3d. The use of the return R(\ud835\udfbd\u2071) is to amplify (or to reduce) those variations. Meaning that if R(\ud835\udfbd\u2071) is high, it will boost the result and makes the neural network confident that \ud835\udf3d progress in the right direction.", "However there is an issue with the return R(\ud835\udfbd\u2071).", "Suppose that we are in a trajectory where the rewards of the early steps are negative and those towards the end of the trajectory are positive in such a manner that the total R = 0 !In this case \u2207J(\ud835\udf3d) will be 0 and the neural network will not learn any new values for \ud835\udf3d from this situation.To solve this problem, we use a discounted rewards at each step, starting from the current state until the end of the trajectory.", "This will gives us a new version of \u2207J(\ud835\udf3d):", "Now the neural network will be able to learn from the trajectories.Still there is more room for improvement, but we need to do some more math.", "What if we push our thinking deeper? The Rt (return starting at step t) is not bad, but we are not really sure what value of Rt is considered good enough to be taken into consideration?!", "Take the example of the robot at the beginning of the article, was using the rockets to jump a good solution ? How do we know ? How good was it ?Similarly, if you take the example of 100m sprinter who scored 10s, did he do good or bad? If it was good then how good was it ?One way to give a meaning to this number is by comparing it to a reference, or what we call a baseline.", "Baselines can take several forms, one of them is the expected performance or in other terms the average performance. If the sprinter got 10s but the average is 12s, then he did very good, conversely if the average is 8s then he did poorly.", "Let\u2019s denote the baseline as b\ud835\udc61, the gradient of the objective function becomes:", "Now let\u2019s think about it for a second, R\ud835\udc61 is the rewards as a result of taking action a at step t.On the other hand b\ud835\udc61 represents the average of results of all actions.This strangely look similar to Q(s, a) which is the value of action a taken at state s, and V(s) which is the value of the state, or the average of all rewards (caused by all actions taken) at state s.", "The equation can be rewritten as", "If we look closely at the equation above, we see that \ud835\uded1(a|s) is what performs the action (remember \ud835\uded1 is the probability of action a is taken at state s), while Q(s, a)-V(s) tells us how valuable it is.", "Computation of the Critic can have different flavors :", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7642bdb2b3d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----7642bdb2b3d2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@fatihkilic?utm_source=medium&utm_medium=referral", "anchor_text": "Fatih K\u0131l\u0131\u00e7"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Gradient"}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566", "anchor_text": "Function Approximation in Reinforcement Learning"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Based Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7642bdb2b3d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----7642bdb2b3d2---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/actor-critic?source=post_page-----7642bdb2b3d2---------------actor_critic-----------------", "anchor_text": "Actor Critic"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7642bdb2b3d2---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7642bdb2b3d2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----7642bdb2b3d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----7642bdb2b3d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7642bdb2b3d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7642bdb2b3d2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7642bdb2b3d2--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-actor-critic-7642bdb2b3d2&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}