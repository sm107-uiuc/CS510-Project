{"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "time": 1683017969.94398, "path": "towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8/", "webpage": {"metadata": {"title": "Neural networks are fundamentally (almost) Bayesian | by Chris Mingard | Towards Data Science", "h1": "Neural networks are fundamentally (almost) Bayesian", "description": "We present evidence from 3 papers which strongly suggests that Deep Neural Networks generalise due to a strong bias in the prior over parameters towards functions with low complexity."}, "outgoing_paragraph_urls": [{"url": "https://chrismingard.medium.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99", "anchor_text": "post", "paragraph_index": 25}], "all_paragraphs": ["Deep neural networks (DNNs) have been extraordinarily successful in many different situations \u2014 from image recognition and playing chess to driving cars and making medical diagnoses. However, in spite of this success, a good theoretical understanding of why they generalise (learn) so well is still lacking.", "In this post, we summarise results from three papers, which provide a candidate for a theory of generalisation in DNNs [1,2,3]. For busy or knowledgable readers, Figures 1\u20137, their captions and the conclusion are sufficient to understand the main argument.", "We will first explain inductive biases \u2014 mechanisms that selects a function that an agent thinks is most likely to explain some training data. We then state that for highly expressive learning agents such as DNNs (which are known to be able to express many possible functions), a good inductive bias is needed to guarantee generalisation.", "We present experiments on real-world datasets which suggest that DNNs behave like Bayesian learning agents (with a prior over parameters). This prior favours simple functions, and this is the source of the good inductive bias in DNNs.", "It is worth noting that many theories suggest that SGD is the main source of this inductive bias (e.g. it performs some hidden regularisation, see [1] for a literature review). Our experiments are strong evidence against this claim, but do show that tuning the hyperparameters of stochastic optimisers can have a minor effect on the post-trained DNNs \u2014 and may amplify the bias already present in the prior over parameters.", "In supervised learning, the problem is to fit a function to some training data S. One of the simplest supervised learning tasks is 1D regression (i.e. fitting a line to some example points). However, as is clear from Figure 1, there are many possible functions f that fit the training data (any function that passes through the black points). However, there is only one \u2018true\u2019 function f* that actually generated the black points. In the case of Figure 1, f* was a (slightly) perturbed sine function, and the grey points show its behaviour away from the training data (they are part of the test data).", "Then, given some learning system, and some datapoints, how can we decide which function f is most likely to be f*? The closer we are to f*, the better the network will have generalised. This is the same statement as \u2018good generalisation is accurately predicting unseen data.\u2019", "In Figure 1, we use two polynomial fitters to find a candidate for f*. The 5th order polyfit fits a function that is fairly close to f* (the dotted orange line) because there are 6 parameters and 5 datapoints (which almost perfectly constrains the problem), whereas the 50th order polyfit (with 51 parameters) can choose between a much larger set of functions. Such algorithms that can fit many functions to S are called highly expressive. Without a mechanism to guide highly expressive algorithms to f*, they will (in probability) fit a function that generalises poorly (simply as they have so many to choose from, most of which will be poor approximations for f*), as is the case for the 50th order polyfit.", "So, one way of controlling what f looks like is by controlling the expressivity of the model \u2014 the number of functions it can fit to the training data S. If we know that f* is much more simple than a 50th order polynomial, we can just reduce the order of the polyfit algorithm, removing the number of incorrect functions the learning algorithm can fit. This is related to the bias-variance tradeoff, which we will argue has no place in our discussion of DNNs.", "This is because DNNs are highly expressive. They usually have many more parameters than datapoints, and are capable of fitting many different functions to S. In Figure 1, we see a DNN with >10000 parameters performing the same task as the polyfit algorithms. The blue line shows the result \u2014 a simple function that, whilst not a sine curve, isn\u2019t as bad as the polynomial fitter (which had just 50 parameters). Just to check that this particular DNN isn\u2019t too small to fit many different functions, we also trained it on a training set consisting of the black points and the red points. It had no problem fitting a function to both. This suggests that it is highly expressive, and yet generalises fairly well (chooses f that is close to f*).", "When just fitting the black points, the DNN could have fit any function (e.g. the red line), but it selected a very simple function. When we force it to pass through the black and red points, it can do this successfully, both implying high expressivity and a preference towards the simplest possible functions. For more information about quantifying the simplicity of functions, see [2,3].", "This example, and substantial evidence in [2,3], suggests that the DNN has an inductive bias towards \u2018simple\u2019 functions.", "An inductive bias is a mechanism that causes algorithms to prefer certain functions as candidates for f*. If the inductive bias selects functions f that are close to f* then the inductive bias is good. There is insufficient inductive bias in the polyfit algorithm \u2014 hence no generalisation!", "In summary, learning agents that are highly expressive need an inductive bias in order to generalise well. It is thought that an inductive bias towards simple functions is most likely to work well in the real world.", "Before we continue, we should develop a proper formalism. In a supervised learning setting, we have some data distribution D that consists of inputs (in the space X) and labels (in the space Y), and some function", "defined for any x\u2208 D. f* is the \u201ctrue\u201d function that defines how inputs should be mapped to their labels.", "Below is an example (MNIST), where D contains 28x28 greyscale images of the handwritten digits 0\u20139, with their labels. Note that, while there is some ambiguity as to when a particular 28x28 image can be counted in D, the black square certainly isn\u2019t. f* represents the \u2018true\u2019 labelling of the images \u2014 it is a function from pixel values in the 28x28 images to labels.", "As we have already discussed, a DNNs job is to find a function f that is (sufficiently) close to f*. In a supervised learning setting, this is done by providing some subset S\u2282 D (\u2018training data\u2019) and attempting to minimise the loss function on S, which measures the difference between f and f* on S. We typically test this function f on a test set E\u2282 D, to see how close it is to f*.", "If the inductive bias of the DNN is good, then f will be close to f* on E.", "In [3], it was shown that 2 layer Fully Connected Networks are capable of fitting any function to 10000 images in MNIST (i.e. labelling the images in any way), and despite this, they overwhelmingly find functions that generalise well.", "We hypothesised in the previous section that DNNs have an inductive bias towards simple functions \u2014 if we assume that functions in the real world are simple (e.g. a simple pattern explains what makes the digit 9 a nine and not a seven) then this would explain why DNNs generalise.", "We found that a good way to answer this question was to ask the following one: how many parameters are associated with functions that generalise well? The following text explains the argument, but Figure 3 might make it clearer.", "We first need to define a few quantities. For some Deep Neural Network N and dataset D:", "P\u1d66 ( f ) is the probability that N expresses f on the dataset D, upon randomly sampling network parameters (typically from i.i.d. Gaussians)", "P\u1d66 ( f ) can be related to a volume in parameter space V\u1d66 ( f ): if network parameters are randomly sampled from a Gaussian distribution, then V\u1d66 ( f ) is a volume with Gaussian measure, and equal to P\u1d66 ( f ). We use the subscript \u2018B\u2019 to denote that this is our Bayesian Prior \u2014 which is a type of inductive bias.", "This gives a measure of the \u2018volume\u2019 in parameter space for different functions f. Extensive empirical evidence and analytic results in [2,3] suggest the following: simple functions will have a corresponding higher \u2018volume\u2019 of parameters. See this post for a summary of these results. Given that simple functions generalise better, then there will be, in effect, a greater \u2018volume\u2019 in parameter space for functions with good generalisation than functions with bad generalisation. So, can we take advantage of this?", "We can imagine performing Bayesian inference, taking P\u1d66 ( f ) as the prior:", "where P(S | f) = 1 if f is consistent with S, else 0. P\u1d66(S) is the probability of randomly initialising to a function consistent with S. This equation is perhaps better understood with reference to Figure 3: it says that the probability that a function f best describes training data S is given by the volume of f divided by the total number of functions consistent with S.", "Very informally, this means that if \u2018more\u2019 parameters give a function f, and f accurately models data in the training set S, then we should assign it a higher posterior probability P\u1d66 ( f | S) than another function accurate on S but with \u2018fewer\u2019 parameters. This posterior probability is our probability estimate for how likely that f is actually f* \u2014 which, looking at the above equation is strongly dependent on our prior P\u1d66 ( f ).", "Then, if P\u1d66 ( f ) is biased towards functions with good generalisation, P\u1d66( f | S) will be too. To emphasise our earlier point, P\u1d66 ( f ) is our inductive bias \u2014 which is called a prior in the context of Bayesian inference. The subscript \u2018B\u2019 stands for \u2018Bayes\u2019.", "However, even if this is true, this does not guarantee good generalisation. DNNs are trained by a stochastic optimiser like SGD. While it may seem likely that functions with higher P\u1d66( f | S) are more likely to be found by SGD, this is not self-evident. Take a look at Figure 4: if the loss landscape behaves somewhat like the one displayed in the cartoon, then functions with higher \u2018volumes\u2019 will be more likely to be found, and because they generalise well, the DNN will generalise well in expectation. Arguments and empirical evidence in [1] suggest this is true (with some conditions).", "We must compare the following quantities. Consider a DNN N, training set S and test set E\u00b9:", "P\u2092\u209a\u209c ( f | S ) is the probability that a stochastic optimiser like SGD finds a function f on E after training to 100% accuracy on S.", "P\u1d66 ( f | S) is the probability that N expresses f on E, upon randomly sampling network parameters from i.i.d. Gaussians until N is 100% accurate on S. [This is an equivalent definition to that given in the previous section].", "If P\u2092\u209a\u209c ( f | S ) \u2248 P\u1d66 ( f | S) then the reason that DNNs generalise well is that there is a greater volume of parameters (volume with Gaussian measure) that correspond to functions that generalise well. SGD is then just more likely to find functions with \u2018more\u2019 associated parameters. Again, the reader may wish to refer back to Figures 3 and 4 for cartoons of the argument.", "For full and formal treatment, see [1]. Hopefully though, this section is sufficient to understand the key points.", "\u00b9Note that we use the test set E to measure differences in the functions post-training (as they will all be the same on S). In effect, we are coarse-graining the functions on data from the same distribution as S.", "Here we give an example of the argument above, in the hope that it might aid intuition. If everything is clear though, skip ahead to the results!", "Consider the dataset MNIST \u2014 the handwritten digits from 0\u20139 with their labels. For our experiments, we then binarise it, so even numbers are classified as 0 and odd numbers as 1.", "Let the training set S consist of 10000 randomly selected images, and the test set E consist of 100 randomly selected images.", "Then, consider training a DNN N on S to 100% accuracy. Record the classification (the function) on E. This is equivalent to sampling from P\u2092\u209a\u209c ( f | S ). Do this multiple times (say around 10\u2075) to build up a good approximation to P\u2092\u209a\u209c ( f | S ).", "To calculate P\u1d66 ( f | S), one needs a different approach. It would be nice to randomly sample parameters until N is 100% accurate on S, record the classification and repeat (similar to how we calculate P\u2092\u209a\u209c ( f | S )) but unfortunately, this would take an infeasible amount of time. We also cannot analytically calculate the volumes P\u1d66 ( f ). Instead, we accurately approximate this process with the Gaussian Processes approximation. See [1] for a detailed explanation of this.", "Our results can be broken down into some very simple experiments. Our main type of experiment tests whether P\u2092\u209a\u209c ( f | S ) \u2248 P\u1d66 ( f | S). We find that this is true over a wide range of datasets (MNIST, Fashion-MNIST, IMDb movie review dataset and the ionosphere dataset), architectures (Fully Connected, Convolutional, LSTM), optimisers (SGD, Adam, Adadelts etc.), training schemes (including overtraining) and optimiser hyperparameters (e.g. batch size, learning rate).", "We show the main result in Figure 5 \u2014 for a fully connected network on MNIST. The main result is shown in 5a: each datapoint represents a specific function on a test set of size 100, after training on a training set S of size 10000, either with SGD, or by Bayesian inference. It shows that P\u2092\u209a\u209c ( f | S ) \u2248 P\u1d66 ( f | S). 5b shows that P\u1d66 ( f | S) varies over many orders of magnitude, and is highest for functions with good generalisation. 5c shows that simple functions generalise better. 5d shows P\u1d66 ( f | S) from 5a in a different way, and 5e,f show the same type of experiment found in 5a but with different hyperparameters (different optimiser and loss function respectively).", "We also show in Figure 6 that changing hyperparameters associated with the optimiser (in this case, batch size) does affect the relationship between P\u2092\u209a\u209c ( f | S ) and P\u1d66 ( f | S). We find that smaller batch sizes cause functions with high P\u1d66 ( f | S) to have even higher P\u2092\u209a\u209c ( f | S ) than for larger batch sizes. However, this effect is much smaller than the effect of P\u1d66 ( f | S). Note that the cross-entropy loss function used in Figure 6 means that the approximations to P\u1d66 ( f | S) are less accurate (due to the EP approximation), hence the less strict agreement.", "Finally, Figure 7 shows that P\u2092\u209a\u209c ( f | S ) \u2248 P\u1d66 ( f | S) for a range of architectures and datasets \u2014 including ConvNets and LSTMs. As above, the EP approximation is required to estimate P\u1d66 ( f | S), introducing error. The greater scatter present in the figures is not strong evidence against the main claim \u2014 given that the optimisers do not find functions with low P\u1d66 ( f | S) it seems sensible to assume that the main contribution to their inductive bias is P\u1d66 ( f | S). Also, note that Figure 5b shows that P\u1d66 ( f | S) varies over 200 orders of magnitude (and equivalent figures in [1] show similar results for all datasets in Figure 7) \u2014 it would require a very strong inductive bias in SGD to overcome this, and this is not supported in the rest of the figures.", "For many more experiments with different datasets, architectures and optimisers, see [1].", "In conclusion, we have presented strong evidence that DNNs generalise because of a strong inductive bias due to the architecture. More specifically:", "Because P\u2092\u209a\u209c ( f | S )\u2248 P\u1d66 ( f | S) on a wide range of architectures, datasets and optimisers, we conclude that the effect of the optimiser is much weaker than the prior over parameters P\u1d66 ( f ). Thus, DNNs primarily generalise because of P\u1d66 ( f ), and not because of the stochastic optimisers \u2014 which are great for finding functions consistent with training sets \u2014 still a very impressive property!", "That said, changing hyperparameters associated with optimisers (e.g. batch size, learning rate, overtraining) can make small changes to the approximate equality between P\u2092\u209a\u209c ( f | S ) and P\u1d66 ( f | S), and thus lead to small changes in generalisation.", "It seems likely that there will be some types of DNNs, or regions in parameter-space, for which the effects of the optimisers become more significant. We hope to investigate these in future work.", "Feel free to send any comments or suggestions to christopher.mingard@queens.ox.ac.uk", "All images in this post were created by the author, most of which can also be found in [1].", "This blog post attempts to explain the work in [1] as closely as possible, but of course, some simplifications have been made in the process. I encourage interested readers to read the paper!", "The experiments shown in Figures 5,6 and 7 have, as mentioned in the main text, been performed for a large range of real-world architectures and datasets. However, there are regimes in which they have not been performed extensively \u2014 for example, the chaotic regime in tanh-activated DNNs. It is entirely possible that some of these, the stochastic optimiser may play a greater role in the inductive bias, and this will be the subject of future work.", "Many thanks to Joar Skalse and Guillermo Valle-Perez for assistance in the process of writing this post!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DPhil student at Oxford University in Machine Learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbee9a172fad8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chrismingard.medium.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Chris Mingard"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10----bee9a172fad8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "https://chrismingard.medium.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99", "anchor_text": "post"}, {"url": "https://arxiv.org/abs/2006.15191", "anchor_text": "https://arxiv.org/abs/2006.15191"}, {"url": "https://arxiv.org/abs/1909.11522", "anchor_text": "https://arxiv.org/abs/1909.11522"}, {"url": "https://arxiv.org/abs/1805.08522", "anchor_text": "https://arxiv.org/abs/1805.08522"}, {"url": "https://medium.com/tag/bayesian-deep-learning?source=post_page-----bee9a172fad8---------------bayesian_deep_learning-----------------", "anchor_text": "Bayesian Deep Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bee9a172fad8---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/generalisation?source=post_page-----bee9a172fad8---------------generalisation-----------------", "anchor_text": "Generalisation"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----bee9a172fad8---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/tag/explore?source=post_page-----bee9a172fad8---------------explore-----------------", "anchor_text": "Explore"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&user=Chris+Mingard&userId=3befb9679b10&source=-----bee9a172fad8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&user=Chris+Mingard&userId=3befb9679b10&source=-----bee9a172fad8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbee9a172fad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bee9a172fad8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bee9a172fad8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bee9a172fad8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bee9a172fad8--------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Mingard"}, {"url": "https://chrismingard.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "85 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5e3e3335a2ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-are-fundamentally-bayesian-bee9a172fad8&newsletterV3=3befb9679b10&newsletterV3Id=5e3e3335a2ce&user=Chris+Mingard&userId=3befb9679b10&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}