{"url": "https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b", "time": 1682993850.927115, "path": "towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b/", "webpage": {"metadata": {"title": "Calculating Gradient Descent Manually | by Chi-Feng Wang | Towards Data Science", "h1": "Calculating Gradient Descent Manually", "description": "Without understanding the math behind deep learning, we cannot really appreciate all the intricacies behind the code. By the end of this article, you will be able to minimize loss functions yourself!"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3", "paragraph_index": 13}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/the-beginners-guide-to-gradient-descent-c23534f808fd", "anchor_text": "this article on gradient descent", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163", "anchor_text": "this article on setting learning rates", "paragraph_index": 38}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1", "paragraph_index": 49}], "all_paragraphs": ["Here\u2019s our problem. We have a neural network with just one layer (for simplicity\u2019s sake) and a loss function. That one layer is a simple fully-connected layer with only one neuron, numerous weights w\u2081, w\u2082, w\u2083\u2026, a bias b, and a ReLU activation. Our loss function is the commonly used Mean Squared Error (MSE). Knowing our network and our loss function, how can we tweak the weights and biases to minimize the loss?", "In Part 1, we learned that we have to find the slope to our loss (or cost) function in order to minimize it. We found that our cost function is:", "In Part 2, we learned how to find the partial derivative. This is important because there are more than one parameter (variable) in this function that we can tweak. We need to find the derivative of the cost function with respect to both the weights and biases, and partial derivatives come into play.", "In Part 3, we learned how to find derivatives of vector equations. Both the weights and biases in our cost function are vectors, so it is essential to learn how to compute the derivative of functions involving vectors.", "Now, we finally have all the tools we need to find the derivative (slope) of our cost function!", "We need to approach this problem step by step. Let\u2019s first find the gradient of a single neuron with respect to the weights and biases.", "The function of our neuron (complete with an activation) is:", "Where it takes x as an input, multiplies it with weight w, and adds a bias b.", "This function is really a composition of other functions. If we let f(x)=w\u2219x+b, and g(x)=max(0,x), then our function is neuron(x)=g(f(x)). We can use the vector chain rule to find the derivative of this composition of functions!", "The derivative of our neuron is simply:", "There are two parts to this derivative: the partial of z with respect to w, and the partial of neuron(z) with respect to z.", "There are two parts to z: w\u2219x and +b. Let\u2019s look at w\u2219x first.", "w\u2219x, or the dot product, is really just a summation of the element-wise multiplication of every element in the vector. In other words:", "This is once again a composition of functions, so we can write v = w\u2297x and u=sum(v). We\u2019re trying to find the derivative of u with respect to w. We\u2019ve learned about both of these functions \u2014 element-wise multiplication and summation \u2014 before in Part 3. Their derivatives are:", "(Go back and review them if you don\u2019t remember how they\u2019re derived)", "Therefore, by the vector chain rule:", "That\u2019s it! Now, let\u2019s find the derivative of z= u+b where u=w\u2219x with respect to both the weights w and the bias b. Remember that the derivative of a function with respect to a variable not in that function is zero, so:", "That\u2019s it! Those two are the derivatives of u with respect to both the weights and biases.", "The max(0,z) function simply treats all negative values as 0. The graph would thus look something like this:", "Looking at that graph, we can immediately see that the derivative is a piecewise function: it\u2019s 0 for all values of z less than or equal to 0, and 1 for all values of z greater than 0, or:", "Now that we have both parts, we can multiply them together to get the derivative of our neuron:", "Voila! We have our derivative for a neuron with respect to its weights! Similarly, we can use the same steps for the bias:", "And there you go! We now have the gradient of a neuron in our neural network!", "Our loss function, defined in Part 1, is:", "We can immediately identify this as a composition of functions, which require the chain rule. We\u2019ll define our intermediate variables as:", "*Note, the u and v here are different from the u and v used in the previous section.", "Let\u2019s compute the gradient with respect to the weights w first.", "u is simply our neuron function, which we solved earlier. Therefore:", "v(y,u) is simply y-u. Therefore, we can find its derivative (with respect to w) using the distributive property and substituting in the derivative of u:", "Finally, we need to find the derivative of the whole cost function with respect to w. Using the chain rule, we know that:", "Let\u2019s find the first part of that equation, the partial of C(v) with respect to v first:", "From above (Image 16), we know the derivative of v with respect to w. To find the partial of C(v), we multiply the two derivatives together:", "Since the max function is on the second line of our piecewise function, where w\u2219x+b is greater than 0, the max function will always simply output the value of w\u2219x+b:", "Finally, we can move the summation inside our piecewise function and tidy it up a little:", "That\u2019s it! We have our derivative with respect to the weights! However, what does this mean?", "w\u2219x+b-y can be interpreted as an error term \u2014 the different between the predicted output of the neural network and the actual output. If we call this error term ei, our final derivative is:", "Here, the greater the error, the higher the derivative. In other words, the derivative represents the slope, or how much we have to move our weights by in order to minimize our error. If our neural network has just begun training, and has a very low accuracy, the error will be high and thus the derivative will be large as well. Therefore, we will have to take a big step in order to minimize our error.", "You might notice that this gradient is pointing in the direction of higher cost, meaning we cannot add the gradient to our current weights \u2014 that will only increase the error and take us a step away from the local minimum. Therefore, we must subtract our current weights with the derivative in order to get one step closer to minimizing our loss function:", "Here, \u03b7 represents the learning rate, which we as programmers can set. The larger the learning rate, the bigger the step. However, setting a too-large learning rate may result in taking too big a step and spiraling out of the local minimum For more information, check out this article on gradient descent and this article on setting learning rates.", "Once again, we have our intermediate variables:", "We also have the value of the derivative of u with respect to the bias that we calculated previously:", "Similarly, we can find the derivative of v with respect to b using the distributive property and substituting in the derivative of u:", "Again, we can use the vector chain rule to find the derivative of C:", "The derivative of C with respect to v is identical to the one we calculated for the weights:", "Multiplying the two together to find the derivative of C with respect to b, and substituting in y-u for v, and max(0, w\u2219x +b) for u, we get:", "Once again, because the second line explicitly states that w\u2219x+b>0, the max function will always simply be the value of w\u2219x+b.", "Just like before, we can substitute in an error term, e = w\u2219x+b-y:", "Just like the derivative with respect to the weights, the magnitude of this gradient is also proportional to the error: the bigger the error, the larger step towards the local minimum we have to take. It is also pointing towards the direction of higher cost, meaning that we have to subtract the gradient from our current value to get one step closer to the local minimum:", "Congratulations on finishing this article! This was most likely not an easy read, but you\u2019ve persisted until the end and have succeeded in doing gradient descent manually!", "As I\u2019ve said in Part 1 of this series, without understanding the underlying math and calculations behind each line of code, we cannot truly understand what \u201ccreating a neural network\u201d really means or appreciate the complex intricacies that support each function that we write.", "I hope that these equations and my explanations make sense and have helped you understand these calculations better. If you have any questions or suggestions, don\u2019t hesitate to leave a comment below!", "If you haven\u2019t already, read Parts 1, 2, and 3 here:", "If you like this article, don\u2019t forget to leave some claps! Do leave a comment below if you have any questions or suggestions :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Student at UC Berkeley; Machine Learning Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6d9bee09aa0b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@reina.wang?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09----6d9bee09aa0b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://c1.staticflickr.com/2/1834/42271822770_6d2a1d533f_b.jpg", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3"}, {"url": "https://www.desmos.com/calculator", "anchor_text": "Source"}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/the-beginners-guide-to-gradient-descent-c23534f808fd", "anchor_text": "this article on gradient descent"}, {"url": "https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163", "anchor_text": "this article on setting learning rates"}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1: Introduction"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2: Partial Derivatives"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3: Vector Calculus"}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "here"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6d9bee09aa0b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6d9bee09aa0b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/calculus?source=post_page-----6d9bee09aa0b---------------calculus-----------------", "anchor_text": "Calculus"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----6d9bee09aa0b---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/math?source=post_page-----6d9bee09aa0b---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----6d9bee09aa0b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----6d9bee09aa0b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6d9bee09aa0b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6d9bee09aa0b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6d9bee09aa0b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/@reina.wang/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F827df2c647b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcalculating-gradient-descent-manually-6d9bee09aa0b&newsletterV3=9ddaaec52a09&newsletterV3Id=827df2c647b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}