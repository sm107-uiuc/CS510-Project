{"url": "https://towardsdatascience.com/audio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de", "time": 1682997212.979701, "path": "towardsdatascience.com/audio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de/", "webpage": {"metadata": {"title": "Audio AI: isolating instruments from stereo music using Convolutional Neural Networks | by Ale Koretzky | Towards Data Science", "h1": "Audio AI: isolating instruments from stereo music using Convolutional Neural Networks", "description": "This is the second article under the \u2018Audio AI\u2019 series I began back in March and it can be considered Part 2 after my first article on vocal isolation using CNNs. If you haven\u2019t read that one yet, I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785", "anchor_text": "vocal isolation using CNNs", "paragraph_index": 0}, {"url": "https://www.izotope.com/en/blog/music-production/stems-and-multitracks-whats-the-difference.html", "anchor_text": "Izotope, on Stems vs Multitracks", "paragraph_index": 4}, {"url": "https://www.native-instruments.com/en/specials/stems/", "anchor_text": "STEM format", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Auditory_masking", "anchor_text": "Auditory masking", "paragraph_index": 19}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111459/", "anchor_text": "this resource", "paragraph_index": 22}, {"url": "https://sigsep.github.io/sigsep-mus-eval/index.html", "anchor_text": "SAR, SIR, SDR", "paragraph_index": 24}, {"url": "https://sigsep.github.io/sigsep-mus-eval/usage.html", "anchor_text": "museval toolbox", "paragraph_index": 24}, {"url": "https://gstreamer.freedesktop.org/", "anchor_text": "GStreamer", "paragraph_index": 61}], "all_paragraphs": ["This is the second article under the \u2018Audio AI\u2019 series I began back in March and it can be considered Part 2 after my first article on vocal isolation using CNNs. If you haven\u2019t read that one yet, I highly recommend you start there!", "As a quick recap, in that first article, I showed you that we can build a pretty-small-for-the-task Convolutional Neural Network (~300k parameters) to perform vocal isolation in real-time. We tricked this network into \u2018thinking\u2019 it was solving a simpler problem and eventually we got this kind of results:", "Now, how do we go from here to reverse-engineering any professionally produced stereo mix into 4 instrument layers, more specifically, into the main 4 stems?", "Before moving forward, a bit of context on Stems.", "Stems are stereo recordings sourced from mixes of multiple individual tracks. For example, a drum stem will typically be a stereo audio file that sounds like all of the drum tracks mixed together. In most cases, additional processing such as equalization, compression, and time-based effects is included\u2026[Izotope, on Stems vs Multitracks].", "Where do stems come from? Well, stems have been used for decades in recording studios, as a way to control, process and manipulate entire groups of instruments, streamlining and simplifying the mixing, mastering and production process. Some will use stems, submixes, subgroups, or busses interchangeably. In addition to the workflow aspect,", "stems have gained some popularity as a format on its own, that can expand the creative possibilities, particularly in remixing and DJing.", "In this context, many players have been trying to develop the space, including Native Instruments with its STEM format.", "As artists and labels become more comfortable and less threatened by a growing ecosystem that embraces collaboration and understands the creative and business potential of derivatives, stems have been experiencing some renewed attention.", "The convention is to represent a mix by 4 stems:", "I received great feedback for my first article\u2019s journey structure so I\u2019ll try to do something similar here. In this case, however, \u2018A\u2019 is now our trained, ready-to-go vocal isolation model and \u2018Z\u2019, the Stems isolation one.", "This hypothesis should be broken down into 2 actually:", "If both 1) and 2) are true, we should be in a pretty good place.", "Previously, I showed you that we can create binary masks that result in a perceptually decent reconstruction of the vocals when applied to the original mix. We then trained our CNN to learn how to estimate these binary masks, being that a much simpler task compared to estimating the actual vocal\u2019s magnitude spectra.", "In this context, our input-output representation and resulting training set looked like this:", "Following the same process described in the first article for building our training set, we now have the same input as before but 4 sets of possible target outputs, one for each stem.", "In this case, the youtube scraping approach didn\u2019t give us much and we had to invest a lot of time and energy finding (and also synthesizing) perfectly aligned mix-stems pairs.", "Let\u2019s take the drums binary mask for instance. Now a value of \u20181\u2019 indicates predominant presence of drums content at a given frequency bin \u2014 timeframe location, and a value of \u20180\u2019 indicates predominant presence of something else at the given location. When talking about a frequency bin \u2014 timeframe location, remember we are literally referring to an individual element (pixel) in a binary mask, which corresponds to some value in the magnitude STFT domain.", "Now that we have multiple sources, there are many different methods by which we can generate our binary masks. We actually ended up with 7 different variations, under 2 main principles:", "a) Assume only one source can be present at a given frequency bin \u2014 timeframe location. In this scenario, you can think of all 4 sources competing for presence at every single location. Even though this is a huge over-simplification of reality, it works surprisingly well in some cases. (in terms of perception, the competition for presence is actually not that far from reality when thinking in terms of Auditory masking.)", "b) Sources are allowed to share a frequency bin \u2014 timeframe location. Or in other words, there can be multiple sources present at any given location. This is obviously more in line with reality.", "Since b) does not rely on the sources competing against each other, the binarization heuristics are based on a magnitude ratio criteria between each individual source and the original mix\u2019s magnitude STFT at each frequency bin \u2014 timeframe location, and in some cases, conditioned on the neighbor regions in the time-frequency space.", "Both principles have their pros and cons. The contents and demos in this article used a combination of proprietary methods under principle b). For a comprehensive overview and references on time-frequency masking, you can check this resource.", "Now going back to our hypotheses, how do we validate hypothesis 1? Two approaches come to mind.", "1) Using audio source separation metrics like SAR, SIR, SDR. These were introduced by the source separation academic community a few years back, with the goals of standardizing the criteria for assessing the quality of estimated sources recovered from a source separation process, as well as for the purpose of benchmarking. The museval toolbox implements these metrics.", "2) A less formal (but not necessarily less effective when building a specific application) approach is to build small test sets and carry listening sessions comparing the original sources and the ones recovered through binary masking on the original mix. With this in place, you can establish some scoring based on the specific quality vectors you are interested in.", "So what was the verdict for hypothesis 1? You\u2019ll find out later\u2026", "Now that we have our Stems training set, how do we plug this input/output representation into some compatible architecture that\u2019s going to learn the mix-to-stems task?", "Well, once again, there\u2019s more than one way to do this and I\u2019ll show you a few approaches we went for during our design.", "We know that our first model for vocal isolation worked well. Before thinking of reinventing the wheel, let\u2019s first validate that the same CNN architecture we used for learning vocal binary masks behaves similarly with other stems. (hypothesis 2)", "We can train the exact same architecture we used for vocals, for isolating drums, bass and others, by simply changing our target binary masks", "For instance, for the drums stem, we\u2019ll be presenting X(mix) at the input and y(drums) at the output.", "Once our 4 independent models have been trained, we can load the pre-trained weights into 4 identical model placeholders. From there we can connect these models in a way that they share a common input, while concatenating their output layers so that at every inference step, we obtain the binary masks for all 4 stems simultaneously. Think of these 4 models as becoming branches of a new, larger model (this is literally what happens here).", "Looking at the output layer of this new model, you can see that our dimensionality is now 2052 (4 x 513). Because we know exactly how these model branches are wired, we can easily retrieve each stem\u2019s binary mask by simple indexing.", "Below, I show you the code to build the above architecture using Keras\u2019 Functional API. I\u2019ll leave it to you as homework to load the weights from a pre-trained model into new model placeholders. It should be relatively straightforward using the load_weights( ) / get_weights() / set_weights() functions in Keras.", "although the final model has close to 1.3M parameters, the learning process took place in architectures of ~300K parameters only", "Draw your own conclusions but the points that I\u2019d to make here are:", "One thing you may be asking though\u2026 \u201cWhy not just keep those 4 models separate during inference?\u201d", "Well, it is a totally valid question. The answer is, for this particular application, we needed to perform the mix-to-stems deconstruction as part of a real-time pipeline connected to several other processes, including playback. Because of this, we wanted all stems to be generated at the exact same time to avoid buffering and synchronization issues. On top of that, we had to run this on a single GPU. Having all 4 models separate would have made things more challenging on both sides. If you\u2019re building an API for example, and you don\u2019t care about synchronization or real-time, you have a lot more flexibility.", "Let\u2019s say we want to stress-test our ability to generate all binary masks at once using the single stem architecture. What we can do is increase the output dimensionality to 2052, so that we can present all 4 binary masks at the output during training. This increase in the output dimensionality, however, will bring the total number of parameters up, to ~0.5M approximately. Not that this is necessarily an issue but, as someone who says \u201cdecouple your hypotheses!\u201d 3 times a day, we should first see what happens when we train this model under comparable conditions.", "There are many different ways in which we can try to reduce the number of parameters. We can, for example, decrease the dimensionality of the last FC layer before the output, from 256 to 77, as shown below:", "During training, we present X(mix) at the input and a concatenation of all 4 stems\u2019 binary masks at the output (2052 = 513 x 4). Some points to be made here:", "The 2 bullet points above are partially validated hypotheses, completely open to debate. They come from consistently observed behaviors during our experimentation.", "In this case, the first 2 convolutional layers are stem-agnostic. The model has total discretion in how it will share those layers\u2019 weights among all 4 stems. After the second convolutional layer, we create 4 branches of 2 convolutional layers each, one for each stem. Although we are not explicitly telling the model which branch corresponds to which stem, the model will be forced to optimize each branch to a specific stem based on the target binary masks at the output.", "The reasoning behind this design was based on the known behavior of CNNs with structured data in the spatial domain, where the learning process derives into a hierarchical decomposition of features; from coarse and primitive to detailed and specific. Our hypothesis was, we can help the model learn better by persuading it to split the task between stem-agnostic filtering operations and stem-specific ones. By inspecting some of the feature maps out of the first convolutional stage, we observed no clear stem-specific patterns. However, as we moved deeper, most of the feature maps computed at the output of each branch revealed clear stem-specific patterns. This should not be a surprise, although it is very cool to see it :)", "Like in the case of Model 2, the optimization is global and not stem-specific.", "Perceptually, the results of this model are comparable to those from model 1. I\u2019d say our hypothesis on the effectiveness of this particular design was partially validated but not conclusive.", "For model 1, we merged independently trained models, remember?", "What if we trained that same full architecture from scratch?", "Not bad either. Some points worth mentioning here:", "In the first article, I explained how our initial vocal isolation design ended up being some sort of regression-classification hybrid.", "\u201cWe are asking the model to \u201cclassify pixels\u201d at the output as vocal or non-vocal, although conceptually (and also in terms of the loss function used -MSE- ), the task is still a regression one\u201d", "By looking at our output representation, what prevents us from actually treating our problem as a classification one? At the end of the day, we need to choose between two discrete values at the output, 0 or 1. Therefore, we can transform our stems isolation problem into a multi-label classifier.", "Think about the vocal isolation model for instance, with output dimensionality equal to 513. Think about what those values represent. We know they are all part of the same thing, right? Just points in a binary mask that later map into magnitude spectra, blah & blah\u2026 Well, another way to interpret those values is, they are labels or categories that represent the presence of content in different buckets.", "An analogy might help crystallize this:", "What changes do we need to make to transform our problem into a multi-label classifier?", "Now, when we do an inference pass to estimate a middle frame\u2019s binary mask, what we get is the probability of a given source being present per frequency-bin. In order to build the final binary mask, prior to the sliding window and concatenation mechanic described in the first article, we need to re-binarize these outputs by simple thresholding:", "Treating our problem as a multi-label classifier results in some significant improvements in the quality of isolation compared to the MSE loss, regression \u2014 based model equivalents.", "For this work, real-time operation was an important design constraint. This required us to spend a good amount of time doing software architecture design and implementation work, in addition to Machine Learning. Concretely, we had to ensure 2 things:", "Making sure that the process of", "\u2026 [reading audio, segmenting it, transforming it, predicting binary masks using our ML models, reconstructing stems back into time-domain and extracting additional features from the estimated stems including onsets, pitch, transcription, etc.. all this in Stereo] \u2026", "was faster than the audio playback rate. In pursuing this, having a fast-inference ML model is obviously highly desired (therefore the reduced # of model parameters in our designs), although that is just one piece of the puzzle\u2026 hello multi-threading, sliding windows, buffering, synchronization, etc. To make this happen we built an optimized C/C++ pipeline on top of GStreamer, which was far from painless, but beautiful once we saw the light at the end of the tunnel. Our final solution ended up being ~2.5X faster than playback.", "This is where things get real\u2026 In the first case, all we care about is performance, but you have no constraints in terms of access to your input data ahead of time. If your process is faster than your playback or delivery rate, you implement the required queueing mechanisms to save your data to retrieve it at a later time.", "Now, what happens when your input is being streamed to you as it is being recorded and you need to perform demixing for a live music installation for example? Well, because you can no longer look ahead, if you need future context to predict the present, you\u2019ll be introducing latency! What can we do to avoid (reduce) that latency then?", "Going back to our input-output representations for training our models, one alternative is to ONLY rely on past context to learn to predict our binary masks, as shown on the left. That way, we pretty much limit latency to whatever recording/playback buffering stages our pipeline has.", "The big question is: is the isolation quality affected when relying on the past only? Yes, significantly! And this should be no surprise, as music is time-series data. Any given timeframe will be more predictable if we know what happened before and after it. However, this still worked well enough for our purposes. [ Now you can picture me ranting on the importance of real Feature Engineering, like I did on my first article..]", "Sorry to be this boring but they all have their pros and cons\u2026 Some models are more sensitive than others to certain types of content and mixing conditions. Others achieve better isolation at the expense of higher degradation in the reconstructed sources, etc. Overall, they all learned the task well enough with the exception of model 2, probably due to its limited number of parameters (~300k parameters) and high-dimensional output. In any case, my goal here was not to give you the holy grail of audio source separation (I\u2019ll probably keep it to myself when I get there :)) but to provide you with some key insights and intuitions related to this particular problem, as well as to some considerations while designing artificial neural networks in general.", "In terms of hypothesis 1, the final answer is, the binary masking method used for vocals generalizes relatively well for bass but not as well for drums and others. In most cases, the degradation you hear in the drums (especially in the highs/cymbals) is more related to the binary masking and not to the ML models. In the case of others, that same binary masking results in some degree of interference from other sources, especially vocals. You can hear this in the examples, which leaves in evidence that the \u2018garbage in\u2026 garbage out\u2026\u2019 principle definitely applies in ML.", "Don\u2019t hesitate in leaving questions. I\u2019ll keep writing Audio AI stuff as time allows!", "ps: once again, big shoutouts to Naveen Rajashekharappa and Karthiek Reddy Bokka for their contributions to this work.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of AI/ML & Audio Science Innovation @Splice . Mentor @Techstars | Advisor @BrkThroughT | USC, Fulbright alum. Love for audio, music and travel."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F584ababf69de&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----584ababf69de--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ale.koretzky?source=post_page-----584ababf69de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Ale Koretzky"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a05d06f496d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&user=Ale+Koretzky&userId=1a05d06f496d&source=post_page-1a05d06f496d----584ababf69de---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785", "anchor_text": "vocal isolation using CNNs"}, {"url": "https://www.izotope.com/en/blog/music-production/stems-and-multitracks-whats-the-difference.html", "anchor_text": "Izotope, on Stems vs Multitracks"}, {"url": "https://www.native-instruments.com/en/specials/stems/", "anchor_text": "STEM format"}, {"url": "https://en.wikipedia.org/wiki/Auditory_masking", "anchor_text": "Auditory masking"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111459/", "anchor_text": "this resource"}, {"url": "https://hal.inria.fr/inria-00544230/document", "anchor_text": "https://hal.inria.fr/inria-00544230/document"}, {"url": "https://sigsep.github.io/sigsep-mus-eval/index.html", "anchor_text": "SAR, SIR, SDR"}, {"url": "https://sigsep.github.io/sigsep-mus-eval/usage.html", "anchor_text": "museval toolbox"}, {"url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "anchor_text": "binary cross entropy"}, {"url": "https://gstreamer.freedesktop.org/", "anchor_text": "GStreamer"}, {"url": "https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785", "anchor_text": "vocal isolation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----584ababf69de---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----584ababf69de---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/audio?source=post_page-----584ababf69de---------------audio-----------------", "anchor_text": "Audio"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----584ababf69de---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----584ababf69de---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&user=Ale+Koretzky&userId=1a05d06f496d&source=-----584ababf69de---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&user=Ale+Koretzky&userId=1a05d06f496d&source=-----584ababf69de---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----584ababf69de--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F584ababf69de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----584ababf69de---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----584ababf69de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----584ababf69de--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----584ababf69de--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----584ababf69de--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----584ababf69de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ale Koretzky"}, {"url": "https://medium.com/@ale.koretzky/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "764 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a05d06f496d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&user=Ale+Koretzky&userId=1a05d06f496d&source=post_page-1a05d06f496d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e5ff42ee152&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de&newsletterV3=1a05d06f496d&newsletterV3Id=7e5ff42ee152&user=Ale+Koretzky&userId=1a05d06f496d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}