{"url": "https://towardsdatascience.com/gpt-3-101-a-brief-introduction-5c9d773a2354", "time": 1683011557.756168, "path": "towardsdatascience.com/gpt-3-101-a-brief-introduction-5c9d773a2354/", "webpage": {"metadata": {"title": "GPT-3 101: a brief introduction. It has been almost impossible to avoid\u2026 | by David Pereira | Towards Data Science", "h1": "GPT-3 101: a brief introduction", "description": "Let\u2019s start with the basics. GPT-3 stands for Generative Pretrained Transformer version 3, and it is a sequence transduction model. Simply put, sequence transduction is a technique that transforms an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/transformers-141e32e69591", "anchor_text": "This article", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "paper", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/gpt-3-the-first-artificial-general-intelligence-b8d9b38557a1", "anchor_text": "GPT-3 is only about 1000x too small", "paragraph_index": 6}, {"url": "https://commoncrawl.org/", "anchor_text": "Common Crawl", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Main_Page", "anchor_text": "English-language Wikipedia", "paragraph_index": 7}, {"url": "https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential", "anchor_text": "spanning some 6 million articles, and making up only 0.6 percent of its training data", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1606.06031v1", "anchor_text": "LAMBADA", "paragraph_index": 7}, {"url": "https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122", "anchor_text": "it can excel in task-agnostic performance without fine tuning", "paragraph_index": 8}, {"url": "https://productsolving.substack.com/p/openais-gpt-3-will-change-how-we", "anchor_text": "priming", "paragraph_index": 11}, {"url": "https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29", "anchor_text": "prompt engineering", "paragraph_index": 11}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "this", "paragraph_index": 13}, {"url": "https://github.com/openai/gpt-2/", "anchor_text": "much smaller version of GPT-2 along with sampling code", "paragraph_index": 14}, {"url": "https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential", "anchor_text": "models built from this data produce text that is shockingly biased", "paragraph_index": 17}, {"url": "https://twitter.com/math_rachel", "anchor_text": "Rachel Thomas", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/bias-in-ai-much-more-than-a-data-problem-de6ef950c848", "anchor_text": "articles", "paragraph_index": 19}, {"url": "https://www.codingrights.org/decolonising-ai-a-transfeminist-approach-to-data-and-social-justice/", "anchor_text": "political design of AI systems", "paragraph_index": 19}, {"url": "https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/#:~:text=Certainly%2C%20OpenAI's%20advancements%20haven't,training%20costs%20exceeding%20%2412%20million.", "anchor_text": "GPT-3 alone is estimated to have a memory requirement exceeding 350GB and training costs exceeding $12 million.", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/is-deep-learning-too-big-to-fail-8930505d7ab1", "anchor_text": "Is Deep Learning too big to fail", "paragraph_index": 22}, {"url": "https://dpereirapaz.medium.com/membership", "anchor_text": "consider a membership", "paragraph_index": 25}, {"url": "https://www.linkedin.com/in/dpereirapaz/", "anchor_text": "https://www.linkedin.com/in/dpereirapaz/", "paragraph_index": 27}], "all_paragraphs": ["Let\u2019s start with the basics. GPT-3 stands for Generative Pretrained Transformer version 3, and it is a sequence transduction model. Simply put, sequence transduction is a technique that transforms an input sequence to an output sequence.", "GPT-3 is a language model, which means that, using sequence transduction, it can predict the likelihood of an output sequence given an input sequence. This can be used, for instance to predict which word makes the most sense given a text sequence.", "A very simple example of how these models work is shown below:", "INPUT: It is a sunny and hot summer day, so I am planning to go to the\u2026", "PREDICTED OUTPUT: It is a sunny and hot summer day, so I am planning to go to the beach.", "GPT-3 is based on a specific neural network architecture type called Transformer that, simply put, is more effective than other architectures like RNNs (Recurrent Neural Networks). This article nicely explains different architectures and how sequence transduction can highly benefit from the Transformer architecture GPT-3 uses.", "Transformer architectures are not really new, as they became really popular 2 years ago because Google used them for another very well known language model, BERT. They were also used in previous versions of OpenAI\u2019s GPT. So, what is new about GPT-3? Its size. It is a really big model. As OpenAI discloses on this paper, GPT-3 uses 175 billion parameters. Just as a reference, GPT-2 \u201conly\u201d used 1,5 billion parameters. If scale was the only requisite to achieve human-like intelligence (spoiler, it is not), then GPT-3 is only about 1000x too small.", "Using this massive architecture, GPT-3 has been trained using also huge datasets, including the Common Crawl dataset and the English-language Wikipedia (spanning some 6 million articles, and making up only 0.6 percent of its training data), matching state-of-the-art performance on \u201cclosed-book\u201d question-answering tasks and setting a new record for the LAMBADA language modeling task.", "What really makes GPT-3 apart from previous language models like BERT is that, thanks to its architecture and massive training, it can excel in task-agnostic performance without fine tuning. And here is when magic comes. Since it was released, GPT-3 has been applied in a broad range of scenarios, and some developers have come with really amazing use case applications. Some of them are even sharing the best ones on github or their own websites for everyone to try:", "A non exhaustive list of applications based on GPT-3 are shown below:", "The first question that comes to mind as someone working in the IT services market when seeing all these incredible GPT-3 based applications is clear: will software engineers run out of jobs due to AI improvements like these? The first thing that comes to my mind here is that software engineering is not the same as writing code. Software engineering is a much profound task that implies problem solving, creativity, and yes, writing the code that actually solves the problem. That being said, I do really think that this will have an impact on the way we solve problems through software, thanks to priming.", "Just as humans need priming to recognize something we have never noticed before, GPT-3 does too. The concept of priming will be key for making this technology useful, providing the model with a partial block of code, a good question on the problem we want to solve, etc. Some authors are already writing about the concept of \u201cprompt engineering\u201d as a new way to face problem solving through AI in the style of GPT-3. Again, an engineering process still requires much more that what it is currently solved by GPT-3, but it will definitely change the way we approach coding as part of it.", "GPT-3 has not been available for much time (and actually, access to its API is very restricted for the moment) but it is clearly amazing what developer\u2019s creativity can achieve by using this model capabilities. Which brings us to the next question. Should GPT-3 generally available? What if this is used for the wrong reasons?", "Not so long ago, OpenAI wrote this when presenting its previous GPT-2 model:", "\u201cDue to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code. We are not releasing the dataset, training code, or GPT-2 model weights.\u201d", "As of today, OpenAI still acknowledges this potential implications but is opening access to its GPT-3 model through a beta program. Their thoughts on this strategy can be found in this twitter thread:", "It is good to see that they clearly understand that misuse of generative models like GPT-3 is a very complex problem that should be addressed by the whole industry:", "Despite having shared API guidelines with the creators that are already using the API, and claiming that applications using GPT-3 are subject to review by OpenAI before going live, they acknowledge that this is a very complex issue that won\u2019t be solved by technology alone. Even the Head of AI @ Facebook entered the conversation with a few examples on how, when being prompted for writing tweets over just one word (jews, black, women, etc.) GPT-3 can show harmful biases. This might have to do with the fact that GPT-3 has been trained on data filtered by Reddit and that \u201cmodels built from this data produce text that is shockingly biased.\u201d", "And this is not the only threat. Advanced language models can be used to manipulate public opinion, and GPT-3 models and their future evolutions could imply huge risks for democracy in the future. Rachel Thomas shared an excellent talk on the topic that you can find here:", "Data Bias is not the only problem with language models. As I mentioned in one of my previous articles, the political design of AI systems is key. In the case of GPT-3 this might have huge implications on the future of work and also on the lives of already marginalized groups.", "As a funny (or maybe scary) note, even GPT-3 thinks GPT-3 should be banned!", "Going back to the architecture of GPT-3, training a model of 175 billion parameters is not exactly cheap in terms of computational resources. GPT-3 alone is estimated to have a memory requirement exceeding 350GB and training costs exceeding $12 million.", "It is clear that the results are amazing but, at which cost? Is the future of AI sustainable in terms of the compute power needed? Let me finish this article by using some sentences that I wrote for my \u201cIs Deep Learning too big to fail?\u201d article:", "Let\u2019s not forget that more data does not necessarily means better data. We need quality data: unbiased and diverse data which can actually help AI benefit a lot of communities that are far from getting access to the state-of-the-art compute power like the one needed to play AlphaStar.", "Only when we use efficient algorithms (therefore accessible to the vast majority of citizens) trained with biased and diverse data will Deep Learning be too big to fail. And it will be too big because it will then serve those who are too big to be failed: the people.", "If you enjoyed reading this piece, please consider a membership to get full access to every story while supporting me and other writers on Medium.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data & Intelligence Partner at NTT DATA Europe & Latam. All opinions are my own. https://www.linkedin.com/in/dpereirapaz/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5c9d773a2354&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dpereirapaz.medium.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": ""}, {"url": "https://dpereirapaz.medium.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "David Pereira"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8c8a1a860bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&user=David+Pereira&userId=f8c8a1a860bc&source=post_page-f8c8a1a860bc----5c9d773a2354---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@franckinjapan", "anchor_text": "https://unsplash.com/@franckinjapan"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "Visualizing A Neural Machine Translation Model"}, {"url": "https://twitter.com/JayAlammar", "anchor_text": "@JayAlammar"}, {"url": "https://towardsdatascience.com/transformers-141e32e69591", "anchor_text": "This article"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/gpt-3-the-first-artificial-general-intelligence-b8d9b38557a1", "anchor_text": "GPT-3 is only about 1000x too small"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "anguage Models are Few-Shot Learners"}, {"url": "https://commoncrawl.org/", "anchor_text": "Common Crawl"}, {"url": "https://en.wikipedia.org/wiki/Main_Page", "anchor_text": "English-language Wikipedia"}, {"url": "https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential", "anchor_text": "spanning some 6 million articles, and making up only 0.6 percent of its training data"}, {"url": "https://arxiv.org/abs/1606.06031v1", "anchor_text": "LAMBADA"}, {"url": "https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122", "anchor_text": "it can excel in task-agnostic performance without fine tuning"}, {"url": "https://productsolving.substack.com/p/openais-gpt-3-will-change-how-we", "anchor_text": "priming"}, {"url": "https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29", "anchor_text": "prompt engineering"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "this"}, {"url": "https://github.com/openai/gpt-2/", "anchor_text": "much smaller version of GPT-2 along with sampling code"}, {"url": "https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential", "anchor_text": "models built from this data produce text that is shockingly biased"}, {"url": "https://twitter.com/math_rachel", "anchor_text": "Rachel Thomas"}, {"url": "https://towardsdatascience.com/bias-in-ai-much-more-than-a-data-problem-de6ef950c848", "anchor_text": "articles"}, {"url": "https://www.codingrights.org/decolonising-ai-a-transfeminist-approach-to-data-and-social-justice/", "anchor_text": "political design of AI systems"}, {"url": "https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/#:~:text=Certainly%2C%20OpenAI's%20advancements%20haven't,training%20costs%20exceeding%20%2412%20million.", "anchor_text": "GPT-3 alone is estimated to have a memory requirement exceeding 350GB and training costs exceeding $12 million."}, {"url": "https://towardsdatascience.com/is-deep-learning-too-big-to-fail-8930505d7ab1", "anchor_text": "Is Deep Learning too big to fail"}, {"url": "https://dpereirapaz.medium.com/membership", "anchor_text": "consider a membership"}, {"url": "https://medium.com/tag/gpt-3?source=post_page-----5c9d773a2354---------------gpt_3-----------------", "anchor_text": "Gpt 3"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5c9d773a2354---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5c9d773a2354---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/openai?source=post_page-----5c9d773a2354---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&user=David+Pereira&userId=f8c8a1a860bc&source=-----5c9d773a2354---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&user=David+Pereira&userId=f8c8a1a860bc&source=-----5c9d773a2354---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5c9d773a2354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5c9d773a2354---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5c9d773a2354--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5c9d773a2354--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5c9d773a2354--------------------------------", "anchor_text": ""}, {"url": "https://dpereirapaz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dpereirapaz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David Pereira"}, {"url": "https://dpereirapaz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "377 Followers"}, {"url": "https://www.linkedin.com/in/dpereirapaz/", "anchor_text": "https://www.linkedin.com/in/dpereirapaz/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8c8a1a860bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&user=David+Pereira&userId=f8c8a1a860bc&source=post_page-f8c8a1a860bc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc63af6ad46d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-101-a-brief-introduction-5c9d773a2354&newsletterV3=f8c8a1a860bc&newsletterV3Id=c63af6ad46d6&user=David+Pereira&userId=f8c8a1a860bc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}