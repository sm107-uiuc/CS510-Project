{"url": "https://towardsdatascience.com/yolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73", "time": 1683003483.372875, "path": "towardsdatascience.com/yolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73/", "webpage": {"metadata": {"title": "YOLO Made Simple: Interpreting the You Only Look Once Paper | by Anirudh S | Towards Data Science", "h1": "YOLO Made Simple: Interpreting the You Only Look Once Paper", "description": "YOLO, unlike the other Object Detection models, is blazingly fast. We'll go through the YOLO paper, pondering over minute details. Made simple!"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "https://arxiv.org/abs/1312.4400", "paragraph_index": 21}, {"url": "https://hackerstreak.com/batch-normalization-how-it-really-works/", "anchor_text": "Batch Normalization", "paragraph_index": 26}, {"url": "https://hackerstreak.com/kaggle-cactus-detection/", "anchor_text": "How to build a simple Convolution Neural Network, train it on a kaggle dataset and achieve 99.9% accuracy!", "paragraph_index": 29}], "all_paragraphs": ["Unlike the state of the art R-CNN model, the \u201cYOLO: Unified, Real-Time Object Detection\u201d or \u201cYOLOv1\u201d presents an end-to-end solution to object detection and classification. Meaning that we can train a single model to detect and classify directly from the input image and is fully differentiable. Examples of traditional methods of object detection are running a classifier on different parts of an image and at different scales. Neat, right?! All you need is just a classifier.", "As simple as it sounds, it\u2019s extremely inefficient to run classifiers hundreds of times on a single image to localize objects. But YOLOv1 deals with it smartly. It detects and classifies with a single forward pass of the image and runs in real-time. Hence the name \u201cYou Only Look Once\u201d. We\u2019ll look at everything described by the paper in detail.", "The authors compare YOLO\u2019s working to human perception. We, humans, glance at a scene and instantly get an overview of what\u2019s present, where, who\u2019s doing what and a whole lot more. The human visual cortex is amazing, isn\u2019t it? YOLOv1 predicts what objects are present and where they\u2019re in the image in just one go by treating the object detection and classification problems as regression. Simply put, you give an image to the YOLO model, it passes through a bunch of layers and the final output will be the class predictions and bounding box coordinates. Here, the authors crisply define YOLO\u2019s working as", "Straight from image pixels to bounding box coordinates and class probabilities.", "YOLO deals with object detection by using an elegant process of dividing the image into a grid of S x S cells. And YOLO restricts the input to square images only.", "Each cell produces class and bounding prediction for objects if their centre falls inside that particular cell. This method is powerful as it enables YOLO to detect multiple objects in an image and classify them simultaneously. Yet, dividing the image into a greater number of cells will produce more fine-grained predictions. Each cell in the grid is responsible for predicting the bounding box parameters, the confidence that an object is present and the class probabilities. The resulting bounding box prediction consists of the x and y coordinates of the box\u2019s centre, sqrt(width), sqrt(height) and an object probability score.", "Note: YOLOv1 predicts the square root of the bounding box\u2019s width and height relative to the image. The reason is explained in the Loss Function section below.", "If there are 20 classes (C=20), the output of the cell is [x, y, \u221aw, \u221ah, object probability, C1, C2, C3,\u2026\u2026., C20]. The aforementioned probabilities are conditional class probabilities. To clarify, it is the probability that the object belongs to a particular class given that the object is present in the cell. Of course, each cell in the grid of cells predicts a similar list of items.", "But there\u2019s one more thing. In YOLOv1, each cell predicts not one but B bounding boxes. And each of these bounding boxes has [x, y, \u221aw, \u221ah, object probability]. However, YOLO predicts the class probabilities only once per cell irrespective of the number of bounding boxes. Consequently, each cell\u2019s output now has more items. To illustrate, if B=2 and C=20, the output grows to become [x1, y1, \u221aw1, \u221ah1, obj. prob1, x2, y2, \u221aw2, \u221ah2, obj. prob2, C1, C2,\u2026., C20].", "For detection on PASCAL VOC with 20 classes, it predicts a 7 x 7 grid of cells with 2 bounding boxes for each one of them. Considering the predictions of all the cells, the output shape will be a cuboidal volume having dimensions (7 x 7 x 30). The two bounding boxes contribute ten terms and the class probabilities have twenty terms since C=20. This sums up to thirty which explains the \u201c30\u201d in the third dimension.", "The x and y coordinates of the centre of the bounding box are relative to the top-left corner of that grid cell rather relative to the image\u2019s top-left corner. Each cell predicts the coordinates relative to its position and the coordinates act as offsets to the cell\u2019s position.", "If we divide an image into 3 x 3 grid cells, as shown above, the centre of the object falls inside the centre grid cell. And if we again assume that each grid cell\u2019s width and height is A, the coordinates of the object\u2019s centre is (0.6A, 0.6A) relative to the cell\u2019s top-left corner. The model predicts the coordinates with a value between 0 and 1 which is a fraction of A. Therefore, coordinates (0.6, 0.6) denote 60% of A\u2019s length to the right and 60% down. These coordinates can be converted relative to the whole image since we know which cell predicts the box and its relative coordinates.", "For the above example, the box\u2019s centre relative to the image is (A+0.6*A, A+0.6*A). The former A that\u2019s added to 0.6*A is the distance of the cell\u2019s top-left corner from that of the image\u2019s top-left corner. Thus, the sum gives us the coordinates of the box\u2019s centre relative to the whole image. But the height and width of the bounding boxes are predicted relative to the whole image. For the above \u201ccat\u201d example, the bounding box\u2019s height is almost two-thirds of the image\u2019s height. And the box\u2019s width is one-third of the image\u2019s width.", "Therefore, YOLO will predict the width and height as 1/3rd and 2/3rd of the image\u2019s width (W) and height (H) respectively. Thus the width and height prediction means \u221a(0.33*W), \u221a(0.66*H). Finally, the probability that an object is also represented as a number between 0 and 1. This object probability is multiplied with the Intersection over Union (IoU) of the predicted box with the ground truth to give the confidence score. The IoU is a score that tells how much the predicted box overlaps with the ground truth box. Its value also falls between 0 and 1 denoting no overlap and complete overlap respectively.", "A confidence score of 1 represents 100% confidence and 0, 0% confidence. The higher this value is, the more confident the cell is that there\u2019s an object. This confidence score is multiplied with the conditional class probability to produce the probability score that a given class is present.", "Since YOLOv1 came out in 2015, it follows a typical convolutional architecture but innovated the way it predicts the output. It has 24 convolutional layers, 4 max-pooling layers and two fully connected layers, one with 4,096 neurons and the other with 1,470 neurons. The model takes in input colour images of size 448 x 448 for object detection. As we saw earlier, the YOLOv1 predicts a cuboidal output from its final fully connected layer. That\u2019s done by reshaping the output of the last fully connected layer with 1,470 neurons into a (7 x 7 x 30) cuboid for PASCAL VOC. Explicitly, we can see that the final layer has 1,470 neurons because it needs to be reshaped to 7 x 7 x 30=1,470.", "The feature extractor is built with convolution layers of different filter sizes, with follow up max-pooling layers after some of them for spatial reduction. The usual stuff! Only the first convolution layer has 7 x 7 filters in the YOLO model. And all the others have 3 x 3 filters. Rather than just using alternate 3 x 3 convolutions and max pool layers, the network uses 1 x 1 convolution.", "The authors mention that their architecture was inspired by the GoogLeNet which introduced the Inception module.", "Our network architecture is inspired by the GoogLeNet model for image classification", "But unlike GoogLeNet, YOLOv1 doesn\u2019t use inception blocks. Instead, it employs 1 x 1 convolution to reduce the channel depth of the feature maps after applying a large number of 3 x 3 filters. The 1 x 1 filters have a very small receptive field (just a single pixel) but they\u2019re used mainly to reduce the computation load on the layers that follow the 3 x 3 convolution layers. Also, they help in introducing a non-linearity without changing the receptive field.", "Don\u2019t quite get it? We\u2019ll look in detail why it\u2019s beneficial.", "The concept of 1 x 1 convolution was introduced in the paper \u201cNetwork-in-Network\u201d by Min et al. Take a look at the paper here https://arxiv.org/abs/1312.4400", "Note: The below example explains 1 x 1 convolution with a small model with convolution layers. Do not confuse this example model with YOLO\u2019s architecture", "For example, consider that an input image of size (100,100,3) is fed to a 3 x 3 convolution layer with 128 filters with zero paddings. We zero pad to produce output feature maps of the same spatial dimensions as the input (100,100). Let\u2019s ignore the batch dimension for simplicity. Each filter produces a (100,100,1) output feature map after convolving with the input image. Since we have 128 such filters, the filter outputs append to the channel dimension to produce an output of shape (100,100,128). The weights of this layer should be of size (3, 3, 3, 128) which is (filter_x_size, filter_y_size, input_channels, number of filters).", "So far so good (just the normal convolution!).", "Now, if we feed this output again to the next 3 x 3 convolution layer with 128 filters, its weights will have to be (3, 3, 128, 128).", "The YOLOv1 model uses a dropout between the two fully connected layers to prevent overfitting. But it doesn\u2019t use any other techniques like Batch Normalization which can accelerate training.", "Now that we\u2019ve seen what\u2019s a 1 x 1 convolution, let\u2019s move on to cover other stuff about the network.", "We saw at the start that the network has 24 convolution layers, 4 max-pool layers and 2 FC layer.", "How to build a simple Convolution Neural Network, train it on a kaggle dataset and achieve 99.9% accuracy!", "The authors pre-trained the first twenty convolution layers on the ImageNet dataset at an input resolution of 224 x 224. It is only half the resolution of YOLOv1\u2019s detection input which is 448 x 448. Which is obviously because the ImageNet images are of size 224 x 224.", "We pre-train the convolutional layers on the ImageNet classification task at half the resolution (224 \u00d7 224 input image) and then double the resolution for detection \u2014 The YOLOv1 Authors", "Since only the first 20 convolution layers are used for transfer learning, it can operate on input images of any resolution.", "The pretraining helps the convolution filters to learn patterns from the ImageNet dataset. As it contains a huge number of images belonging to over a thousand classes, convolution layers can learn a lot of useful features. Pretraining and transfer learning give a good performance boost to YOLO for detection.", "For pretraining we use the first 20 convolutional layers from the Figure followed by a average-pooling layer and a fully connected layer \u2014 The YOLOv1 Authors", "After pretraining on the ImageNet, the average-pooling and the fully connected prediction layer are removed. And they\u2019re replaced with four 3 x 3 convolution layers and two fully connected layers.", "Note: The output dimension is 7 x 7 x 30 only for PASCAL VOC dataset using S=7, B=2. If any of the parameters S (number of grid cells S x S), B or the number of classes in the dataset changes, this output shape would also change.", "These cascaded convolutions and max-pool layers reduce the spatial dimension of the feature map from 448 x 448 to the required 7 x 7 size.", "All the layers in the network except the final fully connected layer use the \u201cLeaky-Relu\u201d activation function. And the final layer has a linear activation.", "Leaky-relu\u2019s graph is slightly different from the Rectified Linear Unit (Relu). Have a look!", "One important thing to notice is the final layer\u2019s output. As we noted before, it has a linear activation and its output is reshaped to form a 7 x 7 x 30 tensor. Finally, the YOLOv1 model was trained for 135 epochs on the PASCAL VOC dataset. Some of the training techniques followed by the authors are quoted below.", "YOLO\u2019s loss function may seem intimidating but it\u2019s quite simple. The loss used is called sum-squared loss and is used for all the tasks in YOLOv1. The authors quote that it is easier to optimize sum-squared than, say, log-likelihood. But they have also mentioned that it has some drawbacks.", "The sum-squared error weighs localization and classification errors equally. And, since many grid cells in an image don\u2019t contain any objects, the sum-squared error tries to make the confidence score of these cells to zero. This means that their loss will dominate the gradients and it doesn\u2019t let the model converge. To solve this issue, the authors introduce the parameters \u03bbcoord and \u03bbnoobj.", "These two parameters weigh the different terms in the loss function to keep the loss due to the cells with no object low. And also weighs the coordinate loss more.", "Let\u2019s break it down and understand what each term in the loss means.", "The first term calculates the sum-squared loss of between the predicted coordinates and the ground truth coordinates. It is done for all the bounding boxes in all the cells. The resulting sum is multiplied by \u03bbcoord. The authors used a value of 5 for \u03bbcoord. The \u201c1 obj i,j\u201d term represents the presence of an object in the ith row and jth column cell.", "The second term calculates the sum-squared loss of the predicted height and width. The model predicts the square root of the width and height rather than directly predicting their values. This is done to alleviate a certain problem that the authors mention.", "The sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviation in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly", "Let\u2019s assume that there are two ground-truth boxes with height 10.2 and 2.2 respectively. For this example let\u2019s exclude the widths of the boxes. And let\u2019s say that the model predicts the heights as 10.1 and 2.1. Now, according to the loss term, the error is (\u221a10.1-\u221a10.2)\u00b2 for the larger box and (\u221a2.1-\u221a2.2)\u00b2. You can see that the values of the losses are 2.4 x 10^-4 and 1.16 x 10^-3 respectively. As you can see, predicting the square root of the height and width weighs small errors in smaller boxes more.", "The first term in this calculates the sum-squared error between the predicted confidence score and the ground truth for each bounding box in each cell. This error term corresponds to the error of the cells in which there\u2019s an object. Hence the term \u201c1 obj i,j\u201d is used.", "Similarly, the second term calculates the sum-squared error of the cells which do not contain any objects. The term \u201c1 noobj i,j\u201d denote the absence of objects in ith column and jth row cell. This sum is weighed by the \u03bbnoobj term to make this loss lesser. The \u03bbnoobj was assigned a value of 0.5 by the authors.", "The final term computes the same loss for the class probabilities. It iterates over all the classes in each cell and calculates the sum-squared loss. This loss term also has \u201c1 obj i,j\u201d. Which means that the cells that don\u2019t have any objects will not contribute to the classification loss.", "I hope that this article gave a deep insight into the working of YOLO. If you find any errors, kindly report them in the comments. Errata, if any, is genuinely unintentional.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Always Believing there's more to learn!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F55f72886ab73&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----55f72886ab73--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----55f72886ab73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860----55f72886ab73---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/yolo-sparklers-new-year-1758212/", "anchor_text": "Kanielse in pixabay"}, {"url": "https://unsplash.com/@wildfernstudio?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Charis Gegelman"}, {"url": "https://unsplash.com/images/animals/cat?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1506.02640.pdf", "anchor_text": "https://arxiv.org/pdf/1506.02640.pdf"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "https://arxiv.org/abs/1312.4400"}, {"url": "https://hackerstreak.com/batch-normalization-how-it-really-works/", "anchor_text": "Batch Normalization"}, {"url": "https://hackerstreak.com/kaggle-cactus-detection/", "anchor_text": "How to build a simple Convolution Neural Network, train it on a kaggle dataset and achieve 99.9% accuracy!"}, {"url": "https://arxiv.org/pdf/1506.02640.pdf", "anchor_text": "https://arxiv.org/pdf/1506.02640.pdf"}, {"url": "https://arxiv.org/abs/1506.02640", "anchor_text": "https://arxiv.org/abs/1506.02640"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "https://arxiv.org/abs/1312.4400"}, {"url": "https://hackerstreak.com/", "anchor_text": "https://hackerstreak.com"}, {"url": "https://medium.com/tag/yolo?source=post_page-----55f72886ab73---------------yolo-----------------", "anchor_text": "Yolo"}, {"url": "https://medium.com/tag/object-detection?source=post_page-----55f72886ab73---------------object_detection-----------------", "anchor_text": "Object Detection"}, {"url": "https://medium.com/tag/ai?source=post_page-----55f72886ab73---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----55f72886ab73---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----55f72886ab73---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&user=Anirudh+S&userId=1a92cae35860&source=-----55f72886ab73---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&user=Anirudh+S&userId=1a92cae35860&source=-----55f72886ab73---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F55f72886ab73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----55f72886ab73---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----55f72886ab73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----55f72886ab73--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----55f72886ab73--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----55f72886ab73--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----55f72886ab73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/@baakchsu.sprx77/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "127 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff34e096e7feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-made-simple-interpreting-the-you-only-look-once-paper-55f72886ab73&newsletterV3=1a92cae35860&newsletterV3Id=f34e096e7feb&user=Anirudh+S&userId=1a92cae35860&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}