{"url": "https://towardsdatascience.com/machine-learning-and-veganism-2a1e03e0b262", "time": 1683017428.271573, "path": "towardsdatascience.com/machine-learning-and-veganism-2a1e03e0b262/", "webpage": {"metadata": {"title": "Machine Learning and Veganism. Analyzing the Vegan conversation on\u2026 | by Joe Cowell | Towards Data Science", "h1": "Machine Learning and Veganism", "description": "This blog post corresponds to my third individual project at Metis. The project itself is an exploration of text data by the use of Natural Language Processing (NLP) and unsupervised learning. My\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/JustAnotherArchivist/snscrape", "anchor_text": "snscrape", "paragraph_index": 2}, {"url": "https://github.com/josephpcowell/cowell_proj_4/blob/main/Py_Files/helper_functions.py", "anchor_text": "code here", "paragraph_index": 6}, {"url": "https://github.com/josephpcowell/cowell_proj_4/blob/main/Py_Files/pipeline.py", "anchor_text": "Here", "paragraph_index": 8}, {"url": "https://pyldavis.readthedocs.io/en/latest/readme.html", "anchor_text": "pyLDAvis", "paragraph_index": 15}, {"url": "https://github.com/josephpcowell/cowell_proj_4", "anchor_text": "Github repository", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/categorizing-fraudulent-credit-card-transactions-8c6490645fd7", "anchor_text": "the blog on my third project", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/josephpcowell/", "anchor_text": "LinkedIn", "paragraph_index": 28}, {"url": "https://twitter.com/josephpcowell", "anchor_text": "Twitter", "paragraph_index": 28}], "all_paragraphs": ["This blog post corresponds to my third individual project at Metis. The project itself is an exploration of text data by the use of Natural Language Processing (NLP) and unsupervised learning. My specific project used Tweets with keyword \u201cvegan\u201d from 2016\u20132020 to understand the vegan conversation over the past five years. In this blog post, I will discuss the following topics:", "I\u2019ve been on a plant-based diet for over a year, and while honing in on an NLP project, I thought it would be fun to look into the vegan conversation on Twitter. People primarily go vegan for three reasons: their health, the environment, and the animals. So, I was hoping to see these categories pop out after performing topic modeling. Let\u2019s get into it\u2026", "Collecting Tweets: Twitter API vs. snscrape \ud83d\udc26", "Initially, I was collecting Tweets with the Twitter API. It was pretty fun to explore the streaming functionality within the API (collecting Tweets based on a specific keyword as they are tweeted). However, I needed archived Tweets for my project, and this is where the Twitter API fell flat. With the most basic, free access, I was only able to collect information within the past week, and there was a limit on the number of Tweets I could collect.", "For my analysis, I wanted a lot more information, so snscrape came to the rescue. There\u2019s no need for an access token or API key, and it\u2019s easy to access public Tweets from any time period.", "After installing snscrape, it was just a matter of manipulating one line of code to collect what I needed. This is an example of what I used:", "I modified this query to collect Tweets from different years, but followed up with a function I defined ,txt_to_df(), to establish a pipeline that converts these files into a DataFrame with only the specific information I cared about. You can dive into the code here.", "The NLP process involved a lot of tweaking, but there were some basic concepts that really helped me dive into the project. I\u2019ll uncover some tips as I talk about my process and what I learned.", "First, there are a few different libraries that make NLP possible: NLTK, SpaCy, Gensim, and a few more. For this project, I utilized NLTK, so if this specific library is of interest to you, keep reading. Also, my first tip is to create an NLP pipeline class to save time when refining the model. Here is the one I used in my project with the help of the Metis instructors.", "To start, you need a corpus (collection of documents) of documents (phrase to describe your texts; my documents were Tweets). The NLTK library has a lot of capabilities, with tokenization being one of its more important functionalities for my purposes. Tokenization splits the document into tokens (these could be words, sentences, bigrams, etc.), which is then fed into a vectorizer, creating a vector for each document containing information about the tokens. The most simple vectorizer is the Count Vectorizer. This vectorizer turns a document into a vector of the counts of each token within that document. The result is a sparse matrix, a matrix containing mostly zeros. If you developed an NLP project, you probably know all these terms, but when I started, these terms were foreign and having a basic description really helped.", "Let\u2019s move on to the pipeline. A pipeline is nice because there are various tokenizers and stemmers to process documents, and customizing a cleaning function based on the documents is important as well. Quick side note: a stemmer helps break down similar words into a single token (i.e. running, ran, and run would be reduced to their stem: run).", "It is important to note that documents and desired output can vary from project to project, so no particular tokenizer, stemmer, or cleaning function is perfect for everything. However, I found the Tweet Tokenizer about five days into my project and it helped out a lot.", "So, what ended up working for me? My goal was to create topics amongst the Tweets, so my vectorizer was a TF-IDF vectorizer (term frequency-inverse document frequency). TF-IDF ranks tokens that occur in many documents lower, thus removing importance from universal terms. Obviously, the Tweet Tokenizer was amazing, which helped understand the structure of tweets in order to tokenize better. Lastly, it was important to customize stop words, which are words like \u2018the\u2019 and \u2018or\u2019 that have no meaning, but I added \u2018vegan\u2019, since that term is irrelevant when all of the Tweets are about veganism. Some additional exploration led me to add more words to the stop words, which helped the topic modeling.", "Here is a snip of my code that utilizes the NLPPipe class that I created:", "Another tip: within the vectorizer, you can use max_df and min_df to have the vectorizer ignore tokens that appear above and below a specific threshold within the documents.", "Topic modeling was a lot of fun within this project. I compared LDA and NMF, but ending up sticking to NMF for my final topics. LDA (Latent Dirichlet allocation), on a very basic level, assigns topics to words and then returns the probability of a document being in a specific topic based on the words within that document. There is a very nice package called pyLDAvis that makes visualizing topics using LDA extremely simple. However, the LDA process is known to work better on larger documents, so I opted for NMF.", "NMF (non-negative matrix factorization) utilizes linear algebra and the concept of matrix multiplication to reduce the features of the document-term matrix to create topics. The image below outlines how SVD (singular value decomposition) works, which is the basis of NMF.", "When applying this to the project, I was able to specify how many topics I wanted to create, and NMF would provide a document-topic matrix with probabilities of each document being labelled as a specific topic. This is where the human element is very important, one must look at the words within the topics to decided what the topic is about.", "After looking at my topics, I realized I had to fine-tune my pipeline. I would add stop words that would appear in multiple topics and had no meaning, I would change the number of topics through NMF to see how topics would change as I added more, and I even changed my cleaning function to remove mentions completely so I wouldn\u2019t have Twitter usernames in my topics. Also, I used my domain knowledge to understand that the words \u2018impossible\u2019 and \u2018beyond\u2019 refer to vegan food companies and not the english definition of those terms.", "In the end, 23 topics came out that made sense, which I further bucketed into broader categories.", "With all of my documents categorized into topics, it was time to draw some conclusions. For topic modeling, it is very common to use t-SNE for visualizing the results. t-SNE is an algorithm that\u2019s makes visualizing multidimensional data in two dimensions possible and easy. It\u2019s not necessarily dimensional reduction, because the algorithm tries to emphasize separations in the data, but it\u2019s a great way to create basic visuals for higher dimensional data.", "The t-SNE algorithm was the most time-consuming algorithm of the project (from a computation aspect), but it provided an x and y coordinate for each document to allow for easy plots. Below is the progression of my corpus over time visualized through the use of t-SNE and plotted with seaborn. Take a look at the bottom right corner and you will see the \u2018Fake\u2019 Foods and Animal Activism topics consuming the conversation in 2020.", "Using my data, I also calculated the percent of the documents in each topic, which emphasized the point in the previous gif through a line chart.", "I thought it was interesting how vegans started talking more about animal activism along with \u2018fake\u2019 foods, although the \u2018fake\u2019 food trend itself wasn\u2019t too surprising on its own.", "Out of all of my projects so far, this was definitely the most exploratory. There was no R\u00b2 value to beat or any recall metrics to increase, it was merely a project about learning from text. It\u2019s interesting to see that no \u2018environmental\u2019 topic exposed itself during the analysis, especially since the world is so concerned about the environment, but I guess the Twitter vegans are more concern about animals and the food they\u2019re eating.", "Anyway, if you are learning about NLP or starting a project of your own, I hope this blog helped.", "Check out the Github repository for all of the code behind the project. I try to make my notebooks easy to follow, but I\u2019ll admit that these notebooks aren\u2019t as clean as my other projects.", "As I\u2019m writing this, I only have one week left at the Metis data science bootcamp. It\u2019s been a whirlwind of information, and I\u2019m hoping to expand upon that foundation and keep you all informed along the way. Feel free to check out the blog on my third project which is about a Kaggle competition and credit card fraud. Also, come back for more updates as I continue to traverse the world of data science.", "Feel free to reach out:LinkedIn | Twitter", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist by way of Music and Materials Science & Engineering."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2a1e03e0b262&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://josephpcowell.medium.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": ""}, {"url": "https://josephpcowell.medium.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Joe Cowell"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8d9fac39820&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&user=Joe+Cowell&userId=f8d9fac39820&source=post_page-f8d9fac39820----2a1e03e0b262---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@matthiaszomer", "anchor_text": "Matthias Zomer"}, {"url": "https://www.pexels.com/photo/nature-farmer-dutch-hd-wallpaper-66627/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels.com"}, {"url": "https://developer.twitter.com/", "anchor_text": "Twitter API"}, {"url": "https://github.com/JustAnotherArchivist/snscrape", "anchor_text": "snscrape"}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK"}, {"url": "https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/", "anchor_text": "t-SNE"}, {"url": "https://github.com/JustAnotherArchivist/snscrape", "anchor_text": "snscrape"}, {"url": "https://github.com/josephpcowell/cowell_proj_4/blob/main/Py_Files/helper_functions.py", "anchor_text": "code here"}, {"url": "https://github.com/josephpcowell/cowell_proj_4/blob/main/Py_Files/pipeline.py", "anchor_text": "Here"}, {"url": "https://pyldavis.readthedocs.io/en/latest/readme.html", "anchor_text": "pyLDAvis"}, {"url": "https://github.com/josephpcowell/cowell_proj_4", "anchor_text": "Github repository"}, {"url": "https://towardsdatascience.com/categorizing-fraudulent-credit-card-transactions-8c6490645fd7", "anchor_text": "the blog on my third project"}, {"url": "https://www.linkedin.com/in/josephpcowell/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/josephpcowell", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2a1e03e0b262---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/topic-modeling?source=post_page-----2a1e03e0b262---------------topic_modeling-----------------", "anchor_text": "Topic Modeling"}, {"url": "https://medium.com/tag/nltk?source=post_page-----2a1e03e0b262---------------nltk-----------------", "anchor_text": "Nltk"}, {"url": "https://medium.com/tag/nmf?source=post_page-----2a1e03e0b262---------------nmf-----------------", "anchor_text": "Nmf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2a1e03e0b262---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&user=Joe+Cowell&userId=f8d9fac39820&source=-----2a1e03e0b262---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&user=Joe+Cowell&userId=f8d9fac39820&source=-----2a1e03e0b262---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2a1e03e0b262&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2a1e03e0b262---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2a1e03e0b262--------------------------------", "anchor_text": ""}, {"url": "https://josephpcowell.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://josephpcowell.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Joe Cowell"}, {"url": "https://josephpcowell.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8d9fac39820&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&user=Joe+Cowell&userId=f8d9fac39820&source=post_page-f8d9fac39820--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff8d9fac39820%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-veganism-2a1e03e0b262&user=Joe+Cowell&userId=f8d9fac39820&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}