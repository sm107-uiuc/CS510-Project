{"url": "https://towardsdatascience.com/hyperparameter-tuning-explained-d0ebb2ba1d35", "time": 1683002018.5873191, "path": "towardsdatascience.com/hyperparameter-tuning-explained-d0ebb2ba1d35/", "webpage": {"metadata": {"title": "Hyperparameter Tuning Explained \u2014 Tuning Phases, Tuning Methods, Bayesian Optimization, and Sample Code! | by Moto DEI | Towards Data Science", "h1": "Hyperparameter Tuning Explained \u2014 Tuning Phases, Tuning Methods, Bayesian Optimization, and Sample Code!", "description": "This is another post to pick up tips introduced in a new book \u201cData Analysis Techniques to Win Kaggle\u201d, authored by three high-rank Kagglers (not including myself thus this is not a personal\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.amazon.co.jp/dp/4297108437", "anchor_text": "Data Analysis Techniques to Win Kaggle", "paragraph_index": 4}, {"url": "https://medium.com/@daydreamersjp/a-new-book-data-analysis-techniques-to-win-kaggle-is-a-current-best-and-complete-for-table-data-4af66a88388", "anchor_text": "other post", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/what-is-the-best-starter-model-in-table-data-ml-lessons-from-a-high-rank-kagglers-new-book-f08b821db797", "anchor_text": "the starter models", "paragraph_index": 10}, {"url": "https://sites.google.com/view/lauraepp/parameters", "anchor_text": "This web page of Laurae++", "paragraph_index": 11}, {"url": "https://www.linkedin.com/in/yu-hsuan-t-471633138/", "anchor_text": "Yu-Hsuan Ting", "paragraph_index": 15}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV", "paragraph_index": 23}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "of", "paragraph_index": 23}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "sklearn.model_selection", "paragraph_index": 23}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "of", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "sklearn.model_selection", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1807.02811", "anchor_text": "Bayesian optimization", "paragraph_index": 29}, {"url": "https://github.com/hyperopt/hyperopt", "anchor_text": "hyperopt", "paragraph_index": 33}, {"url": "https://github.com/optuna/optuna", "anchor_text": "optuna", "paragraph_index": 33}, {"url": "https://github.com/SheffieldML/GPyOpt", "anchor_text": "gpyopt", "paragraph_index": 33}, {"url": "https://github.com/JasperSnoek/spearmint", "anchor_text": "spearmint", "paragraph_index": 33}, {"url": "https://scikit-optimize.github.io/", "anchor_text": "scikit-optimize", "paragraph_index": 33}, {"url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "anchor_text": "nested cross-validation", "paragraph_index": 38}, {"url": "https://fr.linkedin.com/in/motoharu-dei-358abaa", "anchor_text": "https://fr.linkedin.com/in/motoharu-dei-358abaa", "paragraph_index": 45}], "all_paragraphs": ["Hyperparameters are important parts of the ML model and can make the model gold or trash.", "In this post, I will discuss:", "2. Four Basic Methodologies of Hyperparameter Tuning", "3. K-folding in Hyperparameter Tuning and Cross-validation", "This is another post to pick up tips introduced in a new book \u201cData Analysis Techniques to Win Kaggle\u201d, authored by three high-rank Kagglers (not including myself thus this is not a personal promotion! :) )", "For the full table of contents of the book itself, see my other post.", "How we tune hyperparameters is a question not only about which tuning methodology we use but also about how we evolve hyperparameter learning phases until we find the final and best.", "It should depend on the task and how much score change we actually see by hyperparameter change but here\u2019re common steps we should keep in mind:", "Then, you\u2019ll have another question: \u201cwhat are the hyperparameters baselines and which parameters are worth tuning?\u201d", "Parameters are different per model, therefore I cannot discuss parameters for every single model here. It is always the data scientists\u2019 job to watch out for the selection of parameters. It should start with a decent understanding of model algorithm and model documentations.", "In this post, I will just leave a few resources about the GBDT models, xgboost, lightbgm, and catboost, the models I used to discuss as the starter models.", "This web page of Laurae++ is an awesome start-off place of all the time in case of xgboost/lightgbm.", "Analytic Vidhya also provides rich content about hyperparameters of GBDT models:", "There are many other random web pages once you start googling. Here\u2019s the one from Towards Data Science I found informative with a comparison list of three main GBDT models.", "The exhibit below summarizes my opinion about what parameters are important with graded importance, their favorable baseline choices, and tuning ranges.", "(For table development my friend and colleague Yu-Hsuan Ting gave me great advice. Thank you!)", "People using Python package to model GBDT usually go with either of original function version (\u2018original API\u2019) or sklearn wrapper version (\u2018sklearn API\u2019) which enables the function use equivalent to other sklearn ML model APIs.", "Mostly you can choose either according to your taste, but keep in mind that other than catboost package, original API and sklearn API may have different parameter names, even when they represent the same parameters. I included both parameter names in the summary above.", "With manual tuning, based on the current choice of parameters and their score, we change a part of them, train the model again, and check the difference in the score, without the use of automation in the selection of parameters to change and value of new parameters.", "Some examples of manual tuning given in the book were:", "Once you knew other approaches discussed below, you might say why we do manual tuning if it is far from the optimal approach to reach global best parameters, but in practice, this is well used in the early phase to know the sensitivity against the hyperparameter change, or in the last phase to tune up.", "Also, surprisingly, a lot of top Kagglers prefer using manual tuning to doing grid search or random search.", "Grid search is an approach where we start from preparing the sets of candidates hyperparameters, train the model for every single set of them, and select the best performing set of hyperparameters.", "Setting parameters and evaluation is usually done automatically through supporting libraries such as GridSearchCV of sklearn.model_selection.", "The disadvantage is that it is:", "Randomized search is, on the other hand, an approach where we prepare the sets of candidates hyperparameters just as grid search, but next the hyperparameter set is randomly selected from prepared hyperparameters search space. Repeat the random selection, model training, and evaluation by the designated number of times we want to search the hyperparameters. Finally, select the best performing hyperparameter set.", "We can control the randomness by assigning density function of parameters instead of specific value, e.g. uniform distribution or normal distribution.", "Here again, setting parameters and evaluation is usually done automatically through supporting libraries such as RandomizedSearchCV of sklearn.model_selection.", "Advantage of the use of random search is:", "The basic concept of Bayesian optimization is \u201cif we searched some points randomly and knew some of them were more promising than else, why don\u2019t we take a second look around them?\u201d", "In Bayesian optimization, it starts from random and narrowing the search space based on Bayesian approach.", "If you know Bayesian theorem, you can understand it just updates the prior distribution of the belief about possible hyperparameter to the posterior distribution by the starting random searches.", "Advantage of Bayesian optimization approach is:", "There are common two python libraries to do Bayesian optimization, hyperopt and optuna. There are also other names such as gpyopt, spearmint, scikit-optimize .", "Here is a sample code using hyperopt.", "In any approaches for hyperparameter tuning discussed above, in order to avoid overfitting, it is important to Kfold the data first, repeat the training and validation over the training folds data and out-of-fold data.", "Also, if keep using the same folds split in cross-validation (in order to compare the models) as well, your model with selected hyperparameters may have overfitted to the folds but have no chance to recognize it.", "Therefore, it is important to change the folds splits from hyperparameter tuning to cross-validation, by changing the random number seed.", "Another approach may be to do nested cross-validation. In nested cross-validation, there are two hierarchical cross-validation loops: outer and inner.", "A huge drawback of nested cross-validation is it significantly increases the run time, by the factor of the number of inner loop folds. I personally feel doing nested cross-validation is too much, the benefit is rather marginal for its additional time to run, and is not a mainstream even in Kaggle competition.", "The approaches we take in hyperparameter tuning would evolve over the phases in modeling, first starting with a smaller number of parameters with manual or grid search, and as the model gets better with effective features taking a look at more parameters with randomized search or Bayesian optimization, but there\u2019s no fixed rule how we do.", "Models will have a lot of hyperparameters, therefore finding important parameters and their search range is not an easy job. The popular models like GBDT family, though, are well elaborated and we know enough of them where to start and go.", "We have to worry the data folding not to overfit the model, then it is a must to change the fold splits from hyperparameter tuning to model selection cross-validation.", "Let me know if you find something is missed; it will help me improve this summary and provide better information to readers!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist and actuary with 15+ yrs experience in media, marketing, insurance, and healthcare. LinkedIn: https://fr.linkedin.com/in/motoharu-dei-358abaa"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd0ebb2ba1d35&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@daydreamersjp?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daydreamersjp?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Moto DEI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fecff78021d2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&user=Moto+DEI&userId=ecff78021d2d&source=post_page-ecff78021d2d----d0ebb2ba1d35---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.amazon.co.jp/dp/4297108437", "anchor_text": "DATA ANALYSIS TECHNIQUES TO WIN KAGGLE"}, {"url": "https://www.amazon.co.jp/dp/4297108437", "anchor_text": "Data Analysis Techniques to Win Kaggle"}, {"url": "https://medium.com/@daydreamersjp/a-new-book-data-analysis-techniques-to-win-kaggle-is-a-current-best-and-complete-for-table-data-4af66a88388", "anchor_text": "other post"}, {"url": "https://towardsdatascience.com/what-is-the-best-starter-model-in-table-data-ml-lessons-from-a-high-rank-kagglers-new-book-f08b821db797", "anchor_text": "the starter models"}, {"url": "https://sites.google.com/view/lauraepp/parameters", "anchor_text": "This web page of Laurae++"}, {"url": "https://sites.google.com/view/lauraepp/parameters", "anchor_text": "https://sites.google.com/view/lauraepp/parameters"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/", "anchor_text": "\u201cComplete Guide to Parameter Tuning in XGBoost with codes in Python\u201d"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/", "anchor_text": "Which algorithm takes the crown: Light GBM vs XGBOOST?"}, {"url": "https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e", "anchor_text": "\u201cAn Example of Hyperparameter Optimization on XGBoost, LightGBM and CatBoost using Hyperopt\u201d"}, {"url": "https://www.linkedin.com/in/yu-hsuan-t-471633138/", "anchor_text": "Yu-Hsuan Ting"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "of"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "sklearn.model_selection"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "of"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "sklearn.model_selection"}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"}, {"url": "https://arxiv.org/abs/1807.02811", "anchor_text": "Bayesian optimization"}, {"url": "https://github.com/fmfn/BayesianOptimization", "anchor_text": "https://github.com/fmfn/BayesianOptimization"}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "but not necessarily"}, {"url": "https://github.com/hyperopt/hyperopt", "anchor_text": "hyperopt"}, {"url": "https://github.com/optuna/optuna", "anchor_text": "optuna"}, {"url": "https://github.com/SheffieldML/GPyOpt", "anchor_text": "gpyopt"}, {"url": "https://github.com/JasperSnoek/spearmint", "anchor_text": "spearmint"}, {"url": "https://scikit-optimize.github.io/", "anchor_text": "scikit-optimize"}, {"url": "https://irsae.no/blog-report-international-summer-school-on-bayesian-modelling/", "anchor_text": "https://irsae.no/blog-report-international-summer-school-on-bayesian-modelling/"}, {"url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "anchor_text": "nested cross-validation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d0ebb2ba1d35---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----d0ebb2ba1d35---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/python?source=post_page-----d0ebb2ba1d35---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----d0ebb2ba1d35---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d0ebb2ba1d35---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&user=Moto+DEI&userId=ecff78021d2d&source=-----d0ebb2ba1d35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&user=Moto+DEI&userId=ecff78021d2d&source=-----d0ebb2ba1d35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd0ebb2ba1d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d0ebb2ba1d35---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d0ebb2ba1d35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daydreamersjp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daydreamersjp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Moto DEI"}, {"url": "https://medium.com/@daydreamersjp/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "453 Followers"}, {"url": "https://fr.linkedin.com/in/motoharu-dei-358abaa", "anchor_text": "https://fr.linkedin.com/in/motoharu-dei-358abaa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fecff78021d2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&user=Moto+DEI&userId=ecff78021d2d&source=post_page-ecff78021d2d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7abec96ee7b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-explained-d0ebb2ba1d35&newsletterV3=ecff78021d2d&newsletterV3Id=7abec96ee7b0&user=Moto+DEI&userId=ecff78021d2d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}