{"url": "https://towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e", "time": 1683005920.9525409, "path": "towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e/", "webpage": {"metadata": {"title": "Intuitive Explanation of BERT- Bidirectional Transformers for NLP | by Renu Khandelwal | Towards Data Science", "h1": "Intuitive Explanation of BERT- Bidirectional Transformers for NLP", "description": "In this post, we will use an intuitive approach to understand the advancement in NLP, including BERT. The pre-training strategies that make BERT so powerful and popular and BERT fine-tuning for most\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "anchor_text": "Word2Vec and GloVe", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f", "anchor_text": "Encoder and Decoder", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a", "anchor_text": "The attention mechanism", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f", "anchor_text": "Transformers", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1905.05583.pdf", "anchor_text": "How to Fine-Tune BERT for Text Classification?", "paragraph_index": 38}], "all_paragraphs": ["In this post, we will use an intuitive approach to understand the advancement in NLP, including BERT. The pre-training strategies that make BERT so powerful and popular and BERT fine-tuning for most of the NLP tasks.", "Consider if you want to learn a new language, say Hindi and you know English very well.", "The first thing is to understand the meaning of every word of the new language in the context of the known language. You will also understand synonyms and antonym of the language for a better vocabulary. This will help you to understand the semantic or meaning-related relationships. This is the basic concept used in Word2Vec and GloVe", "The next step would be to translate simple and short sentences from English to Hindi. You would listen to each word in the English sentence, and then based on the training, you would translate each word by word from English to Hindi. This is the same concept used in Encoder and Decoder.", "You can now translate short sentences, but to translate longer sentences, you need to pay attention to certain words in the sentence to understand the context better. This is done by adding an Attention mechanism to the Encoder-Decoder model. The attention mechanism allows you to pay attention to specific input words in the sentence to do a better job translating but still reading word by word in a sentence.", "You are now good at translating and would like to increase the speed and accuracy of your translation. You need some sort of parallel processing as well as have an awareness of the context to understand long term dependencies. Transformers addressed this requirement.", "Let\u2019s look at two sentences below", "Letter of recommendation was sent to your address", "Leaders across the globe need to address their people on COVID-19.", "The same word \u201caddress\u201d has a different meaning in a different context. You need to look at the sentence as a whole to understand the syntax and semantics. ELMo- Embedding from Language Models looks at the entire sentence to understand the syntax, semantics, and context to increase the accuracy of the NLP tasks.", "You now started to learn the language by reading lots of text(Transfer learning). The learning gained is now transferred and fine-tuned to be applied to a variety of language tasks like classifying text, translating text, etc., this model was Universal Language Model Fine Tuned(ULM-Fit)", "You use Transformers for speed, accuracy and to understand long term dependency and also learn from a vast corpus of words to have a strong understanding of the language, this model is Generative Pre-Trained Transformers(GPT). It only uses the decoder part of the Transformer. You also apply this knowledge gained(Transfer learning) and start to interpret the word from left to right(Uni-directional).", "As you learn different aspects of the language, you realize that getting exposed to a variety of text is very helpful to apply Transfer learning. You start reading books to build a strong vocabulary and understanding of the language. When certain words in a sentence are masked or hidden, then based on your knowledge of the language and reading the full sentence from left to right and from right to left(Bidirectional). You can now predict the masked words with better accuracy(Masked Language Modeling). It is like filling in the blanks. You can also predict when the two sentences are related or not(Next Sentence Prediction). This is a simple working of BERT: Bidirectional Encoder Representations from Transformers.", "This was an intuitive explanation of different advancements in NLP.", "BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "BERT architecture is a multi-layer bidirectional Transformer encoder. We have two versions of BERT: BERT base and BERT large.", "BERT base has 12 Encoders with 12 bidirectional self-attention heads and 110 million parameters", "BERT large has 24 Encoders with 24 bidirectional self-attention heads and 340 million parameters", "BERT is a two-step framework: pre-training and fine-tuning.", "A \u201csequence\u201d refers to the input token sequence to BERT, which can be a single sentence or two sentences packed together", "The first token of every sequence is always a unique classification token [CLS]. Sentence pairs are packed together into a single sequence and are separated using a special token [SEP]. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.", "Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks.", "BERT uses two unsupervised strategies: Masked Language Model(MLM) and Next Sentence prediction(NSP) as part of pre-training.", "During pre-training, the BERT model is trained on unlabeled data over different pre-training tasks. BERT is pre-trained from unlabeled data extracted from BooksCorpus (800M words) and English Wikipedia (2,500M words)", "Bidirectional conditioning in BERT allows each word to indirectly \u201csee itself.\u201d To train a deep bidirectional representation, we use MLM to mask 15% of the input tokens at random and then predict those masked tokens.", "MLM is like fill in the blanks where we randomly mask 15% of the input tokens to predict the original vocabulary id. In BERT, we predict the masked tokens rather than reconstructing the entire input. We use [MASK] tokens only for pre-training, and they are not used for fine-tuning as they will create a mismatch. To mitigate this issue, we do not always replace the masked words with actual [MASK] token.", "Of the 15% randomly chosen masked tokens,", "MLM is also referred to as a Cloze task", "NSP is used for understanding the relationship between sentences during pre-training. When we have two sentences A and B, 50% of the time B is the actual next sentence that follows A and is labeled as IsNext, and 50% of the time, it is a random sentence from the corpus labeled as NotNext.", "NSP is helpful in NLP tasks like Question Answering (QA) and Natural Language Inference (NLI).", "There are two strategies that we can apply to pre-trained language representations for downstream tasks: feature-based and fine-tuning.", "BERT uses the fine-tuning approach. The fine-tuning method works better as it allows the language model to be tweaked through backpropagation.", "For fine-tuning the BERT model, we first initialize with the pre-trained parameters, and then all of the parameters are fine-tuned using labeled data from the downstream tasks.", "Each downstream task has separate fine-tuned models, but are initialized with the same pre-trained parameters. For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end.", "Fine-tuning is adding a layer of untrained neurons as a feedforward layer on top of the pre-trained BERT.", "Pre-training is expensive and is a one-time procedure, but fine-tuning is inexpensive.", "BERT is designed to pre-train deep bidirectional representations using Encoder from Transformers. BERT pre-training uses an unlabeled text by jointly conditioning on both left and right context in all layers. The pre-trained BERT model can be fine-tuned with an additional output layer to create state-of-the-art models for a wide range of NLP tasks.", "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "How to Fine-Tune BERT for Text Classification?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A Technology Enthusiast who constantly seeks out new challenges by exploring cutting-edge technologies to make the world a better place!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcdc1efc69c1e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://arshren.medium.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----cdc1efc69c1e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@laviperchik?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lavi Perchik"}, {"url": "https://unsplash.com/s/photos/child-reading-book?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "anchor_text": "Word2Vec and GloVe"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "anchor_text": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space"}, {"url": "https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f", "anchor_text": "Encoder and Decoder"}, {"url": "https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a", "anchor_text": "The attention mechanism"}, {"url": "https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://github.com/google-research/bert", "anchor_text": "google-research/bertThis is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in\u2026github.com"}, {"url": "https://arxiv.org/pdf/1905.05583.pdf", "anchor_text": "How to Fine-Tune BERT for Text Classification?"}, {"url": "https://mccormickml.com/2019/07/22/BERT-fine-tuning/", "anchor_text": "https://mccormickml.com/2019/07/22/BERT-fine-tuning/"}, {"url": "https://medium.com/tag/bert?source=post_page-----cdc1efc69c1e---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/nlp?source=post_page-----cdc1efc69c1e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----cdc1efc69c1e---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cdc1efc69c1e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cdc1efc69c1e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&user=Renu+Khandelwal&userId=31b07253bc35&source=-----cdc1efc69c1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&user=Renu+Khandelwal&userId=31b07253bc35&source=-----cdc1efc69c1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcdc1efc69c1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cdc1efc69c1e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cdc1efc69c1e--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://arshren.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.9K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1cb44d62203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e&newsletterV3=31b07253bc35&newsletterV3Id=b1cb44d62203&user=Renu+Khandelwal&userId=31b07253bc35&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}