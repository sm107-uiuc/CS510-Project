{"url": "https://towardsdatascience.com/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20", "time": 1682993997.645117, "path": "towardsdatascience.com/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20/", "webpage": {"metadata": {"title": "Stochastic Policy Gradients | Towards Data Science", "h1": "Self Learning AI-Agents IV: Stochastic Policy Gradient", "description": "Stochastic Policy Gradients are used in Deep Reinforcement Learning to control AI agents in continuous action spaces where there is an infinite amount of possible actions."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Deep Q-Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "anchor_text": "Deep (Double) Q-Learning", "paragraph_index": 0}, {"url": "https://github.com/artem-oppermann/Deep-Reinforcement-Learning", "anchor_text": "GitHub Repository", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Deep Q-Learning", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes", "paragraph_index": 33}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 38}], "all_paragraphs": ["With Deep Q-Learning and Deep (Double) Q-Learning we are able to control an AI in discrete action spaces, where the possible actions may be as simple as going left or right, up or down. Despite these simple possibilities the AI agents are still able to accomplish astonishing tasks such as playing Atari Games with a super-human performance or beat the worlds best human player in the board game Go.", "However many real-world applications of reinforcement learning such as training of robots or self-driving cars require an agent to select optimal actions from continuous spaces. Let\u2019s discuss the term continuous action space on an example.", "When you\u2019re driving your car and you turn the wheel, you can control how much you turn the wheel. This leads to a continuous action space: e.g., for each positive real number x in some range, \u201cturn the wheel x degrees to the right\u201d. Or how much do you press the gas pedal? That\u2019s a continuous input also.", "Remember: Continuous action space means that there (theoretically) an infinite amount of possible actions.", "In fact, most actions that we will ever encounter in real life are from the continuous action space. That is why it is so important to understand how we can train an artificial intelligence that selects an action if there are an infinite amount of possibilities.", "This is where Stochastic Policy Gradients algorithms show their benefits.", "This example of OpenAI\u2019s Gym \u201cMountainCarContinuous\u201d problem was solved with Stochastic Policy Gradients, presented here. The well-documented source code can be found in my GitHub Repository. I have chosen MountainCarContinuous as an example because the training time for this problem is very low and you can reproduce it yourself very quickly. If you want to practice after reading this article, clone the repository and execute src/policy gradients/stochastic/ stochastic_pg.py to start the algorithm.", "In Markov Decision Processes I introduced the AI agent as a neural network that interacts with the Environment (Computer game, chessboard, real life etc. ) by observing it\u2019s States s (Screen pixels, board-configuration etc.) and taking Actions a, based on the current observable state s.", "For each Action a in a State s the AI agent receives a Reward. The amount of the Reward tells the Agent the quality of his Action in this particular State with regards to solving the given objective, e.g. learning how to walk or winning a computer game. The Actions in any given State are determined by the Policy \u03c0.", "In Markov Decision Processes I introduces the Policy as the strategy of the AI that determines his movement from one state s to the next state s\u2019, across the whole sequence of all possible states s_1, ..., s_n in the environment.", "In Deep Q-Learning the agent follows the policy \u03c0 which tell to take actions in a state s which correspond to the highest action-value Q(s,a). The action-value function is the expected return (sum of reward across all states) we obtain by starting in state s, taking action a and then following the policy \u03c0 (Compare Markov Decision Processes).", "In the case of stochastic policies, the basic idea is to represent the policy by a parametric probability distribution:", "The distribution stochastically selects action a in state s according to a parameter vector \u03b8. An example of the policy as a probability distribution would be the Gaussian distribution, where we stochastically pick an action a, as a sample from this distribution (Fig. 1). This yields action a be a continuous variable.", "Remember: In contrast to Deep Q-Learning the policy \u03c0 is now a direct mapping/function from a state s to an action a.", "But how do we determine if the current policy \u03c0 is a good one? To do this we must define a performance function for \u03c0, which we call J(\u03b8).", "Let\u2019s discuss a simple case, where we want to measure the quality/performance of \u03c0 only for one step of the agent (from the state s to the next state s\u2019). In this case, we can define the quality function as follows:", "The second line in the equation is nothing else than the execution of the expectation operator E on the expected action-value r(s,a) for the action a in state s. s is selected from the environment, while a is selected according to the policy \u03c0. R_a_s is the reward for the action a in state s.", "Please note: r(s,a) means the same as Q(s,a) or q(s,a) but only for one step-process.", "It must be considered that in deep reinforcement learning the environment is stochastic, meaning taking an action does not guarantee that the agent will end up in the state he intends to. It\u2019s up to the environment to decide to a certain degree where the agent will actually end up. Because the action-value r(s,a) depends also on the next state s\u2019 (See Eq. 17 in Markov Decision Processes) we must average the reward R_a_s overall transitional probabilities p(s):=p(s\u2192s\u2019) from state s to the next state s\u2019. Furthermore, because R_a_s depends also on the action, we must average the reward over all possible \u03c0(a,s).", "Policy gradient algorithms typically proceed by sampling this stochastic policy and adjusting the policy parameters in the direction of greater cumulative reward.", "Now that we have defined the performance of the policy \u03c0, we can go further and discuss how the optimal policy can be learned. Since \u03c0(\u03b8) depends on some parameters \u03b8 (which are in most cases the weights and biases of a neural network) we must find optimal \u03b8 which maximize the performance.", "The basic idea behind the policy gradient method is to adjust these parameters \u03b8 of the policy in the direction of the performance gradient.", "If we calculate the gradient of J(\u03b8) we obtain the following expression:", "Because we want to find \u03b8 which maximize the performance we must update \u03b8 doing gradient ascent \u2014 in contrast to gradient descent where we want to find parameters which minimize a predefined loss function.", "Now that we know how to improve the policy for a one-step-process, we can continue with the case, where we consider the process of movement of the AI agent across the whole sequence of states.", "Actually, this case is not that difficult, if we keep in mind that the sum of (discounted) rewards that we would receive during this process of movements from one state to another following a policy \u03c0, is the exact definition of the action-value function Q(s,a). This yield the following definition of the policy gradient for the multi-step-process, where the single expected reward r(s,a) is exchanged with the expected accumulative return (sum of rewards) Q(s,a).", "The algorithms that use the update rule according to Eq. 4 are called the Actor-Critic Algorithms. The policy \u03c0(a|s) is called the Actor because the Actor determines the action that must be taken in a state s. Meanwhile, the Critic Q(s,a), serves the purpose of \u201ccriticizing\u201d the action of the Actor by giving it a quality-value.", "As you can see the gradient of J(\u03b8) scales with this quality-value. A high quality-value suggest that the taken action a in state s was in fact a good choice and the update of \u03b8 in the direction of performance can be increased. The opposite applies to the small quality value.", "In reality, we can not know Q(s,a) in advance. Hence we must approximate it by a function Q_w(s,a) which depends on parameters w. In general Q_w(s,a) can be estimated by a neural network.", "That yields a new definition for the performance gradient:", "In summary, a stochastic policy gradient algorithm tries to accomplish two following things:", "The whole Actor-Critic algorithm can be expressed by the following pseudo-code:", "The crucial part of the algorithm happens in the for-loop which represents the lifetime of the AI-agent. Let\u2019s discuss each step more in depth:", "The vanilla implementation of the Actor-Critic-Algorithm is known for having a high variance. One possibility to reduce this variance is to subtract from the action-value Q(s,a) the state-value V(s) (Eq. 7). The state value was defined in Markov Decision Processes as the expected total reward the AI agent will receive if it starts its progress in the state s, without considering the actions.", "This new term is defined as the Advantage and can be inserted into the gradient of the performance. Using the advantage has shown promising results in reducing the variance of the algorithm.", "On the other side with the advantage, we introduce the problem of the requirement to have a third function approximator like a neural network to estimate V(s). However, it can be shown that the expected temporal difference error of V(s) (Eq. 9) is nothing else than the advantage.", "This can be shown by using one definition of Q(s,a) which is the expected value of r+\u03b3V(s\u2019). By subtracting the remaining term V(s) we obtain the previous definition of the advantage A (Eq. 10).", "In the end, we can insert the temporal difference error of V(s) into the gradient of J(s). Doing so we kill two birds with one stone:", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb53f088fce20&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----b53f088fce20---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb53f088fce20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=-----b53f088fce20---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb53f088fce20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=-----b53f088fce20---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Part I: Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Part II: Deep Q-Learning"}, {"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "anchor_text": "Part III: Deep (Double) Q-Learning"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Deep Q-Learning"}, {"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "anchor_text": "Deep (Double) Q-Learning"}, {"url": "https://github.com/artem-oppermann/Deep-Reinforcement-Learning", "anchor_text": "GitHub Repository"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Deep Q-Learning"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Markov Decision Processes"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b53f088fce20---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b53f088fce20---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b53f088fce20---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----b53f088fce20---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b53f088fce20---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb53f088fce20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=-----b53f088fce20---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb53f088fce20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=-----b53f088fce20---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb53f088fce20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----b53f088fce20---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=-----b53f088fce20---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Written by Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----b53f088fce20---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=-----b53f088fce20---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Artem Oppermann"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Regularization in Deep Learning \u2014 L1, L2, and DropoutA Guide on the Theory and Practicality of the most important Regularization Techniques in Deep Learning"}, {"url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "\u00b79 min read\u00b7Feb 19, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&user=Artem+Oppermann&userId=619319ac8220&source=-----377e75acc036----0-----------------clap_footer----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036?source=author_recirc-----b53f088fce20----0---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=-----b53f088fce20----0-----------------bookmark_preview----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b53f088fce20----1---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----b53f088fce20----1-----------------bookmark_preview----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b53f088fce20----2---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b53f088fce20----2-----------------bookmark_preview----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bayes-theorem-the-holy-grail-of-data-science-55d93315defb?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Artem Oppermann"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/bayes-theorem-the-holy-grail-of-data-science-55d93315defb?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "Bayes\u2019 Theorem: The Holy Grail of Data ScienceIntuitive derivation of the Bayes\u2019 Theorem"}, {"url": "https://towardsdatascience.com/bayes-theorem-the-holy-grail-of-data-science-55d93315defb?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": "\u00b78 min read\u00b7Dec 22, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55d93315defb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayes-theorem-the-holy-grail-of-data-science-55d93315defb&user=Artem+Oppermann&userId=619319ac8220&source=-----55d93315defb----3-----------------clap_footer----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bayes-theorem-the-holy-grail-of-data-science-55d93315defb?source=author_recirc-----b53f088fce20----3---------------------2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "36"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55d93315defb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayes-theorem-the-holy-grail-of-data-science-55d93315defb&source=-----b53f088fce20----3-----------------bookmark_preview----2eb1cdfc_61f8_43ed_84f3_a1157f4f780d-------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "See all from Artem Oppermann"}, {"url": "https://towardsdatascience.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----b53f088fce20----0-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----b53f088fce20----1-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b53f088fce20----0---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b53f088fce20----0-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b53f088fce20----1---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----b53f088fce20----1-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----2-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----b53f088fce20----2---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----b53f088fce20----2-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----3-----------------clap_footer----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b53f088fce20----3---------------------30dd0043_89ea_41d7_a50a_766e253deacf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----b53f088fce20----3-----------------bookmark_preview----30dd0043_89ea_41d7_a50a_766e253deacf-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b53f088fce20--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----b53f088fce20--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}