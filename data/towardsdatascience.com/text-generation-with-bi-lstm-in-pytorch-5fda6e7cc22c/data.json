{"url": "https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c", "time": 1683012516.4902961, "path": "towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c/", "webpage": {"metadata": {"title": "Text Generation with Bi-LSTM in PyTorch | by Fernando L\u00f3pez | Towards Data Science", "h1": "Text Generation with Bi-LSTM in PyTorch", "description": "A step-by-step guide to develop a text generation model by using PyTorch\u2019s LSTMCells to create a Bi-LSTM model from scratch"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3", "anchor_text": "From a LSTM Cell to a Multilayer LSTM Network with PyTorch", "paragraph_index": 6}, {"url": "https://www.gutenberg.org/", "anchor_text": "Gutenberg Project", "paragraph_index": 8}, {"url": "https://www.gutenberg.org/cache/epub/46205/pg46205.txt", "anchor_text": "link to the book", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1908.04332.pdf", "anchor_text": "LSTM vs. GRU vs. Bidirectional RNN for script generation", "paragraph_index": 54}, {"url": "https://www.sciencedirect.com/science/article/pii/S1319157820303360", "anchor_text": "The survey: Text generation models in deep learning", "paragraph_index": 55}], "all_paragraphs": ["\u201cThere is no rule on how to write. Sometimes it comes easily and perfectly: sometimes it\u2019s like drilling rock and then blasting it out with charges\u201d \u2014 Ernest Hemingway", "The aim of this blog is to explain the building of an end-to-end model for text generation by implementing a powerful architecture based on LSTMs.", "The blog is divided into the following sections:", "Over the years, various proposals have been launched to model natural language, but how is this? what does the idea of \u201cmodeling natural language\u201d refer to? We could think that \u201cmodeling natural language\u201d refers to the reasoning given to the semantics and syntax that make up the language, in essence, it is, but it goes further.", "Nowadays, the field of Natural Language Processing (NLP) deals with different tasks that refer to reasoning, understanding and modeling of language through different methods and techniques. The field of NLP (Natural Language processing) has been growing extremely fast in this past decade. It has been proposed in plenty of models to solve different NLP tasks from different perspectives. Likewise, the common denominator among the most popular proposals is the implementation of Deep Learning based models.", "As already mentioned, NLP field addresses a huge number of problems, specifically in this blog we will address the problem of text generation by making use of deep learning based models, such as the recurrent neural networks LSTM and Bi-LSTM. Likewise, we will use one of the most sophisticated frameworks today to develop deep learning models, specifically we will use the LSTMCell class from PyTorch to develop the proposed architecture.", "If you want to dig into the mechanics of the LSTM, as well as how it is implemented in PyTorch, take a look at this amazing explanation: From a LSTM Cell to a Multilayer LSTM Network with PyTorch", "Given a text, a neural network will be fed through character sequences in order to learn the semantics and syntactics of the given text. Subsequently, a sequence of characters will be randomly taken and the next character will be predicted.", "First, we are going to need a text which we are going to work with. There are different resources where you can find different texts in plain text, I recommend you take a look at the Gutenberg Project.", "In this case, I will use the book called Jack Among the Indians by George Bird Grinnell, the one you can find here: link to the book. So, the first lines of chapter 1 look like:", "As you can see, the text contains uppercase, lowercase, line breaks, punctuation marks, etc. What is suggested to do is to try to adapt the text to a form which allows us to handle it in a better way and which mainly reduces the complexity of the model that we are going to develop. So we are going to transform each character to its lowercase form. Also, it is advisable to handle the text as a list of characters, that is, instead of having a \u201cbig string of characters\u201d, we will have a list of characters. The purpose of having the text as a sequence of characters is for better handling when generating the sequences which the model will be fed with (we will see this in the next section in detail).", "As we can see, in line 2 we are defining the characters to be used, all other symbols will be discarded, we only keep the \u201cwhite space\u201d symbol. In lines 6 and 10 we are reading the raw file and transforming it into its lowercase form. In the loops of lines 14 and 19 we are creating and string which represents the entire book and generating a list of characters. In line 23 we are filtering the text list by only keeping the letters defined in line 2.", "So, once the text is loaded and preprocessed, we will go from having a text like this:", "to have a list of characters like this:", "Well, we already have the full text as a list of characters. As it\u2019s well known, we cannot introduce raw characters directly to a neural network, we require a numerical representation, therefore, we need to transform each character to a numerical representation. For this, we are going to create a dictionary which will help us to save the equivalence \u201ccharacter-index\u201d and \u201cindex-character\u201d.", "As we can notice, in lines 11 and 12 the \u201cchar-index\u201d and \u201cindex-char\u201d dictionaries are created.", "So far we have already shown how to load the text and save it in the form of a list of characters, we have also created a couple of dictionaries that will help us to encode-decode each character. Now, it is time to see how we will generate the sequences that will be introduced to the model. So, let\u2019s go to the next section!", "The way in which the sequences are generated depends entirely on the type of model that we are going to implement. As already mentioned, we will use recurrent neural networks of the LSTM type, which receive data sequentially (time steps).", "For our model, we need to form sequences of a given length which we will call \u201cwindow\u201d, where the character to predict (the target) will be the character next to the window. Each sequence will be made up of the characters included in the window. To form a sequence, the window is sliced one character to the right at a time. The character to predict will always be the character following the window. We can clearly see this process in Figure 1.", "Well, so far we have seen how to generate the character sequences in a simple way. Now we need to transform each character to its respective numerical format, for this we will use the dictionary generated in the preprocessing phase. This process can be visualized in Figure 2.", "Great, now we know how to generate the character sequences using a window that slides one character at a time and how we transform the characters into a numeric format, the following code snippet shows the process described.", "Fantastic, now we know how to preprocess raw text, how to transform it into a list of characters and how to generate sequences in a numeric format. Now we go to the most interesting part, the model architecture.", "As you already read in the title of this blog, we are going to make use of Bi-LSTM recurrent neural networks and standard LSTMs. Essentially, we make use of this type of neural network due to its great potential when working with sequential data, such as the case of text-type data. Likewise, there are a large number of articles that refer to the use of architectures based on recurrent neural networks (e.g. RNN, LSTM, GRU, Bi-LSTM, etc.) for text modeling, specifically for text generation [1, 2].", "The architecture of the proposed neural network consists of an embedding layer followed by a Bi-LSTM as well as a LSTM layer. Right after, the latter LSTM is connected to a linear layer.", "The methodology consists of passing each sequence of characters to the embedding layer, this to generate a representation in the form of a vector for each element that makes up the sequence, therefore we would be forming a sequence of embedded characters. Subsequently, each element of the sequence of embedded characters will be passed to the Bi-LSTM layer. Subsequently, a concatenation of each output of the LSTMs that make up the Bi-LSTM (the forward LSTM and the backward LSTM) will be generated. Right after, each forward + backward concatenated vector will be passed to the LSTM layer from which the last hidden state will be taken to feed the linear layer. This last linear layer will have as activation function a Softmax function in order to represent the probability of each character. Figure 3 show the described methodology.", "Fantastic, so far we have already explained the architecture of the model for text generation as well as the implemented methodology. Now we need to know how to do all this with the PyTorch framework, but first, I would like to briefly explain how the Bi-LSTM and the LSTM work together to later see how we would do it in code, so let\u2019s see how a Bi-LSTM network works.", "The key difference between a standard LSTM and a Bi-LSTM is that the Bi-LSTM is made up of 2 LSTMs, better known as \u201cforward LSTM\u201d and \u201cbackward LSTM\u201d. Basically, the forward LSTM receives the sequence in the original order, while the backward LSTM receives the sequence in reverse. Subsequently and depending on what is intended to be done, each hidden state for each time step of both LSTMs can be joined or only the last states of both LSTMs will be operated. In the proposed model, we suggest joining both hidden states for each time step.", "Perfect, now we understand the key difference between a Bi-LSTM and an LSTM. Going back to the example we are developing, Figure 4 represents the evolution of each sequence of characters when they are passed through the model.", "Great, once everything about the interaction between Bi-LSTM and LSTM is clear, let\u2019s see how we do this in code using only LSTMCells from the great PyTorch framework.", "So, first let\u2019s understand how we make the constructor of the TextGenerator class, let\u2019s take a look at the following code snippet:", "As we can see, from lines 6 to 10 we define the parameters that we will use to initialize each layer of the neural network. It is important to mention that input_size is equal to the size of the vocabulary (that is, the number of elements that our dictionary generated in the preprocessing contains). Likewise, the number of classes to be predicted is also the same size as the vocabulary and sequence_length refers to the size of the window.", "On the other hand, in lines 20 and 21 we are defining the two LSTMCells that make up the Bi-LSTM (forward and backward). In line 24 we define the LSTMCell that will be fed with the output of the Bi-LSTM. It is important to mention that the hidden state size is double compared to the Bi-LSTM, this is because the output of the Bi-LSTM is concatenated. Later on line 27 we define the linear layer, which will be filtered later by the softmax function.", "Once the constructor is defined, we need to create the tensors that will contain the cell state (cs) and hidden state (hs) for each LSTM. So, we proceed to do it as follows:", "Fantastic, once the tensors that will contain the hidden state and cell state have been defined, it is time to show how the assembly of the entire architecture is done, let\u2019s go for it!", "First, let\u2019s take a look at the following code snippet:", "For a better understanding, we are going to explain the assembly with some defined values, in such a way that we can understand how each tensor is passed from one layer to another. So say we have:", "so the x input tensor will have a shape:", "then, in line 2 is passed the x tensor through the embedding layer, so the output would have a size:", "It is important to notice that in line 5 we are reshaping the x_embedded tensor. This is because we need to have the sequence length as the first dimension, essentially because in the Bi-LSTM we will iterate over each sequence, so the reshaped tensor will have a shape:", "Right after, in lines 7 and 8 the forward and backward lists are defined. There we will store the hidden states of the Bi-LSTM.", "So it\u2019s time to feed the Bi-LSTM. First, in line 12 we are iterating over forward LSTM, we are also saving the hidden states of each time step (hs_forward). In line 19 we are iterating over the backward LSTM, at the same time we are saving the hidden states of each time step (hs_backward). You can notice that the loop is done in the same sequence, the difference is that it\u2019s read in reversed form. Each hidden state will have the following shape:", "Great, now let\u2019s see how to feed the latest LSTM layer. For this, we make use of the forward and backward lists. In line 26 we are iterating through each hidden state corresponding to forward and backward which are concatenated in line 27. It is important to note that by concatenating both hidden states, the dimension of the tensor will increase 2X, that is, the tensor will have the following shape:", "Finally, the LSTM will return a hidden state of size:", "At the very end, the last hidden state of the LSTM will be passed through a linear layer, as shown on line 31. So, the complete forward function is shown in the following code snippet:", "Congratulations! Up to this point we already know how to assemble the neural networks using LSTMCell in PyTorch. Now it\u2019s time to see how we do the training phase, so let\u2019s move on to the next section.", "Great, we\u2019ve come to training. To perform the training we need to initialize the model and the optimizer, later we need to iterate for each epoch and for each mini-batch, so let\u2019s do it!", "Once the model is trained, we will need to save the weights of the neural network to later use them to generate text. For this we have two options, the first is to define a fixed number of epochs and then save the weights, the second is to determine a stop function to obtain the best version of the model. In this particular case, we are going to opt for the first option. After training the model under a certain number of epochs, we save the weights as follows:", "Perfect, up to this point we have already seen how to train the text generator and how to save the weights, now we are going to the top part of this blog, the text generation! So let\u2019s go to the next section.", "Fantastic, we have reached the final part of the blog, the text generation. For this, we need to do two things: the first is to load the trained weights and the second is to take a random sample from the set of sequences as the pattern to start generating the next character. So let\u2019s take a look at the following code snippet:", "So, by training the model under the following characteristics:", "As we can see, the generated text may not make any sense, however there are some words and phrases that seem to form an idea, for example:", "Congratulations, we have reached the end of the blog!", "Throughout this blog we have shown how to make an end-to-end model for text generation using PyTorch\u2019s LSTMCell and implementing an architecture based on recurring neural networks LSTM and Bi-LSTM.", "It is important to comment that the suggested model for text generation can be improved in different ways. Some suggested ideas would be to increase the size of the text corpus to be trained, increase the number of epochs as well as the memory size for each LSTM. On the other hand, we could think of an interesting architecture based on Convolutional-LSTM (maybe a topic for another blog).", "[1] LSTM vs. GRU vs. Bidirectional RNN for script generation", "[2] The survey: Text generation models in deep learning", "Machine Learning Engineer | Data Scientist | Software Engineer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5fda6e7cc22c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://ferneutron.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Fernando L\u00f3pez"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd606f5d846f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=post_page-d606f5d846f2----5fda6e7cc22c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fda6e7cc22c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5fda6e7cc22c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fda6e7cc22c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=-----5fda6e7cc22c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral", "anchor_text": "Patrick Tomasso"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/FernandoLpz/Text-Generation-BiLSTM-PyTorch", "anchor_text": "https://github.com/FernandoLpz/Text-Generation-BiLSTM-PyTorch"}, {"url": "https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3", "anchor_text": "From a LSTM Cell to a Multilayer LSTM Network with PyTorch"}, {"url": "https://www.gutenberg.org/", "anchor_text": "Gutenberg Project"}, {"url": "https://www.gutenberg.org/cache/epub/46205/pg46205.txt", "anchor_text": "link to the book"}, {"url": "https://arxiv.org/pdf/1908.04332.pdf", "anchor_text": "LSTM vs. GRU vs. Bidirectional RNN for script generation"}, {"url": "https://www.sciencedirect.com/science/article/pii/S1319157820303360", "anchor_text": "The survey: Text generation models in deep learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----5fda6e7cc22c---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5fda6e7cc22c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/lstm?source=post_page-----5fda6e7cc22c---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/bi-lstm?source=post_page-----5fda6e7cc22c---------------bi_lstm-----------------", "anchor_text": "Bi Lstm"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----5fda6e7cc22c---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fda6e7cc22c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5fda6e7cc22c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fda6e7cc22c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5fda6e7cc22c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fda6e7cc22c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd606f5d846f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=post_page-d606f5d846f2----5fda6e7cc22c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F14c367392dfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&newsletterV3=d606f5d846f2&newsletterV3Id=14c367392dfa&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5fda6e7cc22c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Written by Fernando L\u00f3pez"}, {"url": "https://ferneutron.medium.com/followers?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "537 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd606f5d846f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=post_page-d606f5d846f2----5fda6e7cc22c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F14c367392dfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c&newsletterV3=d606f5d846f2&newsletterV3Id=14c367392dfa&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5fda6e7cc22c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Fernando L\u00f3pez"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Hypothesis Testing: Z-ScoresA guide to understanding what hypothesis testing is and how to interpret and implement the z-test"}, {"url": "https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "\u00b78 min read\u00b7Aug 29, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F337fb06e26ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhypothesis-testing-z-scores-337fb06e26ab&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----337fb06e26ab----0-----------------clap_footer----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab?source=author_recirc-----5fda6e7cc22c----0---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F337fb06e26ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhypothesis-testing-z-scores-337fb06e26ab&source=-----5fda6e7cc22c----0-----------------bookmark_preview----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5fda6e7cc22c----1---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----5fda6e7cc22c----1-----------------bookmark_preview----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----5fda6e7cc22c----2---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----5fda6e7cc22c----2-----------------bookmark_preview----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Fernando L\u00f3pez"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "SHAP: Shapley Additive ExplanationsA step-by-step guide for understanding how SHAP works and how to interpret ML models by using the SHAP library"}, {"url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": "\u00b712 min read\u00b7Jul 11, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a2a271ed9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshap-shapley-additive-explanations-5a2a271ed9c3&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----5a2a271ed9c3----3-----------------clap_footer----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3?source=author_recirc-----5fda6e7cc22c----3---------------------60c862d2_3b15_400d_a613_b2eddb3d384b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a2a271ed9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshap-shapley-additive-explanations-5a2a271ed9c3&source=-----5fda6e7cc22c----3-----------------bookmark_preview----60c862d2_3b15_400d_a613_b2eddb3d384b-------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "See all from Fernando L\u00f3pez"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Building An LSTM Model From Scratch In PythonHow to build a basic LSTM using Basic Python libraries"}, {"url": "https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b717 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F1dedd89de8fe&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuilding-a-lstm-from-scratch-in-python-1dedd89de8fe&user=Youssef+Hosni&userId=859af34925b7&source=-----1dedd89de8fe----0-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dedd89de8fe&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuilding-a-lstm-from-scratch-in-python-1dedd89de8fe&source=-----5fda6e7cc22c----0-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Coucou Camille"}, {"url": "https://medium.com/codex?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Time Series Prediction Using LSTM in PythonImplementation of Machine Learning Algorithm for Time Series Data Prediction."}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b76 min read\u00b7Feb 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&user=Coucou+Camille&userId=d796c2fbb274&source=-----19b1187f580f----1-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&source=-----5fda6e7cc22c----1-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Prateek Gaurav"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM]Link to Part 1of this article: NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] Link to Part 3 of this article: NLP: Zero\u2026"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b78 min read\u00b7Mar 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&user=Prateek+Gaurav&userId=966fe9bb6729&source=-----77fd60fc0b44----0-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----5fda6e7cc22c----0---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&source=-----5fda6e7cc22c----0-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----1-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----5fda6e7cc22c----1---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----5fda6e7cc22c----1-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5fda6e7cc22c----2---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----5fda6e7cc22c----2-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----3-----------------clap_footer----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----5fda6e7cc22c----3---------------------f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----5fda6e7cc22c----3-----------------bookmark_preview----f017f03e_224d_4c0e_b6c6_f077b6b0b7c3-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----5fda6e7cc22c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}