{"url": "https://towardsdatascience.com/nanohub-rec-b91e1a0beb06", "time": 1683003865.227702, "path": "towardsdatascience.com/nanohub-rec-b91e1a0beb06/", "webpage": {"metadata": {"title": "Building a Hybrid Recommender System for nanoHUB.org | by Xufeng Wang | Towards Data Science", "h1": "Building a Hybrid Recommender System for nanoHUB.org", "description": "This article documents the conceptualization, thought process, and results of my project during my tenure at Insight Data Science as a Fellow (Jan. \u2014 March. 2020). nanoHUB.org (www.nanohub.org) is an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.insightdatascience.com/", "anchor_text": "Insight Data Science", "paragraph_index": 0}, {"url": "http://www.nanohub.org", "anchor_text": "www.nanohub.org", "paragraph_index": 1}, {"url": "https://nanohub.org/about", "anchor_text": "https://nanohub.org/about", "paragraph_index": 2}, {"url": "https://nanohub.org/resources/tools", "anchor_text": "https://nanohub.org/resources/tools", "paragraph_index": 8}, {"url": "https://link.springer.com/chapter/10.1007/978-3-642-13287-2_4", "anchor_text": "\u201clong-tail\u201d problem", "paragraph_index": 24}, {"url": "https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed", "anchor_text": "Machine Learning for Recommender System", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/association-rule-mining-be4122fc1793", "anchor_text": "Association Rule Mining", "paragraph_index": 31}, {"url": "https://stats.stackexchange.com/questions/256012/item-item-collaborative-filtering-vs-market-basket-analysis", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Zipf%27s_law", "anchor_text": "Zipf\u2019s Law", "paragraph_index": 36}, {"url": "http://www.nanoHUB.org", "anchor_text": "www.nanoHUB.org", "paragraph_index": 62}], "all_paragraphs": ["This article documents the conceptualization, thought process, and results of my project during my tenure at Insight Data Science as a Fellow (Jan. \u2014 March. 2020).", "nanoHUB.org (www.nanohub.org) is an online platform for nanotechnology education/research. With over 1.4 million annual visitors, it is one of the largest and most active online community for nanotechnology.", "\u201cOur mission is to accelerate innovation through user-centric science & engineering. Our vision is to make science & engineering products usable, discoverable, reproducible and easy to create for learners, educators, researchers, and business professionals.\u201d \u2014 nanoHUB.org mission statement, https://nanohub.org/about", "When dived into the user behavior for these 1.4 million visitors, I am surprised by the high churn rate. As shown in figure below, ~73% of all visitors only visited 1 item before leaving the site. In another word, the bounce rate is very high, and this is not a good news.", "Moreover, these bounces occur not all at homepage, but scattered around various individual items across the site. Majority of the traffic is driven from search engines \u2014 for example, when a student search for a nanotechnology term on Google, several nanoHUB items may show up. Therefore, this is not a UX problem with the homepage design, but a general problem with visitors losing interest after viewing one item.", "Attracting visitors by placing relevant items within a click\u2019s reach is the main goal of a recommender system, and looks like it is exactly what nanoHUB needs.", "Increasing visitor retention rate is beneficial to nanoHUB\u2019s business model. As an online platform whose contents rely on voluntary contribution from community contributors, the contributor population is also equally important comparing to the visitor population. Therefore, when designing the recommender system, the contributor\u2019s benefits cannot be forgotten.", "In general, contributors desire their contents to be used and recognized. It is particular difficult for new contents to capture attention. If a recommender system is built without care, the popular items will get more recommendation traffic and become even more popular in this so-called \u201crich gets richer\u201d problem in recommender system (see Baptiste Rocca\u2019s excellent article below for details). Clearly, while designing an effective recommender system, the contributor\u2019s interests must also be kept in mind, that is, a fair re-distribution of the recommender traffic to promote new and under-used items.", "Lastly, similar to contributor\u2019s interest, nanoHUB\u2019s business model prefers a promotion of a special kind of items \u2014 online simulation tools (https://nanohub.org/resources/tools). These tools are the star-products of nanoHUB and have long-term business importance and impact. I\u2019d like to recommend them to uses to promote them whenever possible.", "In summary, three business KPIs are pursuited in this project:1. Recommender effectiveness: This KPI can be evaluated based on the accuracy of the ML models.2. Coverage: This KPI is defined as how uniform the items are covered by recommender traffic.3. Promotion to special products: This KPI is defined as the amount of traffic increase expected for the special kind of items \u2014 the simulation tools.", "I\u2019d like to take a moment and address the question \u201cis it really fair to artificially keep the coverage uniform across all items\u201d. In my opinion, no. The quality of the items of course varies, and it is natural for high quality contents to receive more visitors. Artificially keeping all items to receive the same traffic will dilute the content-quality viewed by the users. Doing so will therefore hurt the business in the long term. The strategy however is to initially provide a uniform coverage as a starting point, and low quality contents will receive low ratings. At one point down the road, the rating of the item will become a feature for the recommender system, and the coverage will be adjusted accordingly to the item\u2019s rating. This is beyond the scope of this 3-week Insight project and will be implemented in next phase.", "To build a recommender system, the visitor-item interaction data is needed. This data tells us which visitor viewed which item at what time. Since nanoHUB requires no registration for people to view items with the exception of simulation tools, I cannot rely on registered user\u2019s web data to form a training set. A data set formed as such is incomplete and heavily biased toward simulation tools, since visitors are required to register and login in order to use simulation tools. Moreover, the use of web cookies are recently added for nanoHUB and still in experimental phase, so I cannot rely on cookies to form user activity sessions. An ad-hoc approach is therefore needed.", "To keep it simple, I decided to use the web log table from nanoHUB OLAP database. This web log table is a filtered version of raw Apache web log and consists of 400 million rows spanning across 17 years.", "After inspecting the web logs, I realize there are two challenges during this data cleaning process:1. How to effectively aggregate logs to form user sessions.2. Removal of bot/crawler traffic", "User sessions are defined as a segment of continuous activity by a visitor. For example, IP 123.456.789.11 visited item A, item B, item C at certain times. These form the user-item interaction matrix needed for building the recommender system. Any session having less than 2 items are not useful for forming the user-item interaction matrix and are thus removed. To prevent erroneous joining of two visitors sharing the same IP address, which can happen in, for example, a university lab, session ends when an IP address become inactive after 12 hours.", "To handle the large amount of data, SQLAlchemy is used to syphon data from OLAP database into local HPC environment, and Spark is used to mapreduce logs into sessions. Under a single node with 20 cores and 64 GB RAM, this mapreduce process can be completed within 8 hours.", "One observation during the data exploration process is the discovery of remainent bot/crawler traffic. The filtered web log in SQL is supposed to have all bot/crawler traffic removed. However, this is found not to be the case. From raw Apache web log to SQL, the process relied mostly on self-identification of the bot/crawler. For example, two logs from Bing bot and Knowledge AI are shown in figure below.", "Not all bot/crawler self-identify. In figure above, one instance of a suspected bot behavior is detected, showing periodic (~ 1 sec) crawling behavior starting from the home page. Such records must be identified and removed, since they are non-human and bias the results toward connectivity of web graph. Two strategies are deployed to combat these non-self-identified bots/crawlers: 1. clustering-based technique for bot detection, and 2. use of user-item collaborative filtering technique.", "To get an overview of suspected bot/crawler traffic, I plot, for each of the IP address, number of items visited vs. average time spend. These aggregated metrics are rather crude representation of an IP address\u2019 behavior. Sessions can be used instead, but if an IP address is detected to have bot/crawler behavior, all its sessions are discarded regardless. These metrics are chosen also to avoid large amount of data points. Overall, the number of items visited vs. average time spend plots in scatter and KDE forms are shown above, with only the > 100 items visited portion displayed \u2014 I will call them \u201chigh usage population\u201d from now. From the KDE plot, two distinct high usage population groups can be seen. One, marked with red dash curve, circling the population that exhibit low average time spend per item and high usage. This corresponds to high likelihood that the IP is a crawler. On the other hand, the other, marked with blue dash curve, circling the population that exhibit high average time spend per item and high usage. This is likely a normal, active user.", "Without doing detailed clustering analysis, I apply a straightforward cutoff removing IPs having larger than 200 items and less than 40 seconds average time spend from the dataset.", "Overall, the suspected bot/crawler population is minor comparing to the overall sessions. In addition, the user-item collaborative filtering technique has certain ability to guard against crawlers. The intuition is, bots/crawlers behave in a similar manner, viewing large amount of items that are closely connected with web graph space and, likely, concentrated around homepage. This results in large sessions with similar contents. Normal visitors have much smaller sessions and diverse contents. Under user-item collaborative filtering, the bots/crawlers are thus filtered out when looking for visitors with similar behavior as a normal visitor.", "Since bot detection is not the main topic of this project, I will not complicate this effort further and use the cleaned data set, ~ 2 million sessions in total, to construct a recommender system for nanoHUB.", "Note: Small datasets can have many unique challenges. I recently learned of an interesting project predicting the adoption of accessory dwelling units in California by Minting who run into a small dataset problem and solved it using various ML techniques.", "Basic EDA are conducted to understand the data, gain insights, confirm intuitions, and fish for abnormalities. These include the following:1. Histogram of # of items viewed per session: This is shown in Figure 2 and confirms the high-bounce rate user behavior.2. Tracking of known users to validate session formation: I check the sessions from known users (myself included) and visually examine the activities. The behaviors can be intuitively explained, and the sessions are deemed accurate from this observation.3. Feature exploration, session length: The time duration between items can be interpreted as the time spend by the user on that item. This may correlates to the sentiment of the user toward the item, since attracted users pay more attention to the item, resulting in longer sessions. This however depends on various factors including the item content length, assumption that the user did not disengage for another activity (take a lunch break, visit some other websites, etc.). Therefore, this feature is not included in the study.4. Feature exploration, content similarity: When examining adjacent activities in chronical order (item + item viewed next), high content similarity is discovered. This is discussed in later section and become an important feature to improve the recommender accuracy.", "One key issue common to website traffic is the \u201clong-tail\u201d problem: there are few popular items with high visitor traffic, and lots of under-visited items with low traffic. This also happens to nanoHUB as shown in Figure 6.", "This \u201clong-tail\u201d problem is undesirable for nanoHUB\u2019s contributors and hurt their motivation towards contributing contents, which negatively affects nanoHUB\u2019s business model that relies on voluntary contributor\u2019s contents to thrive. Contributor prefer their own contents to have exposure, wide and fast. \u201cWide\u201d exposure means the number of visitors viewing the item. This is a long term metric also based on the merit of the item \u2014 if the item is of low quality, it is understandably and rightfully to have low traffic. This merit-based metric is the rating of the item and, due to time constraint, not implemented in the recommender system. \u201cFast\u201d exposure means a newly published content need to be \u201cpushed\u201d to the relevant audience in an active manner, instead of passively waiting to be discovered. By modifying the coverage and actively promote under-visited and new items, the recommender system can give contributors a fast exposure of their new contents.", "There are various models for recommender systems, ranging from collaborative filtering techniques to neural network. For a review of these techniques, I recommend the series on Machine Learning for Recommender System by Pavel. For a demonstration of how a neural network based recommender system can offer effective and customized experience for users, I\u2019d like to point you to my friend, Dr. Sid Wang\u2019s recent project: A story recommender system for commaful.", "For this work, I select the user-item collaborative filtering technique for its simplicity and interpretability. In addition, as discussed in the appendix, it offers certain degree of robustness against bot/crawler traffic, which I only removed in a very simple and ad-hoc way.", "The basic principle of user-item collaborative filtering technique is illustrated in Figure 7. In order to recommend next items to a visitor (\u201cA\u201d), simply find the most similar visitor (\u201cB\u201d) and recommend visitor B\u2019s items to visitor A. This is accomplished through a kNN search, with each item being a feature (dimensions), and each user being a data point.", "The overall flow is illustrated in Figure 8. The ~ 2 million sessions are divided into 85%-15% train-test splits. The train dataset is used to construct the user-item interaction matrix for collaborative filtering. The test dataset has 1 item removed from each of the session and hold out for testing. For each the session in test dataset, the recommender gives 5 recommended items. These recommended items are compared with the hold out result, and a score of 1 is assigned if hold out result matches any of the 5 recommended items, and 0 otherwise. In the end, the standard user-item collaborative filtering technique gives an accuracy of ~ 15% with 5 items recommended.", "Not too bad. Let\u2019s take a moment to look into the error and see if the error can be reduced. The strategy is therefore that of a hybrid recommender system. For various aspects that the standard user-item collaborative filtering lacks, I replace the least similar item with another from a stronger predictor. As a result, the hybrid recommender system should outperform the the standard user-item collaborative filtering.", "Association rule mining, also known as Market Basket Analysis or apriori algorithm, is a technique for discovering associations among items. I recommend article Association Rule Mining by Surya for a review of this concept. Essentially, from the user-item interaction matrix, it is able to tell statements (rules) such as \u201cif items A and B are selected by the visitor, I am highly certain that item C is also selected based on my observation of such occurrences.\u201d Intuitively, the certainty of such statement is called \u201cconfidence\u201d, and the number of observations of such rules is called \u201csupport.\u201d Examples of strong association rules mined and their distribution are shown in Figure 10.", "Why association rule mining over user-item collaborative filtering? The user-item collaborative filtering relies on user similarity for recommendation. On the other hand, association rule mining relies on item to item correlation for recommendation. With the above understanding, another curious question arises: \u201cwhat advantages of association rule mining offer over traditional item-item collaborative filtering?\u201d", "Indeed, the difference between association rule mining and item-item collaborative filtering is minor and often under-discussed. One of the best discussions I have found so far is from a CrossValidated post here. I\u2019d like to offer some of my opinions in addition:1. Association rule mining gives two intuitive metrics: confidence and support. These metrics, due to their intuitive nature, are very good handles in selecting what rules to include. For example, in this work, I select confidence cutoff at 85%, and support at 200 counts. This intuitively translate to saying \u201cany association rules that has a true positive rate of 85% and occurs more than 200 times in my dataset, I will straightforwardly apply them regardless of other conditions.\u201d2. Association rule mining is robust against dominance of popular items. For example, if items A & B always occur together as a fact, but the visitor used items (A, X, Y, Z). (X, Y, Z) can be very popular items, and the user-item or item-item collaborative filtering techniques may recommend based on (X, Y, Z) and disregard (A). Since we applied a support cutoff at 200 counts, the association rule strengthened the prediction for mid-popularity items.", "An \u201cah-ha\u201d moment appeared during EDA process for me: if there is no effective recommender system in place, how does the visitor hop from one item to the next? Whatever that answer is should be an important feature to be considered.", "In order to discover the connecting activities between items, I check back to the web activity logs for answers. This time, the events between item visits hold the clues. By sub-sampling a month (2019/3/1\u20132019/4/1) worth of web logs, ~ 60% of connecting events are either text searches, author name URL clicks, or content tag clicks. I therefore decided to use title text-similarity as the key feature to build a content-based model. The inclusion of author name as a feature is dropped due to time constraint and also to avoid dominance of popular authors \u2014 \u201crich gets richer\u201d problem but in terms of authors. The inclusion of tags as a feature is discarded, since nanoHUB is planning on a tag overhaul. The quality of tags is also very low \u2014 there are more tags than total number of items on nanoHUB, and many authors wildly tag anything unrelated from \u201cyoga\u201d to their own names.", "Standard NLP processes are applied in preparation to turn titles into usable features. Using SpaCy, stopwords are first removed, but stemming and lemmatization are not applied since many words are highly technical. For example, \u201ctunneling\u201d is a physics term, and reducing it to \u201ctunnel\u201d may not be appropriate. After stopwords are removed, the word frequency is plotted out as shown in the left plot in Figure 12. The word frequency of item titles, unsurprisingly, mostly follows Zipf\u2019s Law. A violation is seen for the most frequent words, and these are mostly domain key words associated with nanotechnology. In another word, these contain little information and of little help when separating one item from another.", "To speed up things, I decided to remove the popular words and keep only the rare ones, but I also need to make sure the rare words can cover a good portion of all the items. The item coverage vs. applying a document frequency cutoff is shown on the right side of Figure 12. By including rare words that only occur in ~ 1% of all titles, almost all items can be represented. Doing so reduce the vocabulary and increase computing speed.", "After reducing full titles into representations by rare words, I am ready to analyze them for similarity. Between two titles, the similarity is defined as (# of shared words)/(# of the words in the shorter title). This is a rather crude but fast definition of similarity.", "T-test is carried out: 1. neighboring pairs of items in temporal order from each session in training set, and 2. random pairs of items from all titles. The resulting distributions are shown in Figure 13.", "It is quite evident that the null hypothesis of the two sampling distributions occur by chance can be rejected. The item and the next item chosen by the visitor have similar titles.", "The task for the recommender is therefore trivial from now on. Given the previous item, the recommender search among all items and find one with most similar title. If that similarity score is higher than a cutoff (in this work, similarity cutoff = 0.75), the item is accepted as a recommendation. Otherwise, no recommendation based on content-similarity is made. This follows the general strategy of a hybrid recommender system : attempt to replace the least similar item with one from a stronger predictor if such predictor exists.", "Finding the item with the most similar title can be a computationally expensive task. For a population of 8000 items, for example, (8000*8000/2\u20138000) = over 31 million evaluations must be made. This is still manageable with parallel processing, but with the number of items growing exponentially over the years, this can become an impossible task quickly. The advantage of using such pre-generated table is speed, since it effectively placed the burden of calculating title similarity offline, i.e. a weekly re-computation of the similarity table. In online evaluation, the task of computing title similarity is done on-the-fly. This however does not mean one needs to evaluate (1*8000\u20131) = 7999 times, since instead of comparing with general item population, one can compare it with candidate items from user-item collaborative filtering, which reduces the number of evaluations needed. In this project, I opt for online evaluation.", "Note that by introducing the content-based modeling in such way, another weakness of the standard collaborative filtering technique is addressed: the lack of temporal order of items as a feature. Indeed, the particular order of the items being visited by the visitor is of no value to the collaborative filtering technique, since the user-item interaction matrix removes any temporal order and duplicates. In reality, what the visitor decides to view next, as evident from the neighbor-sampled item pairs, is highly correlated in terms of content similarity. Therefore, the introduction of content-based modeling addressed a shortcoming of the standard user-item collaborative filtering technique and strengthened the recommender system.", "Another observation follows from the previous title similarity aspect: one major reason the titles are similar is that a significant portion of them are lectures. Figure 15 shows the types of items from the neighboring pairs in case of correctly and incorrectly recommended within the test data set. Lecture-to-lecture is a significant portion of the error.", "Visitor\u2019s behavior associated with lectures is fairly predictable: visitors tend to watch the next lecture in the series. This can be concluded simply by observing the items from neighboring-item pairs, and it makes intuitive sense.", "In order to capture the trend that visitors likely watch the next lecture in the series, I implemented functions to detect lectures and find the next one in series. The function scans all items, capturing titles in forms of \u201cABC 123 lecture 09\u2026..\u201d, \u201cXYZ 123: title\u201d, etc. For cases where lecture number does not exist, such as \u201cXYZ 123: title\u201d, lectures are ordered by number of visitors in descending order. This is based on the assumption that the number of viewers decreases as lecture series continues.", "Besides improving the recommender accuracy, business KPIs (2) and (3) must be met:2. Coverage: This KPI is defined as how uniform the items are covered by recommender traffic.3. Promotion to special products: This KPI is defined as the amount of traffic increase expected for the special kind of items \u2014 the simulation tools.", "If the recommender system is deployed as-is, that is, tuned to maximize accuracy without regard to other factors, the resulting traffic pattern is labeled as \u201cbefore\u201d and shown in Figure 17. Each dot shown in Figure 17 is an individual item. The \u201ccurrent traffic\u201d is the number of visitors received by the item from web activity log. The \u201ccurrent traffic\u201d for each item is then equally divided into 5 and assigned to each of the 5 recommended items. This is a highly optimal case, assuming the visitor will click through the recommender. As a result, the original traffic is re-distributed, which is labeled as \u201ctraffic from recommender\u201d. Evidently, the \u201crich gets richer\u201d problem exists \u2014 the already popular items also receives more re-directed traffic from recommender.", "To promote a less visited item, the previous replacement method is use, except, instead of replacing the least similar item with one from a stronger predictor, it is now replaced with a less visited item. This less visited item, of course, will not be as good as the one it replaces in terms of recommender performance. The hope is, by replacing the least similar item within the 5 recommended item list, the amount of accuracy loss will be small and the amount of coverage gain will be big. In another word, I\u2019d like to trade a small drop in accuracy for a large gain in coverage.", "When selecting this less visited item, a delicate balance between relevance and coverage must be kept. In order to select an item that is less visited, but also relevant, the items are first ranked based on their visibility, which is defined as average # of visitors per day. This establishes a priority queue for the items \u2014 rarer items will have priority when being selected for recommendation. Each of the items are run through the recommender system to obtain 5 items of most relevance. This is a single-item recommendation as shown in Figure 17. To not confuse it with the main recommender, I will refer this table as \u201cless visited item recommender list\u201d. When issuing recommendations to a visitor, the \u201cless visited item recommender list\u201d is traversed starting from the least visited item, and if any of the recommended items matches any of the items viewed by the visitor in the session log, the less visited item is selected to replace the least similar item in the recommender list.", "The resulting coverage improvement is shown in Figure 17. Before the coverage optimization (labeled \u201cbefore\u201d), the \u201crich gets richer\u201d problem exists. After the optimization (labeled \u201cafter\u201d), the recommender traffic is much more balanced across all items, ensuring all items enjoy good visibility to visitors through the recommender system.", "Another business metric of important is the promotion of simulation tools. To keep the pipeline simple, simulations tools, regardlessly of popularity, is placed at the top of the \u201cless visited item recommender list\u201d. This gives them top priority in being selected.", "With all model routines described above, the overall pipeline is shown in Figure 18.", "After introducing rule-based modeling, content-based modeling, and association rule mining, the overall hybrid recommender system reached an accuracy of 32% comparing to the initial 13% with standard collaborative filtering technique. After replacing the least similar item with a less-visited item, the accuracy suffers a small lose to 29%, but the coverage increased significantly to 85%. This is the \u201caccuracy-coverage\u201d tradeoff mentioned in previous section.", "One detail about the pipeline is the arrangement of the models (marked in black) following the standard collaborative filtering. There is no direct comparison to gauge quantitatively the similarity score among the items picked from various techniques. One assumption is, the subsequent models are stronger predictors than the standard collaborative filtering technique. Therefore, the insertions always occur at the beginning of the queue, and removal is always at the end.", "Due to time constraint, the recommender system is prototyped within a Jupyter notebook environment and is not suitable for production. This, however, does not mean the recommender system is not designed with production in mind. I\u2019d like to touch on the topic briefly here on my plans to bring this model into production, which is the next phase of this project.", "In order to make it suitable for production, the recommender system is built with streamlined, simpler models rather than computationally expensive, advanced models such as neural networks. As discussed previously, this decision places some limitations on our model: collaborative filtering does not consider the specific temporal order of events, but advanced techniques such as LSTM and Markov Chain models can. The visitor viewed items have been shown to be correlated in temporal order, and this missing feature may help improving the accuracy of the model further. This tradeoff among model complexity, interpretability, and deployability to production is a non-trivial optimization and requires future work. For now, I stick to the less complex models for better interpretability and deployability.", "Another important aspect of the production is computational cost. If the recommendation is to be made based on visitor\u2019s session log, then this recommendation is online, meaning computing the next 5 items based on the visitor\u2019s current session data real time. This can be a computationally demanding task. Something curious occured when I train-test the model in a slightly different way. Instead of using each visitor\u2019s full session log, the session is broken into tuples of 2. For example, if the visitor viewed items, in temporal order, (1,2,3,4), then the resulting tuples are (1,2), (2,3), (3,4). Each tuple can be viewed as a new mini-session. The testing set is formed in the same way and the second item in the tuple (item later in time) is hold out. If the recommender system is trained with such 2-tuples, the resulting accuracy is ~19%.", "This is a curious observation, since it indicates a Markov property of visitor\u2019s item preferences, that is, the visitor\u2019s next item likely depends and only depends on the previous item viewed. The assumption of Markov property of visitor\u2019s behavior can bring the burden of recommender computation from online to offline at the cost of ~10% of drop in accuracy. If I am willing to make such tradeoff, the solution towards production becomes much simpler:1. Recommender offline calculates 5 items for each item based on the 2-tuples from visitor sessions generated from web log. This refresh can, for example, be invoked by cron and performed once every week.2. The resulting recommendations can be stored in SQL table with primary key being item ID, and one column containing the 5 recommended items.3. The recommendation SQL table can be pulled by frontend and display on live website.", "In this project, a hybrid recommender system is prototyped for nanoHUB.org, an online education and research platform with over 1.4 million visitors per year. The hybrid recommender system is composed of user-item collaborative filtering, content-based model, rule-based model, and association rule mining. An additional module modifies the final recommendations to optimize coverage and simulation tool promotion. Overall, the recommender system achieves an accuracy of 29% with 85% coverage. The recommender system meets the targeted business KPIs, including recommendation accuracy, coverage, and special content promotion. I hope this new feature can enhance the annual 1.4 million user\u2019s nanoHUB browsing experience and learn more about nanotechnology!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Xufeng Wang is currently a Data Scientist at nanoHUB (www.nanoHUB.org) and an Insight Data Science Fellow."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb91e1a0beb06&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wang159.purdue.1?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wang159.purdue.1?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Xufeng Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f20c2c61518&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&user=Xufeng+Wang&userId=6f20c2c61518&source=post_page-6f20c2c61518----b91e1a0beb06---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.insightdatascience.com/", "anchor_text": "Insight Data Science"}, {"url": "http://www.nanohub.org", "anchor_text": "www.nanohub.org"}, {"url": "https://nanohub.org/about", "anchor_text": "https://nanohub.org/about"}, {"url": "https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada", "anchor_text": "Introduction to recommender systemsOverview of some major recommendation algorithms.towardsdatascience.com"}, {"url": "https://nanohub.org/resources/tools", "anchor_text": "https://nanohub.org/resources/tools"}, {"url": "https://medium.com/@mintingye_43620/home-sweet-home-747df73d8808", "anchor_text": "Home Sweet Home!Predicting adoption of accessory dwelling units in Californiamedium.com"}, {"url": "https://link.springer.com/chapter/10.1007/978-3-642-13287-2_4", "anchor_text": "\u201clong-tail\u201d problem"}, {"url": "https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed", "anchor_text": "Machine Learning for Recommender System"}, {"url": "https://medium.com/@yuwang2020/a-story-recommender-system-for-commaful-c9fd08045aaf", "anchor_text": "A story recommender system for commaful:You read story, we read youmedium.com"}, {"url": "https://towardsdatascience.com/association-rule-mining-be4122fc1793", "anchor_text": "Association Rule Mining"}, {"url": "https://stats.stackexchange.com/questions/256012/item-item-collaborative-filtering-vs-market-basket-analysis", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Zipf%27s_law", "anchor_text": "Zipf\u2019s Law"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&user=Xufeng+Wang&userId=6f20c2c61518&source=-----b91e1a0beb06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&user=Xufeng+Wang&userId=6f20c2c61518&source=-----b91e1a0beb06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb91e1a0beb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b91e1a0beb06---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b91e1a0beb06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wang159.purdue.1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wang159.purdue.1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Xufeng Wang"}, {"url": "https://medium.com/@wang159.purdue.1/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8 Followers"}, {"url": "http://www.nanoHUB.org", "anchor_text": "www.nanoHUB.org"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f20c2c61518&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&user=Xufeng+Wang&userId=6f20c2c61518&source=post_page-6f20c2c61518--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F6f20c2c61518%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanohub-rec-b91e1a0beb06&user=Xufeng+Wang&userId=6f20c2c61518&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}