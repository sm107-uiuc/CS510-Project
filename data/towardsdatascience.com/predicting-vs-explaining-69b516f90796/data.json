{"url": "https://towardsdatascience.com/predicting-vs-explaining-69b516f90796", "time": 1682997410.290074, "path": "towardsdatascience.com/predicting-vs-explaining-69b516f90796/", "webpage": {"metadata": {"title": "Predicting vs. Explaining. And Why Data Science Needs More\u2026 | by Phoebe Wong | Towards Data Science", "h1": "Predicting vs. Explaining", "description": "I recently stumbled across this really juicy debate on natural language processing that took place a few years ago between the old guard of the field, Noam Chomsky, who\u2019s considered as \u201cthe father of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Noam_Chomsky", "anchor_text": "the father of modern linguistics", "paragraph_index": 0}, {"url": "http://norvig.com/", "anchor_text": "Director of Research at Google", "paragraph_index": 0}, {"url": "https://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/?source=post_page---------------------------", "anchor_text": "said the following", "paragraph_index": 0}, {"url": "http://norvig.com/chomsky.html", "anchor_text": "long essay", "paragraph_index": 3}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf", "anchor_text": "two cultures in statistical modeling", "paragraph_index": 4}, {"url": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35179.pdf", "anchor_text": "another paper", "paragraph_index": 4}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf", "anchor_text": "An Introduction to Statistical Learning", "paragraph_index": 6}, {"url": "http://web.cse.ohio-state.edu/~stiff.4/cse3521/norvig-chomsky.html", "anchor_text": "neat example", "paragraph_index": 17}, {"url": "http://www.stat.cmu.edu/~larry/=stat401/Causal.pdf", "anchor_text": "another neat example", "paragraph_index": 18}, {"url": "https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.2.3", "anchor_text": "many statistical techniques", "paragraph_index": 22}, {"url": "https://hdsr.mitpress.mit.edu/pub/a7gxkn0a", "anchor_text": "example on the movie industry", "paragraph_index": 23}, {"url": "https://hdsr.mitpress.mit.edu/pub/a7gxkn0a", "anchor_text": "textual analysis of academic preprints", "paragraph_index": 24}, {"url": "http://blog.datadive.net/interpreting-random-forests/", "anchor_text": "lots", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "of", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27", "anchor_text": "work", "paragraph_index": 25}, {"url": "https://athey.people.stanford.edu/research", "anchor_text": "Susan Athey at Stanford", "paragraph_index": 25}, {"url": "http://bayes.cs.ucla.edu/WHY/", "anchor_text": "Judea Pearl", "paragraph_index": 26}, {"url": "https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/", "anchor_text": "Pearl argues", "paragraph_index": 26}, {"url": "https://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf", "anchor_text": "one of his papers", "paragraph_index": 26}, {"url": "https://phoebetwong.github.io/", "anchor_text": "https://phoebetwong.github.io/", "paragraph_index": 30}], "all_paragraphs": ["I recently stumbled across this really juicy debate on natural language processing that took place a few years ago between the old guard of the field, Noam Chomsky, who\u2019s considered as \u201cthe father of modern linguistics,\u201d and the new guard, Peter Norvig, Director of Research at Google. When commenting on where the field was heading, Chomsky said the following:", "\u201cSuppose that somebody says he wants to eliminate the physics department and do it the right way. The \u201cright\u201d way is to take endless numbers of videotapes of what\u2019s happening outside the video, and feed them into the biggest and fastest computer, gigabytes of data, and do complex statistical analysis \u2014 you know, Bayesian this and that \u2014 and you\u2019ll get some kind of prediction about what\u2019s gonna happen outside the window next. In fact, you get a much better prediction than the physics department will ever give. Well, if success is defined as getting a fair approximation to a mass of chaotic unanalyzed data, then it\u2019s way better to do it this way than to do it the way the physicists do, you know, no thought experiments about frictionless planes and so on and so forth. But you won\u2019t get the kind of understanding that the sciences have always been aimed at \u2014 what you\u2019ll get at is an approximation to what\u2019s happening.\u201d", "Chomsky reinforced that sentiment repeatedly elsewhere: that the current definition of success in natural language processing \u2014 namely predictive accuracy \u2014 is not science. Throwing \u201csome immense corpus of text\u201d into a \u201ccomplicated machine\u201d is merely \u201capproximating unanalyzed data,\u201d or \u201cbutterfly collecting,\u201d that would not lead to \u201creal understanding\u201d of the language. He argues that the main goal of science is to \u201cdiscover explanatory principles\u201d of how a system actually works, and the \u201cright approach\u201d to achieve that goal is to \u201clet the theory guide the data\u201d: study the system\u2019s basic nature by abstracting away \u201cirrelevant intrusions\u201d through carefully designed experiments \u2014 the same way modern science has been conducted since Galileo. In his own succinct words: \u201cJust trying to deal with the unanalyzed chaotic data is unlikely to get you anywhere, just like as it wouldn\u2019t have gotten Galileo anywhere.\u201d", "Norvig then responded to Chomsky\u2019s claims in a long essay that challenges Chomsky\u2019s notion of scientific success and justifies the use of probabilistic models to represent human language processing. Norvig points out that virtually all major application areas in language processing \u2014 search engines, speech recognition, machine translation, and question answering \u2014 are dominated by trained probabilistic models because they preform way better than the theoretical/logical rule-based old tools. He argues that Chomsky\u2019s measure of success in science \u2014 the emphasis on providing insights by answering why while downplaying the importance of describing how \u2014 is misguided by quoting Richard Feynman: \u201cPhysics can progress without the proofs, but we can\u2019t go on without the facts.\u201d And, perhaps a bit of a low blow, Norvig likens Chomsky\u2019s focus on the \u201cdeep whys\u201d to Bill O\u2019Reilly\u2019s religiosity, and mentions that probabilistic models \u201ccreate several trillion dollars of revenue each year, while the offspring of Chomsky\u2019s theories generate well under a billion,\u201d referencing Chomsky\u2019s books sold on Amazon.", "Norvig speculates that Chomsky\u2019s disdain for \u201cBayesian this and that\u201d really came from this schism between the two cultures in statistical modeling described by Leo Breiman: 1) the data modeling culture, which assumes that nature is a black box in which variables are associated stochastically, and the modelers\u2019 job is to identify the model that best fit these underlying associations; 2) the algorithmic modeling culture assumes that the associations in the black box are too complex to be described by a simple model, and the modelers\u2019 job is to use the algorithm that can best estimate output with input variables, without the expectation that the true underlying associations of the variables inside the black box can be understood. Norvig suspects Chomsky doesn\u2019t have a beef with probabilistic models (the data modeling culture) per se, but rather algorithmic models with \u201cquadrillions of parameters\u201d that are not easily interpretable, and therefore not useful for addressing the \u201cwhy\u201d questions. Norvig and Breiman belong to camp #2, who believe that systems like languages are too complex, random, and contingent to be represented by a small set of parameters; to abstract away the complexity is akin to \u201cfabricating a mystical facility that is exactly tuned to the eternal realm\u201d that doesn\u2019t exist, and so \u201cmisses the point about what language is, and how it works.\u201d Norvig doubled down on this point in another paper, in which he argues that \u201cwe should stop acting as if our goal is to author extremely elegant theories, and instead embrace complexity and make use of the best ally we have: the unreasonable effectiveness of data.\u201d He points out that in speech recognition, machine translation, and virtually all applications of machine learning to Web data, simple models like \u201cn-gram models or linear classifiers based on millions of specific features perform better than elaborate models that try to discover general rules.\u201d", "What fascinates me the most about this debate isn\u2019t what Chomsky and Norvig disagree on, but rather what they agree on: they both agree that analyzing massive amounts of data with statistical learning methods without understanding the variables tend to yield better predictions than the theoretical approach that attempts to model how the variables relate to each other. And I\u2019m not the only one who\u2019s stumped by this: many people with math and science background I\u2019ve talked to also find this to be rather counterintuitive: shouldn\u2019t the approach that\u2019s best at modeling the underlying structural relationships also have the most predictive power? Or how could we predict anything accurately without actually knowing how things work?", "Even in academic fields like economics and other social sciences, the concepts of predictive power and explanatory power are often conflated \u2014 models showing high explanatory power are often assumed to be highly predictive. But the approach to building the best predictive model is totally different from the approach to building the best explanatory model, and modeling decisions often result in trade-offs between the two goals. To illustrate the methodological differences, here\u2019s a short summary of prediction and inference modeling in \u201cAn Introduction to Statistical Learning.\u201d(ISL)", "The foundational principle of predictive models is relatively straightforward: to estimate Y using a set of readily available input X. If the error term of X averages to zero, Y can be predicted using:", "where \u0192 is the systematic information on Y provided by X, which results in \u0176 (prediction of Y) when given X. The exact functional form of \u0192 is usually not relevant as long as it accurately predicts Y, and \u0192 is treated as a \u201cblack box.\u201d", "The accuracy of this type of model can be decomposed into two parts: reducible error and irreducible error:", "In order to improve the predictive accuracy of the model, the main objective is to minimize the reducible error by using the most appropriate statistical learning techniques to estimate \u0192.", "When our goal is to understand the relationship between X and Y, i.e. how Y changes as a function of X, \u0192 cannot be treated as a \u201cblack box\u201d because we can\u2019t identify the effect of X on Y without knowing the functional form of \u0192.", "Almost always the case, parametric methods are used to estimate \u0192 when modeling inferences. Parametric refers to how this approach simplifies the estimation of \u0192 by assuming a parametric form of \u0192 and estimating \u0192 via the assumed parameters. There are two general steps in this approach:", "1. Make an assumption about the functional form of \u0192. The most common assumption is that \u0192 is linear in X:", "The most common approach to fitting the model is ordinary least squares (OLS).", "You might already be wondering: how do we know if \u0192 has a linear form? Because the true form of \u0192 is unknown, we don\u2019t actually know, and if the model we chose is too far from the true \u0192, our estimates will be biased. So then why would we want to make such a strong assumption in the first place? That\u2019s because there\u2019s an inherent trade-off between a model\u2019s flexibility and interpretability. Flexibility refers to the range of shapes that a model can produce to fit the many different possible functional forms of \u0192, and so the more flexible a model is, the better fit it can produce, which improves its predictive accuracy. But a more flexible model is generally more complex and requires more parameters for fitting, and the estimates of \u0192 often become too complicated for the associations of any individual predictors to be interpretable. On the other hand, the parameters in a linear model are relatively simple and interpretable, even though it does not do a very good job in predicting accurately. Here\u2019s a great chart in ISL that illustrates this trade-off in different statistical learning models:", "As you can see, the more flexible machine learning models with better predictive accuracy such as Support Vector Machine and Boosting methods are also very low on interpretability. And by making strong assumption on the functional form of \u0192 to make the model more interpretable, inference modeling is also giving up predictive accuracy in the process.", "But wait! Even if you use a highly interpretable model with very good fit, you still can\u2019t use those statistics as the stand-alone evidence of causality. That\u2019s because of the old, tired cliche of \u201ccorrelation isn\u2019t causation\u201d and here\u2019s a neat example: suppose you have data on the lengths of a hundred flag poles, the lengths of their shadows, and the sun\u2019s position. You know that the length of the shadow is caused by the pole\u2019s length and the sun\u2019s position, but even if you set the pole\u2019s length as dependent variable and the shadow length as independent variable, your model would still get a really good fit with statistically significant coefficients, etc. That\u2019s why causal inferences cannot be made by statistical models alone and require background knowledge \u2014 the inferred causality must be justified by some prior theoretical understanding of the relationship. Data analysis and statistical modeling for causal inferences are therefore often heavily guided by theoretical models.", "\u2026AND even if you do have a solid theoretical justification for saying X causes Y, identifying the causal effect is still often super tricky. That\u2019s because estimating the causal effect involves identifying what would have happened in a counterfactual world in which X didn\u2019t take place, which is by definition unobservable. Here\u2019s another neat example: suppose you want to identify the effect of vitamin C on health. You have data on whether someone takes vitamins (X=1 if they do; 0 otherwise), and some binary health outcomes (Y=1 if they\u2019re healthy; 0 otherwise), that looks like this:", "Y\u2081 represents the health outcomes of those who take vitamin C and Y\u2080 is the outcome of those who don\u2019t. To identify the effect of vitamin C on health, we\u2019d estimate the mean treatment effect:", "But in order to do that, we\u2019d need to know what the health outcomes of those who are taking vitamin C would have been had they not taken any vitamin C, and vice versa (or E(Y\u2080|X = 1) and E(Y\u2081|X = 0)), which are indicated as the asterisks in the table and represent the unobservable counterfactual outcomes. The mean treatment effect (\ud835\udec9) cannot be consistently estimated without these inputs.", "Worse yet, now imagine that people who are already healthy tend to take vitamin C, and already unhealthy people tend not to. In this scenario, the estimates would show a strong treatment effect, even if vitamin C actually has no effect on health at all. Here, prior health is called a confounding factor that affects both vitamin C intake and health (X and Y), which leads to biased estimates of \ud835\udec9.", "The safest way to produce a consistent estimate of \ud835\udec9 is by randomizing treatment through an experiment, so that X is independent of Y. When treatment is randomly assigned, the untreated group\u2019s outcome, on average, serves as an unbiased proxy to the counterfactual outcomes of the treatment group and ensures there is no confounding factor. A/B testing is guided by that insight. But randomized experiments are not always possible (or ethical, say if we want to study the health effects of smoking or eating too many chocolate chip cookies), and in those cases, causal effects would have to be estimated from observational data with often non-randomized treatments. There are many statistical techniques that identify causal effects in non-experimental settings by constructing the counterfactual outcomes or simulating random treatment assignments in the observational data, but as you can imagine, the results of these types of analyses are often not very robust or reproducible. And more importantly, these layers of methodological hurdles are not designed to improve the predictive accuracy of the model, but to present evidence of causality through the combination of logic and statistical inferences.", "It\u2019s also much easier to measure the success of a predictive model than a causal model \u2014 while there are standard performance metrics for predictive models, it\u2019s much more challenging to assess relative success of causal models, which perhaps makes them a whole lot less emotionally gratifying to do. But even though causal inferences are tricky to make, it doesn\u2019t mean we should stop trying. The main point here is that predictive and causal models serve very different purposes and require very different data and statistical modeling processes, and often we need to do both. This example on the movie industry illustrates the point: movie studios use predictive models to project box office revenues in order to anticipate the financial outcome of a theatrical release, assess the financial risks/return of their movie portfolio, etc., but predictive models are not very useful for understanding the structure and dynamics of the movie market and inform investment decisions; that\u2019s because in the earlier stages of the movie production process (usually years before the release date) when investment decisions are made, the variance of possible outcomes is really high, and so the accuracy of predictive models based on inputs from the early stages is highly degraded. Predictive models are the most accurate near the theatrical release date when most production decisions have already been made, i.e. when the prediction is no longer particularly actionable. On the other hand, causal inference modeling allows studios to learn how different production characteristics could affect potential revenue during the early stages of the production process, and therefore are crucial for informing their production strategies.", "Judging from the current state of quantitative research literature, it\u2019s not hard to see why Chomsky is upset \u2014 predictive models are now dominating both academia and industry. This textual analysis of academic preprints finds that the fastest growing quantitative research fields are increasingly focusing on predictions in this decade. For example, in AI, papers mentioning terms related to \u201cprediction\u201d have grown by >2x, while papers mentioning terms related to \u201cinference\u201d have fallen by half since 2013. Data science curriculums today largely ignore causal inference methodologies, and the data science industry mostly expects practitioners to focus on predictive models. Even the high profile data science competitions like Kaggle and the Netflix prize are always based on improving predictive performance metrics.", "On the other hand, there are still many fields where not enough attention has been paid on empirical prediction and can benefit from the advances made in machine learning and predictive modeling. But to frame the current state of affairs as a cultural war between \u201cTeam Chomsky\u201d and \u201cTeam Norvig\u201d seems like a false choice \u2014 there is no reason why we could only choose one and there are plenty of opportunities for cross-fertilization between the two cultures. There are lots of work being done to make machine learning models more interpretable, and I\u2019m personally especially excited by the work done by Susan Athey at Stanford that applies machine learning techniques in causal inference methodology (I\u2019ll be writing about them in the next few weeks \u2014 stay tuned!).", "I can\u2019t think of a better way to end on a high note and bring this blog full circle than plugging Judea Pearl\u2019s work. Pearl led the AI research work in the 1980s that allowed machines to reason probabilistically using Bayesian networks, but has since became the biggest critic on how AI\u2019s sole focus on probabilistic associations is handicapping advances. Echoing Chomsky\u2019s sentiment, Pearl argues that \u201call the impressive achievements of deep learning amount to just fitting a curve to data,\u201d and AI today is stuck doing the exact same things (to predict and diagnose/classify) that machines already knew how to do 30 years ago \u2014 only marginally better at them \u2014 yet prediction and diagnosis are \u201cmerely the tip of human intelligence.\u201d He believes that the key to making truly intelligent machines that think like humans is to teach machines to think about cause and effect, so that machines can ask counterfactual questions, plan experiments, and find new answers to scientific questions. His work in the past three decades focuses on building a formal language for machines in order to make causal reasoning possible, similar to his work on Bayesian networks that enabled machines to make probabilistic associations. In one of his papers, he claims that:", "\u201cthe bulk of human knowledge is organized around causal, not probabilistic relationships, and the grammar of probability calculus is insufficient for capturing those relationships\u2026 It is for this reason that I consider myself only a half-Bayesian.\u201d", "Seems like data science would benefit from having more half-Bayesians.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Vice President, Data Science @ Minerva CQ https://phoebetwong.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F69b516f90796&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----69b516f90796--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@phoebetwong?source=post_page-----69b516f90796--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phoebetwong?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Phoebe Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a83326316c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&user=Phoebe+Wong&userId=7a83326316c0&source=post_page-7a83326316c0----69b516f90796---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://link.springer.com/article/10.1007/s00787-018-1264-3", "anchor_text": "foetal alcohol spectrum disorders"}, {"url": "https://en.wikipedia.org/wiki/Noam_Chomsky", "anchor_text": "the father of modern linguistics"}, {"url": "http://norvig.com/", "anchor_text": "Director of Research at Google"}, {"url": "https://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/?source=post_page---------------------------", "anchor_text": "said the following"}, {"url": "http://norvig.com/chomsky.html", "anchor_text": "long essay"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf", "anchor_text": "two cultures in statistical modeling"}, {"url": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35179.pdf", "anchor_text": "another paper"}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf", "anchor_text": "An Introduction to Statistical Learning"}, {"url": "http://web.cse.ohio-state.edu/~stiff.4/cse3521/norvig-chomsky.html", "anchor_text": "neat example"}, {"url": "http://www.stat.cmu.edu/~larry/=stat401/Causal.pdf", "anchor_text": "another neat example"}, {"url": "https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.2.3", "anchor_text": "many statistical techniques"}, {"url": "https://hdsr.mitpress.mit.edu/pub/a7gxkn0a", "anchor_text": "example on the movie industry"}, {"url": "https://hdsr.mitpress.mit.edu/pub/a7gxkn0a", "anchor_text": "textual analysis of academic preprints"}, {"url": "http://blog.datadive.net/interpreting-random-forests/", "anchor_text": "lots"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "of"}, {"url": "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27", "anchor_text": "work"}, {"url": "https://athey.people.stanford.edu/research", "anchor_text": "Susan Athey at Stanford"}, {"url": "http://bayes.cs.ucla.edu/WHY/", "anchor_text": "http://bayes.cs.ucla.edu/WHY/"}, {"url": "http://bayes.cs.ucla.edu/WHY/", "anchor_text": "Judea Pearl"}, {"url": "https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/", "anchor_text": "Pearl argues"}, {"url": "https://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf", "anchor_text": "one of his papers"}, {"url": "https://medium.com/tag/data-science?source=post_page-----69b516f90796---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----69b516f90796---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----69b516f90796---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/statistics?source=post_page-----69b516f90796---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----69b516f90796---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&user=Phoebe+Wong&userId=7a83326316c0&source=-----69b516f90796---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&user=Phoebe+Wong&userId=7a83326316c0&source=-----69b516f90796---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----69b516f90796--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F69b516f90796&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----69b516f90796---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----69b516f90796--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----69b516f90796--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----69b516f90796--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----69b516f90796--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----69b516f90796--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phoebetwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phoebetwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Phoebe Wong"}, {"url": "https://medium.com/@phoebetwong/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "930 Followers"}, {"url": "https://phoebetwong.github.io/", "anchor_text": "https://phoebetwong.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a83326316c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&user=Phoebe+Wong&userId=7a83326316c0&source=post_page-7a83326316c0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa216e7519b38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-vs-explaining-69b516f90796&newsletterV3=7a83326316c0&newsletterV3Id=a216e7519b38&user=Phoebe+Wong&userId=7a83326316c0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}