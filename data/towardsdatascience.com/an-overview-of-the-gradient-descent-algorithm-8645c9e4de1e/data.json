{"url": "https://towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e", "time": 1682995957.529686, "path": "towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e/", "webpage": {"metadata": {"title": "An overview of the Gradient Descent algorithm | by Nishit Jain | Towards Data Science", "h1": "An overview of the Gradient Descent algorithm", "description": "Optimizing parameters is the ultimate goal of every machine learning algorithm. You want to get the optimum value of the slope and the intercept to get the line of best fit in linear regression\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.linkedin.com/in/thenishitjain", "anchor_text": "www.linkedin.com/in/thenishitjain", "paragraph_index": 28}], "all_paragraphs": ["Optimizing parameters is the ultimate goal of every machine learning algorithm. You want to get the optimum value of the slope and the intercept to get the line of best fit in linear regression problems. You also want to get the optimum value for the parameters of a sigmoidal curve in logistic regression problems. So what if I told you that Gradient Descent does it all?", "Before learning how it works, let\u2019s first make clear the meaning of the words Gradient and Descent along with some other key terms.", "Loss Function: A function that returns the cost associated with the model and measures how well our model is doing on the training data. If the cost is too high, it means that the predictions by our model are deviating too much from the observed data. In any machine learning algorithm, our ultimate mission is to minimize the loss function. Various loss functions which we use are:", "Learning Rate: This is the hyperparameter that determines the steps the gradient descent algorithm takes. Gradient Descent is too sensitive to the learning rate. If it is too big, the algorithm may bypass the local minimum and overshoot. If it too small, it might increase the total computation time to a very large extent. We will see the effect of the learning rate in depth later in the article.", "Gradient: Basically, it is a measure of the steepness of a slope. And technically, when we sum up all the first-order derivatives of all the variables in a function, it gives us gradient. For example, if we consider linear regression, we have two parameters, slope, and the intercept, to minimize. So, we calculate derivatives w.r.t. both slope & the intercept and then sum them up to get the gradient for it.", "Descent: To optimize parameters, we need to minimize errors. The aim of the gradient descent algorithm is to reach the local minimum (though we always aim to reach the global minimum of the function. But if a gradient descent algorithm once attains the local minimum, it is nearly impossible to reach the global minimum.). The algorithm accomplishes this by an iterative process of calculating step size at every iteration. And, this iterative calculation of step size to reach a local minimum (or in other words, descending to the point of minimum) is known as the descent (Enough of that going down the hill example).", "Now, let\u2019s get into the technicalities of the algorithm. We will be using first-order derivatives to calculate the gradient. For the demonstration, we will be using NumPy to apply gradient descent on a linear regression problem.", "Let\u2019s generate a randomized dataset first using the NumPy\u2019s random function and plot it to visualize our dataset distribution with a scatter plot.", "In the above code, we have taken two NumPy arrays for independent and dependent variables, that is X and y respectively. It is clear by looking at the graph that the relationship is linear. We know that a linear relation can be put into a function as follows:", "We can calculate the values for theta_0 & theta_1 using the analytical method as shown below:", "Then why do we need Gradient Descent if we can calculate optimum parameters analytically? It is because the analytical method turns out to be hugely computationally expensive when we have a dataset with millions of data points. Gradient descent, on the other hand, gives us similar results while minimizing the computation time massively. Let\u2019s solve for 0 and 1 using gradient descent and see for ourselves.", "Mathematically, the cost function and the gradient can be represented as follows:", "Our goal here is to minimize the cost function in a way that it comes as close to zero as possible. Having a high negative value is also as bad as a high positive value for the cost function. So, in order to keep the value of cost function >=0, we are squaring it up. We could have done the same using absolute but there are two main reasons why we are not doing so.", "We are putting in \u00bd so that the \u20182\u2019 coming from the derivation of the cost function while minimizing it for calculating gradient, cancels out, and makes gradient more concise. Anyway, it is a constant does not matter in calculating theta\u2019s value (as we are differentiating it).", "Also, we are averaging the function over m, which is the total number of data points in the training dataset. This is so that the value of the calculated gradient does not change the scale and always averages out to a central value in case a new data point comes in.", "Before we implement gradient descent, knowing the intuition behind it is a must. Since we are now done with that part, let\u2019s dive into the actual implementation of it.", "We want to calculate the value for 0 and 1 but we can have multiple features (>=2). In that case, the general formula to calculate consecutive step sizes will be", "where \u03b1 is the learning rate. We can now infer from the formula that the \u03b1 influences the size of the step a lot as we are multiplying the gradient with the alpha for every iteration.", "Enough of mathematics, let us start with the actual code. To start, we will initialize a random value for the parameters to be optimized, 0 and 1. We will write two functions to calculate cost and gradient descent by iterating and store them in two distinct NumPy arrays. The above-mentioned formulae have been used in calculating the cost and the consecutive values for theta using the gradient.", "In the code, we can see that we have run 3000 iterations. But, that was something passed to the function explicitly. In real-life scenarios, we do not do that. The actual stop point for gradient descent to stop running should be when step size approaches zero.", "Let us check how the L2 Loss reduces along with increasing iterations by plotting a graph.", "In the above graph, we see that initially, the error reduces significantly. But as iterations increase, there is not much reduction seen in the error. It nearly stabilizes after a certain value, or we can say that it decreases negligibly.", "That is the beauty of the gradient descent. When it starts approaching the local minimum, it starts to take very very small steps towards it in order to not overstep it. On the other hand, when it is far away, to reduce computation time it takes bigger steps to reach the local minimum faster.", "So, to select the best possible value of the number of iterations using this graph, we use the elbow method. Simply, the point at which the value of MSE starts to decrease negligibly is our value for the number of iterations (the elbow of the graph).", "There are various types of Gradient Descent as well. What we did above is known as Batch Gradient Descent. The other types are:", "Gradient Descent can be used to optimize parameters for every algorithm whose loss function can be formulated and has at least one minimum. Also, we cleared up our understanding of the infamous gradient descent loss function equation.", "We learned how to use a visual method to estimate the number of iterations required. Finally, we got to know the actual workings behind the scenes of the famous gradient descent algorithm.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learning from Data | Data Scientist | Wanderer | LinkedIN: www.linkedin.com/in/thenishitjain"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8645c9e4de1e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thenishitjain?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thenishitjain?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Nishit Jain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32b26764953a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&user=Nishit+Jain&userId=32b26764953a&source=post_page-32b26764953a----8645c9e4de1e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/", "anchor_text": "Stochastic Gradient Descent."}, {"url": "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/", "anchor_text": "Mini Batch Gradient Descent."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8645c9e4de1e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----8645c9e4de1e---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8645c9e4de1e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----8645c9e4de1e---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----8645c9e4de1e---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&user=Nishit+Jain&userId=32b26764953a&source=-----8645c9e4de1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&user=Nishit+Jain&userId=32b26764953a&source=-----8645c9e4de1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8645c9e4de1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8645c9e4de1e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8645c9e4de1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thenishitjain?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thenishitjain?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nishit Jain"}, {"url": "https://medium.com/@thenishitjain/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "108 Followers"}, {"url": "http://www.linkedin.com/in/thenishitjain", "anchor_text": "www.linkedin.com/in/thenishitjain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32b26764953a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&user=Nishit+Jain&userId=32b26764953a&source=post_page-32b26764953a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd113167357b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-overview-of-the-gradient-descent-algorithm-8645c9e4de1e&newsletterV3=32b26764953a&newsletterV3Id=d113167357b9&user=Nishit+Jain&userId=32b26764953a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}