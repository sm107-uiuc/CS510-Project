{"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687", "time": 1683004086.2495801, "path": "towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687/", "webpage": {"metadata": {"title": "A no-frills guide to most Natural Language Processing Models \u2014 The LSTM Age \u2014 Seq2Seq, InferSent, Skip-Thought, Quick-Thought, ELMo, Flair, and ULMFiT | by Ilias Miraoui | Towards Data Science", "h1": "A no-frills guide to most Natural Language Processing Models \u2014 The LSTM Age \u2014 Seq2Seq, InferSent, Skip-Thought, Quick-Thought, ELMo, Flair, and ULMFiT", "description": "As LSTMs gained popularity, the NLP community improved language models by moving from shallow static embeddings, that only partially took into account the context of a word, to more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.coursera.org/learn/nlp-sequence-models/home/welcome", "anchor_text": "Andrew Ng\u2019s free Sequence Models Course", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence Learning with Neural Networks", "paragraph_index": 7}, {"url": "https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf", "anchor_text": "Skip-Thought Vectors", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "An efficient framework for learning sentence representations", "paragraph_index": 15}, {"url": "https://github.com/facebookresearch/InferSent", "anchor_text": "Facebook Research\u2019s GitHub", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "Supervised learning of universal sentence representations from natural language inference data", "paragraph_index": 20}, {"url": "https://allennlp.org/elmo", "anchor_text": "AllenNLP", "paragraph_index": 21}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "Tensorflow Hub", "paragraph_index": 24}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "Deep contextualized word representations", "paragraph_index": 27}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair python library", "paragraph_index": 30}, {"url": "https://www.aclweb.org/anthology/N19-4010.pdf", "anchor_text": "Contextual String Embeddings for Sequence Labeling", "paragraph_index": 33}, {"url": "https://github.com/fastai/fastai", "anchor_text": "Fast.ai Python Library", "paragraph_index": 38}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "tutorials", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "Universal Language Model Fine-tuning for Text Classification", "paragraph_index": 41}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/", "paragraph_index": 45}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/", "paragraph_index": 45}], "all_paragraphs": ["As LSTMs gained popularity, the NLP community improved language models by moving from shallow static embeddings, that only partially took into account the context of a word, to more contextualized/dynamic word representations that leveraged the surrounding context to create deeper representations.", "While this may still contain some jargon unexplained, information about the various concepts should be easily accessible via other posts.", "Seq2seq stands out in this list in that it doesn\u2019t generate embeddings per se. However, it can be a valuable weapon in one\u2019s NLP toolkit as it can easily be adapted for particular use-cases.", "Google introduced Seq2seq in the context of Machine Translation. It is one of the first models to use attention (leveraged extensively by the more recent transformer architecture). The structure is divided between an encoder/decoder and the two of them are related to a layer of attention using beam search (cf. the image below or Andrew Ng\u2019s free Sequence Models Course for additional details on the attention mechanism).", "Seq2seq is well-known for having revolutionized machine translation (outperforming years of work of statisticians within a few months on Google Translate). It can also easily be tweaked for other tasks where the output is a sequence (i.e. conversation or summarization).", "\u001eAdvantages:- Excellent performance to generate sequences (it lays the foundation of the transformer model)- Attention layer allows better handling of long-term dependencies than \u201ctraditional\u201d LSTM models", "Disadvantages:- Cannot be used directly (no embedding) and has to be tweaked for each use-case- Supervised problem: Requires labeled data", "Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks (2014), NIPS", "Skip-Thought Vectors build on the skip-gram training method that was used in static word embeddings and extend it to sentences. Instead of using a word and trying to predict a context, Skip-Thought uses a sentence and tries to predict surrounding sentences.", "The model encodes the words in a sentence using a Recurrent Neural Network (often a GRU or a LSTM). It then uses a decoder conditioned on the output of the encoder (aka it uses the same embedding layers as the encoder) to generate the best possible target outputs (the next and the previous sentences). Using the difference between the actual encoded sentences and the predicted embeddings, the model is trained to predict its surrounding sentences as accurately as possible.", "Quick Thoughts is another model that was developed later and that uses a similar encoder/decoder strategy. The difference is that it is trained on a more classic classification task, closer to skip-gram and negative sampling. For each sentence, another target sentence is given into a decoder (same weights as the encoder) and both the encoded original sentence and the \u201cdecoded\u201d sentence are then fed into a classifier for \u201ccomparison\u201d(i.e. is the original sentence adjacent to the target sentence or not).", "Given the difference in training, Quick Thoughts is often seen as more adapted for discriminative tasks while Skip Thoughts is better for generative tasks.", "Advantages:- Sentence-level embeddings can be useful for certain tasks- Generic and Robust Sentence Embeddings: Decent accuracy on many downstream tasks (semantic similarity, classification, etc.)", "Disadvantages:- The LSTM structure and the fact that the embeddings are at the sentence-level makes it slow to train, use a lot of memory and make it hard to train for long sentences- It was one of the first sentence embeddings and hence is not considered State Of The Art (SOTA) anymore- The task it is trained on, namely predicting the neighboring sentence, is hard (even for a human) so it is generally not the best embedding (although Quick Thought simplifies the problem a little bit)- With Skip Thought, a sentence is embedded in a vector with 2400 dimensions, significantly more than most other types of embeddings- Struggles with long-term context dependencies (vs. transformer-based models for long sentences)", "Jamie Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler, Skip-Thought Vectors (2015), NIPS", "Lajanugen Logeswaran, Honglak Lee, An efficient framework for learning sentence representations (2018), ICLR", "Using the Stanford Natural Language Inference (SNLI) corpus, InferSent is built by using a bi-LSTM layer (with max-pooling) to encode pair of sentences (both sentences use the same encoder). A few dense layers are added and the model is trained to predict whether the two sentences embody \u201cneutral\u201d, \u201ccontradiction\u201d or \u201centailment\u201d. Words are usually first transformed into static word-embeddings (GloVe embeddings in the original case) before the vectors are fed into the bi-LSTM layers. As a result, InferSent embeddings provide deep semantic representations of sentences.", "Created by Facebook, InferSent generalizes well to other tasks as well and the pre-trained version is easily accessible on Facebook Research\u2019s GitHub.", "Advantages:- Sentence-level embeddings may be helpful for certain tasks- The model performs particularly well on paraphrase detection and entailment tasks", "Disadvantages:- The complex Bi-LSTM structure makes it slow to train and generate embeddings- The output is an embedding of 4096 dimensions which is significantly more than almost all the other language models- Does not perform as well as ELMo or Flair on many tasks such as sentiment analysis, semantic relatedness, caption retrieval, etc.- Supervised training makes that it is hard to reproduce on custom datasets / for more specific contexts- Struggles with long-term context dependencies (vs. transformer-based models for long sentences)", "Conneau, Alexis, et al., Supervised learning of universal sentence representations from natural language inference data (2017), Association for Computational Linguistics", "ELMo uses a deep Bi-LSTM architecture to create contextualized embeddings. As stated by AllenNLP, ELMo representations are: \u201cContextual\u201d (depends on the context the word is used), \u201cDeep\u201d (trained via a deep neural network), and \u201cCharacter based\u201d (cf. fastText embeddings, to allow for better handling of out-of-vocabulary words).", "To train the model, ELMo uses forward and backward language modeling (one direction of the LSTM predicts the first word, the other direction predicts the last word).", "The model contains numerous layers with residual connections between them. Interestingly, each layer ends up learning a different characteristic of the sentence such as Part-of-Speech Tagging in the first layers and Word-Sense Disambiguation in the later layers.", "Pre-trained ELMo embeddings on the 1 Billion Word Benchmark can easily be used through Tensorflow Hub.", "Advantages:- Deep context-based and character-level word representations adapted for more complex tasks- Shown to perform a lot better than simple embeddings on numerous tasks- Pre-trained version easily accessible", "Disadvantages:- The complex Bi-LSTM structure makes it very slow to train and generate embeddings- Similar models such as Flair (see below) often perform better- Struggles with long-term context dependencies (vs. transformer-based models for long sentences)", "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,Christopher Clark, Kenton Lee, Luke Zettlemoyer, Deep contextualized word representations (2018), NAACL", "Following ELMo\u2019s popularity, Flair was developed by Zalando Research and improved on ELMo by relying more on the character level.", "Similarly to ELMo, Flair learns a character-based Bi-LSTM (also using forward and backward language modeling). In addition, however, the word embedding is computed by giving a word, character by character, into both LSTMs and keeping/concatenating only the last states (i.e. the first and the last characters) of each words to represent the word embedding.", "Flair word embeddings are available in numerous languages and can very easily be generated using the Flair python library (built on PyTorch).", "Advantages:- Deep context-based and character-level word representations adapted for more complex tasks - Given the specificity of the embedding keeping the first and last state only, the embedding handles particularly well out-of-vocabulary tokens and can deal with much smaller dictionaries- Shown to perform better than static embeddings and often ELMo on numerous tasks- Pre-trained version easily accessible", "Disadvantages:- The complex Bi-LSTM structure makes it very slow to train and generate embeddings- Struggles with long-term context dependencies (vs. transformer-based models for long sentences)", "Akbik et al., Contextual String Embeddings for Sequence Labeling (2018), Association for Computational LinguisticsSeq2Seq", "Originating from Fast.ai, ULMFiT, is commonly known for being the first model to really effectively generalize transfer learning for any task.", "ULMFiT relies on a bi-directional LSTM architecture (both forward and backward language modeling). ULMFiT is trained on three sequential tasks: general domain language model pre-training on the Wikipedia dataset, task-specific language model fine-tuning on the IMDb dataset, and finally task-specific classifier fine-tuning on the IMDb dataset.", "Two developments allow ULMFiT to have a great performance: - ULMFiT is fine-tuned by gradually unfreezing its different layers during fine-tuning as training progresses and - it uses \u201cSlanted Triangular Learning Rates (STLR)\u201d, meaning the learning rate starts increasing and later decays in a linear fashion as training advances.", "The main advantage of ULMFiT is the ability and ease with which it can be adapted for various tasks. The particular care put into transfer learning makes ULMFiT a very good contender for many tasks/adaptations.", "A pre-trained model can easily be downloaded and fine-tuned using Fast.ai Python Library and their tutorials.", "Advantages:- Universal: A pre-trained model that is very easily adapted to any task- Excellent performance for an LSTM model- Requires a lot less data than other models to be fine-tuned for a specific domain/task- Easy to access and use thanks to Fast.ai\u2019s fantastic tutorials", "Disadvantages:- The complex Bi-LSTM structure makes it very slow to train and generate embeddings- Struggles with long-term context dependencies (vs. transformer-based models for long sentences)", "Jeremy Howard, Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (2018), Association for Computational Linguistics", "Compared to static embeddings, contextualized representations can be a lot more powerful for more complex tasks and knowing which model to choose can have a significant impact on the performance of a model. As always, try to remember how each model is trained and try to find the one whose primary task resembles the purpose of the application the most. Hopefully, this post helps you gain a better understanding of the pros/cons and use-cases of the numerous LSTM-era models.", "While transformer-based models offer a lot more complex representations that can take into account long-term dependencies, LSTM models offer a very relevant alternative between the complexity of transformer models and the sometimes over-simplistic static embeddings.", "PS: I am currently a Master of Engineering Student at Berkeley and I am still learning about all of this. If there is anything that stands to be corrected or that is not clear, please let me know. You can also email me here.", "Data Enthusiast | Master of Engineering Student @ UC Berkeley | https://imiraoui.github.io/ | https://www.linkedin.com/in/iliasmiraoui/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3af80e77687&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----3af80e77687--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Ilias Miraoui"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a----3af80e77687---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3af80e77687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----3af80e77687---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3af80e77687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=-----3af80e77687---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.coursera.org/learn/nlp-sequence-models/home/welcome", "anchor_text": "Andrew Ng\u2019s free Sequence Models Course"}, {"url": "https://github.com/google/seq2seq", "anchor_text": "Google\u2019s seq2seq GitHub"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence Learning with Neural Networks"}, {"url": "https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf", "anchor_text": "Skip-Thought Vectors"}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "An efficient framework for learning sentence representations"}, {"url": "https://github.com/facebookresearch/InferSent", "anchor_text": "Facebook Research\u2019s GitHub"}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "Supervised learning of universal sentence representations from natural language inference data"}, {"url": "https://allennlp.org/elmo", "anchor_text": "AllenNLP"}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "Tensorflow Hub"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": ""}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "Deep contextualized word representations"}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair python library"}, {"url": "https://www.aclweb.org/anthology/N19-4010.pdf", "anchor_text": "Contextual String Embeddings for Sequence Labeling"}, {"url": "https://github.com/fastai/fastai", "anchor_text": "Fast.ai Python Library"}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "tutorials"}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "Universal Language Model Fine-tuning for Text Classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3af80e77687---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3af80e77687---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----3af80e77687---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3af80e77687---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3af80e77687---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3af80e77687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----3af80e77687---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3af80e77687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----3af80e77687---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3af80e77687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----3af80e77687--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a----3af80e77687---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6c78c71c0a4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&newsletterV3=de0de51bac0a&newsletterV3Id=6c78c71c0a4d&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----3af80e77687---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Written by Ilias Miraoui"}, {"url": "https://medium.com/@ilias.miraoui/followers?source=post_page-----3af80e77687--------------------------------", "anchor_text": "105 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/"}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a----3af80e77687---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6c78c71c0a4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687&newsletterV3=de0de51bac0a&newsletterV3Id=6c78c71c0a4d&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----3af80e77687---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Ilias Miraoui"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "A no-frills guide to most Natural Language Processing Models \u2014 The Pre-LSTM Ice-Age \u2014\u2026A summary of the origins, the use-cases and the advantages/disadvantages of pre-LSTM language models: (R)NNLM, GloVe, Word2Vec & fastText"}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "\u00b76 min read\u00b7Feb 13, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----86055dd5d67c----0-----------------clap_footer----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c?source=author_recirc-----3af80e77687----0---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=-----3af80e77687----0-----------------bookmark_preview----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3af80e77687----1---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----3af80e77687----1-----------------bookmark_preview----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3af80e77687----2---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3af80e77687----2-----------------bookmark_preview----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/verifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Ilias Miraoui"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/verifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "Verifying and Tackling the Assumptions of Linear RegressionLinear regression relies on five main assumptions. Being able to verify and act upon them is especially important."}, {"url": "https://towardsdatascience.com/verifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": "\u00b76 min read\u00b7Feb 18, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32126acea67b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fverifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----32126acea67b----3-----------------clap_footer----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/verifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b?source=author_recirc-----3af80e77687----3---------------------969d3186_35c6_4051_ad16_4a389d3e4a7a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32126acea67b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fverifying-and-tackling-the-assumptions-of-linear-regression-32126acea67b&source=-----3af80e77687----3-----------------bookmark_preview----969d3186_35c6_4051_ad16_4a389d3e4a7a-------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----3af80e77687--------------------------------", "anchor_text": "See all from Ilias Miraoui"}, {"url": "https://towardsdatascience.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----3af80e77687----0-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3af80e77687----1-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Coucou Camille"}, {"url": "https://medium.com/codex?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Time Series Prediction Using LSTM in PythonImplementation of Machine Learning Algorithm for Time Series Data Prediction."}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b76 min read\u00b7Feb 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&user=Coucou+Camille&userId=d796c2fbb274&source=-----19b1187f580f----0-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----3af80e77687----0---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&source=-----3af80e77687----0-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----3af80e77687----1---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----3af80e77687----1-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://soumenatta.medium.com/?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://soumenatta.medium.com/?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Dr. Soumen Atta, Ph.D."}, {"url": "https://blog.devgenius.io/?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Dev Genius"}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "NLTK vs SpaCy: Which Library Is Best for Named Entity Recognition?Named Entity Recognition (NER) is the process of extracting and categorizing named entities such as people, organizations, locations, and\u2026"}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b74 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdev-genius%2F2c8d8faf85ac&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fnltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac&user=Dr.+Soumen+Atta%2C+Ph.D.&userId=f044b1850c1e&source=-----2c8d8faf85ac----2-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----3af80e77687----2---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c8d8faf85ac&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fnltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac&source=-----3af80e77687----2-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----3-----------------clap_footer----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----3af80e77687----3---------------------d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----3af80e77687----3-----------------bookmark_preview----d94e75e7_c789_465f_8d2d_27a1d6dfbf1d-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3af80e77687--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----3af80e77687--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}