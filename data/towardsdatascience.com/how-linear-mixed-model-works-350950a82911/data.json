{"url": "https://towardsdatascience.com/how-linear-mixed-model-works-350950a82911", "time": 1683011871.180193, "path": "towardsdatascience.com/how-linear-mixed-model-works-350950a82911/", "webpage": {"metadata": {"title": "How Linear Mixed Model Works. And how to understand LMM through\u2026 | by Nikolay Oskolkov | Towards Data Science", "h1": "How Linear Mixed Model Works", "description": "This is the seventeenth article from my column Mathematical Statistics and Machine Learning for Life Sciences where I try to explain some mysterious analytical techniques used in Bioinformatics and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences?source=post_page---------------------------", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Frequentist_inference", "anchor_text": "Frequentist statistics", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "from scratch", "paragraph_index": 0}, {"url": "https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html", "anchor_text": "LASSO", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/select-features-for-omics-integration-511390b7e7fd", "anchor_text": "Select Features for OMICs Integration", "paragraph_index": 4}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3014363/", "anchor_text": "GCTA tool", "paragraph_index": 5}, {"url": "https://www.biorxiv.org/content/10.1101/588020v1", "anchor_text": "solved at least for Human Height", "paragraph_index": 5}, {"url": "https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106", "anchor_text": "DESeq", "paragraph_index": 6}, {"url": "http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html", "anchor_text": "DESeq2", "paragraph_index": 6}, {"url": "https://cran.r-project.org/web/packages/MCMCglmm/index.html", "anchor_text": "MCMCglmm", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Simpson's_paradox", "anchor_text": "Simpson\u2019s paradox", "paragraph_index": 8}, {"url": "https://pubmed.ncbi.nlm.nih.gov/12603781/", "anchor_text": "sleep deprivation study", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Bayesian_probability", "anchor_text": "Bayesian statistics", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Frequentist_inference", "anchor_text": "Frequentist", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "next post", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "AIC", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Bayesian_information_criterion", "anchor_text": "BIC", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "next post", "paragraph_index": 26}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms", "paragraph_index": 29}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms", "paragraph_index": 29}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms", "paragraph_index": 29}, {"url": "https://mc-stan.org/", "anchor_text": "Stan", "paragraph_index": 29}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms", "paragraph_index": 29}, {"url": "https://github.com/NikolayOskolkov/HowLinearMixedModelWorks", "anchor_text": "Github", "paragraph_index": 33}, {"url": "https://medium.com/u/8570b484f56c?source=post_page-----350950a82911--------------------------------", "anchor_text": "Nikolay Oskolkov", "paragraph_index": 33}, {"url": "http://linkedin.com/in/nikolay-oskolkov-abb321186?source=post_page---------------------------", "anchor_text": "Linkedin", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "derive the Linear Mixed Model and program it from scratch from the Maximum Likelihood", "paragraph_index": 33}], "all_paragraphs": ["This is the seventeenth article from my column Mathematical Statistics and Machine Learning for Life Sciences where I try to explain some mysterious analytical techniques used in Bioinformatics and Computational Biology in a simple way. Linear Mixed Model (LMM) also known as Linear Mixed Effects Model is one of key techniques in traditional Frequentist statistics. Here I will attempt to derive LMM solution from scratch from the Maximum Likelihood principal by optimizing mean and variance parameters of Fixed and Random Effects. However, before diving into derivations, I will start slowly in this post with an introduction of when and how to technically run LMM. I will cover examples of linear modeling from both Frequentist and Bayesian frameworks.", "Traditional Mathematical Statistics is based to a large extent on assumptions of the Maximum Likelihood principal and Normal distribution. In case of e.g. multiple linear regression these assumptions might be violated if there is non-independence in the data. Provided that data is expressed as a p by n matrix, where p is the number of variables and n is the number of observations, there can be two types of non-independence in the data:", "In both cases, the inverse data matrix needed for the solution of Linear Model is singular, because its determinant is close to zero due to correlated variables or observations. This problem is particularly manifested when working with a high-dimensional data (p >> n) where variables can become redundant and correlated, this is known as the Curse of Dimensionality.", "To overcome the problem of non-independent variables, one can for example select most informative variables with LASSO, Ridge or Elastic Net regression, while the non-independence among statistical observations can be taking into account via Random Effects modelling within the Linear Mixed Model.", "I covered a few variable selection methods including LASSO in my post Select Features for OMICs Integration. In the next section, we will see an example of longitudinal data where grouping of data points should be addressed through the Random Effects modelling.", "LMM and Random Effects modeling are widely used in various types of data analysis in Life Sciences. One example is the GCTA tool that contributed a lot to the research of long-standing problem of Missing Heritability. The idea of GCTA is to fit genetic variants with small effects all together as Random Effect withing LMM framework. Thanks to the GCTA model the problem of Missing Heritability seems to be solved at least for Human Height.", "Another popular example from computational biology is the Differential Gene Expression analysis with DESeq / DESeq2 R package that does not really run LMM but performs a variance stabilization/shrinkage that is one of essential points of LMM. The advantage of this approach is that lowly expressed genes can borrow some information from the highly expressed genes that allows for their more stable and robust testing.", "Finally, LMM is one of the most popular analytical techniques in Evolutionary Science and Ecology where they use the state-of-the-art MCMCglmm package for estimating e.g. trait heritability.", "As we concluded previously, LMM should be used when there is some sort of clustering among statistical observations / samples. This can be, for example, due to different geographic locations where the samples were collected, or different experimental protocols that produced the samples. Batch-effects in Biomedical Sciences is an example of such a grouping factor that leads to non-independence between statistical observations. If not properly corrected for, batch-effects in RNAseq data can lead to totally opposite co-expression pattern between two genes (Simpson\u2019s paradox).", "Another example can be a genetic relation between individuals. Finally, this can be repetitive measurements performed on the same individuals but at different time points, i.e. technical (not biological) replicates.", "As an example of such clustering, we will consider a sleep deprivation study where sleeping time of 18 individuals was restricted, and a Reaction of their organism on a series of tests was measured during 10 days. The data includes three variables: 1) Reaction, 2) Days, 3) Subject, i.e. the same individual was followed during 10 days. To check how the overall reaction of the individuals changed as a response to the sleep deprivation, we will fit an Ordinary Least Squares (OLS) Linear Regression with Reaction as a response variable and Days as a predictor / explanatory variable with lm and display it with ggplot.", "We can observe that Reaction vs. Days has a increasing trend but with a lot of variation between days and individuals. Looking at the summary of the linear regression fit, we conclude that the slope is significantly different from zero, i.e. there is a statistically significant increasing relation between Reaction and Days. The grey area around the fitting line represents 95% confidence interval according to the formula:", "The magic number 1.96 originates from the Gaussian distribution and reflects a Z-score value covering 95% of the data in the distribution. To demonstrate how the confidence intervals are calculated under the hood by ggplot we will implement an identical Linear Regression fit in plain R using predict function.", "However, there is a problem with the fit above. The Ordinary Least Squares (OLS) assumes that all the observations are independent, which will result in uncorrelated and hence Normally distributed residuals. However, we know that the data points on the plot belong to 18 individuals (10 for each), i.e. the data points cluster within individuals and therefore are not independent. As an alternative way, we can fit linear model (lm) for each individual separately.", "We can see that most of the individuals have increasing Reaction profile while some have a neutral or even decreasing profile. Doesn\u2019t it looks strange that the overall Reaction increases while individual slopes might be decreasing? Is the fit above really good enough?", "Did we capture all the variation in the data with the naive Ordinary Least Squares (OLS) Linear Regression model?", "The answer is NO because we have not taken the non-independence between data points into account. As we will see later, we can do it much better with a Linear Mixed Model (LMM) that accounts for non-independence between the samples via Random Effects. Despite the term \u201cRandom Effects\u201d might sound mysterious, we will show below that it is essentially equivalent to introducing one more fitting parameter in the Maximum Likelihood optimization.", "The naive linear fit that we used above is called Fixed Effects modeling as it fixes the coefficients of the Linear Regression: Slope and Intercept. In contrast Random Effects modeling allows for individual level Slope and Intercept, i.e. the parameters of Linear Regression are no longer fixed but have a variation around their mean values.", "This concept reminds a lot about Bayesian statistics where the parameters of a model are random while the data is fixed, in contrast to Frequentist approach where parameters are fixed but the data is random. Indeed, later we will show that we obtain similar results with both Frequentist Linear Mixed Model and Bayesian Hierarchical Model. Another strength of LMM and Random Effects is that the fit is performed on all individuals simultaneously in the context of each other, that is all individual fits \u201cknow\u201d about each other. Therefore, the slopes, intercepts and confidence intervals of the individual fits are affected by their common statistic, shared variance, this is called shrinkage toward the mean, we will cover it in more details when deriving LMM from scratch in the next post.", "We will fit LMM with random slopes and intercepts for the effect of Days for each individual (Subject) using lmer function from lme4 R package. This will correspond to adding the (Days | Subject) term to the linear model Reaction ~ Days that was previously used inside the lm function.", "We can immediately see two types of statistics reported: Fixed and Random Effects. The slope and intercept values for Fixed Effects look fairly similar to the ones obtained above with the OLS Linear Regression. On the other hand, the Random Effects statistics is where the adjustment for non-independence between samples occurs. We can see two types of variance reported: the one shared across slopes and intercepts, Name = (Intercept) and Name = Days, that reflects grouping the data points by Subject, and a Residual variance that remains un-modelled, i.e. we can not further reduce this variance within the given model. Further, comparing the Residual errors between Fixed (lm) and Random (lmer) effects models, we can see that the Residual error decreased for the Random Effects model meaning that we captured more variation in the response variable with the Random Effects model. The same conclusion can be drawn from comparing AIC and BIC values for the two models, again the LMM with Random Effects simply fits the data better. Now let us visualize the difference between Fixed Effects modeling vs. LMM modeling.", "For this reason, we need to visualize confidence intervals of the LMM model. The standard way to build confidence intervals in the Frequentist / Maximum Likelihood framework is via bootstrapping. We will start with the population level (overall / average) fit, and re-run it a number of times using resampling with replacement and randomly removing 75% of samples for each iteration. At each iteration I am going to save LMM fit statistics. After the bootstrapped statistics have been accumulated, I am going to make two plots: first, showing bootstrapped LMM fits against the naive Fixed Effects fit used in the previous section; second, from the accumulated bootstrapped LMM fits I will compute the median, i.e. 50% percentile, as well as 5% and 95% percentiles that will determine the confidence intervals of the population level LMM fit, this will again be plotted versus the naive Fixed Effects fit.", "Above, the Fixed Effects fit (blue line + grey 95% confidence intervals area) is displayed together with the computed bootstrapped LMM fits (left plot), and the summary statistics (percentiles) of the bootstrapped LMM fits (right plot). We can observe that the population level LMM fit (lmer, red line, right plot) is very similar to the Fixed Effects fit (lm, blue line on both plots), the difference is hardly noticeable, they overlap well. However, the computed bootstrapped fits (black thick lines, left plot) and confidence intervals for LMM (red dashed line, right plot) are a bit wider than for the Fixed Effects fit (grey area on both plots). This difference is partially due to the fact that the Fixed Effects fit does not account for individual level variation in contrast to LMM that accounts for both population and individual level variations.", "Another interesting thing is that we observe variations of Slope and Intercept around their mean values:", "Therefore, one can hypothesize that the bootstrapping procedure for building confidence intervals within Frequentist framework can be viewed as allowing the slopes and intercepts to follow some initial (Prior) distributions, and then sampling their plausible values from the distributions. This sounds much like Bayesian statistics. Indeed, bootstrapping is very similar to the working horse of Bayesian statistics which is Markov Chain Monte Carlo (MCMC). In other words, Frequentist analysis with bootstrapping is to a large extent equivalent to Bayesian analysis, we will revisit this later in more details.", "What about individual slopes, intercepts and confidence intervals for each of the 18 individuals from the sleep deprivation study? Here we again plot their Fixed Effects statistics together with the LMM statistics.", "Again, red solid and dashed lines correspond to the LMM fit while blue solid line and the grey area depict Fixed Effects Model. We can see that individual LMM fits (lmer) and their confidence intervals might be very different from the Fixed Effects (lm) model. In other words the individual fits are \u201cshrunk\u201d toward their common population level mean / median, all the fits help each other to have more stable and resembling population level slopes, intercepts and confidence intervals. In the next post, when deriving LMM from scratch, we will understand that this shrinkage toward the mean effect is achieved by adding one more fitting parameter (shared variance) in the Maximum Likelihood optimization procedure.", "Before moving to the Bayesian Multilevel Models, let us briefly introduce the major differences between Frequentist and Bayesian approaches. Frequentist fit used by LMM through lme4 / lmer is based on the Maximum Likelihood principle, where we maximize the likelihood L(y) of observing the data y, which is equivalent to minimizing residuals of the model, the Ordinary Least Squares approach. In contrast, Bayesian linear model is based on Maximum Posterior Probability principle, where we assume the data to be distributed with some likelihood L(y), and add a Prior assumption on the parameters of the Linear Model.", "Here we calculate a probability distribution of parameters (and not the data) of the model, which automatically gives us uncertainties (Credible Intervals) on the parameters.", "Linear Mixed Models (LMM) with Bayesian Prior distributions applied to the parameters are called Bayesian Multilevel Models or Bayesian Hierarchical Models. Here, for implementing Bayesian fitting, we will use brms R package that has an identical to lme4 / lmer syntax. However, an important difference to remember is that fitting LMM via lme4 / lmer applies Maximum Likelihood (ML) principle, i.e. it does not use prior assumptions about the parameters (or one case say, it uses flat Priors), while Bayesian Multilevel Model in brms sets reasonable priors reflecting the data. Another thing worth mentioning is that brms uses probabilistic programming language Stan under the hood. We start with Bayesian population level fitting using brms and display the results:", "Above, we again plot the Fixed Effects population level fit by the blue line and grey area for confidence intervals, we also add the population level Bayesian Multilevel Model using the solid red line for median and red dashed lines for credible intervals. As for the case of bootstrapped LMM fit, we can conclude that population level Bayesian Multilevel fit perfectly overlaps with the Fixed Effects fit, while the Bayesian credible intervals are somewhat wider than the 95% confidence intervals of the Fixed Effects fit. What about individual fits?", "Similarly to the individual bootstrapped Frequentist LMM fits, we can see that the individual Bayesian fits with brms (red solid lines) do not always converge to the Fixed Effects Frequentist fits (blue solid lines), but rather \u201ctry\u201d to align with the overall population level fit (previous plot) in order to be as similar to each other as possible. The Bayesian credible intervals look again sometimes very different compared to the Frequentist Fixed Effects confidence intervals. This is the result of using Bayesian Priors and accounting for non-normality and non-independence in the data via the multi-level modeling.", "In this post, we have learnt that the Frequentist Linear Mixed Model (LMM) and Bayesian Multilevel (Hierarchical) Model are used to account for non-independence and hence non-normality of data points. The models usually provide a better fit and explain more variation in the data compared to the Ordinary Least Squares (OLS) linear regression model (Fixed Effect). While the population level mean fit of the models typically converges to the Fixed Effect model, the individual fits as well as credible and confidence intervals can be very different reflecting better accounting for non-normality in data.", "In the comments below, let me know which analytical techniques from Life Sciences seem especially mysterious to you and I will try to cover them in the future posts. Check the codes from the post on my Github. Follow me at Medium Nikolay Oskolkov, in Twitter @NikolayOskolkov and do connect in Linkedin. In the next post, we are going to derive the Linear Mixed Model and program it from scratch from the Maximum Likelihood, stay tuned.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F350950a82911&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nikolay-oskolkov.medium.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8570b484f56c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&user=Nikolay+Oskolkov&userId=8570b484f56c&source=post_page-8570b484f56c----350950a82911---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences"}, {"url": "https://en.wikipedia.org/wiki/Simpson%27s_paradox", "anchor_text": "Image source: Wikipedia Simpson\u2019s Paradox"}, {"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences?source=post_page---------------------------", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences"}, {"url": "https://en.wikipedia.org/wiki/Frequentist_inference", "anchor_text": "Frequentist statistics"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "from scratch"}, {"url": "https://en.wikipedia.org/wiki/Multicollinearity", "anchor_text": "multicollinearity"}, {"url": "https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html", "anchor_text": "LASSO"}, {"url": "http://anythingbutrbitrary.blogspot.com/2012/06/random-regression-coefficients-using.html", "anchor_text": "image source"}, {"url": "https://towardsdatascience.com/select-features-for-omics-integration-511390b7e7fd", "anchor_text": "Select Features for OMICs Integration"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3014363/", "anchor_text": "GCTA tool"}, {"url": "https://www.biorxiv.org/content/10.1101/588020v1", "anchor_text": "solved at least for Human Height"}, {"url": "https://www.nature.com/news/2008/081105/pdf/456018a.pdf", "anchor_text": "B.Maher, Nature, volume 456, 2008"}, {"url": "https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106", "anchor_text": "DESeq"}, {"url": "http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html", "anchor_text": "DESeq2"}, {"url": "https://cran.r-project.org/web/packages/MCMCglmm/index.html", "anchor_text": "MCMCglmm"}, {"url": "https://en.wikipedia.org/wiki/Simpson's_paradox", "anchor_text": "Simpson\u2019s paradox"}, {"url": "https://pubmed.ncbi.nlm.nih.gov/12603781/", "anchor_text": "sleep deprivation study"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_probability", "anchor_text": "Bayesian statistics"}, {"url": "https://en.wikipedia.org/wiki/Frequentist_inference", "anchor_text": "Frequentist"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "next post"}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "AIC"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_information_criterion", "anchor_text": "BIC"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "next post"}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms"}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms"}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms"}, {"url": "https://mc-stan.org/", "anchor_text": "Stan"}, {"url": "https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf", "anchor_text": "brms"}, {"url": "https://github.com/NikolayOskolkov/HowLinearMixedModelWorks", "anchor_text": "Github"}, {"url": "https://medium.com/u/8570b484f56c?source=post_page-----350950a82911--------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "http://linkedin.com/in/nikolay-oskolkov-abb321186?source=post_page---------------------------", "anchor_text": "Linkedin"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "derive the Linear Mixed Model and program it from scratch from the Maximum Likelihood"}, {"url": "https://medium.com/tag/data-science?source=post_page-----350950a82911---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----350950a82911---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----350950a82911---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/stats-ml-life-sciences?source=post_page-----350950a82911---------------stats_ml_life_sciences-----------------", "anchor_text": "Stats Ml Life Sciences"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----350950a82911---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&user=Nikolay+Oskolkov&userId=8570b484f56c&source=-----350950a82911---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&user=Nikolay+Oskolkov&userId=8570b484f56c&source=-----350950a82911---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F350950a82911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----350950a82911---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----350950a82911--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----350950a82911--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----350950a82911--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----350950a82911--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----350950a82911--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----350950a82911--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----350950a82911--------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "https://nikolay-oskolkov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8570b484f56c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&user=Nikolay+Oskolkov&userId=8570b484f56c&source=post_page-8570b484f56c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff4a74ad409c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-linear-mixed-model-works-350950a82911&newsletterV3=8570b484f56c&newsletterV3Id=f4a74ad409c6&user=Nikolay+Oskolkov&userId=8570b484f56c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}