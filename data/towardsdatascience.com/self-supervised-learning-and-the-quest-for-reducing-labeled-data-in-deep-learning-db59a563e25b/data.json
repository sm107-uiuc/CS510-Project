{"url": "https://towardsdatascience.com/self-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b", "time": 1683002654.2710981, "path": "towardsdatascience.com/self-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b/", "webpage": {"metadata": {"title": "Self-Supervised Learning and the Quest for Reducing Labeled Data in Deep Learning | by Thalles Silva | Towards Data Science", "h1": "Self-Supervised Learning and the Quest for Reducing Labeled Data in Deep Learning", "description": "Let\u2019s start by considering the popular task of classification in Computer Vision. Take the ImageNet database as an example. It contains 1.3 million images from 1000 different classes. For each one of\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 2}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "Krizhevsky et al", "paragraph_index": 3}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "DeepMind\u2019s AlphaStar", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1710.02298.pdf", "anchor_text": "at this paper by DeepMind", "paragraph_index": 10}, {"url": "https://blog.deeplearning.ai/blog/the-batch-google-achieves-quantum-supremacy-amazon-aims-to-sway-lawmakers-ai-predicts-basketball-plays-face-detector-preserves-privacy-1-0-0-0-0", "anchor_text": "THE BATCH", "paragraph_index": 11}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "deep learning top-5 accuracy is around 1.8%", "paragraph_index": 14}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "adversarial examples", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1710.08864.pdf", "anchor_text": "1 single pixel to completely fool the best deep-learning classifiers", "paragraph_index": 19}, {"url": "https://www.facebook.com/epflcampus/videos/1960325127394608", "anchor_text": "talks on self-supervised learning", "paragraph_index": 26}, {"url": "https://www.facebook.com/722677142/posts/10155934004262143/", "anchor_text": "LeCun", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1902.06162", "anchor_text": "pretext task here", "paragraph_index": 39}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/", "paragraph_index": 48}], "all_paragraphs": ["There is one single thing that every Deep Learning practitioner agrees.", "Deep learning models are data inefficient.", "Let\u2019s start by considering the popular task of classification in Computer Vision. Take the ImageNet database as an example. It contains 1.3 million images from 1000 different classes. For each one of these images, there is a single human-annotated label.", "ImageNet was certainly one of the stepping stones for the current deep learning revival. Most of it started in 2012 with the (Krizhevsky et al) paper. Here, ConvNets, for the first time, beat the current state-of-the-art model by a large margin. Among the competitors, it was the single ConvNet based solution. After that, ConvNets became ubiquitous.", "Before deep learning, the ImageNet challenge has always been considered very difficult. Among the main reasons, its large variability stood out. Indeed, to build handcrafted features that could generalize among so many classes of dogs wasn\u2019t easy.", "However, with deep learning, we soon realized that what made ImageNet so hard was actually the secret ingredient to make deep learning so effective. And that is the abundance of data.", "Nevertheless, after years of deep learning research, one thing became clear. The necessity of large databases for training accurate models became a very important concern. And this inefficiency becomes a bigger problem when human-annotated data is required.", "Moreover, the problem with data is everywhere in current deep learning applications. Take DeepMind\u2019s AlphaStar model as another example.", "AlphaStar is a deep learning system that uses supervised and reinforcement learning to play StarCraft II. During training, AlphaStar only sees raw image pixels from the game console. To train it, DeepMind researchers used a distributed strategy where they could train a huge population of agents in parallel. Each agent experienced at least 200 years of real-time StarCraft play (non-stop). AlphaStar was trained with similar constraints a professional player would have. And it was ranked above 99.8% of active players in the official game server \u2014 a huge success.", "Despite all general-purpose techniques used to train the system, one thing was crucial to successfully build AlphaStar (or pretty much any other RL agent) \u2014 the availability of data. In fact, the best reinforcement learning algorithms require many (but many) trials to achieve human-level performance. And that goes directly opposite to the way we humans learn.", "As a consequence, the great successes came on restricted and well-defined scenarios with massive amounts of available data. Take a look at this paper by DeepMind. The best RL methods need nearly 100 hours (10.8 Million frames) of non-stop playing to reach the same performance level a professional human would on a set of Atari Games. Despite recent improvements, this still seems too much.", "For more information on AlphaStar take a look at this short summary from THE BATCH.", "I could bother you with some more examples, but I guess these 2 speak to the point I want to make.", "Current deep learning is predicated on large-scale data. These systems work like a charm when their environment and constraints are met. However, they also fail catastrophically in some weird situations.", "Let\u2019s return to classification on ImageNet for a bit. To contextualize, the database has an estimated human error rate of 5.1%. On the other hand, the current state-of-the-art deep learning top-5 accuracy is around 1.8%. Thus, one could perfectly argue that deep learning is already better than humans on this task. But is it?", "If that is the case, how can we explain such things?", "These examples, that became very popular on the internet, are called adversarial examples. We can think of it as an optimization task designed to fool a machine learning model. The idea is simple:", "How can we change an image previously classified as a \u201cpanda\u201d so that the classifier thinks it is a \u201cgibbon\u201d?", "We can simply think of it as input examples carefully designed to fool an ML model into making a classification mistake.", "As we can see, the optimization is so effective that we can\u2019t perceive (with naked eyes) the difference between the real (left) and the adversarial (right) images. Indeed, the noise, responsible for the misclassification, is not any type of known signal. Instead, it is carefully designed to explore the hidden biases in these models. Moreover, recent studies have shown that in some situations we only need to change 1 single pixel to completely fool the best deep-learning classifiers.", "At this point, we can see that the problems are starting to stack on top of each other. Not only do we need a lot of examples to learn a new task, but we also need to make sure that our models learn the right representations.", "When we see deep learning systems fail like that, an interesting discussion arrives. Obviously, we humans do not get easily fooled by examples like these. But why is that?", "One can argue that when we need to grasp a new task, we don\u2019t actually learn it from scratch. Instead, we use a lot of prior knowledge that we have acquired throughout our lives and experiences.", "We understand about gravity and its implications. We know that if we let a cannonball and a bird feather fall from the same starting point, the cannonball will reach the ground first because of the different effect of the air resistance in both objects. We know that objects are not supposed to float in the air. We understand common sense knowledge about how the world works. You know that if your father has a child, he or she will be your sibling. We know that if we read in a paper that someone was born in the 1900s he/she is probably no longer alive because we know (by observing the world) that people don\u2019t often live more than 120 years.", "We understand causality between events and etc. And most curious, we actually learn many of these high-level concepts very early in life. Indeed, we learn concepts like gravity and inertial with only 6 to 7 months. At this age, interaction with the world is almost none!", "In this sense, it would not be \u201cfair\u201d to compare the performance of algorithms with humans \u2014 some might say.", "In one of his talks on self-supervised learning, Yann LeCun argues that there are at least 3 ways to get knowledge.", "However, if we consider a human infant as an example, interaction at that age is almost none. Nevertheless, infants manage to build an intuitive model of the physics of the world. Thus, high-level knowledge like gravity could only be learned by pure observation \u2014 At least, I haven\u2019t seen any parents teaching physics to a 6-month baby.", "Only later in life, when we master language and start going to school, supervision and interaction (with feedbacks) become more present. But more importantly, when we reach these stages of life, we already have developed a robust model world. And this might be one of the main reasons why humans are so much more data-efficient than current machines.", "As LeCun puts it, reinforcement learning is like the cherry in a cake. Supervised learning is the icing and self-supervised learning is the cake!", "In self-supervised learning, the system learns to predict part of its input from other parts of it input \u2014 LeCun", "Self-supervised learning derives from unsupervised learning. It\u2019s concerned with learning semantically meaningful features from unlabeled data. Here, we are mostly concerned with self-supervision in the context of Computer Vision.", "The general strategy is to transform an unsupervised problem into a supervised task by devising a pretext task. Usually, a pretext task has a general goal. The idea is to make the network capture visual features from images or videos.", "Pretext tasks and common supervised problems share some similarities.", "We know that supervised training requires labels. These, in turn, are usually collected with the effort of human annotators. However, there are many scenarios in which labels are either very expensive or impossible to get. Moreover, we also know that deep learning models are data-hungry by nature. As a direct result, large-scaled labeled datasets have become one of the main walls for further advancements.", "Well, self-supervised learning also requires labels for the training of pretext tasks. However, there is a key difference here. The labels (or pseudo-labels) used to learn pretext tasks have a different characteristic.", "In fact, for self-supervised training, the pseudo-labels are solely derived from the data attributes alone.", "In other words, there is no need for human annotation. Indeed, the main difference between self and supervised learning lies in the source of the labels.", "Recent studies have proposed many pretext tasks. Some of the most common ones include:", "Check out a summary description of each pretext task here.", "During self-supervised training, we challenge the network to learn the pretext task. Again, the pseudo-labels are automatically generated from the data itself and used as the training targets. Once training is over, we often use the learned visual features as transfer knowledge to a second problem \u2014 the downstream task.", "In general, the downstream task can be any supervised problem. The idea is to use the self-supervised features to improve the performance of downstream tasks. Usually, downstream tasks have limited data and overfitting is a big concern. Here, we can see the similarity to common transfer learning using pre-trained ConvNets on large labeled databases like the ImageNet. But with one key advantage.", "With self-supervised training, we can pre-train models on incredibly large databases without worrying about human-labels.", "In addition, there is a stubble difference between pretext and usual classification tasks. In pure classification, the network learns representations with the goal of separating the classes in the feature space. In self-supervised learning, pretext tasks usually challenge the network to learn more general concepts.", "Take the image colorization pretext task as an example. In order to excel in it, the network has to learn general-purpose features that explain many characteristics of the objects in the dataset. These include the objects\u2019 shape, their general texture, worry about light, shadows, occlusions, etc.", "In short, by solving the pretext task, the network will learn semantically meaningful features that can be easily transferred to learn new problems. In other words, the goal is to learn useful representations from unlabeled data before going supervised.", "Self-supervised learning allows us to learn good representations without using large annotated databases. Instead, we can use unlabeled data (which is abundant) and optimize pre-defined pretext tasks. We can then use these features to learn new tasks in which data is scarce.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Vision & Deep Learning. Personal blog: https://sthalles.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdb59a563e25b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----db59a563e25b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thalles.silva?source=post_page-----db59a563e25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca----db59a563e25b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/picjumbo_com-2130229/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=865116", "anchor_text": "free stock photos from www.picjumbo.com"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=865116", "anchor_text": "Pixabay"}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "Krizhevsky et al"}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "DeepMind\u2019s AlphaStar"}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II"}, {"url": "https://arxiv.org/pdf/1710.02298.pdf", "anchor_text": "at this paper by DeepMind"}, {"url": "https://arxiv.org/pdf/1710.02298.pdf", "anchor_text": "Rainbow: Combining Improvements in Deep Reinforcement Learning"}, {"url": "https://blog.deeplearning.ai/blog/the-batch-google-achieves-quantum-supremacy-amazon-aims-to-sway-lawmakers-ai-predicts-basketball-plays-face-detector-preserves-privacy-1-0-0-0-0", "anchor_text": "THE BATCH"}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "deep learning top-5 accuracy is around 1.8%"}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "Attacking Machine Learning with Adversarial Examples"}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "adversarial examples"}, {"url": "https://arxiv.org/pdf/1710.08864.pdf", "anchor_text": "One Pixel Attack for Fooling Deep Neural Networks"}, {"url": "https://arxiv.org/pdf/1710.08864.pdf", "anchor_text": "1 single pixel to completely fool the best deep-learning classifiers"}, {"url": "https://www.youtube.com/watch?v=piYnd_wYlT8&list=PL4-Hw6PNAmgc1NUCsMRbZFOChJmuDnefp&index=12", "anchor_text": "Fooling Image Recognition with Adversarial Examples"}, {"url": "https://drive.google.com/file/d/12pDCno02FJPDEBk4iGuuaj8b2rr48Hh0/view", "anchor_text": "Yann LeCun slides"}, {"url": "https://www.facebook.com/epflcampus/videos/1960325127394608", "anchor_text": "talks on self-supervised learning"}, {"url": "https://drive.google.com/drive/folders/0BxKBnD5y2M8NUXhZaXBCNXE4QlE", "anchor_text": "Yann LeCun"}, {"url": "https://www.facebook.com/722677142/posts/10155934004262143/", "anchor_text": "LeCun"}, {"url": "https://arxiv.org/abs/1902.06162", "anchor_text": "pretext task here"}, {"url": "https://arxiv.org/abs/1902.06162", "anchor_text": "Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey"}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "Attacking Machine Learning with Adversarial Examples"}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----db59a563e25b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----db59a563e25b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----db59a563e25b---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----db59a563e25b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----db59a563e25b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&user=Thalles+Silva&userId=f8db098eb9ca&source=-----db59a563e25b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&user=Thalles+Silva&userId=f8db098eb9ca&source=-----db59a563e25b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdb59a563e25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----db59a563e25b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----db59a563e25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----db59a563e25b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----db59a563e25b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----db59a563e25b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----db59a563e25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/@thalles.silva/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fea9a35433442&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-and-the-quest-for-reducing-labeled-data-in-deep-learning-db59a563e25b&newsletterV3=f8db098eb9ca&newsletterV3Id=ea9a35433442&user=Thalles+Silva&userId=f8db098eb9ca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}