{"url": "https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab", "time": 1683004897.678244, "path": "towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab/", "webpage": {"metadata": {"title": "PyTorch [Tabular] \u2014Multiclass Classification | by Akshaj Verma | Towards Data Science", "h1": "PyTorch [Tabular] \u2014Multiclass Classification", "description": "We will use the wine dataset available on Kaggle. This dataset has 12 columns where the first 11 are the features and the last column is the target column. The data set has 1599 rows. First off, we\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009", "anchor_text": "wine dataset", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/tagged/akshaj-wields-pytorch", "anchor_text": "here.", "paragraph_index": 53}, {"url": "https://www.linkedin.com/in/akshajverma7/", "anchor_text": "LinkedIn", "paragraph_index": 54}, {"url": "https://twitter.com/theairbend3r", "anchor_text": "Twitter", "paragraph_index": 54}, {"url": "https://medium.com/@theairbend3r", "anchor_text": "blogposts", "paragraph_index": 54}], "all_paragraphs": ["We will use the wine dataset available on Kaggle. This dataset has 12 columns where the first 11 are the features and the last column is the target column. The data set has 1599 rows.", "We\u2019re using tqdm to enable progress bars for training and testing loops.", "To make the data fit for a neural net, we need to make a few adjustments to it.", "First off, we plot the output rows to observe the class distribution. There\u2019s a lot of imbalance here. Classes 3, 4, and 8 have a very few number of samples.", "Next, we see that the output labels are from 3 to 8. That needs to change because PyTorch supports labels starting from 0. That is [0, n]. We need to remap our labels to start from 0.", "To do that, let\u2019s create a dictionary called class2idx and use the .replace() method from the Pandas library to change it. Let\u2019s also create a reverse mapping called idx2class which converts the IDs back to their original classes.", "To create the reverse mapping, we create a dictionary comprehension and simply reverse the key and value.", "In order to split our data into train, validation, and test sets using train_test_split from Sklearn, we need to separate out our inputs and outputs.", "Input X is all but the last column. Output y is the last column.", "To create the train-val-test split, we\u2019ll use train_test_split() from Sklearn.", "First we\u2019ll split our data into train+val and test sets. Then, we\u2019ll further split our train+val set to create our train and val sets.", "Because there\u2019s a class imbalance, we want to have equal distribution of all output classes in our train, validation, and test sets. To do that, we use the stratify option in function train_test_split().", "Neural networks need data that lies between the range of (0,1). There\u2019s a ton of material available online on why we need to do it.", "To scale our values, we\u2019ll use the MinMaxScaler() from Sklearn. The MinMaxScaler transforms features by scaling each feature to a given range which is (0,1) in our case.", "Notice that we use .fit_transform() on X_train while we use .transform() on X_val and X_test.", "We do this because we want to scale the validation and test set with the same parameters as that of the train set to avoid data leakage. fit_transform calculates scaling values and applies them while .transform only applies the calculated values.", "Once we\u2019ve split our data into train, validation, and test sets, let\u2019s make sure the distribution of classes is equal in all three sets.", "To do that, let\u2019s create a function called get_class_distribution() . This function takes as input the obj y , ie. y_train, y_val, or y_test. Inside the function, we initialize a dictionary which contains the output classes as keys and their count as values. The counts are all initialized to 0.", "We then loop through our y object and update our dictionary.", "Once we have the dictionary count, we use Seaborn library to plot the bar charts. The make the plot, we first convert our dictionary to a dataframe using pd.DataFrame.from_dict([get_class_distribution(y_train)]) . Subsequently, we .melt() our convert our dataframe into the long format and finally use sns.barplot() to build the plots.", "We\u2019ve now reached what we all had been waiting for!", "First up, let\u2019s define a custom dataset. This dataset will be used by the dataloader to pass our data into our model.", "We initialize our dataset by passing X and y as inputs. Make sure X is a float while y is long.", "Because there\u2019s a class imbalance, we use stratified split to create our train, validation, and test sets.", "While it helps, it still does not ensure that each mini-batch of our model see\u2019s all our classes. We need to over-sample the classes with less number of values. To do that, we use the WeightedRandomSampler.", "First, we obtain a list called target_list which contains all our outputs. This list is then converted to a tensor.", "Then, we obtain the count of all classes in our training set. We use the reciprocal of each count to obtain it\u2019s weight. Now that we\u2019ve calculated the weights for each class, we can proceed.", "WeightedRandomSampler expects a weight for each sample. We do that using as follows.", "Finally, let\u2019s initialize our WeightedRandomSampler. We\u2019ll call this in our dataloader below.", "Before we proceed any further, let\u2019s define a few parameters that we\u2019ll use down the line.", "For train_dataloader we\u2019ll use batch_size = 64 and pass our sampler to it. Note that we\u2019re not using shuffle=True in our train_dataloader because we\u2019re already using a sampler. These two are mutually exclusive.", "For test_dataloader and val_dataloader we\u2019ll use batch_size = 1 .", "Let\u2019s define a simple 3-layer feed-forward network with dropout and batch-norm.", "Initialize the model, optimizer, and loss function. Transfer the model to GPU. We\u2019re using the nn.CrossEntropyLoss because this is a multiclass classification problem. We don\u2019t have to manually apply a log_softmax layer after our final layer because nn.CrossEntropyLoss does that for us. However, we need to apply log_softmax for our validation and testing.", "Before we start our training, let\u2019s define a function to calculate accuracy per epoch.", "This function takes y_pred and y_test as input arguments. We then apply log_softmax to y_pred and extract the class which has a higher probability.", "After that, we compare the the predicted classes and the actual classes to calculate the accuracy.", "We\u2019ll also define 2 dictionaries which will store the accuracy/epoch and loss/epoch for both train and validation sets.", "You can see we\u2019ve put a model.train() at the before the loop. model.train() tells PyTorch that you\u2019re in training mode.", "Well, why do we need to do that? If you\u2019re using layers such as Dropout or BatchNorm which behave differently during training and evaluation (for example; not use dropout during evaluation), you need to tell PyTorch to act accordingly.", "Similarly, we\u2019ll call model.eval() when we test our model. We\u2019ll see that below.", "Back to training; we start a for-loop. At the top of this for-loop, we initialize our loss and accuracy per epoch to 0. After every epoch, we\u2019ll print out the loss/accuracy and reset it back to 0.", "Then we have another for-loop. This for-loop is used to get our data in batches from the train_loader.", "We do optimizer.zero_grad() before we make any predictions. Since the backward() function accumulates gradients, we need to set it to 0 manually per mini-batch.", "From our defined model, we then obtain a prediction, get the loss(and accuracy) for that mini-batch, perform back-propagation using loss.backward() and optimizer.step() .", "Finally, we add all the mini-batch losses (and accuracies) to obtain the average loss (and accuracy) for that epoch. We add up all the losses/accuracies for each mini-batch and finally divide it by the number of mini-batches ie. length of train_loader to obtain the average loss/accuracy per epoch.", "The procedure we follow for training is the exact same for validation except for the fact that we wrap it up in torch.no_grad and not perform any back-propagation. torch.no_grad() tells PyTorch that we do not want to perform back-propagation, which reduces memory usage and speeds up computation.", "To plot the loss and accuracy line plots, we again create a dataframe from the accuracy_stats and loss_stats dictionaries.", "After training is done, we need to test how our model fared. Note that we\u2019ve used model.eval() before we run our testing code. To tell PyTorch that we do not want to perform back-propagation during inference, we use torch.no_grad(), just like we did it for the validation loop above.", "We start by defining a list that will hold our predictions. Then we loop through our batches using the test_loader. For each batch \u2014", "We create a dataframe from the confusion matrix and plot it as a heatmap using the seaborn library.", "Finally, we print out the classification report which contains the precision, recall, and the F1 score.", "Thank you for reading. Suggestions and constructive criticism are welcome. :)", "This blogpost is a part of the series \u2014 \u201d How to train you Neural Net\u201d. You can find the series here.", "You can find me on LinkedIn and Twitter. If you liked this, check out my other blogposts.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9f8211a123ab&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@theairbend3r?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theairbend3r?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Akshaj Verma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe331296a10f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&user=Akshaj+Verma&userId=e331296a10f2&source=post_page-e331296a10f2----9f8211a123ab---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/akshaj-wields-pytorch", "anchor_text": "How to train your neural net"}, {"url": "https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009", "anchor_text": "wine dataset"}, {"url": "https://towardsdatascience.com/tagged/akshaj-wields-pytorch", "anchor_text": "here."}, {"url": "https://www.linkedin.com/in/akshajverma7/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/theairbend3r", "anchor_text": "Twitter"}, {"url": "https://medium.com/@theairbend3r", "anchor_text": "blogposts"}, {"url": "https://www.buymeacoffee.com/theairbend3r", "anchor_text": ""}, {"url": "https://medium.com/tag/data-science?source=post_page-----9f8211a123ab---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9f8211a123ab---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9f8211a123ab---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----9f8211a123ab---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/akshaj-wields-pytorch?source=post_page-----9f8211a123ab---------------akshaj_wields_pytorch-----------------", "anchor_text": "Akshaj Wields Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&user=Akshaj+Verma&userId=e331296a10f2&source=-----9f8211a123ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&user=Akshaj+Verma&userId=e331296a10f2&source=-----9f8211a123ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9f8211a123ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9f8211a123ab---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9f8211a123ab--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9f8211a123ab--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9f8211a123ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theairbend3r?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theairbend3r?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akshaj Verma"}, {"url": "https://medium.com/@theairbend3r/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "373 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe331296a10f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&user=Akshaj+Verma&userId=e331296a10f2&source=post_page-e331296a10f2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F817f2ff0f62b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-tabular-multiclass-classification-9f8211a123ab&newsletterV3=e331296a10f2&newsletterV3Id=817f2ff0f62b&user=Akshaj+Verma&userId=e331296a10f2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}