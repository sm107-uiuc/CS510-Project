{"url": "https://towardsdatascience.com/thoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2", "time": 1682997243.6515, "path": "towardsdatascience.com/thoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2/", "webpage": {"metadata": {"title": "Thoughts on the Two Cultures of Statistical Modeling | by Will Koehrsen | Towards Data Science", "h1": "Thoughts on the Two Cultures of Statistical Modeling", "description": "In the paper: \u201cStatistical Modeling: The Two Cultures\u201d, Leo Breiman \u2014 developer of the random forest as well as bagging and boosted ensembles \u2014 describes two contrasting approaches to modeling in\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www2.math.uu.se/~thulin/mm/breiman.pdf", "anchor_text": "\u201cStatistical Modeling: The Two Cultures\u201d", "paragraph_index": 0}, {"url": "https://scholar.google.com/citations?user=mXSv_1UAAAAJ&hl=en", "anchor_text": "Leo Breiman", "paragraph_index": 0}, {"url": "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf", "anchor_text": "random forest", "paragraph_index": 0}, {"url": "https://www.stat.berkeley.edu/~breiman/bagging.pdf", "anchor_text": "bagging", "paragraph_index": 0}, {"url": "https://pdfs.semanticscholar.org/814c/f172298d11db0ac9b839440ed8f3db93e438.pdf", "anchor_text": "boosted", "paragraph_index": 0}, {"url": "https://cortexintel.com", "anchor_text": "Cortex Building Intelligence", "paragraph_index": 3}, {"url": "http://www2.math.uu.se/~thulin/mm/breiman.pdf", "anchor_text": "reading the article", "paragraph_index": 4}, {"url": "https://www.ibm.com/support/knowledgecenter/en/SS3RA7_17.0.0/clementine/nodes_statisticalmodels.html", "anchor_text": "modeling", "paragraph_index": 5}, {"url": "https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes", "anchor_text": "black box", "paragraph_index": 12}, {"url": "https://github.com/slundberg/shap/tree/master/shap", "anchor_text": "SHAP values", "paragraph_index": 18}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Local Interpretable Model-Agnostic Explanations (LIME)", "paragraph_index": 18}, {"url": "https://www.kaggle.com/learn/machine-learning-explainability", "anchor_text": "Machine Learning Explainability from Kaggle", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Tautology_(language)", "anchor_text": "tautology", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Circular_reasoning", "anchor_text": "circular reasoning", "paragraph_index": 23}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "model interpretation", "paragraph_index": 24}, {"url": "https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/", "anchor_text": "variance inflation factor", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Mutual_information", "anchor_text": "mutual information", "paragraph_index": 26}, {"url": "http://www.thestargarden.co.uk/Heliocentric-models-of-the-Solar-System.html", "anchor_text": "At each step, the model became more complicated", "paragraph_index": 30}, {"url": "https://www.compoundchem.com/2016/10/13/atomicmodels/", "anchor_text": "model of the atom", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Complexity", "anchor_text": "complexity", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "new techniques", "paragraph_index": 31}, {"url": "https://www.isbglasgow.com/10-greatest-scientific-discoveries-and-inventions-of-21st-century/", "anchor_text": "achieved great success", "paragraph_index": 32}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 45}], "all_paragraphs": ["In the paper: \u201cStatistical Modeling: The Two Cultures\u201d, Leo Breiman \u2014 developer of the random forest as well as bagging and boosted ensembles \u2014 describes two contrasting approaches to modeling in statistics:", "At the time of writing in 2001, Breiman estimated 98% of statisticians were in the data modeling group while 2% (himself included) were in the algorithmic modeling culture. The paper is written as a call to arms for statisticians to stop relying solely on data modeling \u2014 which leads to \u201cmisleading conclusions\u201d and \u201cirrelevant theory\u201d \u2014 and embrace algorithmic modeling to solve novel real-world problems arising from massive data sets. Breiman was an academic, working as a statistician at Berkely for 21 years, but he had previously worked for 13 years as a freelance consultant giving him a well-formed perspective on how statistics can be useful in industry.", "Breiman was frustrated because he knew data models were not up to solving new challenges from large-scale data collection and he felt academic statistics was becoming irrelevant by refusing to adopt new tools: complex algorithms with high predictive performance but low explainability. While machine learning and statistics have changed in the 18 years since the paper (I don\u2019t know if the 98/2 split still holds), several interesting points brought up are still relevant to practicing machine learning today particularly for those making the transition from academia to industry. Among the takeaways are:", "The overall lesson from the paper is in line with what I\u2019ve learned applying machine learning in industry (at Cortex Building Intelligence): focus first on model accuracy, and only after building a high-performance model think about explaining it. A highly-complex, accurate model that can\u2019t be fully explained is more valuable than a simple, linear model with no predictive accuracy that we completely understand.", "Following are some of my thoughts on Breiman\u2019s paper. Keep in mind, these are based on much less experience \u2014 1 year in an academic setting (2018) and just over 1 year in industry (2018-present) \u2014 than Breiman was writing with. I\u2019d recommend reading the article (and the criticism it includes) to form your own opinions. Feel free to add comments or experiences about the paper or associated topics in machine learning to this article. Although machine learning may seem to be moving incredibly quickly, there is still valuable information to learn from older papers and books, especially those written by a figure such as Breiman, who played a pivotal role in shaping the field.", "Before we can discuss what makes a good model, we have to understand the 2 goals of modeling:", "The exact balance of these two objectives is situationally dependent: if you\u2019re trying to predict stocks, you might only care if the model is accurate. In a medical setting, learning about the causes of a disease may be the main focus of building a model. Breiman makes the case that the algorithmic approach actually has the upper hand over the data modeling approach for both goals.", "A researcher using a data modeling approach (Breiman considers data models to be linear regression for regression and discriminant analysis or logistic regression for classification) first constructs a plausible mechanism for how the data is generated. That is, the researcher thinks up a linear equation relating the independent variables (features) to the dependent variables (target) from intuition, experience, or domain knowledge.", "The coefficients in the model (feature weights) are found by fitting it to the dataset. The resulting linear equation represents the actual data-generating mechanism \u2014 the black box through which nature produces the dependent and independent variable values. The coefficients are used as a measure of variable importance, showing the effect of a feature on the response.", "Validation, if it occurs at all, in data modeling is done by goodness-of-fit measures such as R\u00b2 or residual analysis \u2014 both measured on the training dataset. Little thought is given to predictive accuracy. Instead, the emphasis is on how well the model explains the phenomenon under study. If the p-values on the coefficients are low enough, then they are \u201csignificant\u201d and the model \u201cbecomes truth\u201d in Breiman\u2019s words with any conclusions made from the model infallible. The whole process is guided by intuition and subjective decisions: instead of letting the data speak, the researchers impose their own personal theories through choices such as which features to use and which data points to throw out as outliers.", "Breiman quotes a textbook by Mosteller and Tukey to summarize his disappointment with data modeling: \u201cThe whole area of guided regression is fraught with intellectual, statistical, computational, and subject matter difficulties.\u201d In other words, data modeling with simple linear models and intuition is not an objective way to learn from data. Yet, this is the approach taken by 98% of academic statisticians according to Breiman! It\u2019s no wonder he was frustrated with his own field.", "The algorithmic modeling approach revolves around one concern: what is the performance of a model on the validation data? For choosing a model, there is no consideration for whether the model represents the underlying mechanism generating the data, only on whether the model can make reliable estimates on new (or hold-out) observations. Breiman credits the rise of the algorithmic culture to the invention of new algorithms such as the random forest (his own work), support vector machine, and neural nets. These are all models that \u2014 at least at the time \u2014 were not well-understood theoretically, but yielded extraordinary predictive accuracy, particularly on large datasets.", "A central idea in the algorithmic community is that nature is a black-box, and our models are also a black box, although one that can give us predictions on new observations. There is little use trying to explain a model that is not accurate, so concentrate primarily on building the model with the greatest performance before focusing on learning anything about nature from it. An accurate model, no matter how complex, is more useful for both prediction (clearly) and for information gathering, because a simple, interpretable model with low accuracy is not capturing anything useful about the problem.", "The algorithmic culture did not grow out of academic statistics, but rather from \u201cyoung computer scientists, physicists, and engineers plus a few aging statisticians.\u201d In other words, people who weren\u2019t afraid to adopt or even invent new techniques for solving novel problems. These were the practitioners rather than the theorists, and they were using neural nets and random forests to solve problems across domains from medicine to genomics to stock markets to astronomy.", "From his time as a consultant, Breiman saw the value of algorithmic modeling, adopting the best tool to solve the problem. He came to see computers as an invaluable tool because of their ability to apply complex techniques to large quantities of data. Upon returning to academia, he was disappointed by the reliance on data models and the lack of emphasis on predictive accuracy. Even if your primary goal is to extract information about nature through modeling, the first priority should be accuracy. A complex, accurate model can teach us about the problem domain because it must have captured some part of the relationship between features and target. Additionally, the algorithmic modeling community has made several interesting discoveries simply through solving problems.", "The first few models I built on the job, I was troubled by a recurring pattern. I was trying to select the \u201cbest\u201d features by measuring validation scores, but, every time I would try a different subset, the overall validation score would stay nearly the same. This was puzzling but occurred repeatedly: varying the features and even trying different values for hyperparameters yielded similar performance. Breiman says that this is nothing to be worried about: for most problems, when using complex models, there are many features and hyperparameters that give roughly the same performance. In other words, the idea of a single best model is a fiction so we shouldn\u2019t worry about finding it.", "While this shouldn\u2019t be a concern for algorithmic models, it raises unsettling questions for anyone relying on data models. Because simple linear models cannot deal well with many features, they require extensive feature selection, usually by a combination of intuition and formal methods. The mapping from features to targets created by choosing the features and calculating coefficients through fitting is assumed to represent ground truth, the data-generating process. However, if there are actually many sets of features that will give the same performance, then how can just one be the ultimate source of truth? In reality, there are many equally good models, so choosing only one is not accurately representing the problem.", "What causes the multiplicity of models? My own experience from building models tells me it\u2019s due to correlated features (two variables are positively correlated when they increase or decrease together). Although linear regression assumes input variables are independent, in real-world datasets, almost all features have some degree of correlation, often quite high. Thus, one feature can substitute for another in a model with no drop in accuracy. Building a single data model and calling it the source of truth misses all the other models that would perform just as well. Algorithmic modelers shouldn\u2019t worry about selecting features: just give them all to your random forest and let it figure out which ones matter. After training, recognize that the fit model is only one possible representation of the mapping from features to targets.", "This is one area where the paper (again from 2001) shows its age. Breiman makes the common claim that the more complex machine learning models are completely unexplainable (particularly random forests and neural networks). When choosing a model, he says that we always have to trade interpretability for increased accuracy. However, the past few years have seen major strides in explaining complex models, particularly SHAP values and Local Interpretable Model-Agnostic Explanations (LIME). These operate on the general principle of building a complex model, then explaining a small part of it (local) using a simpler model such as linear regression.", "(For a class on interpretable machine learning, see Machine Learning Explainability from Kaggle).", "These model-interpretation techniques can work with any model, from random forests to neural networks and provide reasonable interpretations of individual model predictions. At Cortex, we use SHAP values to explain our estimates for the ideal time for buildings to start their HVAC (heating or air conditioning). We have seen high adoption of our recommendations, in part from these easy to understand explanations.", "Breiman\u2019s concerns about lack of explainability were valid, but, just as new algorithms were invented to handle larger datasets, new techniques were developed to peer into complex black-box machine learning models. It was a matter of the algorithms being developed much faster than interpretations. This makes sense \u2014 we needed to make sure the algorithms were accurate before trying to explain them. There is little purpose in explaining the predictions of an inaccurate model. Now that model interpretation techniques have caught up with the algorithms, we can have both reasoning behind predictions and high predictive accuracy.", "While we are on the point of explanation, we should mention that humans are terrible at explaining their decisions. We do give reasons for individual choices, but there is no way these encompass the full combination of environment, genetics, situation, emotions, neurotransmitters, etc. that actually influence decisions. When we ask someone why they were late to work and they tell us \u201cbecause I took a different subway route\u201d, we accept that and usually stop the questioning there. We fail to dig into the reasoning or ask detailed follow-ups, which would lead to more follow-ups. We would need to know someone\u2019s entire life history to explain fully even one choice they made, so clearly we don\u2019t get complete explanations from humans.", "I\u2019ve found that people want any explanation, no matter how flimsy, rather than no explanation. Even if it\u2019s a tautology (boys will be boys) or circular reasoning (I made many spelling mistakes because my spelling is poor) people will accept any sort of justification. In contrast to human reasons, the SHAP values output by a machine learning model are more comprehensive, showing the exact weight assigned to each variable associated with a single prediction. At this point, I would prefer the numbers from these model explanation techniques than the misleading justifications given by humans.", "Instead of worrying about model explainability, perhaps we should be dealing with the much more difficult problem of figuring out human decision making! We have made more progress in interpreting machine learning output than in figuring out the complex web of influences behind an individual\u2019s particular actions. (I\u2019m being a little light-hearted. I think model interpretation is one of the most promising areas in machine learning and am excited about new tools for explaining machine learning to non-technical audiences. My main point is we\u2019ve come a long way towards explaining models since the paper.)", "3. With algorithmic models, more features can benefit performance", "In graduate data science modeling classes, my professors spent a large amount of time on feature selection using techniques like the variance inflation factor or mutual information. In the lab, I saw plenty of feature selection, almost always guided by intuition rather than standardized procedures. The reason might have been sound: linear models tend to not deal with many features well because they don\u2019t have enough capacity to model all the information from the features \u2014 but the methods used were often subjective resulting in models more human-driven than data-driven.", "By comparison, algorithmic models can gain from more features. Breiman points out that more variables mean more information and an effective model should be able to pick out the signal from the noise. ML methods like the random forest can provide accurate predictions with many features, even when the number of variables exceeds the number of data points, as often occurs in genomic datasets. Rather than expending time trying to intuit the important features to leave in, we can give algorithmic models all the features and let it figure out the ones most relevant to the task. Furthermore, we might actually want to create additional features, engineering them from the existing variables, to help the model extract even more information.", "Intuition has no place in algoritmic modeling culture unlike in data models where it might inform the features that go into the model. If we really want to learn from the data, then we have to trust in the data instead of our subjective opinions. Algorithmic modeling does not require us to perform any arbitrary feature selection: instead, we leave in all the features, or even add more, and get better performance with less effort.", "The final main point actually comes in Breiman\u2019s response to criticism (see below), but it\u2019s a crucial one. As our knowledge of the world advances, we need ever more complicated models for prediction and learning information.", "Early models of the universe placed Earth at the center then shifted to the Sun at the center before settling on our current view, where the Milky Way is only one of billions of galaxies in an enormous (possibly infinite) universe. At each step, the model became more complicated as we collected more information that did not fit existing models. Newton\u2019s Laws of Gravitation worked pretty well for hundreds of years until further observations revealed their limitations. Now, we need Einstein\u2019s Theories of Relativity (both special and general) to ensure that our satellites don\u2019t fall out of the sky and so our GPS system remains accurate.", "As other fields have developed more complex models to deal with new difficulties (the model of the atom is another example), statistics should abandon old, linear models when they have outlived their usefulness. Data models worked well for a small subset of problems, but the challenges we face in data science are now much larger. The techniques used to solve them should correspondingly expand. If the rest of science is moving towards greater complexity, why should we assume statistics can continue to operate with the simplest of models? There are a lot of exciting problems developing and solving them requires using whatever tool is most appropriate, or even inventing new techniques for novel obstacles.", "In the best scientific tradition, Breiman includes a substantial amount of criticism from 4 statisticians as an addendum to the paper and then responds to the feedback. It\u2019s worth reading through the criticism and observing the back and forth (civil) arguments. Science advances through open discussion because no single person has all the right answers. It\u2019s through an iterative process of proposing an idea, subjecting it to criticism, improving the idea in response, gathering more data, and opening the floor to more discussion that science has achieved great success. Following is some of the pushback.", "This is a point Breiman willingly concedes: in some situations, a linear model might be appropriate. For example, if we were modeling distance as a function of rate, then this is a linear relationship: distance = rate * time. However, few phenomena in nature follow such a nice mechanism (even the example above almost never holds in the real world.) Linear models can work in the case of very small datasets with very few features, but quickly become obsolete when working on newer problems in areas such as astronomy, climate change, stock market forecasting, natural language processing, etc. where the datasets are large and contain thousands or more variables.", "The algorithmic culture is not about abandoning data models, it\u2019s using the most appropriate model in any situation. If a linear model records the highest predictive accuracy on a dataset, then it\u2019s selected. Breiman\u2019s point is more along the lines that we should not assume ahead of time which is the correct model. Algorithmic modeling is an all-the-above approach to modeling.", "Overfitting is a fundamental issue in machine learning: parameters are always learned on some dataset, which is not indicative of all data for the problem. By choosing a model with the best validation score, we might be inadvertently choosing a model that generalizes poorly to future data. This is not a problem unique to algorithmic models (although it may be easier to overfit with a more complex model since it has more free parameters to train).", "The solution is not to go backwards to simpler models, but to use more robust validation. I prefer cross-validation, using multiple training/testing subsets so the performance is not biased by one random selection. The model may still be overfitting (this should be called the Kaggle effect because it occurs with literally every competition) but a robust validation setup should give a decent indication of performance on new data. It\u2019s also critical to monitor the ongoing performance of the model in production. Periodically checking that the model accuracy has not degraded will allow you to catch model or data drift. Once this occurs, you need to build a new model, gather additional data, or start over on the problem. Overfitting is a serious problem, but it can be tackled with the right approach.", "Much of Breiman\u2019s arguments about extracting information from a complex model relies on the idea of feature importance. This is not defined in the actual paper, but it is addressed in Breiman\u2019s response to the criticism. His definition rests on accuracy. The importance of a feature is measured by the question: does including the feature in the model improve performance?", "Traditionally, variable importance was determined from the coefficients of a linear model. However, we\u2019ve already seen that multiple features can yield the same performance, so using the learned weights as a measure of importance does not capture any single ground truth.", "The area of variable importance is still not solved. There are problems when variables are collinear (highly correlated) because the feature importance might be split between the features. There is not yet a satisfactory method to determine which variables are the most important, but an accuracy-based approach is less subjective than a weights-based approach. SHAP values also provide a per-prediction measure of variable importance, letting us see the exact impact of each feature value from an observation on the output. Predictive feature importance may not be the \u201ctrue\u201d relevance of a feature in nature, but it can give us relative comparisons between variables.", "Some of the statisticians take aim at the idea that the goal of modeling is prediction, arguing for placing greater emphasis on information gathering. My response is a model that has no predictive accuracy cannot provide any useful information about the problem. It might provide model weights, but if they don\u2019t lead to accurate predictions, why would we try to learn anything from them? Instead, we should focus on accuracy first \u2014 so we know our model has learned something useful \u2014 then try to figure out how the model operates. A model must be accurate to give us useful information!", "There is little point in trying to understand a linear model that cannot outperform a simple no-machine learning baseline. Aim for accuracy, then spend as much time as your domain requires on interpreting the model. It is much better to have an accurate model with no explanation than a model that produces nonsense but presents a clear explanation.", "This paper would have been useful for me moving from an academic data science environment to industry. Initially, I spent a lot of time trying to understand the theory behind models or solve problems through intuition rather than aiming for accuracy and letting the data decide the model. Eventually, I learned the most important point of this paper through experience: focus on accuracy first and then explanation. A model must have high predictive performance before it\u2019s worth using for knowledge extraction.", "What this means in practice (particularly for those in industry) is simple: concentrate on setting up a robust validation scheme and find the model that performs the best. Don\u2019t spend too much time worrying about the theory behind the model until you know it works. Furthermore, experience has shown that many models can produce the same accuracy with different sets of features, additional features can improve the performance of complex algorithms, and there is a balance between model interpretability and accuracy, although new techniques have largely closed the gap.", "We all want explanations when we see a prediction or decision. However, we must admit when our knowledge and brains limit us: we simply cannot process the amount of data now facing us and we have to rely on machines to do most of the reasoning for us. Machine learning is a tool used to solve problems with data and we should use the best tools possible. Statistics is an old field, but that does not mean it has to remain stuck in the past: by adopting the latest algorithms, statisticians, even those in academia, can solve challenging new problems arising in modeling.", "As always, I welcome any feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F72d75a9e06c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://www.pexels.com/photo/background-beautiful-blossom-calm-waters-268533/", "anchor_text": "(Source)"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----72d75a9e06c2---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72d75a9e06c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----72d75a9e06c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72d75a9e06c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=-----72d75a9e06c2---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://www2.math.uu.se/~thulin/mm/breiman.pdf", "anchor_text": "\u201cStatistical Modeling: The Two Cultures\u201d"}, {"url": "https://scholar.google.com/citations?user=mXSv_1UAAAAJ&hl=en", "anchor_text": "Leo Breiman"}, {"url": "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf", "anchor_text": "random forest"}, {"url": "https://www.stat.berkeley.edu/~breiman/bagging.pdf", "anchor_text": "bagging"}, {"url": "https://pdfs.semanticscholar.org/814c/f172298d11db0ac9b839440ed8f3db93e438.pdf", "anchor_text": "boosted"}, {"url": "https://cortexintel.com", "anchor_text": "Cortex Building Intelligence"}, {"url": "http://www2.math.uu.se/~thulin/mm/breiman.pdf", "anchor_text": "reading the article"}, {"url": "https://www.ibm.com/support/knowledgecenter/en/SS3RA7_17.0.0/clementine/nodes_statisticalmodels.html", "anchor_text": "modeling"}, {"url": "https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes", "anchor_text": "black box"}, {"url": "https://github.com/slundberg/shap/tree/master/shap", "anchor_text": "SHAP values"}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Local Interpretable Model-Agnostic Explanations (LIME)"}, {"url": "https://www.kaggle.com/learn/machine-learning-explainability", "anchor_text": "Machine Learning Explainability from Kaggle"}, {"url": "https://en.wikipedia.org/wiki/Tautology_(language)", "anchor_text": "tautology"}, {"url": "https://en.wikipedia.org/wiki/Circular_reasoning", "anchor_text": "circular reasoning"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "model interpretation"}, {"url": "https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/", "anchor_text": "variance inflation factor"}, {"url": "https://en.wikipedia.org/wiki/Mutual_information", "anchor_text": "mutual information"}, {"url": "http://www.thestargarden.co.uk/Heliocentric-models-of-the-Solar-System.html", "anchor_text": "At each step, the model became more complicated"}, {"url": "https://www.compoundchem.com/2016/10/13/atomicmodels/", "anchor_text": "model of the atom"}, {"url": "https://en.wikipedia.org/wiki/Complexity", "anchor_text": "complexity"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "new techniques"}, {"url": "https://www.isbglasgow.com/10-greatest-scientific-discoveries-and-inventions-of-21st-century/", "anchor_text": "achieved great success"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----72d75a9e06c2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----72d75a9e06c2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----72d75a9e06c2---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/statistics?source=post_page-----72d75a9e06c2---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----72d75a9e06c2---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72d75a9e06c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----72d75a9e06c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72d75a9e06c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----72d75a9e06c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72d75a9e06c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----72d75a9e06c2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----72d75a9e06c2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Written by Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "38K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----72d75a9e06c2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-the-two-cultures-of-statistical-modeling-72d75a9e06c2&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----72d75a9e06c2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Will Koehrsen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Hyperparameter Tuning the Random Forest in PythonImproving the Random Forest Part Two"}, {"url": "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "12 min read\u00b7Jan 10, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28d2aa77dd74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----28d2aa77dd74----0-----------------clap_footer----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?source=author_recirc-----72d75a9e06c2----0---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28d2aa77dd74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74&source=-----72d75a9e06c2----0-----------------bookmark_preview----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----72d75a9e06c2----1---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----72d75a9e06c2----1-----------------bookmark_preview----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----72d75a9e06c2----2---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----72d75a9e06c2----2-----------------bookmark_preview----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Will Koehrsen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "Random Forest in PythonA Practical End-to-End Machine Learning Example"}, {"url": "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": "21 min read\u00b7Dec 27, 2017"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24d0893d51c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-in-python-24d0893d51c0&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----24d0893d51c0----3-----------------clap_footer----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0?source=author_recirc-----72d75a9e06c2----3---------------------167df5c0_3eaf_4f98_b638_3c36ef370d2b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "61"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24d0893d51c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-in-python-24d0893d51c0&source=-----72d75a9e06c2----3-----------------bookmark_preview----167df5c0_3eaf_4f98_b638_3c36ef370d2b-------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "See all from Will Koehrsen"}, {"url": "https://towardsdatascience.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----72d75a9e06c2----0-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----72d75a9e06c2----1-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----0-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----72d75a9e06c2----0---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----72d75a9e06c2----0-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b717 min read\u00b75 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----1-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----72d75a9e06c2----1---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----72d75a9e06c2----1-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----2-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----72d75a9e06c2----2---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----72d75a9e06c2----2-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----3-----------------clap_footer----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----72d75a9e06c2----3---------------------f10b6f79_c06f_471c_9746_e57b064526cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----72d75a9e06c2----3-----------------bookmark_preview----f10b6f79_c06f_471c_9746_e57b064526cc-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----72d75a9e06c2--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}