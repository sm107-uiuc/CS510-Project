{"url": "https://towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22", "time": 1683014941.07761, "path": "towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22/", "webpage": {"metadata": {"title": "The Exploding and Vanishing Gradients Problem in Time Series | by Barak Or, PhD | Towards Data Science", "h1": "The Exploding and Vanishing Gradients Problem in Time Series", "description": "In this post, we focus on deep learning for sequential data techniques. All of us familiar with this kind of data. For example, the text is a sequence of words, video is a sequence of images. More\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f", "anchor_text": "Understanding of Multilayer perceptron (MLP)", "paragraph_index": 18}, {"url": "https://medium.com/@AI_with_Kain?source=post_page-----8f179c4a135f--------------------------------", "anchor_text": "Nitin Kumar Kain", "paragraph_index": 18}, {"url": "https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f", "anchor_text": "Understanding Neural Networks. From neuron to RNN, CNN, and Deep Learning", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/@nigam.vibhor01?source=post_page-----cd88e90e0a90--------------------------------", "anchor_text": "Vibhor Nigam", "paragraph_index": 19}, {"url": "https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c", "anchor_text": "Back-Propagation is very simple. Who made it Complicated?", "paragraph_index": 20}, {"url": "https://medium.com/@14prakash?source=post_page-----97b794c97e5c--------------------------------", "anchor_text": "Prakash Jay", "paragraph_index": 20}], "all_paragraphs": ["In this post, we focus on deep learning for sequential data techniques. All of us familiar with this kind of data. For example, the text is a sequence of words, video is a sequence of images. More challenging examples are from the branch of time series data, with medical information such as heart rate, blood pressure, etc., or finance, with stock price information. The most common AI approaches for time-series tasks with deep learning is the Recurrent Neural Networks (RNNs). The motivation to use RNN lies in the generalization of the solution with respect to time. As sequences have different lengths (mostly), a classical deep learning architecture such as Multy Layers Perceptrons (MLP) can not be applied without modifying it. Moreover, the number of weights in MLP is absolutely huge! Hence, The RNN is commonly used, where the weights are shared during the entire architecture. A simple RNN architecture is shown below, where V, W, and U are the weights matrices, and b is the bias vector.", "If you are not familiar with RNN, backpropagation, or MLP, please feel free to read references [1]-[3] at the end of the post to fill the gap.", "Training an RNN is done by defining a loss function (L) that measures the error between the true label and the output, and minimizes it by using forward pass and backward pass. The following simple RNN architecture summarizes the entire backpropagation through time idea.", "For a single time step, the following procedure is done: first, the input arrives, then it processes trough a hidden layer/state, and the estimated label is calculated. In this phase, the loss function is computed to evaluate the difference between the true label and the estimated label. The total loss function, L, is computed, and by that, the forward pass is finished. The second part is the backward pass, where the various derivatives are calculated.", "The training of RNN is not trivial, as we backpropagate gradients through layers and also through time. Hence, in each time step we have to sum up all the previous contributions until the current one, as given in the equation:", "In this equation, the contribution of a state at time step k to the gradient of the entire loss function L, at time step t=T is calculated. The challenge during the training is in the ratio of the hidden state:", "Two common problems that occur during the backpropagation of time-series data are the vanishing and exploding gradients. The equation above has two problematic cases:", "In the first case, the term goes to zero exponentially fast, which makes it difficult to learn some long period dependencies. This problem is called the vanishing gradient. In the second case, the term goes to infinity exponentially fast, and their value becomes a NaN due to the unstable process. This problem is called the exploding gradient. In the following two sections, we review two approaches to deal with these problems.", "The following \u201ctrick\u201d tries to overcome the vanishing gradient problem by considering a moving window through the training process. It is known that in the backpropagation training scheme, there are a forward pass and a backward pass through the entire sequence to compute the loss and the gradient. By taking a window, we also improve the training performance from the training duration aspect- where we shortcut it.", "This window is called a \u201cchunk\u201d. During the backpropagation process, we run forward and backward through this chunk of a specific size instead of the entire sequence.", "The Truncated BPTT is much faster than the simple BPTT, and also less complex because we don\u2019t make the contribution of the gradients from faraway steps. The minus of this approach is that dependencies of longer than the chunk length, are not taught during the training process. Another disadvantage is the detection of the vanishing gradients. From looking at the learning curve one can assume that the gradient vanishes, but, maybe the task itself is difficult.", "For the vanishing gradient problem, many other approaches have been suggested, to mention a few of them:", "3. Initialize the weight matrix, W, with an orthogonal matrix, and use this through the entire training (multiplications of orthogonal matrices doesn\u2019t explode or vanish).", "Considering g as the gradient of the loss function with respect to all network parameters. Now, define some threshold and run the following clip condition in the background of the training process. It is a very simple and very effective condition.", "By applying the gradient clipping, we do not change the gradient direction, but only its magnitude. As the hidden state (h) derivative is the part who cause the exploding gradient, it enough to clip the following entity:", "The threshold is a key parameter the designer should manually define. We aim to choose the highest threshold which solves the exploding gradient problem, by looking at the curve of the gradient norm:", "In this post, we explore the vanishing and exploding gradients problem in simple RNN architecture. These two problems belong to the class of open-problem in machine learning and the research in this pattern is very active. The Truncated BPTT and the gradient clipping approaches were discussed, with some tips for implementation.", "Barak Or received the B.Sc. (2016), M.Sc. (2018) degrees in aerospace engineering, and also B.A. in economics and management (2016, Cum Laude) from the Technion, Israel Institute of Technology. He was with Qualcomm (2019\u20132020), where he mainly dealt with Machine Learning and Signal Processing algorithms. Barak currently studies toward his Ph.D. at the University of Haifa. His research interest includes sensor fusion, navigation, deep learning, and estimation theory.", "[1] Understanding of Multilayer perceptron (MLP). Nitin Kumar Kain, at Medium. 2018.", "[2] Understanding Neural Networks. From neuron to RNN, CNN, and Deep Learning. Vibhor Nigam, at Medium. 2018.", "[3] Back-Propagation is very simple. Who made it Complicated? Prakash Jay, at Medium. 2017.", "[6] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. \u201cOn the difficulty of training recurrent neural networks.\u201d International conference on machine learning. 2013.", "[7] Ribeiro, Ant\u00f3nio H., et al. \u201cBeyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness.\u201d International Conference on Artificial Intelligence and Statistics. 2020.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Founder & CEO @ ALMA, AI Researcher."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6b87d558d22&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://barakor.medium.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": ""}, {"url": "https://barakor.medium.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Barak Or, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd9d6efadd96d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&user=Barak+Or%2C+PhD&userId=d9d6efadd96d&source=post_page-d9d6efadd96d----6b87d558d22---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "http://www.barakor.com/", "anchor_text": "www.barakor.com"}, {"url": "https://www.linkedin.com/in/barakor/", "anchor_text": "https://www.linkedin.com/in/barakor/"}, {"url": "https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f", "anchor_text": "Understanding of Multilayer perceptron (MLP)"}, {"url": "https://medium.com/@AI_with_Kain?source=post_page-----8f179c4a135f--------------------------------", "anchor_text": "Nitin Kumar Kain"}, {"url": "https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f", "anchor_text": "Understanding Neural Networks. From neuron to RNN, CNN, and Deep Learning"}, {"url": "https://towardsdatascience.com/@nigam.vibhor01?source=post_page-----cd88e90e0a90--------------------------------", "anchor_text": "Vibhor Nigam"}, {"url": "https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c", "anchor_text": "Back-Propagation is very simple. Who made it Complicated?"}, {"url": "https://medium.com/@14prakash?source=post_page-----97b794c97e5c--------------------------------", "anchor_text": "Prakash Jay"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6b87d558d22---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6b87d558d22---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6b87d558d22---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6b87d558d22---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----6b87d558d22---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&user=Barak+Or%2C+PhD&userId=d9d6efadd96d&source=-----6b87d558d22---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&user=Barak+Or%2C+PhD&userId=d9d6efadd96d&source=-----6b87d558d22---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6b87d558d22&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6b87d558d22---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6b87d558d22--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6b87d558d22--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6b87d558d22--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6b87d558d22--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6b87d558d22--------------------------------", "anchor_text": ""}, {"url": "https://barakor.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://barakor.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Barak Or, PhD"}, {"url": "https://barakor.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "453 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd9d6efadd96d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&user=Barak+Or%2C+PhD&userId=d9d6efadd96d&source=post_page-d9d6efadd96d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fed8e78a964fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22&newsletterV3=d9d6efadd96d&newsletterV3Id=ed8e78a964fb&user=Barak+Or%2C+PhD&userId=d9d6efadd96d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}