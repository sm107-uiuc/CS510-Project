{"url": "https://towardsdatascience.com/fedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4", "time": 1683017912.018621, "path": "towardsdatascience.com/fedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4/", "webpage": {"metadata": {"title": "FedMD: Heterogeneous Federated Learning via Model Distillation | by Kelvin | Towards Data Science", "h1": "FedMD: Heterogeneous Federated Learning via Model Distillation", "description": "In vanilla federated learning [1], the centralized server will send a global model to each participant before training takes place. After every round of federated training, the participants send back\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1910.03581", "anchor_text": "[3] Daliang Li, Junpu Wang. \u201cFedMD: Heterogenous Federated Learning via Model Distillation.\u201d", "paragraph_index": 12}], "all_paragraphs": ["In vanilla federated learning [1], the centralized server will send a global model to each participant before training takes place. After every round of federated training, the participants send back its local gradient to the global model and the server updated it with the average of all the local gradients. Hence, the participants involved in the federated learning process only obtained a generalized global model with no respect for any personalization of their data. One of the challenges in federated learning is data and device heterogeneity, this can pose a problem when a user has rich data but is unable to customize the global model to take advantage of its own personalization (Since gradients are averaged, the averaging effects can get drown out by other gradients other than it\u2019s own).", "With statistical or data heterogeneity, different participants have different data. In order to have a personalized model such as different models created for different participants, statistical heterogeneity needs to be addressed first, which implicitly leads to model heterogeneity. To tackle statistical heterogeneity is to have an individual model for each participant, however, we also need to ensure that the individual model converges to a true global model which is not possible with simple averaging due to client drift.", "In this paper [3], the author focus on a different type of heterogeneity, which is the differences in local models. The author explored and applied two techniques such as transfer learning and knowledge distillation [2] into federated learning. This allows the global model to be universal and also allows every participant to have a customized model with personalization.", "One trivial example without personalization is, assuming we are training a federated learning model for a food recommendation with two participants A and B as shown in Fig 1. Participant A only has data for fruits, participant B only has data for beverages. Clearly both participants have data drawn from different distributions. Hence, using simple averaging of the gradients doesn\u2019t make any of the models unique, as participant A only wants fruit personalization, and participant B only wants drinks personalization and not a mixture of both.", "The author proposes a framework called \u201cFedMD\u201d which allows transfer learning and knowledge distillation to be incorporated into federated learning Fig 2. below.", "The reason for using transfer learning is the scarcity of private data since private datasets can be small and if we can leverage transfer learning on a large public dataset it would be extremely beneficial to the model.", "With knowledge distillation [2], the learned knowledge is communicated based on class scores or probability scores. These newly computed class scores will be used as the new target for the dataset, and with these approaches, we can train any agnostic model to leverage the knowledge learned from one model into another model.", "The FedMD framework requires each participant to 1. Train a unique model on a public dataset to convergence (A large dataset that is publicly accessible), 2. Train its own small private dataset using the unique model, 3. During each round of federated learning, participants to compute class scores on the public dataset and sends the result to a central server, 4. The central server computes and updates the consensus which is the average of the class scores, 5. The updated consensus or class scores will now be the baseline (new public dataset) where the participants will now use for further federated training and fine-tuning.", "The following Fig 3. is a general framework for heterogeneous federated learning used by FedMD framework, where each agent/participant owns a private dataset and a uniquely designed model. Each participant has the class scores computed via knowledge distillation, which is known as the translator. The translator communicates to the central server known as the consensus and the consensus performs an update of the consensus with the average of the class scores computed from each participant.", "The full algorithm of the FedMD framework from paper [3] is shown below.", "Experiments are carried out on 10 participants, each with unique convolution networks that differ by the number of channels and layers. These 10 participants are first trained on the public dataset and achieve a test accuracy around 99% on MNIST and 76% on CIFAR10 achieve state-of-the-art accuracy, and secondly trains its model on its own small private dataset. It was shown in Fig 4. below that the curve slowly approaching the optimal test accuracy of FedMD framework when FedMD framework is used.", "FedMD is a framework that allows participants to have a unique, independently, and privately designed model in federated learning. This framework relaxes the statistical and model heterogeneity challenges in federated learning. The model architecture of the participants need not be the same as opposed to vanilla federated learning e.g. Federated averaging (FedAvg), and this is achieved by using transfer learning and knowledge distillation. FedMD is one framework that allows participants to craft their own model to meet its distinct specification in federated learning, which further addresses some of the concerns such as scarcity of private data and model privacy.", "[3] Daliang Li, Junpu Wang. \u201cFedMD: Heterogenous Federated Learning via Model Distillation.\u201d", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning enthusiast. Writing tips are welcomed."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe84676183eb4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e84676183eb4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kelvinbksoh?source=post_page-----e84676183eb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Kelvin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ea38e941e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&user=Kelvin&userId=7ea38e941e0c&source=post_page-7ea38e941e0c----e84676183eb4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1910.03581.pdf", "anchor_text": "link"}, {"url": "https://arxiv.org/pdf/1910.03581.pdf", "anchor_text": "link"}, {"url": "https://towardsdatascience.com/introduction-to-federated-learning-and-challenges-ea7e02f260ca", "anchor_text": "https://towardsdatascience.com/introduction-to-federated-learning-and-challenges-ea7e02f260ca"}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \u201cDistilling the knowledge in a neural network.\u201d arXiv preprint arXiv:1503.02531 (2015)."}, {"url": "https://arxiv.org/pdf/1910.03581", "anchor_text": "[3] Daliang Li, Junpu Wang. \u201cFedMD: Heterogenous Federated Learning via Model Distillation.\u201d"}, {"url": "https://medium.com/tag/federated-learning?source=post_page-----e84676183eb4---------------federated_learning-----------------", "anchor_text": "Federated Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e84676183eb4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e84676183eb4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/knowledge-distillation?source=post_page-----e84676183eb4---------------knowledge_distillation-----------------", "anchor_text": "Knowledge Distillation"}, {"url": "https://medium.com/tag/privacy-preserving?source=post_page-----e84676183eb4---------------privacy_preserving-----------------", "anchor_text": "Privacy Preserving"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&user=Kelvin&userId=7ea38e941e0c&source=-----e84676183eb4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&user=Kelvin&userId=7ea38e941e0c&source=-----e84676183eb4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe84676183eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e84676183eb4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e84676183eb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e84676183eb4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e84676183eb4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e84676183eb4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e84676183eb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kelvin"}, {"url": "https://medium.com/@kelvinbksoh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "32 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ea38e941e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&user=Kelvin&userId=7ea38e941e0c&source=post_page-7ea38e941e0c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8b60176305e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4&newsletterV3=7ea38e941e0c&newsletterV3Id=8b60176305e1&user=Kelvin&userId=7ea38e941e0c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}