{"url": "https://towardsdatascience.com/regularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c", "time": 1682996568.1835148, "path": "towardsdatascience.com/regularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c/", "webpage": {"metadata": {"title": "Regularization for Neural Networks with Framingham Case Study | by Rachel Draelos, MD, PhD | Towards Data Science", "h1": "Regularization for Neural Networks with Framingham Case Study", "description": "In this post, I discuss L1, L2, elastic net, and group lasso regularization on neural networks. I describe how regularization can help you build models that are more useful and interpretable, and I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)", "anchor_text": "reducing overfitting.", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "Overfitting", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitting.svg", "anchor_text": "an example of overfitting in a classification problem", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "memorizes all of the examples", "paragraph_index": 6}, {"url": "https://glassboxmedicine.com/2019/01/17/introduction-to-neural-networks/", "anchor_text": "this post", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "\u201cridge regression.\u201d", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Elastic_net_regularization", "anchor_text": "using both L1 and L2 regularization at the same time", "paragraph_index": 25}, {"url": "http://www.columbia.edu/~my2550/papers/glasso.final.pdf", "anchor_text": "Ming Yuan and Yi Lin. \u201cModel selection and estimation in regression with grouped variables.\u201d J. R. Statist. Soc. B (2006).", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Lasso_(statistics)#Group_lasso", "anchor_text": "\u201cgroup lasso\u201d is a technique that allows you to zero out entire groups of variables", "paragraph_index": 30}, {"url": "https://glassboxmedicine.com/2019/06/01/everything-you-need-to-know-about-preparing-tabular-data-for-machine-learning-code-included/", "anchor_text": "represented a categorical variable as a one-hot vector", "paragraph_index": 32}, {"url": "https://glassboxmedicine.com/2019/06/01/everything-you-need-to-know-about-preparing-tabular-data-for-machine-learning-code-included/", "anchor_text": "appropriately normalized as described here", "paragraph_index": 34}, {"url": "https://medium.com/@vivek.yadav/wx-b-vs-xw-b-why-different-formulas-for-deep-neural-networks-in-theory-and-implementation-a5ae6995c4ef", "anchor_text": "this post", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1607.00485", "anchor_text": "Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. \u201cGroup Sparse Regularization for Deep Neural Networks.\u201d Neurocomputing (2017).", "paragraph_index": 37}, {"url": "https://bitbucket.org/ispamm/group-lasso-deep-networks/src/master/run_simulation.py", "anchor_text": "a Bitbucket repository for the paper", "paragraph_index": 38}, {"url": "https://bitbucket.org/ispamm/group-lasso-deep-networks/src/master/run_simulation.py", "anchor_text": "this repository", "paragraph_index": 41}, {"url": "http://mathworld.wolfram.com/L2-Norm.html", "anchor_text": "L2 norm", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Framingham_Heart_Study", "anchor_text": "Framingham Heart Study", "paragraph_index": 46}, {"url": "https://glassboxmedicine.com/2019/02/16/measuring-performance-accuracy/", "anchor_text": "accuracy", "paragraph_index": 48}, {"url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "anchor_text": "area under the receiver operating characteristic", "paragraph_index": 48}, {"url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "anchor_text": "average precision", "paragraph_index": 48}, {"url": "https://commons.wikimedia.org/wiki/File:HerdQuit.jpg", "anchor_text": "an oil painting called \u201cThe Herd Quitter\u201d by C.M. Russell", "paragraph_index": 58}, {"url": "https://en.wikipedia.org/wiki/Lasso", "anchor_text": "also called", "paragraph_index": 58}, {"url": "https://glassboxmedicine.com/2019/06/08/regularization-for-neural-networks-with-framingham-case-study/", "anchor_text": "http://glassboxmedicine.com", "paragraph_index": 59}], "all_paragraphs": ["In this post, I discuss L1, L2, elastic net, and group lasso regularization on neural networks. I describe how regularization can help you build models that are more useful and interpretable, and I include Tensorflow code for each type of regularization. Finally, I provide a detailed case study demonstrating the effects of regularization on neural network models applied to real clinical and genetic data from the Framingham study.", "Regularization can improve your neural network\u2019s performance on unseen data by reducing overfitting.", "Overfitting is a phenomenon where a neural network starts to memorize unique quirks of the training data (e.g. training data noise) instead of learning generally-applicable principles. A model that has \u201coverfit\u201d will get high performance on the training data but bad performance on the held-out test data, meaning the model won\u2019t be useful in the real world because it won\u2019t perform well on data it has never seen before. Since the whole point of neural network models is to solve real problems on new data, we want to avoid overfitting so that we obtain a practically useful model.", "Here is an example of overfitting in a classification problem:", "In this classification task we want to learn a line that separates the red dots from the blue dots. The black line represents a good classifier that appears to capture the general principle of where the blue dots and red dots are in the space. The green line represents a classifier that has suffered from overfitting because it\u2019s too specific to this exact training set of red and blue dots.", "Observe how the green classifier line does worse on an unseen test set, even though the unseen test set is following the same general layout of blue and red dots. Also notice that the black classifier line, which has not overfit, still works well on the test set:", "The most extreme version of overfitting happens when a neural network memorizes all of the examples in the training set. This can happen when the neural network has many parameters and is trained for too long.", "Specifically, \u201classo\u201d regularization tries to force some of the weights in the model to be zero.", "Here\u2019s the general setup of regularization for a neural network model, where W represents the weights of your neural network model:", "Every neural network has a loss function that is used in training to adjust the neural network\u2019s weights. The loss function measures how different the neural network\u2019s predictions are from the truth (see this post for review.)", "Regularization merely adds a \u201cregularization term\u201d (shown above in blue) to the loss function.", "Here\u2019s the formula for L1 regularization (first as hacky shorthand and then more precisely):", "Thus, L1 regularization adds in a penalty for having weights of large absolute value. L1 regularization encourages your model to make as many weights zero as possible.", "Here\u2019s an example of how to calculate the L1 regularization penalty on a tiny neural network with only one layer, described by a 2 x 2 weight matrix:", "When applying L1 regularization to regression, it\u2019s called \u201classo regression.\u201d", "Here\u2019s Tensorflow code for calculating the L1 regularization penalty for a weight matrix called weights:", "Here\u2019s the formula for L2 regularization (first as hacky shorthand and then more precisely):", "Thus, L2 regularization adds in a penalty for having many big weights. L2 regularization encourages the model to choose weights of small magnitude.", "Here\u2019s an example of how to calculate the L2 regularization penalty on a tiny neural network with only one layer, described by a 2 x 2 weight matrix:", "If you apply L2 regularization to regression, it\u2019s referred to as \u201cridge regression.\u201d", "Here\u2019s Tensorflow code for calculating the L2 regularization penalty for a weight matrix called weights:", "The Tensorflow function \u201cl2_loss\u201d calculates the squared L2 norm. The squared L2 norm is another way to write L2 regularization:", "Notice that in L1 regularization a weight of -9 gets a penalty of 9 but in L2 regularization a weight of -9 gets a penalty of 81 \u2014 thus, bigger magnitude weights are punished much more severely in L2 regularization.", "Also notice that in L1 regularization a weight of 0.5 gets a penalty of 0.5 but in L2 regularization a weight of 0.5 gets a penalty of (0.5)(0.5) = 0.25 \u2014 thus, in L1 regularization there is still a push to squish even small weights towards zero, more so than in L2 regularization.", "This is why L1 regularization encourages the model to make as many weights zero as possible, while L2 regularization encourages the model to make all the weights as small as possible (but not necessarily zero).", "\u201cElastic net regularization\u201d sounds fancy, but it simply means using both L1 and L2 regularization at the same time:", "Here, we have two lambdas: one that controls the strength of the L1 regularization term, and another that controls the strength of the L2 regularization term. Both of these lambda values can be tuned using the validation set, as described previously.", "Group Lasso was introduced by Yuan and Lin in 2006:", "Ming Yuan and Yi Lin. \u201cModel selection and estimation in regression with grouped variables.\u201d J. R. Statist. Soc. B (2006).", "(That paper has since been cited over 5,000 times.)", "What is group lasso? Recall that L1 regularization is sometimes called \u201classo regularization\u201d and the purpose is to zero out some of the variables. In a similar vein, \u201cgroup lasso\u201d is a technique that allows you to zero out entire groups of variables. All members of a particular variable group are either included in the model together, or excluded from the model (zeroed out) together.", "Here are two situations where group lasso is particularly useful:", "For Categorical Variables: If you\u2019ve represented a categorical variable as a one-hot vector \u2014 i.e., as a collection of binary covariates \u2014 group lasso can ensure that all of the binary covariates corresponding to a single categorical variable get \u201czeroed out\u201d or \u201ckept\u201d together. For example, if you have a categorical variable \u201ccolor\u201d with possible values \u201cred,\u201d \u201cblue,\u201d and \u201cgreen,\u201d then you can represent this categorical variable with a one-hot vector of length three, corresponding to splitting a single \u201ccolor\u201d column into three binary columns: \u201cred yes/no,\u201d \u201cblue yes/no,\u201d and \u201cgreen yes/no.\u201d Group lasso can help you zero out all three columns together, or keep all three columns, treating them as a single unit.", "For Neural Networks: If you\u2019re training a neural network, group lasso can \u201czero out\u201d entire input variables, and help you obtain a more interpretable model.", "The figure below shows \u201cXW\u201d: a 2-dimensional data input matrix X multiplied by a neural network weight matrix W. In this case W maps the 2-dimensional input to a 4-dimensional hidden layer. The input X consists of \u201cPatient A\u201d with variables \u201cBlood Pressure\u201d and \u201cCholesterol\u201d (which have been appropriately normalized as described here.) You can see the first row of the weight matrix highlighted in red, which corresponds to weights that will multiply the blood pressure variable. The second row of the weight matrix is highlighted in blue, which corresponds to weights that will multiply the cholesterol variable. Thus, if we want to \u201czero out\u201d the variable blood pressure, we need to \u201czero out\u201d all four weights in the top row of the weight matrix.", "(Note: in most papers, neural network math is written as WX+b, where W is the weight matrix, X is the input, and b is the bias vector. Why am I writing XW here? Well\u2026In Tensorflow, the implementation of a fully connected layer uses XW instead of WX. See this post for additional comments on notation in theory vs. practice.)", "Here\u2019s a paper describing group lasso applied to neural networks:", "Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. \u201cGroup Sparse Regularization for Deep Neural Networks.\u201d Neurocomputing (2017).", "and here\u2019s a Bitbucket repository for the paper, where the authors explain,", "For each node in the network, we include a regularization term pushing the entire row of outgoing weights to be zero simultaneously. This is done by constraining the L2 norm of the row, weighted by the square root of its dimensionality.", "Here\u2019s the formula for group lasso regularization:", "Here\u2019s one Tensorflow function for the group lasso penalty, from this repository:", "Here\u2019s another Tensorflow implementation of the group lasso penalty:", "Somewhat confusingly, group lasso in this paper uses the L2 norm like in L2 regularization (summing up the squared values of the elements). But traditional \u201classo\u201d uses the L1 norm! What\u2019s going on here?", "Lasso (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.", "\u201cGroup lasso\u201d is performing both variable selection (by zeroing out groups of weights corresponding to particular input variables) and regularization. Also, even though group lasso includes an L2 norm, it is NOT the same as L2 regularization:", "The Framingham Heart Study started in 1948 and has continued to this day. The Framingham study is responsible for much modern knowledge about heart disease, including that eating a healthy diet, maintaining a healthy weight, not smoking, and engaging in regular exercise can reduce the risk of heart disease. The Framingham heart study data set includes clinical variables (e.g. age, smoking status), genetic variables, and heart disease outcomes (e.g. whether or not a patient had a heart attack.)", "In summer 2018, I spent a few months analyzing part of the Framingham heart study data to determine whether predictive models that included both clinical and genetic data would perform better than predictive models built using clinical data alone. I suspected that it would be difficult to see any benefit in predictive performance by adding genetic data on top of clinical data, because:", "I trained various feedforward neural networks on a combination of clinical and genetic data from the Framingham data set to predict heart disease risk. I applied different kinds of regularization to the first layer weight matrix. The table below shows the performance of the different models (Acc = accuracy, AUC = area under the receiver operating characteristic, AP = average precision):", "We can gain even more insights by inspecting heat maps of the first-layer weight matrices for the different regularization approaches. Each row of the heat map corresponds to a different input variable. The top 20 rows (genetic_0 to genetic_19) correspond to weights applied to a learned 20-dimensional representation of the 500,000 genetic input variables. The bottom rows (from SYSBP1 to DIAB_1.0) correspond to weights applied to clinical variables.", "Here are heat maps for the model with (A) no regularization, (B) L1 regularization, \u00a9 L2 regularization, and (D) elastic net regularization:", "Panel A shows the first-layer weight matrix with no regularization applied. We can see that there are large-magnitude positive and negative weights scattered throughout with no particular pattern.", "Panel B shows the first-layer weight matrix with L1 regularization applied. We can see that there are more zero-valued (black) weights. However, there is no particularly strong pattern at the level of variables (rows), which is to be expected as L1 regularization considers the absolute value of each weight independently.", "Panel C shows the first-layer weight matrix with L2 regularization applied. This has a greater number of smaller magnitude weights than the unregularized case.", "Panel D shows the first-layer weight matrix with elastic net regularization applied. For this data set, elastic net regularization ended up achieving the best performance of all the regularization methods considered. It also produced a pretty first-layer weight matrix: all the genetic variables are \u201czeroed out\u201d and the brightest clinical variable is \u201cage\u201d (which is somewhat funny, since I was looking at ~40-year risk of heart disease, so if you start out at age 80 at the beginning of the study period you\u2019re going to have higher risk than someone starting out at age 30.)", "Finally, in Panels E and F we can see the results of two different models trained using group lasso regularization. The models were trained with different random initializations and ended up finding different solutions:", "Thus, this experiment also ended up being a nice demonstration of how neural networks with different random initializations can find different solutions on the same data set.", "Note that in many cases, application of regularization leads to better test set performance, which is not explicitly illustrated in this example. If I had added regularization to the best-performing \u201cclinical data only\u201d model, perhaps it would have resulted in even higher performance.", "The featured image is an oil painting called \u201cThe Herd Quitter\u201d by C.M. Russell of cowboys attempting to lasso a bull. A lasso for livestock is also called a \u201clariat,\u201d \u201criata,\u201d \u201creata,\u201d or simply a \u201crope.\u201d", "Originally published at http://glassboxmedicine.com on June 8, 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a physician with a PhD in Computer Science. My research focuses on machine learning methods development for medical data. I am the CEO of Cydoc."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2c51cca72f7c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rachel-draelos.medium.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Rachel Draelos, MD, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F209c0f742bcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=post_page-209c0f742bcf----2c51cca72f7c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)", "anchor_text": "reducing overfitting."}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "Overfitting"}, {"url": "https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitting.svg", "anchor_text": "an example of overfitting in a classification problem"}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "memorizes all of the examples"}, {"url": "https://glassboxmedicine.com/2019/01/17/introduction-to-neural-networks/", "anchor_text": "this post"}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "\u201cridge regression.\u201d"}, {"url": "https://en.wikipedia.org/wiki/Elastic_net_regularization", "anchor_text": "using both L1 and L2 regularization at the same time"}, {"url": "http://www.columbia.edu/~my2550/papers/glasso.final.pdf", "anchor_text": "Ming Yuan and Yi Lin. \u201cModel selection and estimation in regression with grouped variables.\u201d J. R. Statist. Soc. B (2006)."}, {"url": "https://en.wikipedia.org/wiki/Lasso_(statistics)#Group_lasso", "anchor_text": "\u201cgroup lasso\u201d is a technique that allows you to zero out entire groups of variables"}, {"url": "https://glassboxmedicine.com/2019/06/01/everything-you-need-to-know-about-preparing-tabular-data-for-machine-learning-code-included/", "anchor_text": "represented a categorical variable as a one-hot vector"}, {"url": "https://glassboxmedicine.com/2019/06/01/everything-you-need-to-know-about-preparing-tabular-data-for-machine-learning-code-included/", "anchor_text": "appropriately normalized as described here"}, {"url": "https://medium.com/@vivek.yadav/wx-b-vs-xw-b-why-different-formulas-for-deep-neural-networks-in-theory-and-implementation-a5ae6995c4ef", "anchor_text": "this post"}, {"url": "https://arxiv.org/abs/1607.00485", "anchor_text": "Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. \u201cGroup Sparse Regularization for Deep Neural Networks.\u201d Neurocomputing (2017)."}, {"url": "https://bitbucket.org/ispamm/group-lasso-deep-networks/src/master/run_simulation.py", "anchor_text": "a Bitbucket repository for the paper"}, {"url": "https://bitbucket.org/ispamm/group-lasso-deep-networks/src/master/run_simulation.py", "anchor_text": "this repository"}, {"url": "http://mathworld.wolfram.com/L2-Norm.html", "anchor_text": "L2 norm"}, {"url": "https://en.wikipedia.org/wiki/Framingham_Heart_Study", "anchor_text": "Framingham Heart Study"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3679547/", "anchor_text": "about 50% of the risk of heart disease is genetic"}, {"url": "https://en.wikipedia.org/wiki/Underdetermined_system", "anchor_text": "underdetermined"}, {"url": "https://glassboxmedicine.com/2019/02/16/measuring-performance-accuracy/", "anchor_text": "accuracy"}, {"url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "anchor_text": "area under the receiver operating characteristic"}, {"url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "anchor_text": "average precision"}, {"url": "https://commons.wikimedia.org/wiki/File:HerdQuit.jpg", "anchor_text": "an oil painting called \u201cThe Herd Quitter\u201d by C.M. Russell"}, {"url": "https://en.wikipedia.org/wiki/Lasso", "anchor_text": "also called"}, {"url": "https://glassboxmedicine.com/2019/06/08/regularization-for-neural-networks-with-framingham-case-study/", "anchor_text": "http://glassboxmedicine.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2c51cca72f7c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/regularization?source=post_page-----2c51cca72f7c---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2c51cca72f7c---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2c51cca72f7c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2c51cca72f7c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=-----2c51cca72f7c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=-----2c51cca72f7c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2c51cca72f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2c51cca72f7c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2c51cca72f7c--------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rachel Draelos, MD, PhD"}, {"url": "https://rachel-draelos.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "576 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F209c0f742bcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=post_page-209c0f742bcf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa0377bd1bf3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-for-neural-networks-with-framingham-case-study-2c51cca72f7c&newsletterV3=209c0f742bcf&newsletterV3Id=a0377bd1bf3d&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}