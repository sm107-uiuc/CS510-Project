{"url": "https://towardsdatascience.com/understanding-gradient-descent-and-adam-optimization-472ae8a78c10", "time": 1683009029.801804, "path": "towardsdatascience.com/understanding-gradient-descent-and-adam-optimization-472ae8a78c10/", "webpage": {"metadata": {"title": "Gradient Descent and Adam Optimization | Towards Data Science", "h1": "Understanding Gradient Descent and Adam Optimization", "description": "Intuitive view to Gradient Descent, and basic understanding of Adam Optimization in Deep Learning"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb", "anchor_text": "https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb", "paragraph_index": 1}, {"url": "http://www.iclr.cc/doku.php?id=iclr2015:main", "anchor_text": "ICLR", "paragraph_index": 17}], "all_paragraphs": ["How artificial intelligence has influenced our daily lives in the past decade is something we can only ponder about. From spam filtering to news clustering, computer vision applications like fingerprint sensors to natural language processing problems like handwriting and speech recognition, it is very easy to undermine how big a role AI and data science is playing in our day-to-day lives. However, with an exponential increase in amount of data our algorithms deal with, it is essential to develop algorithms which can keep pace with this rise in complexity. One such algorithm which has caused a notable change to the industry is the Adam Optimization procedure. But before we delve into it, first let us look at gradient descent and where it falls short.", "In case if you aren\u2019t aware of what a cost function is, I would recommend you to go through this blog first, which serves as a great introduction to the topic: https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb", "Suppose we have a convex cost function of 2 input variables as shown above and our goal is to minimize its value and find the value of the parameters (x,y) for which f(x,y) is minimum. What the gradient descent algorithm does is, we start at a specific point on the curve and use the negative gradient to find the direction of steepest descent and take a small step in that direction and keep iterating till our value starts converging.", "I personally find the above analogy to gradient descent very cool, a person starting from the top of a hill and climbing down by the path which enables him to decrease his altitude quickest.", "The formal definition of gradient descent is given alongside, we keep performing the update as required till convergence is reached. We can check convergence easily by checking whether the difference between f(Xi+1) and f(Xi) is less than some number, say 0.0001(the default value if you implement gradient descent using Python). If so, we say that gradient descent has converged at a local minimum of f.", "If you cannot quite grasp the gradient concept or are interested in more in-depth knowledge of cost function and gradient descent, I strongly recommend the following video from my favorite YouTube channel 3Blue1Brown -", "To perform a single step of gradient descent, we need to iterate over all training examples to find out the gradient at a particular point. This is termed as batch gradient descent and was done for many years but with the advent of the era of deep learning and big data, it has become common to have a training set size of the order of millions and this becomes computationally expensive, it may take few minutes to perform a single step of gradient descent. So what is done commonly, is something called a mini-batch gradient descent where we divide the training set into batches of small size and perform gradient descent using those batches individually. This often results in a faster convergence but there\u2019s a major problem here \u2014 We only look at a fraction of the training set while taking a single step and hence, the step may not be towards the steepest decrease of the cost function. This is because we are minimizing the cost based on a subset of the total data, which is not a representative of what\u2019s best for the entire training data. Instead of following a straight path towards the minimum, our algorithm now follows a roundabout path, not always even leading to an optimum and most commonly, overshooting (going past the minimum).", "The following figures alongside show the steps of gradient descent in the 3 different batch size cases, and changes in how the cost function minimizes. In both the figures, it is apparent that the cost function is minimizing, but it oscillates even though in general, it is decreasing. The problem is as follows, Can we somehow \u201csmoothen\u201d out these steps of gradient descent so that it can follow a less noisy path and converge faster? The answer, as you might already have guessed, is Adam Optimization.", "There\u2019s a lot going on here. Let\u2019s quickly break it down. First, let\u2019s see the parameters involved.", "Adam can essentially be broken down as a combination of 2 main algorithms\u2014 Momentum and RMSProp. The momentum step is as follows -", "Suppose beta1=0.9. Then the corresponding step calculates 0.9*current moment + 0.1*current gradient. You can think of this as a weighted average over the last 10 gradient descent steps, which cancels out a lot of noise. However initially, moment is set to 0 hence the moment at the first step = 0.9*0 + 0.1*gradient = gradient/10 and so on. The moment will fail to keep up with the original gradient ,and this is known as a biased estimate. To correct this we do the following, known as bias correction ,dividing by 1 - (beta1 raised to the timestep) -", "Note that 1 - power(beta1,t) approaches 1 as t becomes higher with each step, decreasing the correction effect later and maximizing it at the first few steps.", "The graph alongside pictures this perfectly, the yellow line refers to the moment(estimate) obtained with a smaller beta1, say 0.5 while the green line refers to a beta1 value closer to 1, say 0.9", "RMSProp does a similar thing, but slightly different -", "It also computes a weighted average over the last 1/(1-beta2) examples approximately, which is 100 when beta2=0.99. But it computes the average of the squares of the gradient (a sort of scaled magnitude), and then the same bias correction step.", "Now, in the gradient descent step instead of using the gradient we use these moments as follows -", "Using m_corrected ensures that our gradient moves in the direction of the general trend and does not oscillate about too much while dividing by the square root of the mean of squared magnitudes ensures that the overall magnitude of the steps is fixed and close to unit value. This also adds in adaptive gradient, which I am not going to talk about in detail, it\u2019s just a procedure of changing the magnitude of the steps as we approach convergence. This helps prevent overshooting. Finally, epsilon is added to the denominator to avoid division by 0 in case the estimate of the gradients encountered are too small and are rounded off to 0 by the compiler. The value is deliberately chosen to be very small so as not to affect the algorithm, generally of the order of 10^-8.", "Adam has been in widespread use in Deep Learning models since 2015. It was presented by Diederik Kingma from OpenAI and Jimmy Ba from the University of Toronto in their 2015 ICLR paper \u201cAdam: A method for stochastic gradient optimization\u201d. Adam, as it may sound, has not been named after someone. It is short for \u201cAdaptive Moment Estimation\u201d. The following figure shows it\u2019s effectiveness compared to other minimizing algorithms when applied to a neural network model on the MNIST dataset.", "Adam has been one of the most remarkable achievements in the grounds of optimization. Several incidents where the training of a large model required days have been reduced to hours since usage of Adam. Since it\u2019s inception it has been made the default optimizer used in almost all deep learning libraries. I myself use Adam frequently \u2014 on a handwritten digit classification problem, I found that just by changing my optimizer from mini-batch gradient descent to Adam my training accuracy jumped from 79% to 94%, and number of iterations required reduced to about one-third, a pretty significant change considering that my training data was of size about 10,000, not even close to a million, where the effects would be even more significant!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An aspiring researcher, currently pursuing Bachelors in Computer Engineering, with special interest in Artificial Intelligence."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F472ae8a78c10&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tamoghnobhattacharya2001?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tamoghnobhattacharya2001?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Tamoghno Bhattacharya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F92a115f4e3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&user=Tamoghno+Bhattacharya&userId=92a115f4e3b7&source=post_page-92a115f4e3b7----472ae8a78c10---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/illustrations/artificial-intelligence-brain-think-3382507/", "anchor_text": "https://pixabay.com/illustrations/artificial-intelligence-brain-think-3382507/"}, {"url": "https://www.researchgate.net/figure/Sphere-function-D-2_fig8_275069197", "anchor_text": "https://www.researchgate.net/figure/Sphere-function-D-2_fig8_275069197"}, {"url": "https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb", "anchor_text": "https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb"}, {"url": "https://datascience-enthusiast.com/DL/Optimization_methods.html", "anchor_text": "https://datascience-enthusiast.com/DL/Optimization_methods.html"}, {"url": "https://www.quora.com/What-turns-gradient-descent-into-gradient-ascent-in-deep-learning", "anchor_text": "https://www.quora.com/What-turns-gradient-descent-into-gradient-ascent-in-deep-learning"}, {"url": "https://datascience.stackexchange.com/questions/52884/possible-for-batch-size-of-neural-network-to-be-too-small", "anchor_text": "https://datascience.stackexchange.com/questions/52884/possible-for-batch-size-of-neural-network-to-be-too-small"}, {"url": "https://engmrk.com/mini-batch-gd/", "anchor_text": "https://engmrk.com/mini-batch-gd/"}, {"url": "https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218", "anchor_text": "https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218"}, {"url": "https://medium.com/datadriveninvestor/exponentially-weighted-average-for-deep-neural-networks-39873b8230e9", "anchor_text": "https://medium.com/datadriveninvestor/exponentially-weighted-average-for-deep-neural-networks-39873b8230e9"}, {"url": "http://www.iclr.cc/doku.php?id=iclr2015:main", "anchor_text": "ICLR"}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----472ae8a78c10---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----472ae8a78c10---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----472ae8a78c10---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&user=Tamoghno+Bhattacharya&userId=92a115f4e3b7&source=-----472ae8a78c10---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&user=Tamoghno+Bhattacharya&userId=92a115f4e3b7&source=-----472ae8a78c10---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F472ae8a78c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----472ae8a78c10---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----472ae8a78c10--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----472ae8a78c10--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----472ae8a78c10--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tamoghnobhattacharya2001?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tamoghnobhattacharya2001?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tamoghno Bhattacharya"}, {"url": "https://medium.com/@tamoghnobhattacharya2001/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "51 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F92a115f4e3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&user=Tamoghno+Bhattacharya&userId=92a115f4e3b7&source=post_page-92a115f4e3b7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F68033984c2e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-gradient-descent-and-adam-optimization-472ae8a78c10&newsletterV3=92a115f4e3b7&newsletterV3Id=68033984c2e6&user=Tamoghno+Bhattacharya&userId=92a115f4e3b7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}