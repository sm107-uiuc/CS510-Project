{"url": "https://towardsdatascience.com/navigating-the-sea-of-explainability-649672aa7bdd", "time": 1683000926.61408, "path": "towardsdatascience.com/navigating-the-sea-of-explainability-649672aa7bdd/", "webpage": {"metadata": {"title": "Navigating the Sea of Explainability | by Shir Meir Lador | Towards Data Science", "h1": "Navigating the Sea of Explainability", "description": "Rapid adoption of complex machine learning (ML) models in recent years has brought with it a new challenge for today\u2019s companies: how to interpret, understand, and explain the reasoning behind these\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@Joy_Rimchala", "anchor_text": "Joy Rimchala", "paragraph_index": 0}, {"url": "https://medium.com/@DataLady", "anchor_text": "Shir Meir Lador", "paragraph_index": 0}, {"url": "http://proceedings.mlr.press/v81/buolamwini18a.html", "anchor_text": "GenderShades", "paragraph_index": 1}, {"url": "https://xai.kdd2019.a.intuit.com/", "anchor_text": "XAI 2019", "paragraph_index": 4}, {"url": "https://www.kdd.org/kdd2019/#!", "anchor_text": "KDD 2019", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1606.05386", "anchor_text": "\u00b2", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1704.03296", "anchor_text": "\u00b3", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1705.07857", "anchor_text": "\u2074", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1807.08024", "anchor_text": "\u2075", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "LIME", "paragraph_index": 6}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "Shapley\u2076,", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Integrated Gradients", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1704.02685", "anchor_text": "DeepLIFT", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1503.01161", "anchor_text": "\u2079", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1405.4047", "anchor_text": "\u00b9\u2070", "paragraph_index": 7}, {"url": "https://www.microsoft.com/en-us/research/people/rcaruana/", "anchor_text": "Rich Caruana", "paragraph_index": 7}, {"url": "https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478440", "anchor_text": "\u00b9\u00b9", "paragraph_index": 7}, {"url": "https://dl.acm.org/citation.cfm?id=2788613", "anchor_text": "\u00b9\u00b2", "paragraph_index": 7}, {"url": "https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf", "anchor_text": "This version", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "gradient boosting", "paragraph_index": 7}, {"url": "http://mathworld.wolfram.com/CubicSpline.html", "anchor_text": "splines", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Confounding", "anchor_text": "statistical confounding", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1908.02641", "anchor_text": "\u00b9\u00b3", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1811.10154", "anchor_text": "\u00b9\u2074", "paragraph_index": 12}, {"url": "https://users.cs.duke.edu/~cynthia/", "anchor_text": "Cynthia Rudin", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "\u00b9\u2075", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Rashomon", "anchor_text": "Rashomon", "paragraph_index": 13}, {"url": "https://www.youtube.com/watch?v=wL4X4lG20sM", "anchor_text": "KDD 2019 keynote talk", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "\u00b9\u2075", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1702.08608", "anchor_text": "\u00b9\u2076", "paragraph_index": 16}, {"url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "anchor_text": "partial dependence plots", "paragraph_index": 18}, {"url": "https://dl.acm.org/citation.cfm?id=2788613", "anchor_text": "\u00b9\u00b2", "paragraph_index": 18}, {"url": "https://ai.google/research/people/106045/", "anchor_text": "Been Kim", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1512.00567", "anchor_text": "InceptionV3", "paragraph_index": 19}, {"url": "https://ai.google/research/pubs/pub43022", "anchor_text": "GoogleLeNet", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1810.03292", "anchor_text": "\u00b9\u2077", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1907.09701", "anchor_text": "\u00b9\u2078", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "\u00b9\u2079", "paragraph_index": 21}, {"url": "http://proceedings.mlr.press/v81/buolamwini18a.html", "anchor_text": "PMLR 81:77\u201391", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1606.05386", "anchor_text": "arXiv preprint arXiv:1606.05386", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1704.03296", "anchor_text": "pages 3429\u20133437", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1705.07857", "anchor_text": "pages 6967\u20136976", "paragraph_index": 31}, {"url": "https://arxiv.org/abs/1807.08024", "anchor_text": "Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2019", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1705.07874", "anchor_text": "Advances in Neural Information Processing Systems, 2017", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Pages 3319\u20133328", "paragraph_index": 34}, {"url": "https://arxiv.org/abs/1704.02685", "anchor_text": "3145\u20133153", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1503.01161", "anchor_text": "pages 1952\u20131960", "paragraph_index": 36}, {"url": "https://www.microsoft.com/en-us/research/people/rcaruana/", "anchor_text": "Rich Caruana", "paragraph_index": 37}, {"url": "https://www.microsoft.com/en-us/research/people/paulkoch/", "anchor_text": "Paul Koch", "paragraph_index": 37}, {"url": "https://www.microsoft.com/en-us/research/people/johannes/", "anchor_text": "Johannes Gehrke", "paragraph_index": 37}, {"url": "https://www.microsoft.com/en-us/research/publication/intelligible-models-healthcare-predicting-pneumonia-risk-hospital-30-day-readmission/", "anchor_text": "Pages 1721\u20131730", "paragraph_index": 37}, {"url": "https://arxiv.org/abs/1908.02641", "anchor_text": "arXiv:1908.02641", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1811.10154", "anchor_text": "pages 206\u2013215", "paragraph_index": 39}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "arXiv:1908.01755", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1810.03292", "anchor_text": "NeurIPS 2018", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1907.09701", "anchor_text": "arXiv:1907.09701", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "pages 2673\u20132682", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1809.06514", "anchor_text": "Pages 10\u201319", "paragraph_index": 44}], "all_paragraphs": ["This article is coauthored by Joy Rimchala and Shir Meir Lador.", "Rapid adoption of complex machine learning (ML) models in recent years has brought with it a new challenge for today\u2019s companies: how to interpret, understand, and explain the reasoning behind these complex models\u2019 predictions. Treating complex ML systems as trustworthy black boxes without sanity checking has led to some disastrous outcomes, as evidenced by recent disclosures of gender and racial biases in GenderShades\u00b9.", "As ML-assisted predictions integrate more deeply into high-stakes decision-making, such as medical diagnoses, recidivism risk prediction, loan approval processes, etc., knowing the root causes of an ML prediction becomes crucial. If we know that certain model predictions reflect bias and are not aligned with our best knowledge and societal values (such as an equal opportunity policy or outcome equity), we can detect these undesirable ML defects, prevent the deployment of such ML systems, and correct model defects.", "Our mission at Intuit is powering prosperity around the world. To help small businesses and individuals increase their odds for success, in the last few years Intuit has been infusing AI and ML across its platform and solutions. As data scientists at Intuit, we have a unique privilege and power to develop ML models that make decisions that affect people\u2019s lives. With that power, we also bear the responsibility to make sure our models are held in the highest standards, and are not discriminating in any manner. \u201cIntegrity without compromise\u201d is one of Intuit\u2019s core values. As we grow as an AI/ML-driven organization, machine intelligibility has become a priority for Intuit\u2019s AI/ML products.", "This year, Intuit hosted an Explainable AI workshop (XAI 2019) at KDD 2019. We gleaned many valuable learnings from this workshop that we will start to incorporate in our product and service strategies.", "Interpretability is an active area of research and the description provided below is meant to provide a high level summary of the current state of the field. Interpretability methods fall into two major categories based on whether the model being interpreted is: (a) black box (unintelligible) or (b) glass box (intelligible). In the following section, we will explain and compare each of the approaches. We will also describe how we can use intelligible models to better understand our data. Then we will review a method to detect high-performing intelligible models for any use case (Rashomon curves). Finally, we will compare local and global explanations and feature-based vs. concept-based explanation.", "Black box interpretability methods attempt to explain already-existing ML models without taking into account the inner workings of the model (i.e., the learned decision functions). This class of interpretability methods is model-agnostic and can be integrated easily with a wide variety of ML models, from decision tree-based models to complex neural networks\u00b2 \u00b3 \u2074 \u2075. Applying black box interpretability does not require any changes in the way ML practitioners create and train the models. For this reason, black box interpretability methods enjoy wider adoption among ML practitioners. Black box interpretability methods are also referred to as \u201cpost-hoc\u201d interpretability, as they can be used to interrogate ML models after training and deployment without any knowledge of the training procedures. Examples of black box interpretability methods include LIME\u00b2, Shapley\u2076, Integrated Gradients\u2077, DeepLIFT\u2078, etc. Post-hoc model interpretations are a proxy for explanations. The explanations derived in this manner are not necessarily guaranteed to be human-friendly, useful, or actionable.", "A glass box approach with intelligible ML models requires that models be \u201cinterpretable\u201c upfront (aka \u201cpre-hoc\u201d)\u2079 \u00b9\u2070. The advantage of this approach is the ease with which ML practitioners can tease out model explanations, detect data and/or label flaws, and in some cases, edit the model\u2019s decisions if they do not align with practitioner values or domain knowledge. Rich Caruana, Senior Principal Researcher at Microsoft Research and one of KDD XAI 2019\u2019s keynote speakers, demonstrated how his team built a highly accurate, intelligible, and editable ML model based on generalized additive models (GAMs)\u00b9\u00b9 and applied it to mortality prediction in pneumonia cases\u00b9\u00b2. This version, named also GA2M ( or \u201cGAM on steroids\u201d) is optimized by gradient boosting instead of the cubic splines in the original version, and achieves results comparable to modern ML models (such as random forest or gradient-boosted trees).", "Caruana shared how his team uses intelligible models to better understand and correct their data. For example, the intelligible model learned the rule that patients with pneumonia who have a history of asthma have a lower risk of dying from pneumonia than the general population. This rule is counterintuitive, but reflects a true pattern in the training data: patients with a history of asthma who presented with pneumonia usually were admitted not only to the hospital but directly to the Intensive Care Unit. The aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia in comparison with the general population. Because the prognosis for these patients is better than average, models trained on the data incorrectly learn that asthma lowers mortality risk, when in fact asthmatics have much higher risk (if not aggressively treated).", "If simpler, intelligible models can learn counterintuitive association \u2014 such as, having asthma implies lower pneumonia risk \u2014 more complex neural network-based algorithms can probably do the same. Even if we can remove the asthma bias from the data, what other incorrect things were learned? This is the classic problem of statistical confounding: when a variable (in our case, treatment intensity) is associated with both the dependent and independent variable, causing a spurious association. The treatment intensity is influenced by the variable of asthma, and in turn reduces the risk of mortality.", "This observation illustrates the importance of model intelligibility in high stakes decision-making. Models that captured true but spurious patterns or idiosyncrasies in the data \u2014 such as false association in the pneumonia example or societal biases \u2014 could generate predictions that lead to undesirable consequential outcomes such as mistreating patients. Current ML models are trained to minimize prediction errors on the training data and not on aligning with any human intuition and concepts, so there\u2019s no guarantee that models will align the human\u2019s values. More often than not, ML models trained on human-curated datasets will reflect the defect or bias in the data\u00b9\u00b3. An intelligible model allows these defects to surface during model validation.", "Currently, only a small subset of algorithms \u2014 namely decision tree-based models and generalized additive models (GAMs) \u2014 are intelligible. Decision tree-based models and GAMs are not used in ML applications (such as computer vision, natural language processing, and time series predictions) because the best possible versions of these models currently do not perform at the state-of-the-art-level of complex deep neural network-based models.", "When we\u2019re able to choose between equally-performing intelligible and black box models, the best practice is to choose the intelligible one\u00b9\u2074. How can we know whether a high-performing intelligible model exists for a particular application? Cynthia Rudin, Professor of Computer Science at Duke University and the Institute of Mathematical Statistics (IMS) Fellow 2019 (also a KDD XAI 2019 panelist) proposed a diagnostic tool, called the \u201cRashomon Curve,\u201d\u00b9\u2075 that helps ML practitioners answer this question.", "Let\u2019s first define a few terms. \u201cRashomon effect\u201d denotes the situation in which there exist many different and approximately-equally accurate descriptions to explain a phenomenon. The term \u201cRashomon effect\u201d is derived from a popular Japanese film (Rashomon) known for a plot that involves various characters providing self-serving descriptions of the same incident. A \u201cRashomon set,\u201d defined over the hypothesis space of all possible models in a model class, is a subset of ML models that have training performance close to the best model in the class. The \u201cRashomon ratio\u201d is the cardinality of the Rashomon set divided by the cardinality of all possible models (with varying levels of accuracy). Thus, \u201cRashomon ratio\u201d is defined uniquely for each ML task/dataset pair. When the Rashomon ratio is large, there exist several equally highly accurate ML models to solve that ML task. Some of these highly accurate models within the Rashomon set might have desirable properties such as intelligibility and it may be worthwhile to find such models. Thus, Rashomon ratio serves as an indicator of the simplicity of the ML problem.", "In her KDD 2019 keynote talk, Rudin introduced the \u201cRashomon curve\u201d\u00b9\u2075 (see figure below), a diagnostic curve connecting the log Rashomon ratio of hierarchy of model classes with increasing complexity as a function of the empirical risk (the error rate bound on the model classes).", "When solving an ML problem, one might consider a hierarchy of model classes starting from simpler to more complex model (hypothesis) classes. In the beginning, the model classes remain too simple for the ML task and the model\u2019s error rate continues to decrease with increasing complexity. This observation corresponds to moving along the horizontal part of the Rashomon curve from right to left. In this case, the Rashomon volume grows at about the same rate as the volume of all the set of all possible models (with varying accuracy). In the regime when the ML model classes start to become too complex for the ML tasks, the model error rates remain the same. This corresponds to traversing the vertical part of the Rashomon curve from the top toward the bottom. In this regime, the set of all possible models outgrow the Rashomon set and the Rashomon ratio drops sharply. The turning point in the Rashomon curve (\u201cRashomon elbow\u201d) is a sweet spot where lower complexity (higher log Rashomon ratio) and higher accuracy (low empirical risk) meet. Thus, among the hierarchy of model classes, those that fall in the vicinity of the Rashomon elbow are likely to have the right level of complexity for achieving the best balance of high accuracy with desired properties such as generalizability and interpretability.", "Interpretability methods can provide two types of explanations: local and global\u00b9\u2076. Local explanations describe how a model classifies a single data instance, and answer questions such as, \u201cWhich data element(s) are most responsible for the classification output?\u201d In image classification, this is equivalent to identifying which pixel is responsible for a \u201ccat\u201d image class prediction, and by how much. Local explanations are crucial for investigating ML decisions around individual data points.", "A global explanation, on the other hand, attempts to provide a holistic summarization of how a model generates predictions for an entire class of objects or data sets, rather than focusing on a single prediction and data point.", "The two most popular techniques for global explanations are feature importance and partial dependence plots. Feature importance provides a score that indicates how useful or valuable each feature was in the construction of the model. In models based on decision trees (like random forests or gradient boosting), the more a feature is used to make key decisions within the decision trees, the higher its relative importance. Partial dependence plots (PDP) show the dependence between the target variable and a set of \u201ctarget\u201d features, marginalizing over the values of all other features (the \u201ccomplement\u201d features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the \u201ctarget\u201d features. A partial dependence plot helps us understand how a specific feature value affects predictions, which can be useful for model and data debugging as demonstrated in\u00b9\u00b2.", "Early interpretability methods relied on using input features to construct the explanation. This approach is known as feature-based explanation. A key difficulty with feature-based explanations is that most ML models operate on features, such as pixel values, that do not correspond to high-level concepts that humans can easily understand. In her KDD XAI 2019 keynote, Been Kim, Senior Research Scientist at Google Brain, pointed out that feature-based explanations applied to state-of-the-art complex black-box models (such as InceptionV3 or GoogleLeNet) can yield non-sensible explanations\u00b9\u2077 \u00b9\u2078. More importantly, feature-based explanations for ML problems where the input features have high dimensionality does not necessarily lead to human-friendly explanations.", "Concept-based explainability constructs the explanation based on human-defined concepts rather than a representation of the inputs based on features and internal model (activation) states. To achieve this, the input feature and model internal state and human-defined concept are represented in two vector spaces: (Em) and (Eh), respectively. The functional mapping between these two vector spaces, if it exists, provides a way of extracting human-defined concepts from input features and ML model internal states.", "In her keynote, Kim presented testing with concept activation vector (TCAV), a procedure to quantitatively translate between the human-defined concept space (Eh) and the model internal state (Em)\u00b9\u2079. TCAV requires two main ingredients: (1) concept-containing inputs and negative samples (random inputs), and (2) pretrained ML models on which the concepts are tested. To test how well a trained ML model captured a particular concept, the concept-containing and random inputs are inferenced on subcomponents (layers) of a trained ML model. Then, a linear classifier such as a support vector machine is trained to distinguish the activation of the network due to concept-containing vs. random inputs. A result of this training are concept activation vectors (CAVs). Once CAVs are defined, the directional derivative of the class probability along CAVs can be computed for each instance that belong to a class. Finally, the \u201cconcept importance\u201d for a class is computed as a fraction of the instances in the class that get positively activated by the concept containing inputs vs. random inputs. This approach allows humans to ask whether a model \u201clearns\u201d a particular expressible concept, and how well.", "For example, a human can ask how well a computer vision model \u201cX\u201d learns to associate the concept of \u201cwhite coat\u201d or \u201cstethoscope\u201d in doctor images using TCAV. To do this, human testers can first assemble a collection of images containing white coats and random images, then apply the pretrained \u201cX\u201d on this collection of images to get the predictions, and compute the TCAV scores for the \u201cwhite coat\u201d concept. This TCAV score quantifies how important the concept of \u201cwhite coat\u201d was to a prediction of class \u201cdoctor\u201d in an image classification task. TCAV is an example-driven approach, so it still requires careful selection of the concept data instances as inputs. TCAV also relies on humans to generate concepts to test, and on having the concept be expressible in the concept inputs.", "Concept-based interpretability methods like TCAV are a step toward extracting \u201chuman-friendly\u201d ML explanations. It is up to today\u2019s ML practitioners to make responsible and correct judgment calls on whether model predictions are sensible, and whether they align with our positive values. It is up to us to correct the defects in trained black-box ML models, and TCAV can help illuminate where the flaws are.", "As a community of ML practitioners, it\u2019s our responsibility to clearly define what we want Explainable AI to become, and to establish guidelines for generating explanations that take into consideration what piece of information to use, how (in what manner) to construct the explainability in a way that is beneficial (not harmful or abusive), and when (in which situation/context and to whom) to deliver it. While today\u2019s Explainable AI methods help to pinpoint the defects in ML systems, there\u2019s much work ahead of us.", "For now, here are some tips for bringing explainability to the forefront of today\u2019s practice:", "In the span of a few short years, explainable AI as a field has come a very long way. As co-organizers of this workshop, we were privileged to witness tremendous enthusiasm for explainability in ML. For all of us, explainability can be our \u201ctrue North.\u201d How we can use ML responsibly by ensuring that \u201cour values are aligned and our knowledge is reflected\u201d for the benefit of humanity. This goes beyond achieving end user\u2019 trust or achieving fairness in a narrowly-defined sense. We want to use explainability in conjunction with societal values for the benefit of everyone whose life and livelihood comes into contact with, or is affected by, ML.", "We would like to thank the community of volunteers who helped review the XAI KDD workshop papers in a timely manner. We are also grateful to our workshop speakers and panelists for sharing their knowledge, wisdom and superb content.", "[1] Joy Buolamwini, Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency 2018 PMLR 81:77\u201391.", "[2] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386, 2016.", "[3] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) 2017, pages 3429\u20133437.", "[4] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in Neural Information Processing Systems (NIPS) 2017, pages 6967\u20136976.", "[5] Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image classifiers by counterfactual generation. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2019.", "[6] Scoot Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems, 2017.", "[7] Mukund Sundararajan, Ankur Taly, Qiqi Yan. Axiomatic Attribution for Deep Networks. In Proceedings of the 34th International Conference on Machine Learning (ICML) 2017 Vol. 70, Pages 3319\u20133328.", "[8] Avanti Shrikumar, Peyton Greenside, Anshul Kundaje. Learning Important Features Through Propagating Activation Differences. In ICML 2017 and PMLR Vol 70 pages 3145\u20133153.", "[9] Been Kim, Cynthia Rudin, and Julie A Shah. The Bayesian Case Model: A generative approach for case-based reasoning and prototype classification. In NIPS 2014, pages 1952\u20131960.", "[12] Rich Caruana, Paul Koch, Yin Lou, Marc Sturm, Johannes Gehrke, Noemie Elhadad. Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Pages 1721\u20131730.", "[13] Yair Horesh, Noa Haas, Elhanan Mishraky, Yehezkel S. Resheff, Shir Meir Lador Paired-Consistency: An Example-Based Model-Agnostic Approach to Fairness Regularization in Machine Learning. In arXiv:1908.02641, 2019.", "[14] Cynthia Rudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. In Nature Machine Intelligence (2019) Vol.1, pages 206\u2013215.", "[15] Semenova, Lesia, and Cynthia Rudin. A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. In arXiv preprint arXiv:1908.01755, 2019.", "[17] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim. Sanity Checks for Saliency Maps. In NeurIPS 2018.", "[18] Mengjiao Yang and Been Kim BIM: Towards Quantitative Evaluation of Interpretability Methods with Ground Truth. In arXiv:1907.09701, 2019.", "[19] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In ICML 2018, pages 2673\u20132682.", "[22] Berk Ustun, Alexander Spangher, Yang Liu. Actionable Recourse in Linear Classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT*) 2019, Pages 10\u201319.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F649672aa7bdd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@DataLady?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DataLady?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Shir Meir Lador"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa89cb824340d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&user=Shir+Meir+Lador&userId=a89cb824340d&source=post_page-a89cb824340d----649672aa7bdd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@Joy_Rimchala", "anchor_text": "Joy Rimchala"}, {"url": "https://medium.com/@DataLady", "anchor_text": "Shir Meir Lador"}, {"url": "http://proceedings.mlr.press/v81/buolamwini18a.html", "anchor_text": "GenderShades"}, {"url": "http://gendershades.org/overview.html", "anchor_text": "GenderShades"}, {"url": "https://azure.microsoft.com/en-us/services/cognitive-services/face/", "anchor_text": "Microsoft Cognitive Services Face API"}, {"url": "https://www.faceplusplus.com/", "anchor_text": "FACE++"}, {"url": "https://www.ibm.com/watson/services/visual-recognition/?mhsrc=ibmsearch_a&mhq=face%20recognition", "anchor_text": "IBM watson visual recognition"}, {"url": "https://xai.kdd2019.a.intuit.com/", "anchor_text": "XAI 2019"}, {"url": "https://www.kdd.org/kdd2019/#!", "anchor_text": "KDD 2019"}, {"url": "https://arxiv.org/abs/1606.05386", "anchor_text": "\u00b2"}, {"url": "https://arxiv.org/abs/1704.03296", "anchor_text": "\u00b3"}, {"url": "https://arxiv.org/abs/1705.07857", "anchor_text": "\u2074"}, {"url": "https://arxiv.org/abs/1807.08024", "anchor_text": "\u2075"}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "LIME"}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "Shapley\u2076,"}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Integrated Gradients"}, {"url": "https://arxiv.org/abs/1704.02685", "anchor_text": "DeepLIFT"}, {"url": "https://arxiv.org/abs/1503.01161", "anchor_text": "\u2079"}, {"url": "https://arxiv.org/abs/1405.4047", "anchor_text": "\u00b9\u2070"}, {"url": "https://www.microsoft.com/en-us/research/people/rcaruana/", "anchor_text": "Rich Caruana"}, {"url": "https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478440", "anchor_text": "\u00b9\u00b9"}, {"url": "https://dl.acm.org/citation.cfm?id=2788613", "anchor_text": "\u00b9\u00b2"}, {"url": "https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf", "anchor_text": "This version"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "gradient boosting"}, {"url": "http://mathworld.wolfram.com/CubicSpline.html", "anchor_text": "splines"}, {"url": "https://en.wikipedia.org/wiki/Confounding", "anchor_text": "statistical confounding"}, {"url": "https://arxiv.org/abs/1908.02641", "anchor_text": "\u00b9\u00b3"}, {"url": "https://arxiv.org/abs/1811.10154", "anchor_text": "\u00b9\u2074"}, {"url": "https://users.cs.duke.edu/~cynthia/", "anchor_text": "Cynthia Rudin"}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "\u00b9\u2075"}, {"url": "https://en.wikipedia.org/wiki/Rashomon", "anchor_text": "Rashomon"}, {"url": "https://www.youtube.com/watch?v=wL4X4lG20sM", "anchor_text": "KDD 2019 keynote talk"}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "\u00b9\u2075"}, {"url": "https://arxiv.org/abs/1702.08608", "anchor_text": "\u00b9\u2076"}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "[2]"}, {"url": "https://github.com/google/inception", "anchor_text": "Inception neural network"}, {"url": "https://archive.ics.uci.edu/ml/datasets/census+income", "anchor_text": "census income dataset"}, {"url": "https://github.com/microsoft/interpret", "anchor_text": "InterpertML"}, {"url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "anchor_text": "partial dependence plots"}, {"url": "https://dl.acm.org/citation.cfm?id=2788613", "anchor_text": "\u00b9\u00b2"}, {"url": "https://ai.google/research/people/106045/", "anchor_text": "Been Kim"}, {"url": "https://arxiv.org/abs/1512.00567", "anchor_text": "InceptionV3"}, {"url": "https://ai.google/research/pubs/pub43022", "anchor_text": "GoogleLeNet"}, {"url": "https://arxiv.org/abs/1810.03292", "anchor_text": "\u00b9\u2077"}, {"url": "https://arxiv.org/abs/1907.09701", "anchor_text": "\u00b9\u2078"}, {"url": "https://github.com/tensorflow/tcav", "anchor_text": "https://github.com/tensorflow/tcav"}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "\u00b9\u2079"}, {"url": "https://arxiv.org/abs/1806.07538", "anchor_text": "\u00b2\u2070"}, {"url": "https://arxiv.org/abs/1907.07165", "anchor_text": "\u00b2\u00b9"}, {"url": "https://arxiv.org/abs/1809.06514", "anchor_text": "\u00b2\u00b2"}, {"url": "http://proceedings.mlr.press/v81/buolamwini18a.html", "anchor_text": "PMLR 81:77\u201391"}, {"url": "https://arxiv.org/abs/1606.05386", "anchor_text": "arXiv preprint arXiv:1606.05386"}, {"url": "https://arxiv.org/abs/1704.03296", "anchor_text": "pages 3429\u20133437"}, {"url": "https://arxiv.org/abs/1705.07857", "anchor_text": "pages 6967\u20136976"}, {"url": "https://arxiv.org/abs/1807.08024", "anchor_text": "Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2019"}, {"url": "https://arxiv.org/abs/1705.07874", "anchor_text": "Advances in Neural Information Processing Systems, 2017"}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Pages 3319\u20133328"}, {"url": "https://arxiv.org/abs/1704.02685", "anchor_text": "3145\u20133153"}, {"url": "https://arxiv.org/abs/1503.01161", "anchor_text": "pages 1952\u20131960"}, {"url": "https://arxiv.org/abs/1405.4047", "anchor_text": "arXiv:1405.4047"}, {"url": "https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478440", "anchor_text": "82:398, 371\u2013386"}, {"url": "https://www.microsoft.com/en-us/research/people/rcaruana/", "anchor_text": "Rich Caruana"}, {"url": "https://www.microsoft.com/en-us/research/people/paulkoch/", "anchor_text": "Paul Koch"}, {"url": "https://www.microsoft.com/en-us/research/people/johannes/", "anchor_text": "Johannes Gehrke"}, {"url": "https://www.microsoft.com/en-us/research/publication/intelligible-models-healthcare-predicting-pneumonia-risk-hospital-30-day-readmission/", "anchor_text": "Pages 1721\u20131730"}, {"url": "https://arxiv.org/abs/1908.02641", "anchor_text": "arXiv:1908.02641"}, {"url": "https://arxiv.org/abs/1811.10154", "anchor_text": "pages 206\u2013215"}, {"url": "https://arxiv.org/abs/1908.01755", "anchor_text": "arXiv:1908.01755"}, {"url": "https://arxiv.org/abs/1702.08608", "anchor_text": "arXiv:1702.08608"}, {"url": "https://arxiv.org/abs/1810.03292", "anchor_text": "NeurIPS 2018"}, {"url": "https://arxiv.org/abs/1907.09701", "anchor_text": "arXiv:1907.09701"}, {"url": "https://arxiv.org/abs/1711.11279", "anchor_text": "pages 2673\u20132682"}, {"url": "https://arxiv.org/abs/1806.07538", "anchor_text": "arXiv:1806.07538"}, {"url": "https://arxiv.org/abs/1907.07165", "anchor_text": "arXiv:1907.07165"}, {"url": "https://arxiv.org/abs/1809.06514", "anchor_text": "Pages 10\u201319"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----649672aa7bdd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/explainability?source=post_page-----649672aa7bdd---------------explainability-----------------", "anchor_text": "Explainability"}, {"url": "https://medium.com/tag/data-science?source=post_page-----649672aa7bdd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&user=Shir+Meir+Lador&userId=a89cb824340d&source=-----649672aa7bdd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&user=Shir+Meir+Lador&userId=a89cb824340d&source=-----649672aa7bdd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F649672aa7bdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----649672aa7bdd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----649672aa7bdd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----649672aa7bdd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----649672aa7bdd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DataLady?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DataLady?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shir Meir Lador"}, {"url": "https://medium.com/@DataLady/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa89cb824340d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&user=Shir+Meir+Lador&userId=a89cb824340d&source=post_page-a89cb824340d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcae59e1cb68d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnavigating-the-sea-of-explainability-649672aa7bdd&newsletterV3=a89cb824340d&newsletterV3Id=cae59e1cb68d&user=Shir+Meir+Lador&userId=a89cb824340d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}