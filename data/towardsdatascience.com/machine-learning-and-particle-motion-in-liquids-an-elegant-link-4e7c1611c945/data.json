{"url": "https://towardsdatascience.com/machine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945", "time": 1682994839.014087, "path": "towardsdatascience.com/machine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945/", "webpage": {"metadata": {"title": "Machine Learning and Particle Motion in Liquids: An Elegant Link | by Marco Tavora Ph.D. | Towards Data Science", "h1": "Machine Learning and Particle Motion in Liquids: An Elegant Link", "description": "The gradient descent algorithm is one of the most popular optimization techniques in machine learning. It comes in three flavors: batch or \u201cvanilla\u201d gradient descent (GD), stochastic gradient descent\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent algorithm", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1609.04747.pdf", "anchor_text": "three flavors", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1609.04747.pdf", "anchor_text": "differ in the amount of data", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Langevin_dynamics", "anchor_text": "Langevin Dynamics", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Albert_Einstein", "anchor_text": "Albert Einstein", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": "Paul Langevin", "paragraph_index": 1}, {"url": "https://core.ac.uk/download/pdf/4380833.pdf", "anchor_text": "elegant explanation", "paragraph_index": 2}, {"url": "https://books.google.com.br/books/about/Nonequilibrium_Statistical_Mechanics.html?id=4cI5136OdoMC&redir_esc=y", "anchor_text": "theoretical physics", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Annus_Mirabilis_papers", "anchor_text": "four papers", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/%C3%9Cber_die_von_der_molekularkinetischen_Theorie_der_W%C3%A4rme_geforderte_Bewegung_von_in_ruhenden_Fl%C3%BCssigkeiten_suspendierten_Teilchen", "anchor_text": "one of those papers", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Brownian_motion", "anchor_text": "Brownian motion", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Robert_Brown_(botanist,_born_1773)", "anchor_text": "pollen grains moving in water for example", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Molecular_dynamics", "anchor_text": "molecular dynamics", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Stochastic_calculus", "anchor_text": "stochastic calculus", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": "Paul Langevin", "paragraph_index": 6}, {"url": "https://aapt.scitation.org/doi/10.1119/1.18725", "anchor_text": "another groundbreaking article", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Langevin_equation", "anchor_text": "Langevin Equation", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Langevin_dynamics", "anchor_text": "Gaussian process", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Conservative_force", "anchor_text": "conservative", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Langevin_equation", "anchor_text": "Langevin equation", "paragraph_index": 10}, {"url": "https://cyrille.rossant.net/ipython-cookbook-second-edition/", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://pdfs.semanticscholar.org/e86f/414f860a1a70e16d9718c887f4eb59a51f62.pdf", "anchor_text": "important property", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Stationary_distribution", "anchor_text": "stationary distribution", "paragraph_index": 13}, {"url": "https://stuff.mit.edu/afs/athena/course/6/6.435/www/Hajek88.pdf", "anchor_text": "Hajek", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Simulated_annealing", "anchor_text": "simulated annealing technique", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "certain properties", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Line_search", "anchor_text": "certain protocols", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "SGD", "paragraph_index": 21}, {"url": "https://imgur.com/s25RsOr", "anchor_text": "great animation", "paragraph_index": 26}, {"url": "http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent", "anchor_text": "these other methods", "paragraph_index": 26}, {"url": "https://ieeexplore.ieee.org/document/4048402", "anchor_text": "many proofs", "paragraph_index": 33}, {"url": "https://github.com/marcotav", "anchor_text": "Github", "paragraph_index": 35}, {"url": "https://marcotavora.me/", "anchor_text": "www.marcotavora.me", "paragraph_index": 35}], "all_paragraphs": ["The gradient descent algorithm is one of the most popular optimization techniques in machine learning. It comes in three flavors: batch or \u201cvanilla\u201d gradient descent (GD), stochastic gradient descent (SGD), and mini-batch gradient descent which differ in the amount of data used to compute the gradient of the loss function at each iteration.", "The goal of this article is to describe the progress in the search for global optimizers based on Langevin Dynamics (LD), a modeling approach for molecular motion which has its origins on works by Albert Einstein and Paul Langevin on statistical mechanics in the early 1900s.", "I will provide an elegant explanation, from the perspective of theoretical physics, to why variants of gradient descent are efficient global optimizers.", "There was no sign that a revolution was about to happen. In 1904, had Albert Einstein abandoned physics, it is likely that his fellow scientists wouldn\u2019t even have noticed. Fortunately, that didn\u2019t happen. In 1905, the young patent clerk published four papers that revolutionized science.", "In one of those papers, Einstein derived a model for the so-called Brownian motion, the random motion of suspended particles in a liquid caused by collisions with smaller, fast-moving molecules (pollen grains moving in water for example).", "In this paper, he single-handedly confirmed the existence of atoms and molecules thus giving birth to a new branch of physics called molecular dynamics, and created a brand new field of applied mathematics known as stochastic calculus.", "In 1908, three years after Einstein published his landmark papers, the French physicist Paul Langevin published another groundbreaking article where he generalized Einstein\u2019s theory and developed a new differential equation describing Brownian motion, known today as the Langevin Equation (LE)", "where x is the position of the moving particle, m is its mass, R represents a (random) force generated by collisions with the smaller, fast-moving molecules of the fluid (see the animation above) and F represents any other external force. The random force R is a delta-correlated stationary Gaussian process with the following mean and variance:", "The term \u201cdelta-correlated\u201d means that the forces at two different times have zero correlation. The LE was the first mathematical equation describing thermodynamics out of equilibrium.", "If the particle\u2019s mass is small enough, we can set the left-hand side to zero. Furthermore, we can express a (conservative) force as the derivative of some potential energy. We obtain:", "where \u03b4t is a small time interval, and moving terms around, we obtain the discretized Langevin equation for a particle with small mass:", "Expressed this way, the Langevin equation describes incremental displacements of a particle undergoing Brownian motion.", "A Python implementation of Brownian motion can be found here. To simulate a 2D discrete Brownian process two 1D processes are used. The steps of the codes are:", "An important property of Langevin dynamics is that the diffusion distribution p(x) of the stochastic process x(t) (where x(t) obeys the Langevin equation given above) converges to a stationary distribution, the ubiquitous Boltzmann distribution (BD)", "which concentrates around the global minimum of the potential energy E(x) (from its functional form, we can easily see that the BD peaks on the global minimum of the potential energy E(x)). More precisely, as shown by Hajek and others, if the temperature is slowly decreased to zero following the discrete steps:", "then p(x) will converge to the Boltzmann distribution for large values of n (and x will converge to the global minimum of E(x)). The Langevin equation for time-dependent temperatures is usually interpreted as describing the decay of metastable physical states into the ground state of the system (which is the global minimum of the energy). One can, therefore, use Langevin dynamics to design algorithms that are global minimizers even of potential non-convex function.", "This principle is the basis of the simulated annealing technique, used for obtaining the approximate global optimum of functions.", "I will now switch gears to machine learning optimization algorithms.", "Gradient descent is a simple iterative optimization algorithm for minimizing (or maximizing) functions. In the context of machine learning, these functions are the loss functions (or cost functions). For concreteness, consider a multivariate loss function L(w) defined for all points w around some fixed point p. The GD algorithm is based on the simple property that starting at any point p the function L(w) decreases fastest in the direction of its negative gradient:", "One starts by guessing an initial value for the (unknown) minimum and computes the sequence", "where \u03b3 is the learning rate, which is allowed to change at each iteration n. If the loss function L and its gradient have certain properties and the learning rate variation is chosen following certain protocols, local convergence is guaranteed (convergence to a global minimum is guaranteed only if L is convex since for convex functions any local minimum is also a global one).", "In contrast to the basic GD algorithm, which scans the full dataset at each iteration, SGD and mini-batch GD uses only a subset of the training data. SGD uses a single sample of the training data to update the gradient in each iteration i.e. as it sweeps through the training data, it performs the above update of w for each training example. Mini-batch GD performs parameter updates using mini-batches of training examples.", "Let us put this in mathematical terms. For a general training set", "the loss function has the general form:", "In the case of mini-batch gradient descent, the sum is only over the training examples inside the batch. SGD, in particular, uses one sample only. These procedures have two main advantages versus vanilla GD: they are much faster and can handle datasets that are much larger (since they use either one or a few samples).", "Defining G and g as indicated below, we have in this case:", "In the great animation below by Alec Radford, the SGD convergence is shown together with other methods (these other methods, not mentioned in this article, are more recent improvements of SGD).", "The next (and last) step is crucial for the argument. I omitted more rigorous aspects for the main idea to come across.", "We can write the mini-batch gradient as a sum between the full gradient and a normally distributed \u03b7:", "Now substituting this expression in the GD iteration expression we obtain:", "Comparing the expression for the mini-batch gradient descent iteration with the Langevin equation we can immediately notice their similarity. More precisely, they become identical using the following correspondence:", "Substituting \u03b4t by \u03b3 in the second expression we find that", "SGD or mini-batch gradient descent algorithms are therefore formally analogous to Langevin processes and that explains why, provided the learning rate is varied following the previously mentioned protocols, they have a very high probability of selecting global minima.", "This result is far from new. There are in fact many proofs that the addition of a noise term to the usual gradient descent recursion makes the algorithm converge to global minima. It must be emphasized that it is the \u201ccooling protocol\u201d of the learning rate that provides the crucial \u201cextra\u201d randomization that allows the algorithm to escape local minima and converge to the global one.", "In this article, I showed that by thinking of either stochastic or mini-batch gradient descent as Langevin stochastic processes (with energy identified with the loss function) and including an extra level of randomization via the learning rate, we can understand why these algorithms can work so well as global optimizers. It is an elegant result which shows that examining a problem from multiple viewpoints is often extremely useful.", "My Github and my personal website www.marcotavora.me have (hopefully) some other interesting stuff both about data science and physics.", "As always, constructive criticism and feedback are welcome!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Theoretical physicist, data scientist, and scientific writer."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4e7c1611c945&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://concerningphysicsandmath.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": ""}, {"url": "https://concerningphysicsandmath.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Marco Tavora Ph.D."}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9bfccc40b00d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&user=Marco+Tavora+Ph.D.&userId=9bfccc40b00d&source=post_page-9bfccc40b00d----4e7c1611c945---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/fr/users/ronymichaud-647623/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=578897", "anchor_text": "rony michaud"}, {"url": "https://pixabay.com/fr/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=578897", "anchor_text": "Pixabay"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent algorithm"}, {"url": "https://arxiv.org/pdf/1609.04747.pdf", "anchor_text": "three flavors"}, {"url": "https://arxiv.org/pdf/1609.04747.pdf", "anchor_text": "differ in the amount of data"}, {"url": "https://en.wikipedia.org/wiki/Langevin_dynamics", "anchor_text": "Langevin Dynamics"}, {"url": "https://en.wikipedia.org/wiki/Albert_Einstein", "anchor_text": "Albert Einstein"}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": "Paul Langevin"}, {"url": "https://core.ac.uk/download/pdf/4380833.pdf", "anchor_text": "elegant explanation"}, {"url": "https://books.google.com.br/books/about/Nonequilibrium_Statistical_Mechanics.html?id=4cI5136OdoMC&redir_esc=y", "anchor_text": "theoretical physics"}, {"url": "https://en.wikipedia.org/wiki/Annus_Mirabilis_papers", "anchor_text": "four papers"}, {"url": "https://en.wikipedia.org/wiki/Annus_Mirabilis_papers", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Annus_Mirabilis_papers", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/%C3%9Cber_die_von_der_molekularkinetischen_Theorie_der_W%C3%A4rme_geforderte_Bewegung_von_in_ruhenden_Fl%C3%BCssigkeiten_suspendierten_Teilchen", "anchor_text": "one of those papers"}, {"url": "https://en.wikipedia.org/wiki/Brownian_motion", "anchor_text": "Brownian motion"}, {"url": "https://en.wikipedia.org/wiki/Robert_Brown_(botanist,_born_1773)", "anchor_text": "pollen grains moving in water for example"}, {"url": "https://en.wikipedia.org/wiki/Brownian_motion", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Brownian_motion", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Molecular_dynamics", "anchor_text": "molecular dynamics"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_calculus", "anchor_text": "stochastic calculus"}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": "Paul Langevin"}, {"url": "https://aapt.scitation.org/doi/10.1119/1.18725", "anchor_text": "another groundbreaking article"}, {"url": "https://en.wikipedia.org/wiki/Langevin_equation", "anchor_text": "Langevin Equation"}, {"url": "https://en.wikipedia.org/wiki/Langevin_dynamics", "anchor_text": "Gaussian process"}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Paul_Langevin", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Conservative_force", "anchor_text": "conservative"}, {"url": "https://en.wikipedia.org/wiki/Langevin_equation", "anchor_text": "Langevin equation"}, {"url": "https://cyrille.rossant.net/ipython-cookbook-second-edition/", "anchor_text": "here"}, {"url": "https://github.com/ipython-books/cookbook-2nd/blob/master/chapter13_stochastic/03_brownian.md", "anchor_text": "source"}, {"url": "https://pdfs.semanticscholar.org/e86f/414f860a1a70e16d9718c887f4eb59a51f62.pdf", "anchor_text": "important property"}, {"url": "https://en.wikipedia.org/wiki/Stationary_distribution", "anchor_text": "stationary distribution"}, {"url": "https://en.wikipedia.org/wiki/Boltzmann_distribution", "anchor_text": ""}, {"url": "https://stuff.mit.edu/afs/athena/course/6/6.435/www/Hajek88.pdf", "anchor_text": "Hajek"}, {"url": "https://en.wikipedia.org/wiki/Simulated_annealing", "anchor_text": "simulated annealing technique"}, {"url": "https://en.wikipedia.org/wiki/Simulated_annealing", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "certain properties"}, {"url": "https://en.wikipedia.org/wiki/Line_search", "anchor_text": "certain protocols"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "SGD"}, {"url": "https://imgur.com/s25RsOr", "anchor_text": "great animation"}, {"url": "http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent", "anchor_text": "these other methods"}, {"url": "https://imgur.com/s25RsOr", "anchor_text": "Great animation"}, {"url": "https://imgur.com/s25RsOr", "anchor_text": "source"}, {"url": "https://ieeexplore.ieee.org/document/4048402", "anchor_text": "many proofs"}, {"url": "https://github.com/marcotav", "anchor_text": "Github"}, {"url": "https://marcotavora.me/", "anchor_text": "www.marcotavora.me"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4e7c1611c945---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----4e7c1611c945---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4e7c1611c945---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4e7c1611c945---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4e7c1611c945---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&user=Marco+Tavora+Ph.D.&userId=9bfccc40b00d&source=-----4e7c1611c945---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&user=Marco+Tavora+Ph.D.&userId=9bfccc40b00d&source=-----4e7c1611c945---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4e7c1611c945&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4e7c1611c945---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4e7c1611c945--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4e7c1611c945--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4e7c1611c945--------------------------------", "anchor_text": ""}, {"url": "https://concerningphysicsandmath.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://concerningphysicsandmath.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marco Tavora Ph.D."}, {"url": "https://concerningphysicsandmath.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9bfccc40b00d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&user=Marco+Tavora+Ph.D.&userId=9bfccc40b00d&source=post_page-9bfccc40b00d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F329b50c18616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945&newsletterV3=9bfccc40b00d&newsletterV3Id=329b50c18616&user=Marco+Tavora+Ph.D.&userId=9bfccc40b00d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}