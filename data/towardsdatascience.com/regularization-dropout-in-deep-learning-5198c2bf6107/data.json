{"url": "https://towardsdatascience.com/regularization-dropout-in-deep-learning-5198c2bf6107", "time": 1683016572.4383948, "path": "towardsdatascience.com/regularization-dropout-in-deep-learning-5198c2bf6107/", "webpage": {"metadata": {"title": "Regularization & Dropout in Deep Learning | by Jeremy Zhang | Towards Data Science", "h1": "Regularization & Dropout in Deep Learning", "description": "In the last post, we have coded a deep dense neural network, but to have a better and more complete neural network, we would need it to be more robust and resistant to overfitting. The commonly\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4", "anchor_text": "post", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4", "anchor_text": "post", "paragraph_index": 1}, {"url": "https://github.com/MJeremy2017/deep-learning/tree/main/regularization", "anchor_text": "Github Repo", "paragraph_index": 10}, {"url": "https://github.com/MJeremy2017/deep-learning/tree/main/dropout", "anchor_text": "Github Repo", "paragraph_index": 20}], "all_paragraphs": ["In the last post, we have coded a deep dense neural network, but to have a better and more complete neural network, we would need it to be more robust and resistant to overfitting. The commonly applied method in a deep neural network, you might have heard, are regularization and dropout. In this article, we will together understand these 2 methods and implement them in python.", "(we will directly use the function created in the last post in the following, if you get confused about some of the code, you might need to check the previous post)", "Regularization helps to prevent model from overfitting by adding an extra penalization term at the end of the loss function.", "Where m is the batch size. The shown regularization is called L2 regularization, while L2 applies square to weights, L1 regularization applies absolute value, which has the form of |W|.", "The appended extra term would enlarge the loss when either there are too many weights or the weight becomes too large, and the adjustable factor \u03bb put an emphasis on how much we want to penalize the weights.", "An intuitive understanding would be that in the process of minimizing the new loss function, some of the weights would decrease close to zero so that the corresponding neurons would have very small effect to our results, as if we are training on a smaller neural network with fewer neurons.", "In the forward process, we need only to change the loss function.", "The backward propagation of L2 regularization is actually straight forward, we only need to add the gradient of the L2 term.", "As usual, we test our model on a binary classification case and compare the model with regularization and without.", "Actually when we have the iteration goes up, the model would continue to overfit that causes error in the divide operation, suspecting that in the forward process, result A gets too close to 0.", "In contrast, the model with regularization would not overfit. For the complete implementation and training process please check my Github Repo.", "Dropout prevents overfitting by randomly shutting down some output units.", "In the process above, in each iteration, some units on layer [2] would be randomly muted, meaning that there would be less neurons working in the forward process, thus the overall structure of neural network is simplified.", "Meanwhile, the trained model would be more robust, since the model no longer can rely on any specific neurons anymore (as they could be muted in the process), all other neurons would need to learn in the training.", "You can think of dropout as adding an extra layer to the forward process.", "In the previous sessions, we have the forward equations as following,", "Where g is the activation function. Now with dropout an extra layer is applied to A^[l].", "Where D is the dropout layer. The key factor in the dropout layer is keep_prob parameter, which specifies the probability of keeping each unit. Say if keep_prob = 0.8, we would have 80% chance of keeping each output unit as it is, and 20% chance set them to 0.", "The implementation would be adding an extra mask to the result A. Assume we have an output A^{[l]} with four elements as following,", "And we want to mute the third unit while keeping the rest, what we need is a matrix of the same shape and do an element-wise multiplication as following,", "Some of the modules below are pre-imported, to check the complete code, please go to my Github Repo.", "Here we have D initialized as the same shape as A's , and convert it to 0 and 1 matrix based on the keep_prob .", "Note that after dropout, result A needs to rescale! Because some of the neurons are muted in the process, correspondingly the left neurons need to be augmented in order to match the expected value.", "The backward process is to mask the same function D to the corresponding dA.", "The backward propagation equations remain the same as we\u2019ve introduced in deep dense net implementation. The only difference lies in the matrix D . Except the last layer, all other layers with dropout would apply the corresponding masking D to dA .", "Note that in back propagation, dA also needs to be rescaled.", "The training and evaluating part with dropout, if you are interested, please check my Github link above.", "Both regularization and dropout are widely adopted methods to prevent overfitting, regularization achieves that by adding an extra punishing term at the end of the loss function and dropout by randomly mute some neurons in the forward process in order to make the network more concise.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5198c2bf6107&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----5198c2bf6107---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4", "anchor_text": "post"}, {"url": "https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4", "anchor_text": "post"}, {"url": "https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral", "anchor_text": "fabio"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/MJeremy2017/deep-learning/tree/main/regularization", "anchor_text": "Github Repo"}, {"url": "https://github.com/enggen/Deep-Learning-Coursera", "anchor_text": "https://github.com/enggen/Deep-Learning-Coursera"}, {"url": "https://github.com/MJeremy2017/deep-learning/tree/main/dropout", "anchor_text": "Github Repo"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5198c2bf6107---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python3?source=post_page-----5198c2bf6107---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5198c2bf6107---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----5198c2bf6107---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----5198c2bf6107---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5198c2bf6107&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5198c2bf6107---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5198c2bf6107--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5198c2bf6107--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5198c2bf6107--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-dropout-in-deep-learning-5198c2bf6107&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}