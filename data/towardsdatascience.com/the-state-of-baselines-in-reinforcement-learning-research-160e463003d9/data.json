{"url": "https://towardsdatascience.com/the-state-of-baselines-in-reinforcement-learning-research-160e463003d9", "time": 1683001661.268506, "path": "towardsdatascience.com/the-state-of-baselines-in-reinforcement-learning-research-160e463003d9/", "webpage": {"metadata": {"title": "Baselines in Reinforcement Learning Research | by Nathan Lambert | Towards Data Science", "h1": "Baselines in Reinforcement Learning Research", "description": "Baselines are toolsets that should be shared among practitioners in a field to establish a common ground to compare experiments. Scientific research papers should be judged on the quality of their\u2026"}, "outgoing_paragraph_urls": [{"url": "https://peterattiamd.com/", "anchor_text": "Peter Attia", "paragraph_index": 2}, {"url": "http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf", "anchor_text": "PETS", "paragraph_index": 7}, {"url": "http://www.mujoco.org/", "anchor_text": "MuJoCo", "paragraph_index": 8}, {"url": "https://cs.stanford.edu/group/manips/publications/pdfs/Park_2008_Robotica.pdf", "anchor_text": "contact forces", "paragraph_index": 8}, {"url": "https://mujoco.org/#changelist", "anchor_text": "read how much changes between MuJoCo versions", "paragraph_index": 9}, {"url": "https://github.com/openai/gym/blob/52e66f38081548e38711f51d4439d8bcc136d19e/gym/envs/mujoco/half_cheetah.py#L21", "anchor_text": "model-free implementation", "paragraph_index": 10}, {"url": "https://github.com/kchua/handful-of-trials/blob/77fd8802cc30b7683f0227c90527b5414c0df34c/dmbrl/env/half_cheetah.py#L31", "anchor_text": "model-based baseline", "paragraph_index": 11}, {"url": "https://deepmind.com/about", "anchor_text": "DeepMind", "paragraph_index": 18}, {"url": "https://ai.facebook.com/research", "anchor_text": "Facebook AI", "paragraph_index": 18}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 21}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 21}], "all_paragraphs": ["Baselines are toolsets that should be shared among practitioners in a field to establish a common ground to compare experiments. Scientific research papers should be judged on the quality of their ideas and thoroughness of experimentation, baselines can help. I\u2019ve found in reinforcement learning research, baselines have become a hinderance.", "Recently, I had a paper shedding critical light on common practice in a model-based reinforcement learning \u2014 it was criticized in a very unscientific manner, that will long term hinder the field. The reviewer turned against us because our implementation was not satisfactory on baselines for the reviewer. In the end, the primary difference was simulation versioning, which made the paper\u2019s concepts nearly untenable for a reviewer. This is extremely dangerous to scientific communities, because peak baseline performance in reinforcement learning is often more of a result of parameters tuning rather than insight during replication.", "Deploying baselines as a crutch for discussion is not scientific and only limits the field\u2019s growth. When putting trust in a simulation mechanism over what is being evaluated, a scientist that achieves a good numerical result is ultimately reinforcing potentially bad behavior. Peter Attia has commented on this danger in his podcast multiple times (albeit from the perspective of medicine). The idea is that you can have a quadrant from the options of polarized methods and outcomes, shown below.", "The options we have effect the mental state of scientists in different ways \u2014 some of which are particularly challenging.", "Ultimately, humans get positive reinforcement when they get good outcomes, so they repeat those methods. I can\u2019t help but think some of the algorithms and papers implemented in reinforcement learning fall into 3 (bad method good outcome), so mediocre-at-best methods are reinforced.", "When I spoke with the author of a \u201cstate of the art\u201d reinforcement learning algorithm during my internship at Facebook AI, he told me he changed some policy parameterization in existing code (technically speaking, he added an additional value function to better handle uncertainty) and noticed a drastic improvement in performance, so he ran with it. This turned into a first author paper and a certain amount of reinforcement learning fame. The current academic cycle rewards backtracking and explaining these upticks in performance somehow with citations, but I see this as a risky business. By no means do I criticize the individuals for promoting their work, more am looking at the system for not looking deeper than a single figure. Baselines should be leaned on for evaluation only for algorithms claiming performance there.", "As the center stage of learning algorithms move from simulation focused tasks to real robotic agents in the future, this formula will have to be re-evaluated.", "Back to the reviewer I was struck by. I understand the comment, but I want to point out the likely causes of the performance gap between our RL implementation and the original. In this case, turns out we worked with the original author to re-implement his algorithm PETS in PyTorch from its original TensorFlow (which is potential cause number one, the slight numerical differences between the two computation structures and cause notable differences in the probabilistic rollouts of control algorithms).", "The primary difference we hypothesize is the difference of a simulator. The most common simulator used today in reinforcement learning research is MuJoCo, a research project turned product for multi-body contact forces. Peak performance in some tasks of RL are from exploiting non-physical subtleties in the simulator. Non-physical effects come about because contact forces are incredibly hard to model, and are highly non-linear. When you sample a nonlinear function outside its intended region of support, \u2018messiness\u2019 occurs \u2014 compounding non-linear functions diverge rapidly.", "One can easily read how much changes between MuJoCo versions, but old research is not revisited to update results. Our implementation changed from version 1.31 to 1.51, and we lost peak performance (a factor of almost 40% reward loss on the half-cheetah task). Concerned by this, we compared this to other high-fidelity re-implementations we had, and they mirrored the drop. This difference is not notable as a contribution in academia but holds so much weight in how work is accepted.", "The other item that the reviewer could\u2019ve observed is a difference in how state data is passed through the algorithms learning and control process. The difference is most stark between the subareas of model-based and model-free reinforcement learning. Due to the need for reward to be accessible in online planning, model-based reinforcement learning needs access to all the state variables used during reward calculation without updating the state of the simulator. That results in the following subtle difference in environments \u2014 let\u2019s start with a model-free implementation.", "You can see in the model-based baseline that the observation is changed:", "Most researchers won\u2019t get to the level of this detail, but all the algorithms have been shown to have dramatic changes in performance with different state variables used. For the curious, the state data appended to the observation in the second image is the velocity, which is directly used to compute reward in this case. When model-free algorithms have direct access to reward they can be found to learn less from the remaining context.", "Ultimately flipping any of simulation versions, auto differentiation software package, or environment observation structure can cause simulated results for an algorithm to fall short. A great highlight of this is a baselining paper attempted by a group of top names in the field, found here:", "The astute reader will compare the plots they give to the original plots published, and differences abound. Replicating results in reinforcement learning is almost laughably hard.", "DeepMind\u2019s mission since conception has been:", "but this cannot happen if research is muddled by difference in minor details. I see Facebook AI\u2019s mission as a little less clear, being:", "\u201cto further our fundamental understanding in both new and existing domains, covering the full spectrum of topics related to AI, with the mission of advancing the state-of-the-art of AI through open research for the benefit of all\u201d", "Being at Facebook, I know they are fortunate to have gathered an incredible team that is making huge progress for the field, but the mission statement leaves a lot open to be covered. Over time, core values compound and create the environment for research and contribution. As institutions such as DeepMind, Facebook AI, etc. claim they have long term, audacious goals in these fields, I hope that those members without tight, academic curfew and pressures can frame their work in ways that does not contribute to the problem of over-reliance on baselines.", "More? Subscribe to my newsletter on robotics, artificial intelligence, and society!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F160e463003d9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----160e463003d9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf", "anchor_text": "1"}, {"url": "https://www.cs.toronto.edu/~tingwuwang/mbrl.html", "anchor_text": "2"}, {"url": "http://www.mujoco.org/", "anchor_text": "3"}, {"url": "https://peterattiamd.com/", "anchor_text": "Peter Attia"}, {"url": "https://www.iros2019.org/", "anchor_text": "IROS"}, {"url": "http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf", "anchor_text": "PETS"}, {"url": "https://gym.openai.com/envs/HalfCheetah-v2/", "anchor_text": "source"}, {"url": "http://www.mujoco.org/", "anchor_text": "MuJoCo"}, {"url": "https://cs.stanford.edu/group/manips/publications/pdfs/Park_2008_Robotica.pdf", "anchor_text": "contact forces"}, {"url": "https://mujoco.org/#changelist", "anchor_text": "read how much changes between MuJoCo versions"}, {"url": "https://github.com/openai/gym/blob/52e66f38081548e38711f51d4439d8bcc136d19e/gym/envs/mujoco/half_cheetah.py#L21", "anchor_text": "model-free implementation"}, {"url": "https://github.com/openai/gym/blob/52e66f38081548e38711f51d4439d8bcc136d19e/gym/envs/mujoco/half_cheetah.py#L21", "anchor_text": "source"}, {"url": "https://github.com/kchua/handful-of-trials/blob/77fd8802cc30b7683f0227c90527b5414c0df34c/dmbrl/env/half_cheetah.py#L31", "anchor_text": "model-based baseline"}, {"url": "https://github.com/kchua/handful-of-trials/blob/77fd8802cc30b7683f0227c90527b5414c0df34c/dmbrl/env/half_cheetah.py#L31", "anchor_text": "source"}, {"url": "https://www.cs.toronto.edu/~tingwuwang/mbrl.html", "anchor_text": "Benchmarking Model-Based Reinforcement LearningBenchmarking Model-Based Reinforcement LearningBenchmarking Model-Based Reinforcement Learningwww.cs.toronto.edu"}, {"url": "https://github.com/WilsonWangTHU/mbbl", "anchor_text": "WilsonWangTHU/mbblArxiv Link PDF Project Page Abstract: Model-based reinforcement learning (MBRL) is widely seen as having the potential\u2026github.com"}, {"url": "https://deepmind.com/about", "anchor_text": "DeepMind"}, {"url": "https://ai.facebook.com/research", "anchor_text": "Facebook AI"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----160e463003d9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----160e463003d9---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----160e463003d9---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/science?source=post_page-----160e463003d9---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/robotics?source=post_page-----160e463003d9---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&user=Nathan+Lambert&userId=890b1765e6d&source=-----160e463003d9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&user=Nathan+Lambert&userId=890b1765e6d&source=-----160e463003d9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F160e463003d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----160e463003d9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----160e463003d9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----160e463003d9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----160e463003d9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----160e463003d9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----160e463003d9--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-state-of-baselines-in-reinforcement-learning-research-160e463003d9&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}