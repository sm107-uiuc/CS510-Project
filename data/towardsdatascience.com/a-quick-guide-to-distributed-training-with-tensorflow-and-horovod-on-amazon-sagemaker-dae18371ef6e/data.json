{"url": "https://towardsdatascience.com/a-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e", "time": 1683004756.8720481, "path": "towardsdatascience.com/a-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e/", "webpage": {"metadata": {"title": "A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker | by Shashank Prasanna | Towards Data Science", "h1": "A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker", "description": "In deep learning, more is better. More data, more layers, and more compute power, usually leads to higher accuracy, and better robustness of trained models. I may not be able to help you collect more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://registry.opendata.aws/", "anchor_text": "collect more data", "paragraph_index": 1}, {"url": "https://horovod.readthedocs.io/en/latest/#", "anchor_text": "Horovod", "paragraph_index": 3}, {"url": "https://horovod.readthedocs.io/en/latest/", "anchor_text": "Horovod documentation page", "paragraph_index": 6}, {"url": "https://eng.uber.com/horovod/", "anchor_text": "Horovod blog post", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1802.05799.pdf", "anchor_text": "Horovod paper", "paragraph_index": 11}, {"url": "https://github.com/aws/sagemaker-python-sdk", "anchor_text": "SageMaker Python SDK", "paragraph_index": 32}, {"url": "https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html", "anchor_text": "Amazon SageMaker notebook instance", "paragraph_index": 32}, {"url": "https://sagemaker.readthedocs.io/en/stable/index.html", "anchor_text": "https://sagemaker.readthedocs.io/en/stable/index.html", "paragraph_index": 36}, {"url": "https://aws.amazon.com/cloudwatch/", "anchor_text": "Amazon CloudWatch", "paragraph_index": 46}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/generate_cifar10_tfrecords.py", "anchor_text": "training script", "paragraph_index": 47}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com", "paragraph_index": 57}], "all_paragraphs": ["In deep learning, more is better. More data, more layers, and more compute power, usually leads to higher accuracy, and better robustness of trained models.", "I may not be able to help you collect more data, but I can show how you can do distributed training on a large number of machines to train faster and run more experiments and increase your productivity.", "In this blog post, I\u2019m going to cover how you can run distributed training, without managing infrastructure \u2014 no instances to launch, no clusters to setup, no storage volumes to manage, and no containers to build. Bring in your training scripts, specify the number of GPUs, and let Amazon SageMaker handle the rest.", "In the first part of this guide, I\u2019ll provide step-by-step instructions for updating your training scripts to use the Horovod library. For distributed training to work, training processes on different GPUs need to communicate. Horovod enables this seamless communication and provides a convenient API to prepare your training scripts for distribution training. The changes you make are agnostic to the number of GPUs, so it\u2019s a one time effort.", "In the second part of this guide I\u2019ll show how you can take your updated training scripts and run them at-scale using Amazon SageMaker on as many GPUs as you want, or as little as you need \u2014 just by changing a single line of code.", "Want to follow along and run examples as you read? Jupyter notebook, and training scripts are available here:", "Horovod is a distributed deep learning framework that supports popular deep learning frameworks \u2014 TensorFlow, Keras, PyTorch, and Apache MXNet. The example in this guide uses TensorFlow and Keras. If you\u2019re a PyTorch or MXNet user updating your scripts will follow a very similar process as described here. The Horovod documentation page also includes plenty of examples for other frameworks.", "During distributed training, multiple processes need to communicate with each other. To enable communication between training processes, Horovod uses a communication protocol called Message Passing Interface (MPI). And to average gradients and update all copies of the models, it uses an approach called ring-allreduce (we\u2019ll come back to this). These approaches are not new and have been used for many years by scientists, researchers and engineers working in High-Performance Computing (HPC) to solve problems in computational fluid dynamics, molecular dynamics, computer graphics and others.", "MPI itself defines basic concepts for sending and receiving information between multiple processes in a cluster such as allreduce, allgather, and broadcast. And as you may have deduced from their names \u2014 allgather gathers data from all processes (in the case of deep learning, gradients). Broadcast, broadcasts data (gradients) from one process to every other process. allreduce (conceptually speaking) combines these two operations \u2014 gathers data from all processes, performs a reduction operation (for example, averaging gradients) and then broadcasts (the averaged gradients).", "As you increase the number of training processes, inter-process communications increases and communication overhead starts affecting scaling efficiency.", "The ring all-reduce approach improves upon vanilla allreduce by making communication cost independent of the number of processes in the system. It does this by arranging processes in a logical ring where each process only receives data from it\u2019s \u201cleft\u201d neighbor and sends data to it\u2019s \u201cright\u201d neighbor as illustrated in the accompanying figure.", "The ring-allreduce process for deep learning is described in further detail in the Horovod blog post and the Horovod paper. To use the horovod library, you don\u2019t really need to know how ring-allreduce works, but it always helps to have an intuition about how algorithms and libraries you use work.", "To use update your training script to use the Horovod library, you primarily need to know the following key concepts:", "For the purpose of illustration, let\u2019s take an example of a distributed training job on 2 GPUs \u2014 these could be on the same of different systems, it doesn\u2019t matter. Here\u2019s what happens behind the scenes:", "Step 1: During the forward pass, it\u2019s business as usual. Each copy of the model does a forward pass with a batch_size of data that it receives.", "Step 2: A backward pass is then performed to compute the gradients. But the gradient is NOT used to update the weights yet.", "Step 3: Horovod now does an allreduce operation (average gradients and then broadcast) to all processes. In this example to both GPUs.", "Step 4: The final allreduced gradients are now used to update each model", "By allowing each GPU to train on different batches of data, and allreducing the gradients, you\u2019re effectively training on a larger batch and therefore speeding up training.", "For this demo, I\u2019ll use the CIFAR-10 dataset which consists of 60,000 32x32 images belonging to 10 different classes (6,000 images per class). The training script is available on blog post\u2019s GitHub repository along with Jupyter notebook to run the full example:", "This section describe the changes that were made to the following files to prepare them for distributed training :", "To make it easier for you to follow along, I\u2019ve included the exact same section heading as a comment in the above scripts. Look for \u201cChange NUMBER\u201d", "Put these at the top of your training script to import horovod.", "Initialize horovod and get the total number of GPUs in your cluster. If you\u2019re only running this on CPUs then this will be equal to the total number of instances.", "Pin a GPU to current process.", "Update the learning rate by scaling it by number of GPUs. The effective batch during distributed training is batch_size times hvd.size(). This change is in model_def.py", "By increasing the learning rate, you compensate for the effective increase in batch size.", "The distributed optimizer does the magic of averaging gradients and broadcasting it using allreduce or allgather, and then updating the weights with the averaged gradients. This change is in model_def.py", "You\u2019ll need to divide the total number of images/batch by the number of GPUs.", "Amazon SageMaker will pass these values to the script when it launches a distributed training job.", "You\u2019re now done with the hard part \u2014 modifying your training script to make it distributed ready.", "The rest of the process \u2014 distributed training \u2014 is relatively straightforward using Amazon SageMaker.", "To run a distributed training job using Amazon SageMaker, download and install the SageMaker Python SDK. For a more convenient experience, you can also launch an Amazon SageMaker notebook instance which comes with Jupyter Notebook server, SageMaker Python SDK and popular deep learning frameworks pre-installed.", "Running a SageMaker training job involves only two key steps which I\u2019ll highlight below:", "The following code excerpts are from the following Jupyter Notebook in the blog post\u2019s repository.", "Using the SDK, you need to specify the following details so that Amazon SageMaker can get the requested resources and prepare for training", "There are many more options you can specify in the SageMaker TensorFlow estimator and you can a full list in the documentation: https://sagemaker.readthedocs.io/en/stable/index.html", "After defining the estimator, you\u2019ll need to specify the paths to your training, validation and test datasets are in Amazon S3, and pass it to the estimator\u2019s fit function.", "And you\u2019re done! Sit back and wait for the distributed training job to complete.", "You can (and should) monitor progress, which I\u2019ll cover in the next section, but first, let\u2019s take a closer look at what\u2019s happening behind the scenes.", "Amazon SageMaker does several things for you automatically so that you don\u2019t have to worry about infrastructure level details. Very briefly, SageMaker will:", "When training is initiated, Amazon SageMaker runs the exact same copy of your Horovod-updated training scripts on each instance. Each copy knows it\u2019s unique local rank using hvd.local_rank() and a GPU is pinned to that particular process. Horovod then takes care of performing ring-allreduce and updating the weights on each GPU with averaged gradients.", "One training is done, SageMaker will automatically:", "There are a couple of different options for monitoring jobs:", "If you head over to AWS Console > Amazon SageMaker > Training Jobs you can see a list of currently running jobs and jobs you\u2019ve run in the past. Click on a job and you can see details such as progress status, type of instance, hyperparameters, S3 location for datasets and model artifacts and so on.", "Scroll further down and you can see CPU, GPU and other resource utilizations.", "You\u2019ll also find a link to Amazon CloudWatch dashboard where you can monitor training job logs for all instances. This comes in handy for debugging and diagnosing when something isn\u2019t looking right.", "In the training script you\u2019ll notice that there are two Keras callbacks for logging. First for saving tensorboard log files locally in the container and second for syncing those logs to an Amazon S3 location you specified when invoking the SageMaker estimator function.", "You can now run TensorBoard anywhere you like (your laptop, desktop, EC2 instance) and point it to your Amazon S3 location with your TensorBoard logs. You\u2019ll need to make sure that you have permission to access Amazon S3 and you can set that up using the AWS CLI.", "Run the following command on a machine that has tensorboard installed and has S3 read access permissions:", "Distributed training can save you a time when dealing with large models and datasets. With libraries like Horovod and services like Amazon SageMaker, you can scale training with very little effort. In this blog post, I covered two key concepts:", "I\u2019ll leave you with a few guidelines for choosing the right instances for distributed training:", "For better performance, always favor a single instance with multiple GPUs vs. multiple instances with a single GPU.", "Let\u2019s say you want to run distributed training with 4 GPUs, always prefer a single p3.8xlarge instance rather than 4 x p3.2x large. The benefit of doing this is that when processes need to communicate to do allreduce operation, they are not crossing network barriers to communicate with other instance\u2019s CPUs and GPUs. This will add communication latency that may affect training performance. Similarly, if you want to distribute training to 8 GPUs, choose a single p3.16xlarge or p3dn.24xlarge with 8 GPUs vs. 8 x p3.2xlarge or 2 x p3.8xlarge. These multi-GPU instances include NVIDIA\u2019s NVLink technology that enables high-bandwidth inter-GPU communication to speed up the allreduce operations performed by Horovod.", "Thanks for reading, I hope you enjoyed this guide. All the code and examples are available on GitHub here:", "If you have questions about this guide, suggestions on how to improve it or ideas for new guides, please reach out to me on twitter (@shshnkp), LinkedIn or leave a comment below. Enjoy!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Talking Engineer. Runner. Coffee Connoisseur. Formerly Machine learning @Meta, AWS, NVIDIA, MATLAB, posts are my own opinions. website: shashankprasanna.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdae18371ef6e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Shashank Prasanna"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----dae18371ef6e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://registry.opendata.aws/", "anchor_text": "collect more data"}, {"url": "https://horovod.readthedocs.io/en/latest/#", "anchor_text": "Horovod"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker", "anchor_text": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker"}, {"url": "https://horovod.readthedocs.io/en/latest/", "anchor_text": "Horovod documentation page"}, {"url": "https://eng.uber.com/horovod/", "anchor_text": "Horovod blog post"}, {"url": "https://arxiv.org/pdf/1802.05799.pdf", "anchor_text": "Horovod paper"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker", "anchor_text": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/code/cifar10-tf-horovod-sagemaker.py", "anchor_text": "cifar10-tf-horovod-sagemaker.py"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/code/model_def.py", "anchor_text": "model_def.py"}, {"url": "https://github.com/aws/sagemaker-python-sdk", "anchor_text": "SageMaker Python SDK"}, {"url": "https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html", "anchor_text": "Amazon SageMaker notebook instance"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/cifar10-sagemaker-distributed.ipynb", "anchor_text": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/cifar10-sagemaker-distributed.ipynb"}, {"url": "https://sagemaker.readthedocs.io/en/stable/index.html", "anchor_text": "https://sagemaker.readthedocs.io/en/stable/index.html"}, {"url": "https://aws.amazon.com/cloudwatch/", "anchor_text": "Amazon CloudWatch"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker/blob/master/generate_cifar10_tfrecords.py", "anchor_text": "training script"}, {"url": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker", "anchor_text": "https://github.com/shashankprasanna/distributed-tensorflow-horovod-sagemaker"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dae18371ef6e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----dae18371ef6e---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/horovod?source=post_page-----dae18371ef6e---------------horovod-----------------", "anchor_text": "Horovod"}, {"url": "https://medium.com/tag/sagemaker?source=post_page-----dae18371ef6e---------------sagemaker-----------------", "anchor_text": "Sagemaker"}, {"url": "https://medium.com/tag/aws?source=post_page-----dae18371ef6e---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----dae18371ef6e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----dae18371ef6e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdae18371ef6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dae18371ef6e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dae18371ef6e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dae18371ef6e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dae18371ef6e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shashank Prasanna"}, {"url": "https://medium.com/@shashankprasanna/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "681 Followers"}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd48ce4d9cb5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e&newsletterV3=e0c596ca35b5&newsletterV3Id=d48ce4d9cb5c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}