{"url": "https://towardsdatascience.com/limericking-part-2-topic-modeling-with-lda-45476ab9af15", "time": 1682997257.578887, "path": "towardsdatascience.com/limericking-part-2-topic-modeling-with-lda-45476ab9af15/", "webpage": {"metadata": {"title": "Limericking Part 2: Topic Modeling with LDA | by Max Miller | Towards Data Science", "h1": "Limericking Part 2: Topic Modeling with LDA", "description": "Welcome to part 2 in my quest to build an automatic limerick generator. As I detailed in part 1, I am inspired by the incredible twitter account Limericking, which produces topical limericks based on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/limericking-part-1-context-and-haikus-3eb057c8154f", "anchor_text": "detailed in part 1", "paragraph_index": 0}, {"url": "https://github.com/SproulHimself/satire_classification_project", "anchor_text": "legitimate news stories and satire from sources like the Onion with 98% accuracy", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "scikit learn\u2019s", "paragraph_index": 9}, {"url": "https://www.npr.org/sections/health-shots/2019/07/13/734430818/has-your-doctor-talked-to-you-about-climate-change", "anchor_text": "Has Your Doctor Talked to You About Climate Change", "paragraph_index": 15}, {"url": "https://www.npr.org/2019/07/13/741382999/hurricane-barry-makes-landfall-in-louisiana", "anchor_text": "this article about Hurricane Barry making landfall in Louisiana", "paragraph_index": 16}, {"url": "https://www.npr.org/2019/07/13/740505613/new-white-house-press-secretary-likes-a-challenge", "anchor_text": "White House press secretary Stephanie Grisham", "paragraph_index": 21}], "all_paragraphs": ["Welcome to part 2 in my quest to build an automatic limerick generator. As I detailed in part 1, I am inspired by the incredible twitter account Limericking, which produces topical limericks based on events in the news. I have discovered that I much prefer consuming the news in topical-limerick form; so dour is the news these days, that I am finding it harder and harder to consume news in any other way.", "Unfortunately, the lone genius behind the Limericking twitter account can only produce so many. Each poem has to be crafted for meter, rhyme, and comedic turn, after all, so maybe it shouldn\u2019t be surprising that Limericking only produces a couple of poems a week. I commend the account\u2019s dedication to quality and refusal to publish anything less than a brilliant limerick, but I lament the dearth of content. Is there a way to automate the topical limerick production process (and thereby allow me to be reasonably up to date on current events while continuing to only read light verse)?", "Perhaps there is, but unfortunately a whole series of tricky data science problems need to be solved first. Our limerick generator needs to be able to take in a text and parse it for meaning, coming away with a summary sentence or two (or at least a topic and key words/figures) and then generate meaningful text that has been adjusted to fit the form of the limerick without losing its coherence. I suppose on top of that, you would want the limerick to be humorous, although that might be beyond my abilities.", "In support of my first post I stripped down the task and ended up with a series of functions that produced haikus based on the news articles. My program tried to ascertain the topic of the text by looking at the proper nouns that occur most often in the text (a strategy that usually produced reasonable results) but otherwise fit words into the haiku semi-randomly: the probability of selecting a word was weighted by how often the word appeared in the text. What would be the first step in improving this program? The first step the limerick generator would need to take would be to parse the text for meaning, so the subject of this post will be topic modeling, and in particular a method called Latent Dirichlet Allocation.", "Topic modeling is essentially a complex classification problem, and it presents two challenges. The first is that while computers can do some things with text, like count words, text information is not inherently meaningful to the computer. Humans see meaning in text both on the level of individual words and on a higher order arising from syntax and the relationships between words in a sentence (\u2018the cat bit the dog\u2019 and \u2018the dog bit the cat\u2019 mean very different things despite having all the same words). These forms of meanings are difficult for computers; the first because word meanings are by definition abstract and grounded in human context and the second because the grammatical structures of human language are mind-bogglingly complex.", "There are approaches to natural language processing that avoid meaning altogether and can still achieve powerful results. So called \u2018bag-of-words\u2019 models disregard all grammar or word order and simply represent texts as collections of words, which can then be compared probabilistically. Probabilistic models like Naive Bayes Classifiers look at the relatively frequencies of occurrences of different words in different contexts. In the topic detection use case, you can imagine that domain specific words are used more in some topics than in others: tech articles are likely to have tech words, like \u2018processor\u2019 or \u2018gigabyte\u2019, while political articles are likely to have political words, like \u2018election\u2019 or \u2018collusion\u2019. Assuming you have a well labeled and sufficiently large dataset, models like these can be quite powerful, even on relative subtle questions: a well trained Bayesian classifier could tell between legitimate news stories and satire from sources like the Onion with 98% accuracy.", "This brings us unfortunately to the second challenge, data labeling. In order to train a classifier you need a training set of data that is already well labeled, partitioned into the proper groups. This wouldn\u2019t be a challenge for something like the satire classifier \u2014 anything that comes from a source like the Onion is going to be satire and anything that comes from, say, Reuters isn\u2019t, so the data almost comes labeled by default. With something like topics, it\u2019s a little harder, particularly since even when articles might be tagged into different topical sections by the news organizations that publish them, different organizations might use different classification schemes. Imagine, for instance, the distinction between a \u2018political\u2019 story and a story about Brexit that has been filed in the \u2018international\u2019 section.", "Manually labeling a large data set of news articles would be prohibitively cumbersome. Luckily there are also unsupervised topic modeling methods, including Latent Dirichlet Allocation, that don\u2019t require well labeled training data. LDA and similar methods might be better called \u2018topic clustering\u2019 rather than topic modeling. A computer using LDA can\u2019t \u2018name\u2019 a topic exactly, it can\u2019t take in an article and definitively say that it comes from the Arts section of the newspaper. After all, the whole reason we are considering using LDA is that it doesn\u2019t require pre-labeled data, so the computer has no way of knowing which articles it receives came from the Arts section.", "Instead what LDA and similar methods try to do is group articles together, returning unnamed \u2018topics\u2019 that represent clusters of articles it thinks are similar. Hopefully, the model will group Arts section articles together without ever needing to be told which articles were in the Arts section to begin with. Two quirks of this method are that a) the computer doesn\u2019t, can\u2019t really, return directly interpretable information about the groups it finds (that requires human intervention) and b) there\u2019s no real way to validate the groupings it identifies since it is finding those groupings in an unsupervised manner.", "LDA is a \u2018bag-of-words\u2019 model in that it treats each document or text as essentially just a collection of words without any grammar or word order. It basically conceptualizes \u2018topics\u2019 as probability distributions of words and individual documents/texts as probability distributions of topics. Any given text in this view is essentially being generated by drawing words randomly from certain topics. A statistical package with an LDA function (like scikit learn\u2019s, which I used) tries to find the most sensible groupings of words within topics given a dataset of texts and a number of expected topics.", "You can imagine the workings of an LDA model a little like this: The model identifies a topic based on a cluster of articles which seem to have many words in common. A human interpreter might label this topic \u2018politics\u2019: words like \u2018senate\u2019, \u2018party\u2019 and \u2018vote\u2019 are common within the topic, while words like \u2018crepes\u2019 or \u2018hemoglobin\u2019 don\u2019t commonly appear. Another topic identified by the model might align with movie reviews and correspond with words like \u2018film\u2019, \u2018action\u2019 and \u2018cinematography\u2019. When presented with a new article that is, say, a review of a political thriller, the model would hopefully be able to identify that it most strongly corresponds with those two topics.", "The math underpinning how LDA actually finds the topic clusters is a littler complicated, although like many data science tools, actually implementing it is easy with widely available python libraries. I decided to try to use LDA to improve my haiku generator. The first step was building a training corpus of texts. I had been working with NPR articles because NPR has a convenient, easy to scrape, plain text version of the website whose urls are all identical save for an article id number. Unfortunately, the ids are not sequential and there isn\u2019t an easy to access to list of them. Someone with more web-scraping skills than I have would have found a better way to automatically crawl through the NPR site, I resulted to compiling a list of around 250 article id numbers from recent articles by hand. This is a corpus that is large enough to work with, but nowhere near as large as you would want for this purpose.", "The next step is \u2018vectorizing\u2019 the data, turning each article from a string of characters the computer can\u2019t really work with, into a mathematical object it can. I used a simple \u2018count vectorizer\u2019 which does pretty much what it sounds like: all of the unique words in the entire corpus of texts are represented as columns in a large matrix. The vector corresponding to any given text is like a row in this matrix, with a value in each column corresponding to the count of that column\u2019s word appearing in the text. As such, the matrix is \u2018sparse\u2019: because most possible words do not appear in any given text, there are a lot of zeros. An alternative to the simple count vectorization might be Term Frequency-Inverse Document Frequency (TF-IDF) which essentially weights the entries in the text vector inversely to how often the corresponding words appear in the entire corpus. Thus words common across multiple topics, like \u2018good\u2019 or maybe the names of the days of the weeks, would have less weight while words that are relatively rare are given more weight.", "Finally, running the LDA model is an easy two lines of code using scikit learn. I chose to run the model for 10 topics, and printed out the most significant words in each topic to visually inspect how well they seemed to cohere.", "In some cases, the model had clearly done an excellent job grouping related terms together, particularly with regards to these groups related to current events that I think require no gloss:", "Other topics are less legible on their face, although, one of the unexpected benefits of having a smaller data-set is that it was easy to track down some of the connections. Topic number 7 at first seemed strange, with top words mostly relating to health and medicine, but also containing things like \u2018climate change\u2019. The link was made by this one article, which discusses some of the health consequences of hotter summers and other global warming related stresses: Has Your Doctor Talked to You About Climate Change. Because this was a relatively small training set to work with, a single article like that could make a significant link between otherwise disparate topics within the model. With a larger set of texts and a larger number of topics, it\u2019s likely an LDA model would have grouped the medical and climate change articles as separate topics and thought that the article linking the two subjects was a mix of the two topics.", "Once the model is trained, it can evaluate new texts and classify them by which of its topic clusters it thinks it belongs to. For instance, given this article about Hurricane Barry making landfall in Louisiana, the model thought it most likely belonged in topic #4, a topic with numerous government and politics related words. Not a bad guess, considering that the article discusses federal responses to the hurricane. To better illustrate what was going on under the hood, I wrote a function to return the words from the text that had the largest weights within the chosen topic, and therefore contributed the most to the article being grouped with that topic:", "The out put of this function for my test article helps clarify why it was grouped with that topic:", "The test article was correctly grouped with other news articles and if you look through the words that are most weighted towards the topic cluster it identifies, you see clear examples of \u2018news-speak\u2019: \u2018residents\u2019, \u2018reports\u2019, \u2018officials\u2019, etc. Still, there are some quirks: for one thing, it identifies the day \u2018Thursday\u2019 as significant. Maybe this actually makes sense! Maybe news articles (rather than articles from the arts section) are more likely to specify a particular day of the week. It also seems to me that some words might point the model to a particular topic cluster for the \u2018wrong\u2019 reasons. The word \u2018water\u2019, for instance. Knowing a bit about the articles that went into this model, it doesn\u2019t surprise me that water might be clustered with Trump and other news related words: there have been numerous articles over the summer about the detention centers on the southern border, including discussions of whether the people shamefully imprisoned therein have access to clean water. Yet, in the context of this test article, it\u2019s a different sort of water, rain/flood water, that\u2019s being used to link the article to this topic cluster.", "Of course, the purpose of this whole exercise was to improve my limerick writing function. How would you be able to use this for that end? Two ways. One, this provides a method to filter out less \u2018useful\u2019 words from the article: you want to identify which words are \u2018content\u2019 words and use them more often than words that might be common to articles across all topics. Two, this might unlock extra vocabulary not found within the text of an article. If you\u2019re creating a limerick you may need to look for words that don\u2019t appear in the text in order to find rhymes or to generate new sentences that scan properly. How will the computer know which sorts of words make sense? This might be one way to group words: \u2018I see this article falls into this topic cluster, other words associated with this cluster are\u2026\u2019", "I tried adjusting my haiku generator to use the outputted, highly weighted words as the dictionary of words to draw from when assembling the haikus. With the previous Hurricane Barry article, my new haiku generator produced this poem:", "Given this article about the new White House press secretary Stephanie Grisham, it produced the following:", "Does this process improve my haiku generating? Not really. If anything, it makes it worse for a couple of reasons. My previous generator used words in proportion to their frequency in the text and didn\u2019t throw out any words except for stop words. Now, however, the text words are passed through a vectorizer that throws out words that don\u2019t appear frequently enough in all the texts of the corpus and is using weights that are influenced by the model/corpus overall. As a result, words that are relatively particular to a given article might not make it into the list of words to be used, even though those are exactly the sorts of words you want to appear in the summary haiku! In the long run of the project a strategy might be necessary to unlock a wider dictionary of words to use. It\u2019s clear to me, however, that in order to use this sort of strategy, you just need more: more text, and more diverse text, that can be grouped into a greater number of topic clusters.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F45476ab9af15&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----45476ab9af15--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----45476ab9af15--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----45476ab9af15---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/limericking-part-1-context-and-haikus-3eb057c8154f", "anchor_text": "detailed in part 1"}, {"url": "https://github.com/SproulHimself/satire_classification_project", "anchor_text": "legitimate news stories and satire from sources like the Onion with 98% accuracy"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "scikit learn\u2019s"}, {"url": "https://www.npr.org/sections/health-shots/2019/07/13/734430818/has-your-doctor-talked-to-you-about-climate-change", "anchor_text": "Has Your Doctor Talked to You About Climate Change"}, {"url": "https://www.npr.org/2019/07/13/741382999/hurricane-barry-makes-landfall-in-louisiana", "anchor_text": "this article about Hurricane Barry making landfall in Louisiana"}, {"url": "https://www.npr.org/2019/07/13/740505613/new-white-house-press-secretary-likes-a-challenge", "anchor_text": "White House press secretary Stephanie Grisham"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----45476ab9af15---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----45476ab9af15---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----45476ab9af15---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/limerick?source=post_page-----45476ab9af15---------------limerick-----------------", "anchor_text": "Limerick"}, {"url": "https://medium.com/tag/the-limericking-project?source=post_page-----45476ab9af15---------------the_limericking_project-----------------", "anchor_text": "The Limericking Project"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&user=Max+Miller&userId=dfd5ba1a8332&source=-----45476ab9af15---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&user=Max+Miller&userId=dfd5ba1a8332&source=-----45476ab9af15---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F45476ab9af15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----45476ab9af15---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----45476ab9af15--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----45476ab9af15--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----45476ab9af15--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----45476ab9af15--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----45476ab9af15--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flimericking-part-2-topic-modeling-with-lda-45476ab9af15&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}