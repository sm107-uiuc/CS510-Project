{"url": "https://towardsdatascience.com/topic-modeling-in-nlp-524b4cffeb68", "time": 1683007039.953301, "path": "towardsdatascience.com/topic-modeling-in-nlp-524b4cffeb68/", "webpage": {"metadata": {"title": "Topic Modeling In NLP. With a focus on Latent Dirichlet\u2026 | by Arun Jagota | Towards Data Science", "h1": "Topic Modeling In NLP", "description": "In natural language processing, the term topic means a set of words that \u201cgo together\u201d. These are the words that come to mind when thinking of this topic. Take sports. Some such words are athlete\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In natural language processing, the term topic means a set of words that \u201cgo together\u201d. These are the words that come to mind when thinking of this topic. Take sports. Some such words are athlete, soccer, and stadium.", "A topic model is one that automatically discovers topics occurring in a collection of documents. A trained model may then be used to discern which of these topics occur in new documents. The model can also pick out which portions of a document cover which topics.", "Consider Wikipedia. It has millions of documents covering hundreds of thousands of topics. Wouldn\u2019t it be great if these could be discovered automatically? Plus a finer map of which documents cover which topics. These would be useful adjuncts for people seeking to explore Wikipedia.", "We could also discover emerging topics, as documents get written about them. In some settings (such as news) where new documents are constantly being produced and recency matters, this would help us detect trending topics.", "This post covers a statistically powerful and widely used approach to this problem.", "This approach involves building explicit statistical models of topics and documents.", "A topic is modeled as a probability distribution over a fixed set of words (the lexicon). This formalizes \u201cthe set of words that come to mind when referring to this topic\u201d. A document is modeled as a probability distribution over a fixed set of topics. This reveals the topics the document covers.", "The aim of learning is to discover, from a corpus of documents, good word distributions of the various topics, as well as good topic proportions in the various documents. The number of topics is a parameter to this learning.", "At this stage, it will help to describe how to generate a synthetic document from a learned model. This will reveal key aspects of how this model operates that we haven\u2019t delved into yet.", "First, we\u2019ll pick the topics this document will cover. One way to do this is to first pick a random document from our corpus, then set the new document\u2019s topic proportions to those of the seed document.", "Next, we\u2019ll set the document length, call it n.", "Next, we will repeat the following n times:", "This will emit a sequence of n words. These words will come annotated with the topics they were sampled from.", "The resulting document is gibberish. A bag of words sampled from a mix of topics. That\u2019s not a problem \u2014 it wasn\u2019t meant to be read. It does reveal which words were generated from which topics, which can be insightful.", "In the above, we\u2019ve described a topic as a set of words. We interpret this as: all the words in the set are equiprobable; the remaining words in the lexicon have zero probability.", "Let\u2019s see a 4-word generated document.", "Topic 3\u2019s proportion in this document (25%) is close to its proportion (30%) in its sampling distribution.", "As usual, this is where things get especially interesting.", "First, let\u2019s remind ourselves of the aim of learning. It is to discover, from a corpus of documents, the word distributions of the various topics, and the topic proportions in the various documents. In short, what words describe which topic, and which topics are covered in which document.", "The algorithm we\u2019ll describe is in wide use. It is also not hard to understand. It is a form of Gibbs Sampling.", "This algorithm works by initially assigning the topics to the various words in the corpus somehow, then iteratively improving these assignments. During its operation, the algorithm keeps track of certain statistics on the current assignments. These statistics help the algorithm in its subsequent learning. When the algorithm terminates, it is easy to \u201cread off\u201d the per-topic word distributions and the per-document topic proportions from the final topic assignments.", "Let\u2019s start by describing the statistics mentioned in the previous paragraph. These take the form of two matrices of counts: topic_word and doc_topic. Both are derived from the current assignment of topics to the words in the corpus. topic_word(t,w) counts the number of occurrences of topic t for word w. doc_topic(d,t) counts the number of occurrences of topic t in document d.", "Let\u2019s see a numeric example to make sure we got it right. Below we see a two-document corpus along with an assignment of topics to its words. The lexicon is A, B, C.", "Actually let\u2019s first use this opportunity to muse about some peculiarities we see. In doc 1, notice that A is assigned sometimes to topic 1 and sometimes to topic 2. This is plausible if word A has a high probability in both topics. In doc 2, notice that B is consistently assigned to topic 2. This is plausible if Doc 2 covers only topic 2, and B has a positive probability in topic 2\u2019s distribution.", "Okay, now to the two matrices of counts.", "We\u2019ve bolded some entries that are a bit striking. Perhaps doc2 prefers topic 2. Perhaps topic 2 prefers word C.", "Ok, let\u2019s start explaining the learning. The first step is to label the words in the corpus with randomly-sampled topics. This sounds easy enough. Actually there is a bit more to it. Instead of hard-coding this random-sampling, it is better to sample off suitable prior distributions. This gives us a potentially powerful mechanism to inject domain knowledge or results from external text analyses.", "This priors-based mechanism works as follows. First, we make copies of the two matrices we introduced earlier. Call them prior_topic_word and prior_doc_topic respectively. As before, the entries in these matrices are counts. These counts capture our prior beliefs.", "These prior matrices influence the initial assignment of topics. As learning progresses, this influence diminishes, albeit not to zero.", "How exactly do we sample the initial assignment of topics from these counts? First, we calculate", "P(w|t) is just the fraction of the assignments of topic t whose word is w. P(t|d) is just the fraction of the words in document d whose assigned topic is t.", "Next, we sample the assignments from these. More specifically, we sample the topic for word w in document d from a distribution whose numerator is P(w|t)P(t|d).", "This may be understood as follows. P(w|t)P(t|d) is exactly the probability of generating word w in document d in our generative model. Viewed as a function of t, it captures the likelihood that t was used during this process.", "Now let\u2019s discuss setting the values of these counts in the two prior matrices. For our purposes here, all we care about is that no topic be preferred over another. Such preferences would be unwanted biases. We can achieve this by setting all the counts in each matrix to the same positive number. 1 is the simplest choice. Occam\u2019s razor reasoning.", "Okay, so the topic assignments will be sampled from these counts and come out uniformly random.", "Subsequent to this initial assignment, we will repeatedly do the following in the hopes of improving the assignment and consequently, our models learned from it:", "What is Q(w|t)? It is our current belief in the likelihood of generating the word w from topic t. In fact, good values of Q(w|t) are what we seek. These will form the final topic-specific words-distributions.", "Before starting learning, we captured whatever prior beliefs we had about this distribution into P(w|t). As learning progresses, P(w|t) starts getting revised into the posterior belief Q(w|t).", "Q(t|d)\u2019s explanation is similar. Our prior beliefs about the per-document topic proportions go into P(t|d). As learning progresses, these get revised into Q(t|d).", "How are Q(w|t) and Q(t|d) computed? Consider the assignment of topics to the various words in the corpus at any point during learning. From this assignment of topics, we can calculate the counts in the topic_word and the doc_topic matrices. Next, we do the following", "Note that the \u2018+\u2019 is matrix addition. From the posterior versions of the count matrices, we may compute Q(w|t) and Q(t|d). Just as we computed P(w|t) and P(t|d) from the prior versions.", "How do we know that this iterative process actually improves the topic assignments? We won\u2019t give a proof. Instead, some intuition.", "First note that the overall quality of an assignment of topics to the words in the corpus may be obtained by multiplying out the various Q(w|t)Q(t|d) terms, over all word occurrences w in the corpus. Here d denotes the document in which w appears, and t the topic assigned to that occurrence.", "Next, we\u2019ll reveal that the score function has certain desirable characteristics.", "This Score Function Favors Topic Specificity", "By \u201ctopic specificity\u201d we mean that Q(w|t) is concentrated on only a few topics. This is a desired property of those words that strongly correlate with specific topics. Let\u2019s elaborate. Consider a diverse corpus such as Wikipedia. Say our aim is to discover the wide variety of topics it covers. Consider the word cat. Its topic specificity is high, i.e. it conjures up only a few of these topics. As it should. So the score function\u2019s bias towards favoring topic specificity is a good thing.", "That said, not every word should be topic-specific. For example the. Later we\u2019ll discuss a separate mechanism that will counteract topic specificity in such cases. First, let\u2019s explain the topic-specificity bias. We\u2019ll call topic-specific words informative.", "Consider n >> 1 occurrences of the same word w in the corpus, perhaps spread across many documents. Let T1 and T2 be two different topic assignments to these n occurrences. All the topics in T1 are distinct. Call this set {1,2,3,\u2026,n}. All the topics in T2 are the same, the one that maximizes Q(w|t). Call this topic tmax. T1\u2019s likelihood under Q(w|t) is Q(w|1)Q(w|2)*\u2026*Q(w|n). T2\u2019s likelihood under Q(w|t) is Q(w|tmax)^n. T2\u2019s likelihood can be way higher than T1\u2019s when w is informative and n is not too small.", "This analysis may be summarized as", "This Score Function Favors Document Specificity", "By \u201cdocument specificity\u201d we mean that the document covers only a few topics. Documents tend to be specific. The question is whether the score function (and consequently the learning algorithm) exploits this tendency to do its job better. The answer is yes. As explained below.", "Consider a document d on n words. Let T1 and T2 be two different topic assignments to its words. All the topics in T1 are distinct. Call this set {1,2,3,\u2026,n}. All the topics in T2 are the same, this time the one with the highest proportion in d. Call it tmax. T1\u2019s likelihood under Q(t|d) is Q(1|d)Q(2|d)*\u2026*Q(n|d). T2\u2019s likelihood under Q(t|d) is Q(tmax|d)^n. Clearly T2 can be way higher, especially for large documents.", "This analysis may be summarized as", "Consider an especially common word: the. Reasonably assume it appears in almost all documents. Topic consistency favors all these occurrences being assigned the same topic. Document specificity protests, as this would force all these documents to cover this one topic.", "Let\u2019s imagine how this might play out. Topic consistency might back off its grip for such words. Their assigned topics might just \u201cgo with the flow\u201d, taking on the identity of whichever topic is being covered in the neighborhood. Sure, the topic consistency component of the likelihood of these assignments might decrease. The document specificity portion, on the other hand, would increase.", "We\u2019ve seen that the score function is biased in good ways. This is only helpful if the learning algorithm exploits them well. So let\u2019s pivot to discussing the algorithm a bit more.", "We\u2019ll start by noting that the algorithm works by locally optimizing the global topic assignment quality score. This alone suggests it is paying attention to the biases.", "Next, we\u2019ll run through an iteration or two in a simple example. This will help the reader get a better feel for its \u201clocal optimizing\u201d behavior. It\u2019s more nuanced than some might imagine.", "We\u2019ll set the lexicon to {A, B}. Our corpus will have two documents, AAA (d1) and BBB (d2). (While these may sound silly, this exercise will be instructive.) We\u2019ll aim to fit two topics to the corpus.", "Let\u2019s set all the prior counts to 0.0000001. It will still produce initial assignments that are uniformly random, though simplify our numeric calculations.", "Say the following is the initial assignment", "The learned model from this assignment is", "Now imagine picking a word from d1 and resampling its topic. (We could have chosen d2 instead but the reasoning is similar.) There are three possible outcomes: 121 \u2192 121, 121 \u2192 111, 121 \u2192 122 or 221. The third one is effectively where we started with: swap the topic names and rearrange the order. So the first and the third outcome effectively put us back in the same state. So let\u2019s zoom in on the second one: 121 \u2192 111. The probability of this transition is positive. (In fact quite high as the switch from 2 \u2192 1 improves both the topic likelihood component and also the document likelihood component.) So if we keep repeating this process at some point all of d1 will be assigned the same topic t (=1 or 2).", "Next, we reestimate the various parameters of the model from this assignment t to d1. Q(t|d1) is now 1. Q(A|t) may well be higher than Q(A|1) was when we began this process. We have arrived at a \u201chappy state\u201d from which it is difficult to escape.", "Now let\u2019s look at some variants of this algorithm. These involve modifying step 2 below.", "We can replace step 2 by \u201cset t\u2019 to the topic that maximizes Q(w|t)Q(t|d)\u201d. This is called maximum likelihood estimation. In our setting, this yields a greedy algorithm.", "This variant is attractive in its simplicity and may converge quicker. However, it is more susceptible to getting trapped in suboptimal local optima.", "Our next variant introduces a parameter called temperature, which may be varied to span behaviors ranging from the Gibbs sampler to the greedy algorithm. Therein lies this algorithm\u2019s appeal. It does open up a new issue though: how to set the temperature.", "Setting temperature aside, let\u2019s see how it works. Consider the word w whose current topic t is being considered for re-assignment. For every topic t\u2019 we evaluate", "We won\u2019t explain why the log here. We will note that delta(t\u2019) being less than 0 corresponds to t\u2019 being a better fit than t in this situation. A switch from t to t\u2019 may be viewed as an energy-reducing (or hill-descending) move.", "What happens next? We\u2019ll describe it qualitatively. So now we know delta(t\u2019) for the various values of t\u2019, including t. From these delta-values, the algorithm defines a suitable probability distribution over the topics. This distribution is parameterized by temperature.", "At high temperature, even \u201cup-hill moves\u201d, i.e. moves to topics whose delta-values are positive are permitted. Such moves, while regressing on the current topic assignment, can help escape local energy minima.", "At low temperature, the distribution favors moves whose delta-values are negative. In the zero temperature extreme, this yields greedy behavior.", "To this point, we\u2019ve been set prior counts to 1. Here we consider richer settings.", "Previously we noted that it is desirable for certain words to be topic-specific. Can we find such words and capture their topic-specificity preference into the prior counts? This could speed up subsequent learning.", "Here is a sensible way to do this. First, we\u2019ll set all the prior counts to 1. Next, for each word w, we\u2019ll compute n-n(w). Here n is the total number of documents in the corpus and n(w) the number of documents in which word w appears. Think of n-n(w) as a sort-of \u201cinverse document frequency\u201d of w. Next, we will do the following independently for each of the words. We will pick a topic t randomly from the universe of topics. We\u2019ll add n-n(w) to the prior count word_topic(t,w).", "The idea is to map uncommon words to specific topics. Since the topic choices are random, different words will likely map to different topics.", "Is this prior overly biased? We can easily soften it should we feel that is the case. There are a number of possibilities. One is to replace n-n(w) by log n \u2014 log n(w). A second is to sample more than one topic (albeit not many) when amplifying a word\u2019s prior count.", "What are other areas to consider for improving the model? First, let\u2019s spell out its assumptions:", "Let\u2019s say a bit about each. Topic compatibility can be important. A document is more likely to cover tech companies & computers than tech companies & sports. Word proximity also matters. Two words that repeatedly occur near each other are more likely to be on the same topic than those that appear thousands of words apart. Topic hierarchies model documents better. A common writing pattern is for a document to cover a broad topic together with its various subtopics.", "Below we describe how to address these issues. The relaxations may improve the accuracy of the model in some use cases, though at the potential cost of increasing its complexity or learnability. They also offer opportunities to inject domain knowledge. Plus in some ways even help with the learning!", "Exactly how this plays out depends heavily on the use case.", "This involves adding a Markov model to the mix. Its states are the topics. Its transitions model compatibility among topics in the corpus.", "How is this model used when sampling a word\u2019s topic in a document? We need to extend Q(t|d) suitably. This extension is easiest described by imagining sampling topics from it.", "This sampling may be viewed as a random walk on the Markov model with perhaps occasional restarts. We start the walk by choosing a topic sampled from Q(t|d). Next, we do the following. Most of the time we walk a transition from this topic\u2019s state with the probability that is on that transition. Occasionally we re-sample from Q(t|d), i.e., jump to a new topic.", "This random walk induces a new distribution, call it Q\u2019(t|d, M) which is influenced by both the Markov model M and Q(t|d). The Markov model is a corpus-level construct. The use of Q(t|d) customizes its behavior to the document d.", "How are the parameters of the Markov model learned? This is easier to explain. Our learning algorithm to this point already works at the level of assigning topics to individual words in the documents of the corpus. The Markov model\u2019s parameters are fully determined by this assignment.", "Specifically, for every pair s, t of topic assignments in a document, we increment counts on the arcs s \u2192 t and t \u2192 s unless s equals t in which case only once.", "So as topic assignments improve, the parameters of the topic-compatibility model also improve. The two act synergistically.", "Let\u2019s look at this synergy in a wider scope. As we see more and more documents on the same two topics, the Markov model starts learning that these topics are compatible. This improved learning leads to improved topic assignments elsewhere.", "This can be incorporated by extending Q(w|t) suitably. Specifically, let the choice of t be influenced not only from w but also from words near w.", "Does this complicate our model from the learning perspective? With a suitable assumption, fortunately not. We assume the words in W(d) are conditionally independent given t. Under this assumption we get", "The RHS terms involving t are all of the same forms as before. So learning needs no change!", "The assumption notwithstanding, we get the benefits that come with using context. Chief among them is that the topic assigned to the central word w may be determined more accurately, as now we have more context. Another helpful characteristic, especially early on in the learning, is that we get some topic continuity. As we slide the window W(d) one word at a time, the assigned topic is less likely to change since the context hasn\u2019t changed much. Compared to W(0).", "This is an advanced topic. Our coverage here is partial.", "Here is the key observation. Comparing the word distributions of two topics can help assess whether or not one is a descendant of the other. The descendant\u2019s keywords will tend towards being a subset of the ancestor\u2019s.", "Using this notion, we can (re)learn a hierarchy on the topics any time we want during the learning process. Even a rough learned hierarchy can be better than none.", "As we did for topic compatibility, we can extend our Q(t|d) models to take the learned hierarchy into account. As there, this extension is easiest described by imagining sampling topics for a specific document from it.", "This sampling is a random walk on the hierarchy with perhaps occasional restarts. We start the walk by choosing a topic sampled from Q(t|d). Next, we do the following. Most of the time we walk to a child from this topic. Occasionally we resample from Q(t|d), i.e., jump to a topic somewhere else in the hierarchy.", "This random walk induces a new distribution, call it Q\u2019(t|d, H) which is influenced by both the topic hierarchy H and Q(t|d). The topic hierarchy is a corpus-level construct. The use of Q(t|d) customizes its behavior to the document d.", "The readability of a document is not the primary aim of LDA. Nonetheless, the modeling framework presents opportunities to inject mechanisms that improve it. So let\u2019s take advantage of this.", "Let\u2019s start by surfacing the issues.", "We can maintain topic continuity by adding a Markov model with two states: continue and switch. Continue to continue the current topic, switch to switch to a new one. The arc from continue to itself should have a high probability. The arc from switch to continue should have a near-1 probability as we almost surely want to continue following a switch.", "The probabilities on this model are easy to learn from the topic assignments. Buried in them are the continue and switch events. Such learning dovetails with the learning of the topic assignments, iteratively benefiting each other.", "Leveraging word proximity in documents also helps maintain topic continuity. The reason we chose to cover the approach of this section as well is that it is far simpler to implement than word proximity.", "Readability aside, injecting the topic continuity mechanism also potentially improves the quality of the learned assignments. In effect, it acts as a smoothness regularizer, disfavoring outliers.", "Here is an example, an extension of one we used earlier. Consider the word the, which we assumed occurs in almost all documents in the corpus. Previously we explained why we would like the topic assigned to a particular occurrence of the to \u201cgo with the flow\u201d of the topic in its neighborhood. The topic continuity mechanism adds more weight to this preference, as it explicitly favors \u201cgo with the flow\u201d.", "We can maintain topic coherence by relaxing the bag-of-words assumption. Instead, use a first-order Markov model that models the influence of the current word on the next one. Each topic has its own Markov model.", "The per-topic Markov models are easy to (re)learn from the current topic assignment in the corpus.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F524b4cffeb68&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----524b4cffeb68---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mitchel3uo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Mitchell Luo"}, {"url": "https://unsplash.com/s/photos/tag-cloud?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf", "anchor_text": "http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf"}, {"url": "https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf", "anchor_text": "https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf"}, {"url": "https://medium.com/tag/gibbs-sampling?source=post_page-----524b4cffeb68---------------gibbs_sampling-----------------", "anchor_text": "Gibbs Sampling"}, {"url": "https://medium.com/tag/generative-model?source=post_page-----524b4cffeb68---------------generative_model-----------------", "anchor_text": "Generative Model"}, {"url": "https://medium.com/tag/information-retrieval?source=post_page-----524b4cffeb68---------------information_retrieval-----------------", "anchor_text": "Information Retrieval"}, {"url": "https://medium.com/tag/bayesian-learning?source=post_page-----524b4cffeb68---------------bayesian_learning-----------------", "anchor_text": "Bayesian Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&user=Arun+Jagota&userId=ef9ed921edad&source=-----524b4cffeb68---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&user=Arun+Jagota&userId=ef9ed921edad&source=-----524b4cffeb68---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F524b4cffeb68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----524b4cffeb68---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----524b4cffeb68--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----524b4cffeb68--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----524b4cffeb68--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "685 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-in-nlp-524b4cffeb68&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}