{"url": "https://towardsdatascience.com/recurrent-neural-networks-part-3-1032d4a67757", "time": 1683011405.034226, "path": "towardsdatascience.com/recurrent-neural-networks-part-3-1032d4a67757/", "webpage": {"metadata": {"title": "A tribute to LSTMs - Intro to long-short term memory units | Towards Data Science", "h1": "Recurrent Neural Networks \u2014 Part 3", "description": "In this blog post, we present an introduction to long-short term memory units and the different gates."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-2-5f45c1c612c4", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/vZQxwDHvuug", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-4-39a568034d3b", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 15}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 15}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 15}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 15}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 15}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 15}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 15}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs", "paragraph_index": 16}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation", "paragraph_index": 16}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs", "paragraph_index": 16}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning! Today I want to show you one alternative solution to solve this vanishing gradient problem in recurrent neural networks.", "You already noticed long temporal contexts are a problem. Therefore, we will talk about long short-term memory units (LSTMs). They have been introduced by a Hochreiter in Schmidhuber and they were published in 1997.", "They were designed to solve this vanishing gradient problem in the long term dependencies. The main idea is that you introduce gates that control writing and accessing the memory in additional states.", "So, let\u2019s have a look into the LSTM unit. You see here, one main feature is that we now have essentially two things that could be considered as a hidden state: We have the cell state C and we have the hidden state h. Again, we have some input x. Then we have quite a few of activation functions. We then combine them and in the end, we produce some output y. This unit is much more complex than what you\u2019ve seen previously in the simple RNNs.", "Okay, so what are the main features the LSTM: Given some input x it produces a hidden state h. It also has a cell state Cthat we will look into a little more detail in the next couple of slides, to produce the output y. Now, we have several gates and the gates essentially are used to control the flow of information. There\u2019s a forget gate and this is used to forget old information in the cell state. Then, we have the input gate and this is essentially deciding new input into the cell state. From this, we then compute the updated cell state and the updated hidden state.", "So let\u2019s look into the workflow. We have the cell state after each time point t and the cell state undergoes only linear changes. So there is no activation function. You see there are only one multiplication and one addition on the path of the cell state. So, the cell state can flow through the unit. The cell state can be constant for multiple time steps. Now, we want to operate on the cell state. We do that with several gates and the first one is going to be the forget gate. The key idea here is that we want to forget information from the cell state. In another step, we then want to think about how to actually put new information in the cell state that is going to be used to memorize things.", "So, the forget gate f controls how much of the previous cell state is forgotten. You can see it is computed by a sigmoid function. So, it\u2019s somewhere between 0 and 1. It\u2019s essentially computed with a matrix multiplication of a concatenation of the hidden state and x plus some bias. This is then multiplied to the cell state. So, we decide which parts of the state vector to forget and which ones to keep.", "Now, we also need to put in new information. For the new information, we have to somehow decide what information to input into the cell state. So here, we need two activation functions: One that we call I that is also produced by a sigmoid activation function. Again, matrix multiplication of the hidden state concatenated with the input plus some bias and the sigmoid function as non-linearity. Remember, this value is going to be between 0 and 1 so you could argue that it is kind of selecting something. Then, we have some C tilde which is a kind of update state that is produced by the hyperbolic tangent. This then takes as input some weight matrix W subscript c that is multiplied to the concatenation of hidden and input vector plus some bias. So essentially, we have this index that is then multiplied to the intermediate cell stage C tilde. We could say that the hyperbolic tangent is producing some new cell state and then we select via I which of these indices should be added to the current cell state. So, we multiply with I the newly produced C tilde and add it to the cell state C.", "Now, we update as we\u2019ve just seen the complete cell state using a point-wise multiplication with the forget gate of the previous state. Then, we add the elements of the update cell state that have been identified by I with a point-wise multiplication. So, you see the update of the cell state is completely linear only using multiplications and additions.", "Now, we still have to produce the hidden state and the output. As we have seen in the Elman cell, the output of our network only depends on the hidden state. So, we first update the hidden state by another non-linearity that is then multiplied to a transformation of the cell state. This gives us the new hidden state and from the new hidden state, we produce the output with another non-linearity.", "So, you see these are the update equations. We produce some o which is essentially a proposal for the new hidden state by a sigmoid function. Then, we multiply it with the hyperbole tangent that is generated from the cell state in order to select which elements are actually produced. This gives us the new hidden state. The new hidden state we can then pass through another non-linearity in order to produce the output. You can see here, by the way, that for the update of the hidden state and the production of the new output, we omitted the transformation matrices that are of course required. You could interpret each of these nonlinearities in the network essentially as a universal function approximator. So, we still need the linear part, of course, inside here to reduce vanishing gradients.", "If you want to train all of this, you can go back and use a very similar recipe as we\u2019ve already seen for the Elman cell. So, you use backpropagation through time in order to update all of the different weight matrices.", "Okay. This already brings us to the end of this video. So you\u2019ve seen the long short-term memory cell, the different parts, the different gates, and, of course, this is a very important part of this lecture. So, if you\u2019re preparing for the exam, then I would definitely recommend having a look at how to sketch such a long short-term memory unit. You can see that the LSTM has a lot of advantages. In particular, we can alleviate the problem with the vanishing gradients by the linear transformations in the cell state. By the way, it\u2019s also noteworthy to point out that we somehow include in our long short term memory cell some ideas that we know from computer design. We essentially learn how to manipulate memory cells. We could argue that in the hidden state, we now have the kind of program a kind of finite state machine that then operates on some memory and learns which information to store, which information to delete, and which information to load. So, this is very interesting how these network designs gradually seem to be approaching computer architectures. Of course, there\u2019s much more to say about this. In the next video, we will look into the gated recurrent neural networks which are a kind of simplification of the LSTM cell. You will see that with a slightly slimmer design, we can still get many of the benefits of the LSTM, but much fewer parameters. Ok, so I hope you enjoyed this video and see you next time when we talk about gated recurrent neural networks. Bye-bye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "Character RNNsCNNs for Machine TranslationComposing Music with RNNs", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1032d4a67757&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://akmaier.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----1032d4a67757---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1032d4a67757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=-----1032d4a67757---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1032d4a67757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=-----1032d4a67757---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-2-5f45c1c612c4", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/vZQxwDHvuug", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-4-39a568034d3b", "anchor_text": "Next Lecture"}, {"url": "https://imgflip.com/i/492k7p", "anchor_text": "imgflip"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://imgflip.com/i/48zhqg", "anchor_text": "imgflip"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://folkrnn.org/competition/", "anchor_text": "FolkRNN.org"}, {"url": "https://themachinefolksession.org/tunes/", "anchor_text": "MachineFolkSession.com"}, {"url": "https://github.com/IraKorshunova/folk-rnn/blob/master/soundexamples/successes/The%20Glas%20Herry%20Comment%2014128.mp3", "anchor_text": "The Glass Herry Comment 14128"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs"}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation"}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs"}, {"url": "http://www.pnas.org/content/79/8/2554.full.pdf.", "anchor_text": "http://www.pnas.org/content/79/8/2554.full.pdf."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1032d4a67757---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1032d4a67757---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1032d4a67757---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1032d4a67757---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----1032d4a67757---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1032d4a67757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=-----1032d4a67757---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1032d4a67757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=-----1032d4a67757---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1032d4a67757&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----1032d4a67757---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----1032d4a67757---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Written by Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----1032d4a67757---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-3-1032d4a67757&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----1032d4a67757---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "10 Ideas to Make Money from Large Language ModelsLarge Language Models work, but what can we do with them?"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "\u00b73 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&user=Andreas+Maier&userId=b1444918afee&source=-----86f2cb31bb25----0-----------------clap_footer----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----1032d4a67757----0---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&source=-----1032d4a67757----0-----------------bookmark_preview----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1032d4a67757----1---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1032d4a67757----1-----------------bookmark_preview----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1032d4a67757----2---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1032d4a67757----2-----------------bookmark_preview----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "Gradient Descent and Back-tracking Line SearchAn Introduction to Optimization using Gradient Descent"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": "\u00b713 min read\u00b7Apr 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&user=Andreas+Maier&userId=b1444918afee&source=-----d8bd120bd625----3-----------------clap_footer----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----1032d4a67757----3---------------------1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&source=-----1032d4a67757----3-----------------bookmark_preview----1d5a63cf_351f_4dae_8bad_9fbb6281e3fd-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "See all from Andreas Maier"}, {"url": "https://towardsdatascience.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----1032d4a67757----0-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----1032d4a67757----1-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----1032d4a67757----0---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1032d4a67757----0-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1032d4a67757----1---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----1032d4a67757----1-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----2-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1032d4a67757----2---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----1032d4a67757----2-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "Prateek Gaurav"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM]Link to Part 1of this article: NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] Link to Part 3 of this article: NLP: Zero\u2026"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": "\u00b78 min read\u00b7Mar 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&user=Prateek+Gaurav&userId=966fe9bb6729&source=-----77fd60fc0b44----3-----------------clap_footer----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----1032d4a67757----3---------------------6eeb3e8f_1086_4012_a383_2ecece950476-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&source=-----1032d4a67757----3-----------------bookmark_preview----6eeb3e8f_1086_4012_a383_2ecece950476-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1032d4a67757--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1032d4a67757--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}