{"url": "https://towardsdatascience.com/interpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2", "time": 1682995289.18172, "path": "towardsdatascience.com/interpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2/", "webpage": {"metadata": {"title": "Interpretable AI or How I Learned to Stop Worrying and Trust AI | by Ajay Thampi | Towards Data Science", "h1": "Interpretable AI or How I Learned to Stop Worrying and Trust AI", "description": "In the last five years alone, AI researchers have made significant breakthroughs in areas such as image recognition, natural language understanding and board games! As companies are considering\u2026"}, "outgoing_paragraph_urls": [{"url": "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/566075/gs-16-19-artificial-intelligence-ai-report.pdf", "anchor_text": "Sir Mark Walport", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "image recognition", "paragraph_index": 1}, {"url": "https://www.microsoft.com/en-us/research/uploads/prod/2018/04/08049322.pdf", "anchor_text": "natural language understanding", "paragraph_index": 1}, {"url": "https://www.nature.com/articles/nature16961", "anchor_text": "board games", "paragraph_index": 1}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "criminal justice", "paragraph_index": 1}, {"url": "https://thenextweb.com/artificial-intelligence/2018/03/21/killer-robots-cambridge-analytica-and-facebook-show-us-the-real-danger-of-ai/", "anchor_text": "politics", "paragraph_index": 1}, {"url": "https://www.bloomberg.com/graphics/2016-amazon-same-day/?cmpid=google", "anchor_text": "retail", "paragraph_index": 1}, {"url": "https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms", "anchor_text": "facial recognition", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1607.06520.pdf", "anchor_text": "language understanding", "paragraph_index": 1}, {"url": "https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/", "anchor_text": "nobody really understands how they work", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1606.08813.pdf", "anchor_text": "GDPR \u201cright to explanation\u201d", "paragraph_index": 2}, {"url": "https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/overview", "anchor_text": "Team Data Science Process", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining", "anchor_text": "CRISP-DM", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Data_mining#Process", "anchor_text": "KDD", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/measuring-model-goodness-part-1-a24ed4d62f71", "anchor_text": "measuring model goodness", "paragraph_index": 4}, {"url": "https://www.kaggle.com/spscientist/students-performance-in-exams", "anchor_text": "historical data", "paragraph_index": 10}, {"url": "https://github.com/thampiman/interpretability/blob/master/Features.ipynb", "anchor_text": "source code", "paragraph_index": 10}, {"url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html", "anchor_text": "scikit-learn", "paragraph_index": 12}, {"url": "https://github.com/SauceCat/PDPbox/", "anchor_text": "PDPBox", "paragraph_index": 12}, {"url": "https://github.com/SauceCat", "anchor_text": "Jiangchun Lee", "paragraph_index": 12}, {"url": "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html", "anchor_text": "Iris flower dataset", "paragraph_index": 17}, {"url": "https://github.com/thampiman/interpretability/blob/master/DecisionTrees.ipynb", "anchor_text": "source code", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html", "anchor_text": "dataset", "paragraph_index": 18}, {"url": "https://github.com/thampiman/interpretability/blob/master/DecisionTrees.ipynb", "anchor_text": "source code", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "proposed", "paragraph_index": 21}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Github", "paragraph_index": 21}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions", "anchor_text": "proposed", "paragraph_index": 24}, {"url": "https://github.com/slundberg/shap/", "anchor_text": "Github", "paragraph_index": 24}, {"url": "https://github.com/thampiman/interpretability/blob/master/SHAP_TreeEnsembles.ipynb", "anchor_text": "source code", "paragraph_index": 25}, {"url": "https://github.com/thampiman/interpretability/blob/master/SHAP_TreeEnsembles.ipynb", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/slundberg/shap/tree/master/notebooks", "anchor_text": "SHAP respository", "paragraph_index": 30}, {"url": "https://github.com/thampiman/interpretability/blob/master/Resources.md", "anchor_text": "Github page", "paragraph_index": 32}, {"url": "https://arxiv.org/pdf/1708.01870.pdf", "anchor_text": "paper", "paragraph_index": 33}, {"url": "https://www.turing.ac.uk/people/programme-directors/adrian-weller", "anchor_text": "Adrian Weller", "paragraph_index": 33}, {"url": "https://www.manning.com/books/interpretable-ai", "anchor_text": "Interpretable AI", "paragraph_index": 35}, {"url": "https://medium.com/u/bd9671589b55?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Manning Publications", "paragraph_index": 35}], "all_paragraphs": ["Public trust is a vital condition for Artificial Intelligence to be used productively. \u2014 Sir Mark Walport", "In the last five years alone, AI researchers have made significant breakthroughs in areas such as image recognition, natural language understanding and board games! As companies are considering handing over critical decisions to AI in industries like healthcare and finance, the lack of understanding of complex machine learned models is hugely problematic. This lack of understanding could result in models propagating bias and we\u2019ve seen quite a few examples of this in criminal justice, politics, retail, facial recognition and language understanding.", "All of this has a detrimental effect on trust and this, from my experience, is one of the main reasons why companies are resisting the deployment of AI across the enterprise. Explaining or interpreting AI is a hot topic of research as modern machine learning algorithms are black boxes and nobody really understands how they work. Moreover, there is EU regulation now to explain AI under the GDPR \u201cright to explanation\u201d. In this blog post, I will cover a few techniques that you can add to your data science arsenal to improve model understanding.", "But first off, what process do you follow to build and deploy an AI application? At Microsoft, we follow an agile and iterative methodology called the Team Data Science Process (TDSP) that takes inspiration from processes like CRISP-DM and KDD. In this post, I will zoom into the modeling and deployment stages of the process.", "When developing the model, the first step is learning. You train a model to detect patterns from historical data and this entails going through multiple iterations of training and validation to pick the best model. Once you have a learned model, the next step is testing. This means evaluating the model on a blind set of data that the model hasn\u2019t seen before. I\u2019ve written a blog post on measuring model goodness and you can apply those techniques to quantify business value and share that with the business stakeholders. After proving business value, the next logical step is typically deploying the model into production. Once deployed, a very common issue is that the model does not perform as expected. There are two main reasons for this:", "We can circumvent these problems by introducing an additional step for model understanding before deploying the models in the wild. By interpreting the model, we can gain a much deeper understanding and address problems like bias, leakage and trust.", "Interpretability is the degree to which a human can consistently estimate what a model will predict, how well the human can understand and follow the model\u2019s prediction and finally, how well a human can detect when a model has made a mistake.", "Interpretability though means different things to different people:", "It is important to be mindful of these different personas when you talk about interpretability and model understanding.", "The interpretability of a model can be characterized by how complex the learned response function is to the input features. The graph above shows three different types of response functions:", "Let\u2019s now look at a concrete example. The problem is to predict math, reading and writing grades for high-school students in the U.S. We are given historical data that include features like \u2014 gender, race/ethnicity (which is anonymized), parent level of education, whether the student ate a standard/free/subsidized lunch and the level of preparation for tests. Given this data, I trained a multi-class random forest model [source code].", "In order to explain what the model has learned, one of the simplest techniques is to look at the relative feature importance. Feature importance measures how big an impact a given feature has on predicting the outcome. From the graph above, it looks like the two most important features are \u2014 parent level of education and race/ethnicity. This is useful information but it does not tell us anything about how the grade is influenced by different levels of education and also how race and education interact with each other.", "PDPs show us the marginal effect of features on the predicted outcome. In Python, you can use the implementation in scikit-learn which limits you to gradient boosted regressors/classifiers. A better implementation is PDPBox developed by Jiangchun Lee. The library is inspired by ICEBox, a PDP implementation for R, and supports all scikit-learn machine learning models. You can install this library as follows.", "Now let\u2019s see PDP in action. The plot above shows the influence of different levels of education on predicting grades A and F for math. As you go from left to right on the x-axis, the parent level of education increases, starting from high school going all the way to a master\u2019s degree. You will notice that the impact on predicting grade A increases as the parent level of education increases. You see a similar downward trend for grade F, i.e. the more educated the parent is, the less impact it has on predicting grade F. This analysis shows that parent level of education is a valid feature. You can generate the above plots in Python using the following code snippet.", "Let\u2019s now look at feature interactions on predicting grade A in math. I\u2019ve picked the two most important features here. The parent level of education is on the y-axis and as you go from bottom to the top, the level of education increases. The anonymized race or ethnicity of the student is shown on the x-axis where different points represent different races\u200a\u2014\u200athere are 5 distinct groups in this study. Please note the colors in the heatmap as well\u200a\u2014\u200aviolet/blue represents low impact in predicting grade A and green/yellow represents high impact.", "Marginalizing on group 0, you can see that as the parent level of education increases, the impact on predicting grade A also increases. This makes sense as it shows the level of education has more influence on the grade than the race. The model has therefore learned this correlation well. But what is going on with group 5? It looks like regardless of the level of education, if the student belongs to group 5, then that has a strong influence on predicting grade A. This looks fishy to me and it exposes\u200a\u2014", "It turns out the Kaggle dataset used here is contrived and group 5 isn\u2019t properly represented. Regardless, the main point is that none of these insights would\u2019ve been possible by just looking at feature importance. The above interaction plot can be generated in Python as follows.", "I want to debunk a common misconception about decision trees \u2014 that they are extremely interpretable. Let\u2019s again look at a concrete example \u2014 the Iris flower dataset. The problem is to predict if a given Iris flower is either Setosa, Versicolor or Virginica based on 4 features \u2014 the petal length and width, and the sepal length and width. I\u2019ve trained a simple decision tree for this classification task [source code] and you can easily visualize the trained model in scikit-learn, shown above. As you go down different branches in the tree, you can see how the features influence the model\u2019s decision and it is very easily understood by a layman.", "Let\u2019s now look at a slightly more challenging problem. A hospital wants to use an AI system for early detection of breast cancer. This is an example where model understanding is critical as it is a life or death situation. In the dataset, we are given 30 different features. I\u2019ve again trained a simple decision tree for this binary classification task [source code]. From the visualization below, you can see that as the number of features increases, the complexity of the decision tree also increases and it becomes much harder to interpret.", "You might say that we can reduce the size of the feature space through dimensionality reduction and that\u2019s perfectly valid. But you have to understand that there is a trade-off you are making here \u2014 you are trading off accuracy for simplicity. This may not be the right strategy to take especially for such a critical decision. A decision tree is therefore not a silver bullet to address the interpretability problem.", "Let\u2019s go back to the breast cancer detection problem. Since accuracy is important for such a critical task, what if we trained a more complex ensemble of trees (like random forest or gradient boosted trees) or even a black-box neural network? How can we interpret such complex models?", "Suppose that the complex model you trained learns the decision function as shown above to separate the malignant cells (in red) from the benign cells (in blue). An approach to interpret this complex model is to pick an instance that you want to explain. Then train a simpler linear model that approximates the complex model around that instance. This means that we use the simpler model to explain that one instance by looking at similar cases around it. The learned representation is therefore locally faithful but not globally. This post-hoc explanation technique is called LIME and it stands for Local Interpretable Model-agnostic Explanations. It was proposed in 2016 and has gained a lot of popularity since (5343 stars on Github as of 05-Mar-2019).", "Now let\u2019s take a look at it from a different angle. We\u2019ve got data of different breast cancer cells that we want to classify, represented as x in the block diagram. We are training a model that learns a complex function f that separates the benign cases from the malignant cases. We then train a simple linear surrogate model g that explains one instance x\u2019 by approximating the complex function around that point. The parameters learned by that simpler model is the explanation. This is shown mathematically below.", "In the equation above, x\u2019_i is a binary variable that is used to sample instances around the picked instance and M represents the number of simplified input features. LIME is therefore an additive feature attribution method.", "A framework was proposed by Scott M. Lundberg et al at NeurIPS 2017 that unifies various additive feature attribution methods including LIME. In the paper, SHAP (which stands for SHapley Additive exPlanation) values were used as a unified measure of feature importance. A Python implementation of SHAP was released in 2017 unifying LIME and other techniques like DeepLIFT and tree interpreters. The library is also gaining in popularity since its release (3909 stars on Github as of 03-Mar-2019). You can install SHAP as follows.", "Now for the breast cancer detection problem, I\u2019ve trained a random forest classifier [source code] obtaining an average precision of 100% on the hold out test set \u2014 see PR curve below.", "In order to explain the model, I\u2019ve picked a malignant case to run through the SHAP tree explainer. The model has predicted this case to be malignant with probability 0.9. The base rate of malignancy in this dataset is 0.6251. The explanation provided by SHAP is shown below \u2014 where positive impact is represented by red and negative impact is represented by blue. You can see how different feature values push the base prediction up to 90% certainty. The features with the largest positive impact are \u2014 worst area, worst concave points and mean concave points.", "A similar explanation can be obtained for a benign case as well. I picked a benign cell for which the model predicted so with 100% certainty, i.e. 0% chance that it is malignant. You can see from the explanation below how various feature values pushed the base probability of malignancy down from 0.6251 to 0. The features with the largest negative impact are \u2014 worst concave points, mean perimeter and worst radius. This explanation is great as it gives us a much deeper understanding of how the model arrived at the final prediction.", "The above explanations can be generated using the below code snippet \u2014 the entire source code can be found here.", "You can also use SHAP to visualize the average impact of each of the features in predicting the target classes. This is shown by the plot on the left below. You can see that the two most important feature are \u2014 worst concave points and mean concave points. The plot on the right shows the interaction between these two features in predicting the malignant class.", "SHAP can also be used to explain black box deep learning models. Below is an example of SHAP explaining a convolutional neural network trained on the MNIST dataset. The SHAP DeepLIFT implementation was used to explain the predictions. The pixels in red indicate high impact on predicting the digit. It\u2019s interesting to note that the DeepExplainer model has picked up some good explanations \u2014 for instance, the blank middle for digit 0 and the lack of connection between the two vertical lines for digit 4. There are more cool examples in the SHAP respository.", "I\u2019ve barely scratched the surface in this article and there are more interpretability techniques that you can apply such as:", "I\u2019ve added a lot of resources on this Github page for further reading.", "I want to end with a caveat. In an interesting paper by Adrian Weller, he makes the argument that AI transparency should be a means to a goal and not the goal itself. Dr. Weller draws on multi-agent game theory to show that more transparency could lead to an unfavorable global outcome for all and could expose AI to abuse by malicious actors. It goes without saying that AI systems must be secure and safeguarded against adversarial attacks. This is again another active area of research and deserves a separate blog post!", "Thanks for taking the time to read this article. Hope you enjoyed it.", "Update [Nov 4, 2020]: I have written a book on Interpretable AI with Manning Publications with more interpretability techniques that you can add to your arsenal.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer at Facebook . Author of Interpretable AI . PhD in Signal Processing . Views my own"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe61f9e8ee2c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thampiman?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thampiman?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Ajay Thampi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7df87e0e0bf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&user=Ajay+Thampi&userId=7df87e0e0bf5&source=post_page-7df87e0e0bf5----e61f9e8ee2c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-data-science/inside-ai/home", "anchor_text": "Inside AI"}, {"url": "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/566075/gs-16-19-artificial-intelligence-ai-report.pdf", "anchor_text": "Sir Mark Walport"}, {"url": "https://pixabay.com/photos/hands-together-handshake-give-1947915/", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "image recognition"}, {"url": "https://www.microsoft.com/en-us/research/uploads/prod/2018/04/08049322.pdf", "anchor_text": "natural language understanding"}, {"url": "https://www.nature.com/articles/nature16961", "anchor_text": "board games"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "criminal justice"}, {"url": "https://thenextweb.com/artificial-intelligence/2018/03/21/killer-robots-cambridge-analytica-and-facebook-show-us-the-real-danger-of-ai/", "anchor_text": "politics"}, {"url": "https://www.bloomberg.com/graphics/2016-amazon-same-day/?cmpid=google", "anchor_text": "retail"}, {"url": "https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms", "anchor_text": "facial recognition"}, {"url": "https://arxiv.org/pdf/1607.06520.pdf", "anchor_text": "language understanding"}, {"url": "https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/", "anchor_text": "nobody really understands how they work"}, {"url": "https://arxiv.org/pdf/1606.08813.pdf", "anchor_text": "GDPR \u201cright to explanation\u201d"}, {"url": "https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/overview", "anchor_text": "Team Data Science Process"}, {"url": "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining", "anchor_text": "CRISP-DM"}, {"url": "https://en.wikipedia.org/wiki/Data_mining#Process", "anchor_text": "KDD"}, {"url": "https://towardsdatascience.com/measuring-model-goodness-part-1-a24ed4d62f71", "anchor_text": "measuring model goodness"}, {"url": "https://www.kdd.org/exploration_files/KDDCup08-P1.pdf", "anchor_text": "source"}, {"url": "https://www.kdd.org/kdd-cup/view/kdd-cup-2008", "anchor_text": "KDD competition in 2008"}, {"url": "https://www.kaggle.com/spscientist/students-performance-in-exams", "anchor_text": "historical data"}, {"url": "https://github.com/thampiman/interpretability/blob/master/Features.ipynb", "anchor_text": "source code"}, {"url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html", "anchor_text": "scikit-learn"}, {"url": "https://github.com/SauceCat/PDPbox/", "anchor_text": "PDPBox"}, {"url": "https://github.com/SauceCat", "anchor_text": "Jiangchun Lee"}, {"url": "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html", "anchor_text": "Iris flower dataset"}, {"url": "https://github.com/thampiman/interpretability/blob/master/DecisionTrees.ipynb", "anchor_text": "source code"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html", "anchor_text": "dataset"}, {"url": "https://github.com/thampiman/interpretability/blob/master/DecisionTrees.ipynb", "anchor_text": "source code"}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "proposed"}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "Github"}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions", "anchor_text": "proposed"}, {"url": "https://github.com/slundberg/shap/", "anchor_text": "Github"}, {"url": "https://github.com/thampiman/interpretability/blob/master/SHAP_TreeEnsembles.ipynb", "anchor_text": "source code"}, {"url": "https://github.com/thampiman/interpretability/blob/master/SHAP_TreeEnsembles.ipynb", "anchor_text": "here"}, {"url": "https://github.com/slundberg/shap/tree/master/notebooks", "anchor_text": "SHAP respository"}, {"url": "https://github.com/thampiman/interpretability/blob/master/Resources.md", "anchor_text": "Github page"}, {"url": "https://arxiv.org/pdf/1708.01870.pdf", "anchor_text": "paper"}, {"url": "https://www.turing.ac.uk/people/programme-directors/adrian-weller", "anchor_text": "Adrian Weller"}, {"url": "https://www.manning.com/books/interpretable-ai", "anchor_text": "Interpretable AI"}, {"url": "https://medium.com/u/bd9671589b55?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Manning Publications"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e61f9e8ee2c2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e61f9e8ee2c2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e61f9e8ee2c2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e61f9e8ee2c2---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----e61f9e8ee2c2---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&user=Ajay+Thampi&userId=7df87e0e0bf5&source=-----e61f9e8ee2c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&user=Ajay+Thampi&userId=7df87e0e0bf5&source=-----e61f9e8ee2c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe61f9e8ee2c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e61f9e8ee2c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e61f9e8ee2c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thampiman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thampiman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ajay Thampi"}, {"url": "https://medium.com/@thampiman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "176 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7df87e0e0bf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&user=Ajay+Thampi&userId=7df87e0e0bf5&source=post_page-7df87e0e0bf5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7df87e0e0bf5%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2&user=Ajay+Thampi&userId=7df87e0e0bf5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}