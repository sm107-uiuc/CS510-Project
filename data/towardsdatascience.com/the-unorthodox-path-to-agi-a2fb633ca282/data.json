{"url": "https://towardsdatascience.com/the-unorthodox-path-to-agi-a2fb633ca282", "time": 1683017521.566508, "path": "towardsdatascience.com/the-unorthodox-path-to-agi-a2fb633ca282/", "webpage": {"metadata": {"title": "The unorthodox path to AGI. Ben Goertzel on the TDS podcast | by Jeremie Harris | Towards Data Science", "h1": "The unorthodox path to AGI", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning", "paragraph_index": 0}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://twitter.com/bengoertzel", "anchor_text": "follow Ben on Twitter here", "paragraph_index": 4}, {"url": "https://singularitynet.io/", "anchor_text": "check out SingularityNET here", "paragraph_index": 4}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 4}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 173}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "No one knows for sure what it\u2019s going to take to make artificial general intelligence work. But that doesn\u2019t mean that there aren\u2019t prominent research teams placing big bets on different theories: DeepMind seems to be hoping that a brain emulation strategy will pay off, whereas OpenAI is focused on achieving AGI by scaling up existing deep learning and reinforcement learning systems with more data, more compute.", "Ben Goertzel \u2014a pioneering AGI researcher, and the guy who literally coined the term \u201cAGI\u201d \u2014 doesn\u2019t think either of these approaches is quite right. His alternative approach is the strategy currently being used by OpenCog, an open-source AGI project he first released in 2008. Ben is also a proponent of decentralized AI development, due to his concerns about centralization of power through AI, as the technology improves. For that reason, he\u2019s currently working on building a decentralized network of AIs through SingularityNET, a blockchain-powered AI marketplace that he founded in 2017.", "Ben has some interesting and contrarian views on AGI, AI safety, and consciousness, and he was kind enough to explore them with me on this episode of the podcast. Here were some of my favourite take-homes from the conversation:", "You can follow Ben on Twitter here, check out SingularityNET here, or follow me on Twitter here.", "Jeremie (00:00:00):Hey everyone, welcome back to the podcast. Today we\u2019re talking to Ben Goertzel, who\u2019s actually one of the founders of the AGI research community. He\u2019s been interested in this area for decades, really long before most people were. And he\u2019s done a whole bunch of interesting thinking around what it\u2019s going to take to build an AGI. We\u2019ll be talking about that a fair bit.", "Jeremie (00:00:18):We\u2019ll also be talking about his current AGI initiative, SingularityNET, which is taking a very different, and some might say a radical approach, to building AGI that differs considerably from what we see at DeepMind and OpenAI. Instead of focusing on deep learning or reinforcement learning alone, Ben\u2019s taking an approach that focuses on decentralization, trying to get different AI\u2019s to collaborate together constructively in the hopes that that collaboration is going to give rise through emergence to artificial general intelligence.", "Jeremie (00:00:45):It\u2019s a very interesting strategy with a lot of moving parts, and it\u2019s going to lead to conversations about AI governance, AI risk, AI safety, and some more exotic topics as well, like consciousness and panpsychism, which we\u2019ll get into too. So I\u2019m really looking forward to having this wide-ranging discussion with Ben and I hope you enjoy it as much as I did. Hi, Ben, thanks so much for joining me for the podcast.", "Jeremie (00:01:09):Yeah, me too. I think actually one of the areas that I want to start with, because I think it\u2019s upstream of a lot of other things and it\u2019s something that, people often talk about when is AGI coming, what\u2019s AGI going to look like, what are the risks associated with AGI? But I think upstream of that as a separate conversation about what intelligence is itself, and that seems like a hard thing to pin down. I\u2019d love to get your thoughts on what you think intelligence is and how you define it.", "Ben (00:01:35):So, I do think intelligence as a concept is hard to pin down and I don\u2019t think that matters too much. I think, for example, life is also hard to pin down as a concept. And you could debate whether viruses are really alive, or digital life forms or retroviruses are alive. And I mean, yeah, there\u2019s some things that are really clearly alive and some things that it\u2019s much less useful to think of them as being alive like a rock. And then there are things that are near the border in an interesting way.", "Ben (00:02:11):Biology didn\u2019t grind to a halt because we don\u2019t have an exact definition of life, right? I think similar thing is true with cognitive science and AI. So, I\u2019ve gone through a bit of a journey myself and thinking about intelligence, but that\u2019s a journey that has made me less and less enthusiastic about precisely defining intelligence as being an important part of the quest.", "Ben (00:02:41):When I started out working on the theory of AGI in the 1980s, I wanted to have some mathematical conceptualization of the problem. So, I started looking at basically, okay, intelligence is something that can achieve complex goals and in complex environments. And I think that it\u2019s in the spirit of what Shane Legg wrote about in his thesis Machine super intelligence much later. My conception was a little bit broader because he\u2019s looking more at a reinforcement learning paradigm where an AI is trying to optimize some reward function.", "Ben (00:03:22):And optimizing expected reward is only one species of goal achievement. You could say instead, I\u2019m trying to optimize some function over my future history, which isn\u2019t necessarily a average of a reward. And you can also look at multiple objective functions then you\u2019re looking at, so Pareto optimizing multiple functions defined over your future history.", "Ben (00:03:46):So, going that direction is interesting, it leads you down some rabbit holes of algorithmic information theory and whatnot. Then on the other hand, you can look at intelligence much more broadly. So, my friend Weaver, David Weinbaum had a PhD thesis from Free University of Brussels called Open-Ended Intelligence. And he\u2019s just looking at intelligence as a complex, self-organizing system that is modifying, and extending, and transforming itself and its environment in a way that is synergetic with its environment.", "Ben (00:04:25):And if you look at things from that point of view, achieving goals is one thing that happens in a complex of organizing system of this nature, but the goals may pop up and then go and be replaced by other goals. And the synthesis and interpretation of goals is just part of the overall cognitive self-organization process. So, from that point of view, achieving complex goals and complex environments is part of the story, but it\u2019s not necessarily the whole story. And obsessing on that part of the story could maybe lead you in bad design directions.", "Ben (00:05:09):And you could look at the current focus on deep reinforcement learning, and much of the AI community has, in part, being driven by an overly limited notion of what intelligence is. And of course, successes in that regard may tend to drive that overly limited definition of what intelligence is also. You can then go down the direction of, okay, how do we formalize the notion of open-ended into intelligence? And you can do that.", "Ben (00:05:41):And Weaver in his thesis wrote a bunch about category theory, algebra, topology, formulations, but then that becomes a whole pursuit on its own. And then one has to think as a researcher, how much time do I spend trying to formalize exactly what I\u2019m doing versus trying to do things and build systems. And of course, some balance can be useful there because it is useful to have a broader concept of what you\u2019re doing rather than just the system that you\u2019re building at that point in time.", "Ben (00:06:24):On the other hand, again, going back to the analog with biology, I mean, if you\u2019re trying to build synthetic life forms, it is useful to reflect on what life is and the fundamental nature of metabolism and reproduction and so forth. On the other hand, the bulk of your work is not driven by that, right? I mean, the bulk of your work is like, how do I string these amino acids together? So, I\u2019m attracted by that philosophical side. On the other hand, probably it\u2019s best going to be addressed in synergy with building stuff. And it\u2019s interesting when Shane Legg who went on to co-found Google DeepMind was working with me in the late 1990s when he was my employee in web mining.", "Ben (00:07:08):Shane\u2019s view then was, \u201cWell, if we want to build AGI first, we have to fully understand and formalize the definition of intelligence. He called it cybernance at that point. And then he published the thesis on Machine Super Intelligence. And so, he did, to his satisfaction, formalized the definition of general intelligence. So though I have a lot of issues with his definition, he was happy with it.", "Ben (00:07:32):Then shortly after that, he decided, but that\u2019s actually not the key to creating general intelligence, instead let\u2019s look at how the brain works and follow that. I guess each of us working in the field finds it useful to clarify in our own mind something about how intelligence works and then you go off and pull in a whole lot of other ideas to actually work on. Right?", "Jeremie (00:07:57):Well, that\u2019s really interesting. And I think one of the things that\u2019s really fascinating about, I think the approach that you\u2019re taking here is, it\u2019s almost as if it implies that the moment we specify like a loss function, the moment we get too absolutist about what it is we\u2019re trying to optimize, it creates room for pathology. It creates room for an almost ideological commitment to a certain loss function, which maybe we don\u2019t understand. Is that like a [crosstalk 00:08:23].", "Ben (00:08:24):Yeah. I mean, one thing you find in experimenting with even simple AI systems is, just like a computer program has the disturbing feature of doing what you\u2019re programmed to do, whether they\u2019re what you wanted to do, like an AI system, given a certain objective function, it has the property of figuring out how to optimize that objective function rather than the function you hoped you were giving it, right? So, I mean, it\u2019s common in game-playing. If you sent an AI to play in a game, I mean, it doesn\u2019t care about the spirit of the game, right? It\u2019s just trying to find a way to win, given whatever funny little loopholes there are in the rules of the game.", "Ben (00:09:10):And for a relatively simple game like chess or go, it\u2019s all good, right? Because there\u2019s not that many loopholes, there isn\u2019t that many pieces. If you\u2019re looking at a more complex video game, I mean, very often an AI will find some really insane, weird looking\u2026 That can\u2019t work. Right? But it does work, right? It\u2019s allowed by the rules of the game. And I\u2019ve found this playing with AI, with genetic algorithms, for learning strategies, for games a long time ago, like in the eighties. I mean, others even before that.", "Ben (00:09:46):But that same phenomena, of course, occurs on a larger scale. Right? And there\u2019s a whole bunch of complex papers looking at pathologies of, you\u2019re asking an AI to optimize the reward, but then it finds some way to optimize that reward that wasn\u2019t what you\u2019re thinking. This ties in with wireheading, which is talked about in the transhumanist and the brain computer interfacing community. Right?", "Ben (00:10:14):So, if your goal is to maximize pleasure, define the stimulation of your pleasure center, then why don\u2019t you just stick a wire in your head and stimulate your pleasure center. But that\u2019s very trivial, but there are indefinitely-", "Ben (00:10:33):\u2026 complex versions of that pathology. That\u2019s not an easy problem, right? Because you can look at it like, if you look at the brother issue, so I genuinely want an AGI that I create to, in some sense, I want it to reflect the values that I have now. And even if I say I don\u2019t want it to fully agree with me on everything, I mean, that the notion of fully agree, I want it to understand not fully agree in terms of my own mindset, right? I don\u2019t want it to slice all my children into little strips and stir fry them or something. Right?", "Ben (00:11:11):I mean, there\u2019s a lot of things I clearly don\u2019t want to happen. There are some ways that I don\u2019t mind the AI leading me in a different direction than I was thinking just as I want people wiser than me to lead me in different directions, but formalizing the wiggle room. I want the AI to have in deviating from my values is also hard. And then, like if I had an AI that had exactly the values I have when I was 20, I would be annoyed at that I know, let alone the exact values that the human race had in 1950, let alone 1500. Right?", "Ben (00:11:50):Even if we had an exact formulation of our values, we wouldn\u2019t want the AGI to be pinned to that forever. Right? But then we want those values to evolve over time in synchrony with us. But we\u2019re evolving in directions that we don\u2019t know. So, to formalize all that is infeasible, given tools currently at our disposal leading one to think that formalizing these things, hopefully, can\u2019t be necessary and it can\u2019t be the right way to do it, right? And that leads you in a whole other direction.", "Ben (00:12:26):It\u2019s like, okay, if formalizing ethics and pinning down an AI to formalized ethics leads to all sorts of bizarre conundrums that are very far from resolution, maybe we take a step back and take a more loosey goosey human view of it, which is what we do with ethics in our own lives. And if we take early stage AGI systems and we\u2019re relating to them in a way involving positive emotional bonding, if we have been doing beneficial things like education and healthcare and doing science and so forth, I mean, then are we qualitatively moving that AGI in a positive, ethical direction rather than trying to approach it by formalizing our goals in some precise way, and then guaranteeing that the AI won\u2019t deviate from goals, except in the ways we want it to deviate from them, which we also can\u2019t formalize. Right? I mean, that\u2019s both satisfying and unsatisfying. Right? But that seems to be the reality as I see it now.", "Jeremie (00:13:38):And do you think, because there\u2019s a sense I keep getting from this as you described, like our morality\u2019s shifted dramatically over the last 500 years. There are things we do today that we take for granted that would be considered morally important, like 500 years ago.", "Ben (00:13:55):I mean, clearly. And right now, I mean, I eat meat every day with some moral misgivings, but it\u2019s easy for me to imagine in 50 years, especially once synthetic meat is a thing that really works and it almost works now, it\u2019s easy for me to imagine how the people will look back on eating hamburgers the way we now look back on cannibalism. And we\u2019re like, \u201cOh, but the burger tastes good.\u201d But yeah. I mean, the stone-age tribesmen men might have been like the other guys upper arm tastes good. Right?", "Jeremie (00:14:25):Right. And I mean, to the degree that the co-sign similarity, basically between our moral fabric today and the moral fabric of fifteen hundreds or thirteen hundreds Europe, or wherever we\u2019ve come from, is basically zero. There are not [crosstalk 00:14:37].", "Ben (00:14:37):It\u2019s not zero. Kinship is value. We love our parents and our children. We love our wives and we don\u2019t want to torture our own bodies. There is a core of humanity there, but the subtle thing is, what we identify as that core is not what they would have identified in 1500. They would have been like belief in God is the core. Right? And to me, I\u2019m like, no, that was just some random nonsense you believed back then. That wasn\u2019t the core. Right?", "Jeremie (00:15:11):I guess we do have populations too that would look at, there are individuals within humanity who would say, it\u2019s morally bad to love your family. Like I\u2019m sure you can find out if 7 billion people, a handful who would argue for that. I guess part of what I\u2019m wondering is-", "Ben (00:15:25):Of course, there are transhumanists who believe that. Yeah, because that\u2019s tribal thinking, which is holding us back from moving on to a broader singularity where you\u2019re not clinging to all this DNA stuff. So, yeah, sure.", "Jeremie (00:15:41):Yeah. And I guess where that leads me is the question of, whether you think our moral trajectory, which AGI is going to be a part of that eventually, but is that trajectory like, is the process more important than the end point? Is it more about the stepwise progression or do you think that there\u2019s actually a subset of moral norms that will be preserved throughout because they\u2019re intrinsically good?", "Ben (00:16:02):Clearly the process of transformation is a lot of what\u2019s important to us, right? If we could upgrade our intelligence at any speed, we would probably not choose to multiply our intelligence by 1000 every second, because we would just lose ourselves. That\u2019s basically replacing yourself by a God mind. Which could be a cool thing to do.", "Ben (00:16:34):I mean, if it was like either I exist or the God mind exists, maybe I\u2019ll decided the God mind should exist. But I mean, if you doubled your intelligence every six months or something, then it would be more satisfying from our point of view of our current value system. Right? Because we\u2019re getting continuity though. You\u2019re getting to experience yourself becoming a super intelligent God mind rather than it just happening.", "Ben (00:17:02):And I think that has to do with why we feel like we\u2019re the same person we were when we were three years old. I remember back to around 18 months. But the sense in which I\u2019m the same person as I was at 18 months, I mean, of course you can cherry pick common personality traits. But the main thing though is each point, I thought I was the same guy the day before and the day after. And just changed a little bit. Even at a moment in your life when you have an incredible inflection point, right?", "Ben (00:17:41):I mean, a few times in my life I\u2019ve been through some big consciousness transformation where I felt like within a week I was a whole different person. But still, I didn\u2019t really think I was a whole different person. Right?", "Ben (00:17:53):I still realize that this is still Ben, I\u2019m just in a different state of consciousness. Yeah, that continuity is important to us on a cultural level also. Right? And I think that will continue to be important to us going forward for a while. I mean, whether we will lose our taste for continuity, that\u2019s a big question I don\u2019t have the answer for. You could imagine the taste for continuity continuously going down year by year until by 2100, we\u2019re just a super AI mind who really isn\u2019t attached to itself at all. It doesn\u2019t care about growing it\u2019s intelligence by a factor of a thousand in a second. Right? But now clearly we do care about that and that\u2019s going to drive the progress toward the singularity in a number of different ways. Right?", "Jeremie (00:18:52):Well, and so speaking of progress towards the singularity, there are a number of different organizations now working on that, that big project. I think two of the most, maybe high profile in the media are OpenAI and DeepMind. I know you\u2019ve had a lot of influence on DeepMind in particular in terms of the folks who work there today and the thesis, but I\u2019d love to get your sense of\u2026 So first off, what\u2019s your mental model of how OpenAI and DeepMind are approaching AGI? And then, how does that contrast with the SingularityNET?", "Ben (00:19:19):The first thing I would say is, to me, putting those two in the same category is like putting Winston Churchill and Trump in the same category or something. I mean, they each have their own strengths, but I don\u2019t think those are really comparable organizations. And I think Google as a whole, and not just DeepMind, but Google Brain in the Mountain View office, which is where Bird and transformer neural nets, for example, came out, and a whole lot of other valuable stuff.", "Ben (00:19:50):So, I think Google as a whole has put together by far the best AI organization of any centralized company out there. I mean, of course, academia as a whole is stronger than any one company by a humongous amount in terms of coming up with new AI ideas. But if you\u2019re looking at a company or a government lab, a specific organization, I mean, Google has just done a really good job of pulling in AI experts from a wide variety of different backgrounds.", "Ben (00:20:28):So, I mean, they have deep learning people, they\u2019ve pulled in cognitive architecture people, they\u2019ve pulled in a whole bunch of algorithmic information people, they\u2019ve got Marcus Hutter, who invented the general theory of general intelligence, right? So, I think there\u2019s really a great deal of depth there. And I know there\u2019s some internal rivalry between the mountain view and the UK people, but I think DeepMind is very strong, Google Brain and other teams in Mountain View are also very strong. Google in New York area is very strong.", "Ben (00:21:01):So, all in all, there\u2019s a lot of depth there, and there\u2019s a lot of different approaches being pursued behind the scenes, which are qualitatively different from the things that get a lot of publicity. So, my eldest son, Zarathustra is doing PhD in AI for automated theorem proving. So machine learning to guide theory improving. And his supervisor, Josef urban is an amazing guy who organizes AITP AI for theory proving conference every year.", "Ben (00:21:30):You\u2019ve got a bunch of Google DeepMind people there every year who are doing work on AI for theory proving, connecting that with AI ethics and some of the things we were talking about earlier and that\u2019s a pretty much unconnected with video game playing or with brain modeling or the things that [inaudible 00:21:51] personally is into. So, I think there\u2019s a lot of depth there that really, they want to create AGI. Like Larry and [Sergey 00:22:00] understand what AGI is and what the singularity is. Dennis and Shane understand it very deeply. I think still at a high level they are predominantly committed to deep reinforcement learning and conceptual emulation of how the brain works in modern hardware as an approach to AGI.", "Ben (00:22:23):Now, they\u2019re clearly open-minded enough that they\u2019ve made hires of great people who do not share their prediction, which is to their credit. But still, the vast bulk of their machine is deep re-enforcement learning, crunching a lot of data on a lot of machines and trying to figure out what the brain is doing and figuring out ways to do the same kind of thing in neural nets on a lot of GPUs. Right? And so, I think if that approach is going to work, it would shock me if Google weren\u2019t the ones who got there actually.", "Ben (00:23:03):And now I happen to think that it\u2019s not the best approach, which is a different topic right now. OpenAI on the other hand, I don\u2019t know them as well, but my wife who\u2019s also a AI PhD. My wife and I, actually I\u2019m not an AI PhD, I\u2019m a math PhD. So, she\u2019s more of a certified AI expert than I am. We went to OpenAI\u2019s conference a few years ago in San Francisco. And it felt like a room full of super high IQ, brilliant, passionate, 25 year old guys or younger, and mostly guys literally, who thought that AI had been invented about five years previously and that backpropagation was the only AI learning algorithm there.", "Ben (00:23:51):There was a few guys there who were like, \u201cWell for our hackathon project this weekend, we\u2019re going to enable natural language based authoring of Python programs. Right? So, we\u2019re going to try to seek neuro network to map an element to Python. After three days of hacking, they were like, \u201cWell, we found the irregularities in natural language were a bit more complicated than anticipated.\u201d Right?", "Ben (00:24:21):These guys didn\u2019t know that linguistics or computational linguistics had anything to say, or they may not have known it existed as a field. No, of course, I mean, Ilya Sutskever. Of course there were guys in OpenAI who were very, very deeply knowledgeable, but I felt, at that stage, OpenAI was like a deep neural nets only shop. And they were just hiring more and more people to bang on that, to use that one hammer to bang whatever they could, which is the exact opposite of what DeepMind and Google Brain have done.", "Ben (00:24:58):And then, I mean, the whole thing of GPT-2 is too dangerous to release because it\u2019s going to destroy the world. I mean, spouting bullshit is bad, but it\u2019s not going to destroy the world. Right? And that was invented in Google anyway. I mean, that\u2019s just-", "Ben (00:25:16):Yeah. I mean, that\u2019s just burnt in one direction. Right? And so, I mean, now actually, for some of my, I\u2019m working on a number of things. So, I\u2019m working on a bunch of AGI research that we\u2019ll talk about in a few minutes, and I\u2019m also working on some applied practical AI projects. Like one thing we announced yesterday is a robot called Grace, which is an elder care robot, a collaboration with Hanson Robotics.", "Ben (00:25:40):And so, that\u2019s supposed to go into elder care facilities and hospitals provide social and emotional support, some practical functions. But in that project, I went along to that before I get to AGI, I need a practical dialogue system. I wish we could use OpenAI stuff. I wish we could use GPT-2 or 3, but a high percentage of what they say makes no sense. It\u2019s just bloviating bullshit. And then you can\u2019t put that in an elder care robot. Right?", "Ben (00:26:06):So, I mean, these things that they were claiming are going to destroy the world because they\u2019re so human-like, they\u2019re not good enough to use in almost any practical projects now. And I would say DeepMind has not done that either. On the other hand, GPT-2 I didn\u2019t mind so much because they hadn\u2019t gotten their payday yet. Right? But now they\u2019re already bought by Microsoft. So, why did they still need to overblow things. It\u2019s not financially necessary anymore. Right?", "Jeremie (00:26:43):What is it that you think means or prevents GPT-3 from performing at the level that it needs to? I mean, is it a symbolic [crosstalk 00:26:51]?", "Ben (00:26:52):It has no understanding of anything that it\u2019s talking about. My friend, Gary Marcus, who\u2019s working on the robust AI with Rodney Brooks, he put it beautifully in his article, GPT-3 bloviator. Right? I mean, it just spouts stuff that sounds plausible, but it has no idea what it\u2019s talking about. And so, I mean, you can see, like with multiplying numbers, it can do two by two multiplication. When you get up to four by four multiplication of integers, it\u2019s right 15 to 20% of the time or something. That\u2019s just really, really weird.", "Ben (00:27:31):If you know the algorithm, you\u2019re right almost a hundred percent, unless you make a stupid error. If you don\u2019t know the algorithm, you should be 0%. Right? But what it did, it looked at all the multiplication problems online and memorized the answers. And then it came up with some weird extrapolations and let it do a few problems that weren\u2019t in its training database. Right?", "Ben (00:27:52):It doesn\u2019t understand what multiplication is, or it would never get 15 or 20% multiplication problems right. And you can see that in many other cases. Ask it like, who were the best presidents of the U.S. it\u2019ll answer a lot of good things then it\u2019ll throw a few kings of England in there just for fun. But I mean, because it doesn\u2019t know what of the U.S means.", "Ben (00:28:17):So, the thing is, in the end, it has no more to do with AGI than my toaster oven does. It\u2019s not representing the knowledge in a way that will allow it to make consistently meaningful responses. And that\u2019s not to say that everything in there is totally useless for AGI. It\u2019s just you\u2019re not going to make GPT-4, 5, 6, 7 and get AGI.", "Ben (00:28:46):So, I mean, there\u2019s very interesting work on tree transformers where you use a constituent grammar to bias the generation of a transformer network. And then I\u2019ve been playing with something like that, where you use a whole knowledge hypergraph. You can use a knowledge graph, which is dynamically constructed based on what\u2019s been read to guide the generation of the transformer. And then you have some semantic representation, sone knowledge that\u2019s playing a role in the generative network.", "Ben (00:29:20):Yeah, yeah, yeah, yeah. So, there\u2019s some of the ideas and tools in transforming networks may end up being one component among others in a viable AGI architecture. But on the other hand, OpenAI is not working on those things from what I know. I mean, from what I can see, their philosophy is mostly take the best AI out of the literature and implement it at very, very large scale with a lot of data and a lot of processors. And then it will do new and amazing things. And the thing with that is, it\u2019s semi-true. There\u2019s so much in the AI literature that gave so-so results and will give amazing results once you run it on enough data and enough processor time.", "Ben (00:30:13):But you usually need to rejigger the architecture and rethink things, and add some important new features while you\u2019re in the process of doing that scaling up. So, I was teaching deep neural networks in the nineties in University of Western Australia. I taught a class in neural nets cross-listed computer science in psychology department back when I was an academic. We were doing multilayer perceptrons with recurrent backpropagation, we\u2019re teaching deep neural nets.", "Ben (00:30:46):And you could see you needed to scale up massively because you were running in there on that with like 50 neurons. And it took all your processor time for hours. Right? But the process of scaling it up, it wasn\u2019t like, take that exact code and idea and run a lot more machines. Still people aren\u2019t using recurrent back prop. Right? They came up with other methods.", "Ben (00:31:07):So, at the very high level, I do think we can get to AGI by taking ideas we have now and scaling them up on massive number of machines with a massive amount of data. But when you\u2019re in the weeds, that scaling up involves inventing a whole bunch of new math and algorithms, kind of in the spirit of what was there before. And I think OpenAI is so far sticking a little too closely to like, let\u2019s actually take what someone else invented and just literally-", "Ben (00:31:43):\u2026 scale it up on more machines with more data. Right? At leverages they\u2019re positioned very well in that they have a lot of money, a lot of data and a lot of computers. Right? Actually, I don\u2019t think we need necessarily humongous, conceptual, revolutionary breakthrough to get to AGI, but I think we need more creativity than that. Right? And so, that leads onto my own AGI work, I guess.", "Jeremie (00:32:19):Yeah. I was going to say maybe that gets us to OpenCog to SingularityNET. And I think this is entangled too with the next question I did want to ask, which is, what are some of the risks that you see coming from AGI? I know you\u2019ve been generally more optimistic, maybe than some of the hardcore pessimists in the community, but you see some risks and I\u2019m curious how that plays into your own view about what the most promising route AGI would be.", "Ben (00:32:39):Yeah. Yeah. I mean, there\u2019s two categories. Let me address the risk thing first, because once I start talking about OpenCog and true AGI, it\u2019s hard to stop. So, I mean, there\u2019s two categories of risks in my view. So, one risk is the Nick Bostrom style risk, which is, you do your best to create an ethical AGI using all your math theory and your loving care and common sense, and getting all the politics. Right? And then, you still can\u2019t reduce the uncertainty to zero when you\u2019re creating something that\u2019s in the end, more generally intelligent than you are. Right?", "Ben (00:33:22):I mean, no matter how optimistic you are, there\u2019s some odds that there\u2019s something you can\u2019t foresee that\u2019s going to unfold there. And I mean, I don\u2019t buy Nick\u2019s argument that a superhuman AGI is going to intrinsically have a drive to turn us all into computronium to make more hard drive space for itself. I mean, the whole drive to be megalomaniac and consume our resources and squash your enemies, this is not something that necessarily is there in an engineered system by any means, but still-", "Ben (00:34:06):No, I think that\u2019s bullshit. I mean, I think that emerges in systems and evolve through competition or through some complex mix of competition and cooperation. But if you\u2019re engineering a singleton mind, say, it\u2019s not competing with anything, right? It just didn\u2019t evolve in that competitive way, so it doesn\u2019t have to have that motivational system.", "Ben (00:34:28):So, I don\u2019t think the drivers that caused humans to be that way have to be there for an AGI because, I mean, you and I could compete, but we can\u2019t merge our brains if we want to become a superorganism. Two AGIs that started competing could decide to merge their brains instead. I mean, I don\u2019t think that logic applies to systems that are engineered and don\u2019t evolve in the way that we did.", "Ben (00:34:57):But on the other hand, I have to say there\u2019s an irreducible uncertainty in creating something a hundred times smarter than me. Right? I mean, for all, it could immediately make contact with some even more super intelligent that\u2019s eminent in the quantum fluctuations of elementary particles that we think are random. And that other AI that it contacted could be good or bad from the human point of view. Right?", "Ben (00:35:26):So there\u2019s that. There\u2019s an irreducible uncertainty, which is really hard to put a probability on. And to me, I mean, there\u2019s also an irreducible uncertainty that I wake up in five seconds and realize I got my brain in a vent in some other universe. Right? So, I mean, there\u2019s irreducible uncertainties all around if you reflect on them. But then there\u2019s a more concrete risk, I think, which is that, I mean, humanity could develop malevolent or indifferent AIs in a pretty directly, simply comprehensible way, even before you get to massive super AI.", "Ben (00:36:05):And so, this gets down to my often made observation that the main things that AIs are used for in the world today are selling, killing, spying and crooked gambling. Right? I mean, that you\u2019re advertising, you\u2019re doing surveillance military, and you\u2019re doing Wall Street trading. And so, if this is what\u2019s shaping the mind of the baby AGI, maybe it will end up being a greedy megalomaniacal sociopath, reflecting the cognitive structure of the corporate and government organizations that gave rise to it, right? So, I mean, that\u2019s a palpable risk, right?", "Ben (00:36:46):I mean, you could see if AGIs are spy agencies, Wall Street traders, and advertising companies, which are exacerbating global wealth inequality. And then, I mean, you\u2019ve got a bunch of people in the developing world who have no jobs anymore because robots in the developing world took their jobs. So they\u2019re being subsistence farmers and computer hackers, right? So, I mean, there\u2019s potential dystopic scenarios that don\u2019t need any super AGI.", "Ben (00:37:19):And that from some points of view would be, even look like the most likely scenario, given the nature of world politics and technology development. So, I mean, that says-", "Ben (00:37:35):Yeah, that certainly motivates my approach. And I think this ties in with AI algorithmics in a subtle way. It\u2019s not that subtle, but subtle than is commonly appreciated, because I think big tech companies, even the more beneficial oriented ones run by good-hearted human beings, they\u2019re focusing on those AI approaches that best leverage their unique resources, which is huge amounts of data and huge amounts of compute power.", "Ben (00:38:02):So, if you look at the space of all AI algorithms, maybe there\u2019s some that don\u2019t need that much data or compute. And there\u2019s some that need a lot of data or compute. The big tech companies, they have a fiduciary duty to put their attention on the ones that require a lot of data and compute because that\u2019s where they have more competitive advantage over everyone else.", "Ben (00:38:23):And they have such, if you\u2019re working among these companies, the APIs for leveraging their data and compute power are really slick and so much fun to work with. Like if you\u2019re working at Google, I mean, simple command, and you\u2019re doing a query over the whole web, that\u2019s amazing. Right? So, I mean, of course you\u2019ve got a bias to use those tools, but the result is that the whole AI field is being pushed in a direction which is hard to do if you\u2019re not inside the big tech company. It\u2019s a valid direction, but there may have been other directions, I think there are, that don\u2019t need as much data or compute power. But those don\u2019t have nearly such slick tool chains associated with them.", "Ben (00:39:04):So, for develop, like OpenCog, that I\u2019ll talk about in a moment is a pain in the ass to work with. TensorFlow is much easier to work with. That\u2019s not entirely for fundamental reasons. We don\u2019t have the UI developers. We don\u2019t have all the team that\u2019s needed to make parallelism scale up automatically. Right? So, the approaches the big tech companies like have slicker tools associated with them. So it will attract more developers, even not the ones not working for those big tech companies. So what happens is\u2026 And of course, if you\u2019re a PhD student, if you write a PhD thesis that matches what a big tech company is doing, I mean, you\u2019re more likely to get a good job right away. So why not do that? It\u2019s also interesting, even though there\u2019s other interesting stuff.", "Ben (00:39:48):So, the whole AI field is sucked into, what\u2019s valuable for these companies doing, selling crooked gambling, spying, and supporting murder activities by national governments.", "Jeremie (00:40:00):It\u2019s funny how common these effects are. I mean, sorry, I\u2019m just thinking back to my time in academe and in physics where I was studying interpretations of quantum mechanics. And it was like, that\u2019s like a great career-ender. If you ever want a career ending thing to study is like, go into fundamental quantum mechanics and it\u2019s the same, anyway. It\u2019s so funny.", "Ben (00:40:14):Yeah. Yeah. And that\u2019s an area I\u2019ve studied a lot. And there\u2019s a lot of depth in the interaction between that and quantum computing. I mean, but that\u2019s a whole other fascinating topic. And I think reinforcement learning, it suits really well a company which has a metrics-oriented business model and supervised learning does as well. So, if you\u2019re running an advertising company, sales of all kinds is all about metrics. Like how big is your product line? How many deals have you closed? This advertising channel, how good has the ROI been, right?", "Ben (00:41:04):And so, the Wall Street, obviously as well. One of the beautiful things about working in computational finance, I mean, I like it as a geek. It\u2019s cool because you get immediate feedback on what your algorithms are doing as opposed to medicine where it can be years to get feedback. Because these business areas are metrics-oriented, that\u2019s really driven AI toward things like supervised learning and reinforcement learning where you\u2019re optimizing these quantitative metrics all the time. And it\u2019s not that that\u2019s bad or invalid, but it\u2019s not the only thing that you can do in advanced AI or AGI.", "Ben (00:41:42):So, I mean, other things like, say, hypothesis generation, which is important for medicine or science, it\u2019s just nastier to quantify, computational creativity in general. So like Google DeepDream got a lot of news. It\u2019s creative compared to a lot of things. But in the end, it\u2019s just combining pictures that were found online, right? It\u2019s not that creative. But computational creativity, it\u2019s hard to put a metric on it.", "Ben (00:42:11):And development has been driven by these metrics-driven business models. Now there\u2019s some good about that, right? I mean, having a metric lets you cut through your internal bullshit and in certain ways and it can drive progress. On the other hand, it also drives progress away from valuable things. Like my mom spent her career running social work agencies and she was always fed up by these philanthropic organizations that would only donate to nonprofits that were demonstrating progress according to metrics. But yet it\u2019s very, very hard to show your progress according to metrics if you\u2019re doing, say, enrichment education for low-income single mothers or something.", "Ben (00:42:58):I mean the metrics unfold over the years and it costs a lot of money to follow up the people and see how they\u2019re doing. So, the result there is philanthropic organizations prefer to donate to breast cancer or something where you can quantify progress. So, the thing is, focusing on quantifiable progress is good, but it pushes you to certain things. And in AI, AI is pushing you to reinforcement learning where you\u2019re doing reward maximization and it\u2019s pushing away from education, healthcare and science where it\u2019s harder to immediately quantify. That\u2019s what I see is a short term danger. And in a way focusing on the long-term Nick Bostrom danger is valid, in a way it\u2019s a disinformation campaign to distract your attention from the short-term danger.", "Ben (00:43:52):So, when a big tech company tries to get you to pay attention to super AGI that might kill you 50 years from now and sponsors conferences on that, partly it\u2019s a way of distracting your attention from the damage they\u2019re causing in the world at this exact moment. Right? Although there\u2019s a valid aspect to it too. I mean, world is very tangled up, right?", "Jeremie (00:44:15):Yeah. It\u2019s funny how often problems come down to the fact that important things are often hard to measure. And when we score the functioning, for example, like the economy in terms of the stock market, or GDP, or some metric that\u2019s easily-", "Ben (00:44:48):\u2026 profound mess, profound economic issues. And I can see, like my sister\u2019s a school principal and there\u2019s low-income kids who are just sitting at home watching TV all day, getting no education. And there\u2019s going to be a lot of ramifications to that. But hard to measure, GDP going up is easy to measure.", "Jeremie (00:45:08):Right. Now, I do want to tack into the, the last area I really want to make sure we can touch on which is, which AI risk mitigation strategies you think are most promising. I\u2019m going to focus maybe on the short term risks that you\u2019ve highlighted in terms of-", "Ben (00:45:23):Well, let me say a little bit about my own AGI work now because that will tie into that. So, my approach and my feelings since the late nineties has been what I would call a hybrid approach to AGI is going to be most successful. And there were architectures in the eighties called blackboard architectures where you had sort of a, there were blackboards back then, now there are whiteboards, right? Or Promethean\u2019s boards, they have a lot of things.", "Ben (00:45:58):Imagine a blackboard, and you have a bunch of experts in different areas, and they\u2019re all riding in the same blackboard, and they\u2019re all erasing what each other wrote. And they\u2019re collectively doing a proof for making a picture or something. So, I think each of the historical AI paradigms which I would say neural net supervised learning, unsupervised learning, logical inference, and evolutionary learning and the number of others, each of these paradigms, in my view, is like one of the blind men grabbing part of the elephant of intelligence. Like one guy\u2019s grabbing the nose, the other guy is pulling the ears, one guy is pulling on the others, there\u2019s something, right?", "Ben (00:46:42):So, I mean, I think each of the traditional AI paradigms is getting at a key part of what we need for human-like general intelligence. So, one approach is to try to find ways to make one AI paradigm incorporate what\u2019s good about all the other ones, right? So making neural net do logical reasoning and do evolution. Another approach is to come up with some new meta-algorithm that incorporates what\u2019s good about all these different. And that\u2019s very appealing to me personally, actually. But the approach I think is probably going to succeed first is a hybrid approach, where you\u2019re letting algorithms coming from these different classical AI paradigms cooperate together in real time on updating a common knowledge base.", "Ben (00:47:32):And I think that work in that direction ultimately it\u2019s going to lead to what looks like a single meta-algorithm that incorporates the best of what comes from these different paradigms. But I think we\u2019re going to get there by actually hybridizing different algorithms from different classical AI paradigms and having them work together.", "Ben (00:47:52):So, in OpenCog architecture, what we do is we have a knowledge hypergraph, so it\u2019s a weighted labeled hypergraph. Actually, it\u2019s a meta-graph not a hypergraph because-", "Ben (00:48:06):I was about to. A graph has nodes with links between them, right? A hypergraph has nodes with links but the link can go between more than two nodes. You can have a link spanning three nodes or something. A meta-graph you can have a link pointing to a link or a link pointing to a whole sub-graph, right?", "Ben (00:48:22):So, it\u2019s like the most abstract graph that you can get. So, OpenCog, AtomSpace, it\u2019s a weighted labeled meta-graph. Weights means each noted link could have a set of different quantitative or discrete values associated with it. And labels, these are types associated with nodes and links. And so, we don\u2019t enforce a single type system on the AtomSpace, but you could have a collection of type systems on the M space.", "Ben (00:48:51):So, from a programming language, it looks like a gradual typing system or something where you could have something on typed or you could have something with a partial type. And then the types could act, even new types can be assigned via learning. But you can have type checkers in that whole instrumentation on there. So it\u2019s a weighted labeled knowledge hypergraph and then you allow multiple different AIs to act on this knowledge hypergraph concurrently. And this is where things get interesting because if you have a probabilistic logic system and you have, say, a attractor neural net, and you have a reinforcement learning systems, and you have, say, an automated program learning system, these are working together on the same knowledge hypergraph, I mean, then you need them to be cooperating in a way that leads to what we call cognitive synergy which means they don\u2019t screw each other up. Right?", "Ben (00:49:47):Basically if one of the algorithm gets stuck, the other algorithms should be able to help it overcome whatever obstacle it\u2019s facing. That requires the various algorithms to be sharing some abstraction of their intermediate state with each other. So, it means some abstraction of the intermediate state of each algorithm as it\u2019s operating on the knowledge hypergraph needs to be in the hypergraph itself. Right?", "Ben (00:50:16):And this is where the design gets settled because doing everything in this hypergraph is slow, but doing nothing in there means you just have a multi-modular system with no ability for each algorithm to see the other one\u2019s intermediate state. So, like what abstraction of its state, what portion of each algorithm state goes in the hypergraph versus outside. In a neural net, for example, we developed what we call cognitive module networks, where you, say, if you break a deep neural architecture into layers or something, you may have a node in the hypergraph representing each layer and its parameters. And the piping between layers happens in the hypergraph. But then the spreading of the backpropagation inside the layer happens in some torch object that\u2019s outside the hypergraph.", "Jeremie (00:51:04):Oh, so you\u2019re constantly bouncing back and forth between those frameworks?", "Ben (00:51:06):Yeah. So the nice thing with torch is you have very good access to the compute graph unlike in TensorFlow. So, if you have two different torch neural nets, you represent them by nodes in the hypergraph. And if you compose those torch neural nets, that\u2019s represented by a symbolic composition of the nodes and the nodes in the hypergraph. So, if your reasoning engine comes up with some new way to compose neuro modules, that can be backed out to compensation in torch and the compute graphs, the composition just passes through.", "Ben (00:51:38):So, in math terms, you have a morphism between the torch compute graph and the logic graph within the hypergraph. Right? There\u2019s a lot to work out there. Right? So I just described one little bit of it, which [inaudible 00:51:54] powerful, which our St. Petersburg team published, I guess last year in the paper on cognitive module networks. But you need similar thinking to that, like pairwise for each pair of AI algorithms. Right?", "Ben (00:52:06):And so, how does your evolutionary learning algorithm make use of probabilistic reasoning to do fitness estimation? The hybrid approach mostly has a steep learning curve, right?", "Ben (00:52:20):Because you need to understand this crosscutting knowledge representation, and you have to understand all the algorithms that are playing a role in it. So, where we\u2019re at now with that, actually we came to the very painful and annoying conclusion that we needed to rebuild almost the whole OpenCog system from scratch.", "Ben (00:52:44):Yeah. We\u2019re calling OpenCog Hyperon. I decided to name the versions after obscure elementary particles, instead of numbers. Like Linux has all these funny animals, or Apple has a California suburbs. So, yeah, we\u2019re doing hyper-run when we port to quantum computing, we\u2019ll make it tacky on there\u2019s something [crosstalk 00:53:04].", "Ben (00:53:05):Basically it\u2019s about scalability. I mean, we can do whatever we want with the current OpenCog, but it\u2019s just, it\u2019s too slow in a few different senses. I mean, I think that probably the most obvious thing is we need it to be massively distributed. Like now we can have a knowledge hypergraph and RAM on one machine and we can have a bunch of hypergraphs share a post stress data store in a sort of hub and spokes architecture. But we can\u2019t use the current system across thousands or tens of thousands of machines.", "Ben (00:53:35):And ultimately, for our work with transformer neural nets, we have a pretty big server farm with all these multi GPU servers. For OpenCog, we just can\u2019t use scalable infrastructure now. And it\u2019s obvious we need to. So, part of our bet is just a seven with Deaton there on that. It\u2019s like when you manage to scale them up the right way. Whoa, look at what they can do. Right?", "Ben (00:53:56):So we\u2019re thinking that once we\u2019ve scaled up the OpenCog infrastructure, we\u2019re suddenly going to see the system able to solve a whole bunch of hard problems that it hasn\u2019t been able to so far. So that\u2019s part of it is just scaling up the knowledge hypergraph. And of course, that mostly means scaling up the pattern matching engine across the knowledge hypergraph, right? Because I mean, just scaling up nodes and links isn\u2019t that hard, getting the kinds of pattern matching we need to do, which significantly go beyond what current graph databases support, getting the kind of pattern matching we need to do to scale up across a distributed knowledge hypergraph, not impossible, but it\u2019s work, right?", "Ben (00:54:38):Then the other thing is, and this is more just increment AGI point of view, from the work we\u2019ve been doing putting neural nets, evolutionary learning and logic together, we\u2019ve just come to a much subtler understanding of how the knowledge hypergraph needs to work. So, we\u2019re creating what\u2019s effectively a new programming language, which is, we\u2019ve been referring to atomies because in OpenCog, the nodes and links are called atoms. Atom is the superclass of the node and link subclasses.", "Ben (00:55:14):So, we informally referred to the dialect of scheme we have been using to create and manipulate nodes and links as atomies because both nodes and links are atoms. So this is atomies too, maybe we\u2019ll come up with another name. But we\u2019re understanding better what we need to do in terms of a type system and then a family of index type systems inside the M space to better support integration of neural nets reasoning and evolutionary learning.", "Ben (00:55:54):And so, this led us to dig fairly deep into Idris and Agda and various of these dependently type programming languages. So, we\u2019re looking at how do you do gradual, probabilistic, linear, dependent typing in a reasonably scalable way? Because it seems like if you can do that, then you can get these multiple AI algorithms we\u2019re working with to inter-operate scalably and cleanly.", "Ben (00:56:25):And this comes down to how much of the operation of these AI algorithms can you pack into the type checker of a gradual probabilistic, linear, dependently type language? Because the more of the AI crunching you can fit into the type checker, then you can just make sure that type checker is really, really, efficiently implemented, right? Then this ties in with which aspects of internal state of the AI algorithms are put into the knowledge hypergraph, right?", "Ben (00:56:59):So, we\u2019re digging very deep in deep functional programming literature on that side, as well as into the distributed graph databases. And I think this may be how, in the end, hybridizing different algorithms eventually leads you in the direction of, well, actually what I\u2019ve ended up with bears little resemblance to the algorithms I started with, and we have a meta-algorithm.", "Ben (00:57:30):So, to start, OpenCog Hyperon will certainly be a hybrid. We\u2019re going to keep using torch or if something better comes along, right? And we\u2019re going to use our probabilistic logic network framework. And we can make those work together, but it may be that after a few years of incremental improvement there, we\u2019ve modified the neural evolutionary and logical part enough that you just have something-", "Ben (00:57:58):\u2026 something you want to call a different, more abstract [inaudible 00:58:03]. Because when you cash these things out at the category theory level, I mean, the differences between a neural learning algorithm and a logic inference algorithm are much less than one would think, most of them at the current implementation level. So, part of it is about having an implementation fabric where the underlying commonalities between the algorithms from different paradigms are exposed in the language rather than obscured, which is the current case.", "Jeremie (00:58:39):Yeah, I was going to say, that itself is really interesting because I generally, at least I\u2019ve seen it framed as an adversarial relationship. This idea of neural learning versus symbolic learning or symbolic logic. And what I find cool about this is you\u2019re really fusing the two together and making them play nice in a very formal way.", "Ben (00:58:56):Yeah. We\u2019ve already done that in simple ways. So, I mean, for example, we have the nodes and links and the hypergraph and you can do embedding to embed a node in a vector. Right? And we do that. We tried deep walk and graph convolution networks. Actually, we\u2019re doing it using kernel PCA in a certain way now, so more traditional tools. But you can set that up so that you have a category theory, like you have a morphism between the vector algebra of the vectors and then the probabilistic logic algebra on the graph side.", "Ben (00:59:34):So, what\u2019s interesting, if you\u2019re trying to do logic inference, you have some premises, you have a conclusion that you want to drive or try to drive from the premises, you can make a graph embedding as a vector of the premises, making the embedding of the conclusion. So, then you have a vector for the premises vector of the conclusion. You can look at the midpoint, you can look at the intermediate points along that vector, you map those midpoints back into the knowledge hypergraph and then you look at those as potential intermediate premises for the logic engine to do an inferring from the premises to the conclusion, right?", "Ben (01:00:07):So, you\u2019re using the morphism between graph space and vector space as a method of logical inference control. Right? And that depends how you do that mapping because if you just straightforwardly use Deep Walker GCN to do the mapping, you don\u2019t get a morphism that\u2019s accurate. Right? So there\u2019s a lot of subtle things there.", "Ben (01:00:32):And I think we\u2019re going to get rid of backpropagation and as that is gotten rid of, you\u2019re going to see that replaced with algorithms that met more naturally into the-", "Ben (01:00:49):\u2026 and evolutionary side also. So, this is another subtle point is playing around with InfoGen and other more subtle neural algorithms. So we\u2019ve got an InfoGen, which is a form of gen that learns automatically these semantic, latent variables on degenerative network side. We\u2019ve gotten that to do transfer learning between clinical trials successfully, which is pretty cool.", "Ben (01:01:12):So, you train InfoGain network to predict which patients in, say, a breast cancer clinical trials are going to be helped by which medication. And then using an InfoGain for the semantic latent variables that can be used for patient segmentation in a way that lets you transfer it to a different clinical trial better.", "Jeremie (01:01:36):A different clinical trials still on breast cancer, I assume, right?", "Ben (01:01:44):Or maybe a very different patient population. Right? It might be, you can tell something even for different cancers. I mean, we\u2019re looking at tumor gene expression and there\u2019s a lot of similarity between the tumor gene expression in cancers in different tissues actually. But we\u2019ve been looking at breast cancer mostly because there\u2019s more open data about that than about other things.", "Ben (01:02:03):So, there we\u2019ve got it to work. But what I was coming to is, if you try InfoGain on video data or something, the learning just doesn\u2019t converge. Right? And so then you give up and you\u2019re like, that\u2019s a bad architecture. But maybe it\u2019s not about architecture and backprop, it\u2019s just about learning algorithm. Right? So, I\u2019m suspecting that using flowing point evolutionary learning like CMAES or something on these really complex neural architectures may work better.", "Ben (01:02:34):But if so, that gives you a lot to go on in cognitive synergy, right? Because once you\u2019re using an evolutionary learning algorithm, well, you can use inference for fitness estimation, right? I mean, there\u2019s a lot of openings for other AI tools in your hybrid system to help guide the evolutionary learning in a way that\u2019s more challenging in in the backprop framework.", "Ben (01:02:57):So, yeah, this is something I\u2019m curious about, which is a purely technical point. Like how many promising neural architectures are being discarded just because they\u2019re not suitable-", "Ben (01:03:07):\u2026 for backprop, which is a very good algorithm, but it has its strengths and weaknesses like everything else, right?", "Jeremie (01:03:16):Well, maybe this ties in at least thematically with another kind of contrarian position, at least as you were saying earlier, with respect to the way AGI is looked at or AI is looked at, or intelligence is looked at in the West. Which is, we tend to take just a materialistic view of consciousness. But I do want to touch on this idea of panpsychism that you\u2019ve been a fan of, which is almost as controversial in the West as discarding backdrop is. Do you mind elaborating a little bit on panpsychism and maybe its connection to some of your thinking on AGI, if [crosstalk 01:03:47].", "Ben (01:03:47):I will elaborate on panpsychism, but I have skipped the company I\u2019m now the CEO of, which I must tell you about for a few minutes. So, SingularityNET, so I\u2019ve talked a lot about OpenCog and OpenCog Hyperon, right? And that\u2019s something I\u2019ve been working on a long time. Now, what I\u2019ve been doing the last few years is leading this project called SingularityNET, which is, it\u2019s a blockchain-based AGI platform basically, an AI platform, not just AGI. AI and neural AI, both.", "Ben (01:04:21):So that, lets you basically operate a bunch of Docker containers, each of which has an AI algorithm satisfying a certain API in it. And then these Docker containers can coordinate together, they can outsource work to each other, they can rate each other\u2019s reputation, they can pay each other and someone can query the network. And there\u2019s a peer-to-peer mechanism that passes the query along through the network to see if there\u2019s anyone who can do what Cory asked for.", "Ben (01:04:45):But the whole thing works without any centralized controller. Right? So, it\u2019s a society of minds as AI pioneer Marvin Minsky was talking about.", "Ben (01:04:58):Yeah, yeah. So this ties back to the more political industry structure aspects we were talking about, right? Because it\u2019s important because as we move from neural AI toward AGI, we\u2019re going to be a lot better off as a species if the emerging AGI is not owned or controlled by one actor. Because any single actor is corruptible.", "Ben (01:05:23):And I\u2019m not very corruptible, but if some thugs come to my house and threatened to murder my children if I don\u2019t give them the private keys to my repository, then I probably am corruptible. Right? So, I would rather not be in that position or anyone be in that position. Right? So, I think we want the early stage AGI as it evolves, I think we want it to be more like Linux or the internet than like, say, OSX or some company\u2019s private internal network.", "Ben (01:05:57):And to enable that is challenging, right? Because you\u2019re talking about runtime systems that are using a lot of RAM and processing power and data and network and so forth. So, SingularityNET is a platform that allows a bunch of different nodes in a distributed AI network to cooperate and operate in a purely decentralized way. And it\u2019s out there now, it\u2019s not as sophisticated as it will be. I mean, we\u2019re working with Cardano blockchain.", "Ben (01:06:29):Right now it\u2019s implemented on top of Ethereum blockchain. We\u2019re moving a bunch of the network to Cardano blockchain, partly because it\u2019s faster and cheaper, but partly because we\u2019re introducing some more abstract features that Cardano supports better because their smart contracts are in Haskell, which is a nice abstract language.", "Ben (01:06:47):So, we\u2019re looking at, how does one AI in the network describe, at an abstract level, what it does to the other AI in terms of the resources it uses, the data and texts it spits out, but also what properties its processing fulfill. So, we\u2019re introducing an abstract, dependent-type theory based description language for AIs to describe what they\u2019re doing to each other. Which is supposed to make it easier for one AI to reason about how the combine other AIs to do something, right?", "Ben (01:07:20):So, if we compare it to OpenCog, in OpenCog you have this small type of graph and multiple AI algorithms are tightly integrated on top of it because they have to all understand what each other are doing semantically. SingularityNET is looser integration, right? You have multiple different AIs in the network, they communicate by a description language that tells what each other are doing and why and what properties they obey.", "Ben (01:07:45):But in the end, they can be black boxes. They can have their own knowledge repositories and exposing certain properties. So, I think we want both of those layers. Like you want a society of minds layer with multiple AIs that are semi-transparent with regard to each other. And then within that, you\u2019ll have some things that are doing more neural functions like processing certain data types or doing certain kinds of optimization. And you have some AIs in that network that are serving more general intelligence-type functions, just like we have a cortex and we have a peripheral nervous system and the cerebellum and so forth.", "Ben (01:08:21):So, in that landscape, OpenCog is intended to power the agents that run in SingularityNET network doing the most abstract cognition stuff. But we want a lot of other things in there complimenting them.", "Jeremie (01:08:36):Because I was going to ask you, how does the coherence then emerge from SingularityNET? It sounds like then it\u2019s like OpenCog gives you that coherence, gives you that high level reasoning and then outsources tasks through SingularityNET through the blockchain to other AIs.", "Ben (01:08:50):Yeah. And you can deploy OpenCog through SingularityNET, right? So, I mean, you can have multiple different OpenCog agents running in SingularityNET, but from a pure SingularityNET point of view, OpenCog doesn\u2019t matter. People could deploy whatever they want in there. From the view of, like why I personally created it in the first place, it\u2019s partly because you want a decentralized open way for your OpenCog systems to cooperate with a bunch of other things.", "Ben (01:09:23):So, yeah, SingularityNET is run by, it\u2019s a nonprofit foundation. It\u2019s more like Ethereum foundation. We\u2019ve spun off a for-profit company called True AGI, which is working on building systems using the OpenCog Hyperon framework. So, that\u2019s like a Linux Red Hat thing, right? Where OpenCog Hyperon is open, but just as Red Hat commercialized stuff on top of Linux, True AGI is commercializing or working toward commercializing systems that are built using OpenCog Hyperon and SingularityNET.", "Ben (01:09:55):So, yeah, there\u2019s a lot of layers there. And this actually ties in with your questions about Panpsychism and consciousness in some-", "Ben (01:10:07):\u2026 some indirect ways, because I think\u2026 Yeah, part of the idea underlying Panpsychism, which is the philosophical premise that everything is conscious in its own way. Right? But just like in George Orwell\u2019s Animal Farm, all animals are equal, but some animals are more than others. I mean, in Panpsychism, everything is conscious, but some things may be more conscious than others or differently conscious than others. Right?", "Ben (01:10:39):And so, in that point of view, it can seem ridiculous to say that this coffee cup is conscious, but yet if you dig into quantum field theory, I mean, and quantum information theory, at a certain level, I mean, all these wave functions, they\u2019re interacting with the wave functions that are outside the system, they\u2019re processing information all the time. And they\u2019re incorporating that in some aspects of global coherent state as well as local state.", "Ben (01:11:11):I mean, if you try to boil down consciousness to information processing or distributed coherent awareness, it becomes hard to argue that these processes are absolutely not there in some physical object. Although you could certainly argue they\u2019re there to a greater degree in the human brain or a mouse\u2019s brain. But if you\u2019re talking about a philosophical principle like, is there a conscious versus unconscious dividing line? It\u2019s not clear on what sense that makes sense.", "Ben (01:11:49):Certainly, you can speak about abstract, reflective consciousness, like self-modeling at a cognitive level. And we are doing that in a way that this cup is not, right? So, there are some aspects of the natural language term consciousness that humans have and the coffee cup doesn\u2019t have. So, what becomes subtle in thinking about consciousness is what Chalmers called the hard problem of consciousness, right?", "Ben (01:12:22):So we have various empirical properties we can talk about, like, can you model your mind in a practical sense and answer questions about what you\u2019re doing, and are you exchanging information with your environment and registering that information exchange in your global state? You have all these empirical properties associated with consciousness, then you have what are called qualia, like the experience of existing, right?", "Ben (01:12:53):And what many people do is they correlate the experience of the existing with reflexive self-modeling, which the human brain clearly does in a way that a coffee cup doesn\u2019t. And I think the key aspect differentiating panpsychism from what some more common view of consciousness in the modern West is, as a panpsychist, you tend to think like the basic qualia, the basic experience, I am, is not uniquely associated with like reflective, deliberative, self-modeling type consciousness, but rather it\u2019s associated with the more basic-like information exchange type consciousness that is eminent in every physical system. Right?", "Ben (01:13:44):And so, that\u2019s not incredibly relevant to the everyday AI work that I\u2019m doing now. It will become relevant when you start building a femto scale quantum computers, or maybe even simpler quantum computers. It will certainly become relevant when you start doing brain computer interfacing. But then you can ask yourself questions like, \u201cOkay, this computer that I\u2019ve wired into my head, do I feel it there on the other end in the same way that I feel if I wire another human brain into my head, or does it feel like what I get when I wire a coffee cup into my head,\u201d right?", "Ben (01:14:24):Because I\u2019m guessing if I wire a coffee cup into my brain or wifi it, I\u2019m not going to feel that much of what it is to be a coffee cup. I guess if I wire my brain into yours and increase the bandwidth, I will feel a lot of what it is to be you, which will be weird. Right? What if I wire my brain into a version 3.0 of our grace awakening health elder care robot, right? Will I feel what it is to be an elder care robot? Will it feel something like what it is to be a human, or will it feel like what it is to be a coffee cup? Right?", "Ben (01:15:00):So, I think that the rubber will hit the road with this stuff. And very interesting in terms of, see, things like SingularityNET as a decentralized society of minds, or even think about human society and the global brain of computing and communication and human intelligence cloaking the earth right now. I mean, one could argue the real intelligence is in human society and culture, and we\u2019re all just neurons in the global brain responding to what it sends us on the internet. Right?", "Ben (01:15:30):But what kind of consciousness or experience does the global brain of the earth, or would, say, a decentralized SingularityNET society of minds have? Right? So, in a panpsychist view, you might say, \u201cWell, an OpenCog system, it has a focus of attention, it has a working memory, it\u2019s conscious experience will be a little more like human beings although quite different because it doesn\u2019t grow up with a single body that it\u2019s uniquely associated with. Something like a decentralized SingularityNET network might have its own general intelligence in a way that exceeds an OpenCog or a human, it\u2019s conscious experience would just be very, very different, right?", "Ben (01:16:20):I mean, because it\u2019s not centered on a single knowledge base, let alone a single body. And this gets back to first question of, what is intelligence, right? Because our whole way of conceptualizing intelligence it\u2019s overfitted to organisms like us, that we\u2019re here to control the body and get food and get sex and status and all the things that we do. An OpenCog system, even though it\u2019s very mathematical, ultimately it\u2019s built on the metaphor of human cognitive science, where you got perception action in the long-term memory, working memory.", "Ben (01:16:54):I mean, because that\u2019s what we have to go on. Right? But is that kind of intelligence greater in the fundamental sense than that which would emerge in a SingularityNET network that might get some self-organized emergent structures that we can\u2019t even understand. And this brings us back to Weaver\u2019s notion of open-ended intelligence, right?", "Ben (01:17:22):So, SingularityNET, you would say is more of, it\u2019s more of an open-ended intelligence approach toward an AGI where you\u2019re like, \u201cEveryone around the world, put your stuff in there.\u201d Have it describe what it\u2019s doing using an abstract description language. If it\u2019s going to flourish, it should get a lot of its processing by outsourcing stuff to others in the network rather than being so cystic.", "Ben (01:17:43):And then, it\u2019s trying to make money by providing services. It\u2019s hopefully providing its creator with some money, helping with income inequality if they\u2019re in a developing country. But what does this whole thing develop into? No one\u2019s scripting it. Right?", "Ben (01:17:57):So, that\u2019s also very cool. So, if the breakthrough in AGI happened in a decentralized way would be really awesome and fascinating. I mean, the OpenCog way is a little more determinant. We\u2019re architecting a system model on human cognitive science. We\u2019re going to use it to control these elder care robots, which even have human-like bodies through a partnership of two AGI and the robot company. And so, I mean, that\u2019s a little more determinate. And it may be a level of each, right? We don\u2019t know how this is going to evolve because the robots are going to draw on SingularityNET AI, including OpenCog plus other stuff on the back end.", "Ben (01:18:43):And from the robots for the view, it will just draw on whatever works best for achieving its functions. And we\u2019ll see to what extent that\u2019s OpenCog versus some incomprehensible, self-assembled conglomeration of a thousand agents. Right?", "Jeremie (01:18:59):Well, it\u2019s a beautiful and exciting vision and a very open-ended one too, which is interesting. I guess we\u2019ll have to wait and see how this develops.", "Ben (01:19:07):Yeah. Vision is one thing, but making it work is where I spend most of my time on which is really astoundingly difficult. But it\u2019s amazing the tooling that we have available now. You could have written it down when I started my career, but each step would have just been so slow in terms of compute time and human time using the crappy tools available that time.", "Ben (01:19:32):So, it\u2019s amazing. This is all very, very hard, but it\u2019s amazing that we can even make progress on it now. Right?", "Ben (01:19:41):It\u2019s certainly a fun time to be doing AI as you and all your listeners know.", "Jeremie (01:19:49):Absolutely, yeah. And it\u2019s a great time to be learning about things like this too. All the different approaches to solving this problem. Thanks a lot for sharing yours with us. Do you have any places where you recommend people go if they want to contribute to OpenCog or SingularityNET [crosstalk 01:20:02]?", "Ben (01:20:02):Yeah, absolutely. So, probably our best developed media property is the SingularityNET. So, if you go to singularitynet.io and look at the SingularityNET blog and SingularityNET YouTube channel, which is linked from singularitynet.io, like that will lead you to everything. But regarding OpenCog in particular, there\u2019s an OpenCog wiki site. And you can find it by going to OpenCog.org, then go to the Wiki.", "Ben (01:20:29):And there\u2019s a page of stuff on OpenCog Hyperon, which is our new system. The current OpenCog is in GitHub, it\u2019s all open. And while I\u2019ve been thinking a lot about the new version, the current version is what we\u2019re using inside these nursing assistant robots now. It does something.", "Ben (01:20:46):There are two online conferences that I organized earlier this year, which might be interesting. One was the OpenCogCon online conference, which is just about OpenCog. Then every year, since 2006, I\u2019ve led the artificial general intelligence conference, which is a, and face-to-face until now. But this year, the online AGI 20 conference, you can find all the videos and papers from that online also. And that\u2019s some of my stuff, but also other things from the modern AGI community that I haven\u2019t had time to go into here.", "Jeremie (01:21:22):Well, great. Thanks so much. Really appreciate it. It\u2019s something I\u2019m sure a lot of people will want to check out. And Ben, thanks so much for making the time.", "Ben (01:21:28):Yeah, yeah. Yeah. Thanks for the interview. It\u2019s good fun. There\u2019s always more to cover than you possibly can, but it\u2019s some fun conversation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa2fb633ca282&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a2fb633ca282---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://youtu.be/-VKF1lJhspg", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://en.wikipedia.org/wiki/Instrumental_convergence", "anchor_text": "power-seeking"}, {"url": "https://twitter.com/bengoertzel", "anchor_text": "follow Ben on Twitter here"}, {"url": "https://singularitynet.io/", "anchor_text": "check out SingularityNET here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://wiki.opencog.org/w/The_Open_Cognition_Project", "anchor_text": "OpenCog"}, {"url": "https://opencog.org/2020/07/virtual-opencogcon-july-15-16/", "anchor_text": "OpenCogCon"}, {"url": "https://agi-conf.org/2020/", "anchor_text": "Artificial General Intelligence Conference"}, {"url": "https://medium.com/tag/ben-goertzel?source=post_page-----a2fb633ca282---------------ben_goertzel-----------------", "anchor_text": "Ben Goertzel"}, {"url": "https://medium.com/tag/singularitynet?source=post_page-----a2fb633ca282---------------singularitynet-----------------", "anchor_text": "Singularitynet"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a2fb633ca282---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai-alignment-and-safety?source=post_page-----a2fb633ca282---------------ai_alignment_and_safety-----------------", "anchor_text": "Ai Alignment And Safety"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----a2fb633ca282---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&user=Jeremie+Harris&userId=59564831d1eb&source=-----a2fb633ca282---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&user=Jeremie+Harris&userId=59564831d1eb&source=-----a2fb633ca282---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa2fb633ca282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a2fb633ca282---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a2fb633ca282--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a2fb633ca282--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a2fb633ca282--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122K Followers"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unorthodox-path-to-agi-a2fb633ca282&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.ca/Quantum-Physics-Made-Fundamental-Everything/dp/0735244138", "anchor_text": "Quantum Physics Made Me Do It: A Simple Guide to the Fundamental Nature of Everything2023"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}