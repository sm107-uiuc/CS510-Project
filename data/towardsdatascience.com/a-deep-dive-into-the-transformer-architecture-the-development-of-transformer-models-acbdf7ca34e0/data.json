{"url": "https://towardsdatascience.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0", "time": 1683011307.854192, "path": "towardsdatascience.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0/", "webpage": {"metadata": {"title": "A Deep Dive Into the Transformer Architecture \u2014 The Development of Transformer Models | by James Montantes | Towards Data Science", "h1": "A Deep Dive Into the Transformer Architecture \u2014 The Development of Transformer Models", "description": "It may seem like a long time since the world of natural language processing (NLP) was transformed by the seminal \u201cAttention is All You Need\u201d paper by Vaswani et al., but in fact that was less than 3\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al.", "paragraph_index": 0}, {"url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00576", "anchor_text": "predicting chemical reactions", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "reinforcement learning", "paragraph_index": 0}, {"url": "https://nlpprogress.com/", "anchor_text": "benchmark leaderboard", "paragraph_index": 2}, {"url": "https://ruder.io/nlp-imagenet/", "anchor_text": "better", "paragraph_index": 2}, {"url": "https://thegradient.pub/nlps-clever-hans-moment-has-arrived/", "anchor_text": "worse", "paragraph_index": 2}, {"url": "https://blog.exxactcorp.com/5-types-lstm-recurrent-neural-network/", "anchor_text": "LSTMs", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Arrival_(film)", "anchor_text": "movie Arrival", "paragraph_index": 4}, {"url": "https://github.com/FlxB2/arrival_logograms", "anchor_text": "https://github.com/FlxB2/arrival_logograms", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1906.02243", "anchor_text": "total yearly energy consumption", "paragraph_index": 7}, {"url": "https://github.com/huggingface/transformers#model-architectures", "anchor_text": "HuggingFace repository", "paragraph_index": 7}, {"url": "https://www.semanticscholar.org/paper/Semantic-hashing-Salakhutdinov-Hinton/cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "anchor_text": "2007 Semantic Hashing", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "DenseNet", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "layer normalization", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Dai et al. In 2019", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1802.05751", "anchor_text": "image generation", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "reinforcement learning", "paragraph_index": 24}, {"url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00576", "anchor_text": "chemistry", "paragraph_index": 24}, {"url": "https://github.com/huggingface/transformers#model-architectures", "anchor_text": "HuggingFace", "paragraph_index": 25}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "notebook", "paragraph_index": 25}, {"url": "https://talktotransformer.com/", "anchor_text": "talk to transformer", "paragraph_index": 25}, {"url": "https://play.aidungeon.io/", "anchor_text": "dungeon adventure game", "paragraph_index": 25}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "code-annotated version", "paragraph_index": 25}], "all_paragraphs": ["It may seem like a long time since the world of natural language processing (NLP) was transformed by the seminal \u201cAttention is All You Need\u201d paper by Vaswani et al., but in fact that was less than 3 years ago. The relative recency of the introduction of transformer architectures and the ubiquity with which they have upended language tasks speaks to the rapid rate of progress in machine learning and artificial intelligence. There\u2019s no better time than now to gain a deep understanding of the inner workings of transformer architectures, especially with transformer models making big inroads into diverse new applications like predicting chemical reactions and reinforcement learning.", "Whether you\u2019re an old hand or you\u2019re only paying attention to transformer style architecture for the first time, this article should offer something for you. First, we\u2019ll dive deep into the fundamental concepts used to build the original 2017 Transformer. Then we\u2019ll touch on some of the developments implemented in subsequent transformer models. Where appropriate we\u2019ll point out some limitations and how modern models inheriting ideas from the original Transformer are trying to overcome various shortcomings or improve performance.", "Transformers are the current state-of-the-art type of model for dealing with sequences. Perhaps the most prominent application of these models is in text processing tasks, and the most prominent of these is machine translation. In fact, transformers and their conceptual progeny have infiltrated just about every benchmark leaderboard in natural language processing (NLP), from question answering to grammar correction. In many ways transformer architectures are undergoing a surge in development similar to what we saw with convolutional neural networks following the 2012 ImageNet competition, for better and for worse.", "Transformer represented as a black box. An entire sequence of (x\u2019s in the diagram) is parsed simultaneously in feed-forward manner, producing a transformed output tensor. In this diagram the output sequence is more concise than the input sequence. For practical NLP tasks, word order and sentence length may vary substantially.", "Unlike previous state-of-the-art architectures for NLP, such as the many variants of RNNs and LSTMs, there are no recurrent connections and thus no real memory of previous states. Transformers get around this lack of memory by perceiving entire sequences simultaneously. Perhaps a transformer neural network perceives the world a bit like the aliens in the movie Arrival. Strictly speaking the future elements are usually masked out during training, but other than that the model is free to learn long-term semantic dependencies throughout the entire sequence.", "Transformers do away with recurrent connections and parse entire sequences simultaneously, sort of like the Heptapods in Arrival. You can make your own logograms using the open source python2 repository by FlxB2 (https://github.com/FlxB2/arrival_logograms).", "Operating as feed-forward-only models, transformers require a slightly different approach to hardware. Transformers are actually much better suited to run on modern machine learning accelerators, because unlike recurrent networks there is no sequential processing: the model doesn\u2019t have to process a string of elements in order to develop a useful hidden cell state. Transformers can require a lot of memory during training, but running training or inference at reduced precision can help to alleviate memory requirements.", "Transfer learning is an important shortcut to state-of-the-art performance on a given text-based task, and quite frankly necessary for most practitioners on realistic budgets. Energy and financial costs of training a large modern transformer can easily dwarf an individual researcher\u2019s total yearly energy consumption, at a cost of thousands of dollars if using cloud compute. Luckily, similar to deep learning for computer vision, the new skills needed for a specialized task can be transferred to large pre-trained transformers, e.g. downloaded from the HuggingFace repository.", "The secret sauce in transformer architectures is the incorporation of some sort of attention mechanism, and the 2017 original is no exception. To avoid confusion, we\u2019ll refer to the model demonstrated by Vaswani et al. as either just Transformer or as vanilla Transformer to distinguish it from successors with similar names like Transformer-XL. We\u2019ll start by looking at the attention mechanism and build outward to a high-level view of the entire model.", "Attention is a means of selectively weighting different elements in input data, so that they will have an adjusted impact on the hidden states of downstream layers. The vanilla Transformer implements attention by parsing input word vectors into key, query, and value vectors. The dot product of the key and query provide the attention weight, which is squashed using a softmax function across all attention weights so that the total weighting sums to 1. The value vectors corresponding to each element are summed according to their attention weights before being fed into subsequent layers. That may be a bit complicated to take in all at once, so let\u2019s zoom in and go through it step-by-step.", "Starting with a sequence of words forming a sentence, each element (word) in the sequence is first converted to an embedded representation called a word vector. Word vector embedding is a much more nuanced representation than something like a one-hot encoded bag-of-words model, like used by Salakhutidinov and Hinton in their 2007 Semantic Hashing paper.", "Word embeddings (also sometimes called tokens) are useful because they impart semantic meaning in a numerical way that can be understood by neural networks. Learned word embeddings can contain contextual and relational information, for example as the semantic relationship between \u201cdog\u201d and \u201cpuppy\u201d is roughly equivalent to \u201ccat\u201d and \u201ckitten,\u201d so we may be able to manipulate their word embeddings like so:", "Starting from the top left in the diagram above, an input word is first tokenized by an embedding function, replacing the string \u201cALL\u201d with a numerical vector which will be the input to the attention layer. Note that the only layer which has an embedding function is the first encoder, every other layer just takes the preceding output vectors as inputs. The attention layer (W in the diagram) computes three vectors based on the input, termed key, query, and value. The dot product of key and query, a scalar, is the relative weighting for a given position.", "The attention mechanism is applied in parallel at every element in the sequence, so every other element also has an attention score as well. These attention scores are subjected to a softmax function to ensure that the total weighting sums to 1.0, and then multiplied with the corresponding value vector. The values for all elements, now weighted by their attention scores, are summed together. The resulting vector is the new value in a sequence of vectors forming an internal representation of the input sequence, which will then be passed to a feed-forward fully connected layer.", "Another important detail that may have gotten lost so far is the scaling factor used to stabilize the softmax function, i.e. before inputting values to the softmax function used by the attention layers, the numbers are scaled inversely proportional to the square root of the number of units in the key vector. This is important for making learning work well regardless of the size of the key and query vectors. Without a scaling factor the dot product will tend to be a large value when using long key and query vectors, pushing the gradient of the softmax function into a relatively flat area and making it difficult for error information to propagate.", "As mentioned earlier, a useful consequence of doing away with recurrent connections is that the entire sequence can be processed at once in a feed-forward manner. When we combine the self-attention layer described above with a dense feed-forward layer, we get one encoder layer. The feed-forward layer is composed of two linear layers with a rectified linear unit (ReLU) in between them. That is to say the input is first transformed by a linear layer (matrix multiply), the resulting values are then clipped to always be 0 or greater, and finally the result is fed into a second linear layer to produce the feed-forward layer output.", "Vanilla Transformer uses six of these encoder layers (self-attention layer + feed forward layer), followed by six decoder layers. Transformer uses a variant of self-attention called multi-headed attention, so in fact the attention layer will compute 8 different key, query, value vector sets for each sequence element. These will then be concatenated into one matrix, and put through another matrix multiply that yields the properly sized output vector.", "Decoder layers share many of the features we saw in encoder layers, but with the addition of a second attention layer, the so-called encoder-decoder attention layer. Unlike the self-attention layer, only the query vectors come from the decoder layer itself. The key and value vectors are taken from the output of the encoder stack. Decoder layers also each contain a self-attention layer, just like we saw in the encoder, and the queries, keys, and values feeding into the self-attention layer are generated in the decoder stack.", "Decoder layer, differing from encoder layers in the addition of a encoder-decoder attention sub-layer. Six of these make up the decoder in vanilla Transformer.", "Now we have recipes for both encoder and decoder layers. To build a transformer out of these components, we have only to make two stacks, each with either six encoder layers or six decoder layers. The output of the encoder stack flows into the decoder stack, and each layer in the decoder stack also has access to the output from the encoders. But there are just a few details left to fully understand how the vanilla Transformer is put together.", "Like many other extremely deep neural networks with many parameters, training them can sometimes be difficult when gradients don\u2019t flow through from input to outputs as well as we\u2019d like. In computer vision, this led to the powerful ResNet style of convolutional neural networks. ResNets, short for residual networks, explicitly add the input from the last layer to its own output. In this way the residuals are retained through the entire stack of layers and gradients can more easily flow from the loss function at the output all the way back to the inputs. DenseNet architectures aim to solve the same problem by concatenating input tensors and output tensors together, instead of adding. In the vanilla Transformer model, the residual summing operation is followed by layer normalization, a method for improving training that, unlike batch normalization, is not sensitive to minibatch size.", "Diagram of residual connections and layer normalization. Every sub-layer in the encoder and decoder layers of vanilla Transformer incorporated this scheme.", "In recurrent architectures like LSTMs the model can essentially learn to count and gauge sequence distances internally. Vanilla Transformer doesn\u2019t use recurrent connections and perceives entire sequences simultaneously, so how does it learn which element came from which part of the sequence, especially when sequence length is allowed to vary? The answer is a positional encoding based on a decaying sinusoidal function that is concatenated to the sequence element embeddings. Vaswani et al. also experimented with learned positional encodings with almost identical results, but reasoned that using sinusoidal encodings should allow the model to generalize better to sequence lengths not seen during training.", "The introduction of the vanilla Transformer in 2017 disrupted sequence-based deep learning significantly. By doing away with recurrent connections entirely, transformer architectures are better suited for massively parallel computation on modern machine learning acceleration hardware. It\u2019s surprising that vanilla Transformer could learn long-term dependencies in a sequence at all, and in fact there is an upper limit to the distances over which vanilla Transformer can easily learn relationships.", "Transformer-XL was introduced by Dai et al. In 2019 to address this problem with a new positional encoding, and also incorporated a sort of pseudo-recurrent connection where the key and value vectors depend in part on the previous hidden state as well as the current one. Additional transformer variants include decoder-only transformers (e.g. OpenAI\u2019s GPT and GPT-2), adding bidirectionality (i.e. BERT-based transformers), among others. Transformers were designed for sequences and have found their most prominent applications in natural language processing, but transformer architectures have also been adapted for image generation, reinforcement learning (by modifying Transformer-XL), and for chemistry.", "Hopefully this post has helped you to build intuition for working with modern transformer architectures for NLP and beyond. But you don\u2019t have to be a hero and build and train a new transformer from scratch in order to try out your own ideas. Open source repositories like HuggingFace provide pre-trained models that you can fine-tune to fuel your NLP projects. Google has provided a notebook you can use to tinker with transformer models in their tensor2tensor library. And of course you can always talk to transformer or play a text-based dungeon adventure game with OpenAI\u2019s GPT-2 as dungeon master. Depending on your preferred level of abstraction the above resources may be more than enough, but if you won\u2019t be happy until you put code to concepts you may be interested in a code-annotated version of the paper from Alexander Rush and others at the Harvard NLP lab.", "Interested in HMI, AI, and decentralized systems and applications. I like to tinker with GPU systems for deep learning. Currently at Exxact Corporation."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Facbdf7ca34e0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://james-montantes-exxact.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "James Montantes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d8a2ff208c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=post_page-3d8a2ff208c3----acbdf7ca34e0---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Facbdf7ca34e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=-----acbdf7ca34e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Facbdf7ca34e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=-----acbdf7ca34e0---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://unsplash.com", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al."}, {"url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00576", "anchor_text": "predicting chemical reactions"}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "reinforcement learning"}, {"url": "https://nlpprogress.com/", "anchor_text": "benchmark leaderboard"}, {"url": "https://ruder.io/nlp-imagenet/", "anchor_text": "better"}, {"url": "https://thegradient.pub/nlps-clever-hans-moment-has-arrived/", "anchor_text": "worse"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/5-types-lstm-recurrent-neural-network/", "anchor_text": "LSTMs"}, {"url": "https://en.wikipedia.org/wiki/Arrival_(film)", "anchor_text": "movie Arrival"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://github.com/FlxB2/arrival_logograms", "anchor_text": "https://github.com/FlxB2/arrival_logograms"}, {"url": "https://arxiv.org/abs/1906.02243", "anchor_text": "total yearly energy consumption"}, {"url": "https://github.com/huggingface/transformers#model-architectures", "anchor_text": "HuggingFace repository"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://www.semanticscholar.org/paper/Semantic-hashing-Salakhutdinov-Hinton/cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "anchor_text": "2007 Semantic Hashing"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "DenseNet"}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "layer normalization"}, {"url": "https://blog.exxactcorp.com/", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Dai et al. In 2019"}, {"url": "https://arxiv.org/abs/1802.05751", "anchor_text": "image generation"}, {"url": "https://arxiv.org/abs/1910.06764", "anchor_text": "reinforcement learning"}, {"url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00576", "anchor_text": "chemistry"}, {"url": "https://github.com/huggingface/transformers#model-architectures", "anchor_text": "HuggingFace"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "notebook"}, {"url": "https://talktotransformer.com/", "anchor_text": "talk to transformer"}, {"url": "https://play.aidungeon.io/", "anchor_text": "dungeon adventure game"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "code-annotated version"}, {"url": "https://medium.com/tag/transformer?source=post_page-----acbdf7ca34e0---------------transformer-----------------", "anchor_text": "Transformer"}, {"url": "https://medium.com/tag/architecture?source=post_page-----acbdf7ca34e0---------------architecture-----------------", "anchor_text": "Architecture"}, {"url": "https://medium.com/tag/natural-language-process?source=post_page-----acbdf7ca34e0---------------natural_language_process-----------------", "anchor_text": "Natural Language Process"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Facbdf7ca34e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=-----acbdf7ca34e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Facbdf7ca34e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=-----acbdf7ca34e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Facbdf7ca34e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d8a2ff208c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=post_page-3d8a2ff208c3----acbdf7ca34e0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba54567ee2a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&newsletterV3=3d8a2ff208c3&newsletterV3Id=ba54567ee2a0&user=James+Montantes&userId=3d8a2ff208c3&source=-----acbdf7ca34e0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Written by James Montantes"}, {"url": "https://james-montantes-exxact.medium.com/followers?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "466 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d8a2ff208c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&user=James+Montantes&userId=3d8a2ff208c3&source=post_page-3d8a2ff208c3----acbdf7ca34e0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba54567ee2a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0&newsletterV3=3d8a2ff208c3&newsletterV3Id=ba54567ee2a0&user=James+Montantes&userId=3d8a2ff208c3&source=-----acbdf7ca34e0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "James Montantes"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "3 Reasons to Use Random Forest Over a Neural Network\u2013Comparing Machine Learning versus Deep\u2026Random Forest is a better choice than neural networks because of a few main reasons. Here\u2019s what you need to know."}, {"url": "https://towardsdatascience.com/3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "\u00b75 min read\u00b7Feb 4, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9d65a154d89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89&user=James+Montantes&userId=3d8a2ff208c3&source=-----f9d65a154d89----0-----------------clap_footer----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89?source=author_recirc-----acbdf7ca34e0----0---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9d65a154d89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89&source=-----acbdf7ca34e0----0-----------------bookmark_preview----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----acbdf7ca34e0----1---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----acbdf7ca34e0----1-----------------bookmark_preview----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----acbdf7ca34e0----2---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----acbdf7ca34e0----2-----------------bookmark_preview----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/examining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "James Montantes"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/examining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "Examining the Transformer Architecture \u2014 Part 1: The OpenAI GPT 2 Controversy\u201cRecycling is NOT good for the world. It is bad for the environment, it is bad for our health, and it is bad for our economy. I\u2019m not\u2026"}, {"url": "https://towardsdatascience.com/examining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": "\u00b713 min read\u00b7May 29, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffeceda4363bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexamining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb&user=James+Montantes&userId=3d8a2ff208c3&source=-----feceda4363bb----3-----------------clap_footer----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/examining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb?source=author_recirc-----acbdf7ca34e0----3---------------------9098a037_2b7a_4373_8b4d_5db93366911e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffeceda4363bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexamining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb&source=-----acbdf7ca34e0----3-----------------bookmark_preview----9098a037_2b7a_4373_8b4d_5db93366911e-------", "anchor_text": ""}, {"url": "https://james-montantes-exxact.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "See all from James Montantes"}, {"url": "https://towardsdatascience.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----acbdf7ca34e0----0-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----acbdf7ca34e0----1-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----acbdf7ca34e0----0---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----acbdf7ca34e0----0-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----acbdf7ca34e0----1---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----acbdf7ca34e0----1-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----2-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----acbdf7ca34e0----2---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----acbdf7ca34e0----2-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----3-----------------clap_footer----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----acbdf7ca34e0----3---------------------588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----acbdf7ca34e0----3-----------------bookmark_preview----588f3c8a_6f36_49a6_aa74_0d5d6ce0a854-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----acbdf7ca34e0--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}