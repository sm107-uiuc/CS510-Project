{"url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "time": 1682996078.245844, "path": "towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4/", "webpage": {"metadata": {"title": "Graph Embedding for Deep Learning | by Flawnson Tong | Towards Data Science", "h1": "Graph Embedding for Deep Learning", "description": "There are alot of ways machine learning can be applied to graphs. One of the easiest is to turn graphs into a more digestible format for ML. Graph embedding is an approach that is used to transform\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/what-is-geometric-deep-learning-b2adb662d91d", "anchor_text": "overview of Geometric Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b", "anchor_text": "the prerequisites", "paragraph_index": 0}, {"url": "https://twitter.com/FlawnsonTong", "anchor_text": "my Twitter", "paragraph_index": 1}, {"url": "https://www.reddit.com/r/GeometricDeepLearning/", "anchor_text": "Geometric Deep Learning subreddit", "paragraph_index": 1}, {"url": "https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/", "anchor_text": "this fantastic article by the Gradient", "paragraph_index": 5}, {"url": "https://medium.com/u/b83b72c7aef4?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Aditya Grover", "paragraph_index": 16}, {"url": "http://snap.stanford.edu/index.html", "anchor_text": "Stanford\u2019s SNAP", "paragraph_index": 22}, {"url": "https://medium.com/u/370a562c29b0?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Jian Tang", "paragraph_index": 32}, {"url": "https://medium.com/u/c939be75faee?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Aur\u00e9lien G\u00e9ron", "paragraph_index": 34}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "video on the subject.", "paragraph_index": 34}, {"url": "http://www.linkedin.com/in/flawnson", "anchor_text": "LinkedIn", "paragraph_index": 48}, {"url": "https://www.facebook.com/flawnson", "anchor_text": "Facebook", "paragraph_index": 48}, {"url": "https://www.instagram.com/flaws.non/?hl=en", "anchor_text": "Instagram", "paragraph_index": 48}, {"url": "https://medium.com/@flawnsontong1", "anchor_text": "Medium", "paragraph_index": 48}, {"url": "http://www.flawnson.com", "anchor_text": "my website", "paragraph_index": 49}, {"url": "https://github.com/flawnson", "anchor_text": "GitHub", "paragraph_index": 49}, {"url": "http://mail.google.com", "anchor_text": "flawnsontong1@gmail.com", "paragraph_index": 50}], "all_paragraphs": ["Be sure to read an overview of Geometric Deep Learning and the prerequisites to become familiar with this niche in machine learning.", "Follow my Twitter and join the Geometric Deep Learning subreddit for latest updates in the space.", "There are alot of ways machine learning can be applied to graphs. One of the easiest is to turn graphs into a more digestible format for ML.", "Graph embedding is an approach that is used to transform nodes, edges, and their features into vector space (a lower dimension) whilst maximally preserving properties like graph structure and information. Graphs are tricky because they can vary in terms of their scale, specificity, and subject.", "A molecule can be represented as a small, sparse, and static graph, whereas a social network could be represented by a large, dense, and dynamic graph. Ultimately this makes it difficult to find a silver bullet embedding method. The approachess that will be covered each vary in performance on different datasets, but they are the most widely used in Deep Learning.", "Graphs are the main focus of this series, but if the 3D imaging applications are more your thing, then I recommend this fantastic article by the Gradient.", "If we view embedding as a transformation to a lower dimension, embedding methods themselves are not a type of neural network model. Instead,they are a type of algorithm used in graph pre-processing with the goal to turn a graph into a computationally digestible format. This is because graph type data, by nature, are discrete.", "Machine learning algorithms are tuned for continuous data, hence why embedding is always to a continuous vector space.", "As recent work has shown, there is a variety of ways to go about embedding graphs, each with a different level of granularity. Embeddings can be performed on the node level, the sub-graph level, or through strategies like graph walks. These are some of the most popular methods.", "Deepwalk isn\u2019t the first of it\u2019s kind, but it is one of the first approaches that have been widely used as a benchmark in comparison with other graph learning approaches. Deepwalk belongs to the family of graph embedding techniques that uses walks, which are a concept in graph theory that enables the traversal of a graph by moving from one node to another, as long as they are connected to a common edge.", "If you represent each node in a graph with an arbitrary representation vector, you can traverse the graph. The steps of that traversal could be aggregated by arranging the node representation vectors next to each other in a matrix. You could then feed that matrix representing the graph to a recurrent neural net. Basically, you can use the truncated steps of graph traversals as input for an RNN. This is analogous to the way word vectors in a sentence are put together.", "The approach taken by DeepWalk is to complete a series of random walks using the equation:", "The goal is to estimate the likelihood of observing node vi given all the previous nodes visited so far in the random walk, where Pr() is probability, \u03a6 is a mapping function that represents the latent representation associated with each node v in the graph.", "The latent representations is what becomes the input for a neural network. The neural network, based on what nodes and how often the nodes were encountered during the walk, can make a prediction about a node feature or classification.", "The method used to make predictions is skip-gram, just like in Word2vec architecture for text. Instead of running along the text corpus, DeepWalk runs along the graph to learn an embedding. The model can take a target node to predict it\u2019s \u201ccontext\u201d, which in the case of a graph, means it\u2019s connectivity, structural role, and node features.", "Although DeepWalk is relatively efficient with a score of O(|V|), this approach is transductive, meaning whenever a new node is added, the model must be retrained to embed and learn from the new node.", "You\u2019ve heard of Word2vec now prepare for\u2026 Node2vec (Aditya Grover et al)", "One of the more popular graph learning methods, Node2vec is one of the first Deep Learning attempts to learn from graph structured data. The intuition is similar to that of DeepWalk:", "If you turn each node in a graph into an embedding as you would words in sentence, a neural network can learn representations for each node.", "The difference between Node2vec and DeepWalk is subtle but significant. Node2vec features a walk bias variable \u03b1, which is parameterized by p and q. The parameter p prioritizes a breadth-first-search (BFS) procedure, while the parameter q prioritizes a depth-first-search (DFS) procedure. The decision of where to walk next is therefore influenced by probabilities 1/p or 1/q.", "As the visualization implies, BFS is ideal for learning local neighbors, while DFS is better for learning global variables. Node2vec can switch to and from the two priorities depending on the task. This means that given a single graph, Node2vec can return different results depending on the values of the parameters. As per DeepWalk, Node2vec also takes the latent embedding of the walks and uses them as input to a neural network to classify nodes.", "Experiments demonstrated that BFS is better at classifying according to structural roles (hubs, bridges, outliers, etc.) while DFS returns a more community driven classification scheme.", "Node2vec is one of the many graph learning project that have come out of Stanford\u2019s SNAP research group dedicated to graph analytics. Many of their works have been the origin of many great strides in geometric Deep Learning.", "A modification to the node2vec variant, graph2vec essentially learns to embed a graph\u2019s sub-graphs. This is demonstrated by an equation that is used in doc2vec, a closely related variant, and a point of inspiration for this paper.", "In plain english, this equation can be written as: the probability of the word (wj) appearing in context given document (d) equals the exponential of the document embedding matrix (d~) multiplied by the word embedding matrix (w~j is sampled from the document), divided by the sum of all the exponentials of the document embedding matrix multiplied by the word embedding matrix for each word in the vocab list (V) across all documents.", "Using an analogy with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs (which is then made of nodes).", "These predetermined sub-graphs have a set number of edges, as specified by the user. Once again, it is the latent sub-graph embeddings that are passed into a neural network for classification.", "Unlike the previous embedding techniques, SDNE does not use random walks. Instead, it tries to learn from two distinct metrics:", "The ultimate goal is to capture highly non-linear structures. This is acheived by using deep autoencoders (semi-supervised) to preserve the first order (supervised) and second order (unsupervised) network proximities.", "To preserve first order proximity, the model also a variation of Laplacian Eigenmaps, a graph embedding/dimensionality reduction technique. The Laplacian Eigenmap embedding algorithm applies a penalty when similar nodes are mapped far from each other in the embedded space, thus allowing for optimization by minimizing the space between similar nodes.", "The second order proximity is preserved by passing the adjacency matrix of te graph through an unsupervised autoencoder which has a built in reconstruction loss function it must minimize.", "Together, the first order proximity loss function and the second order reconstruction loss function are jointly minimized to return a graph embedding. The embedding is than learned from by a neural network.", "LINE (Jian Tang et al) explicitly defines two functions; one for first order proximity and another for second order proximity. In the experiments conducted by the original research, second order proximity performed significantly better than first, and it was implied that including higher orders may level off the improvements in accuracy.", "The goal of LINE is to minimize the difference between the input and embedding distributions. This is achieved using KL divergence:", "The visualizations are simple, the math less so. Aur\u00e9lien G\u00e9ron has a great video on the subject. On another note, G\u00e9ron is also one of the few researchers in graph learning, and has combined knowledge graphs and Deep Learning to improve video recommendations while working at YouTube.", "LINE defines two joint probability distributions for each pair of nodes then minimizes the KL divergence of the distributions. The two distributions are the adjacency matrix and the dot product of node embedding. KL Divergence is an important similarity metric in information theory and entropy. The algorithm is used in probabilistic generative models like Variational Autoencoders, which embed inputs of an autoencoder into a latent space, which becomes the distribution.", "Since the algorithm has to define new functions for each increasing order of proximity, LINE doesn\u2019t perform very well if the application needs an understanding of node community structure.", "Nevertheless, the simplicity and effectiveness of LINE are just a couple reasons why it was the most cited paper on WWW of 2015. This work helped inspire interest in Graph Learning as a niche in Machine Learning and eventually Deep Learning in specific.", "HARP is an improvement to the previously mentioned embedding/walking based models. Previous models risked getting stuck in local optima since their objective functions are non-convex. Basically this means, the ball can\u2019t roll to the absolute bottom of the hill.", "Improve the solution and avoid local optima by better weight inizialization.", "Use graph coarsening to aggregate related nodes into \u201csupernodes\u201d", "HARP is essentially a graph-preprocessing step that simplifies the graph to make for faster training.", "After coarsening the graph, it then generates an embedding of the coarsest \u201csupernode\u201d, followed by an embedding of the entire graph (which itself is made of supernodes).", "This strategy is followed for each \u201csupernode\u201d in the entire graph.", "Since HARP can be used in conjunction with previous embedding algorithms like LINE, Node2vec and DeepWalk. The original paper reported marked improvements of up to 14% in classification tasks when combining HARP with various graph embedding methods: a significant leap forward.", "I\u2019ve definitely missed a bunch of algorithms and models, especially since the recent explosion of interest in Geometric Deep Learning and Graph Learning has led to new contributions popping up in publications almost daily.", "In any case, Graph embedding methods are a simple but very effective method of transforming graphs into the optimal format for a machine learning task. Due to their simplicity, they are often quite scalable (at least compared to their convolutional counterparts), and are easy to implement. They can be applied to most networks and graphs without sacrificing performance or efficiency. But can we do better?", "Next up is a dive into the complex and elegant world of Graph Convolutions!", "Follow me on LinkedIn, Facebook, Instagram, and of course, Medium for more content.", "All my content is on my website and all my projects are on GitHub", "I\u2019m always looking to meet new people, collaborate, or learn something new so feel free to reach out to flawnsontong1@gmail.com", "Upwards and onwards, always and only \ud83d\ude80", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Using machine learning to accelerate science one step at a time :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4305c10ad4a4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://flawnsontong.medium.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": ""}, {"url": "https://flawnsontong.medium.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Flawnson Tong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1dcfdd487e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&user=Flawnson+Tong&userId=a1dcfdd487e6&source=post_page-a1dcfdd487e6----4305c10ad4a4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/what-is-geometric-deep-learning-b2adb662d91d", "anchor_text": "overview of Geometric Deep Learning"}, {"url": "https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b", "anchor_text": "the prerequisites"}, {"url": "https://twitter.com/FlawnsonTong", "anchor_text": "my Twitter"}, {"url": "https://www.reddit.com/r/GeometricDeepLearning/", "anchor_text": "Geometric Deep Learning subreddit"}, {"url": "https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/", "anchor_text": "this fantastic article by the Gradient"}, {"url": "https://arxiv.org/pdf/1403.6652.pdf", "anchor_text": "DeepWalk \u2014 Perozzi et al"}, {"url": "https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf", "anchor_text": "Node2vec \u2014 Grover et al"}, {"url": "https://medium.com/u/b83b72c7aef4?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Aditya Grover"}, {"url": "http://snap.stanford.edu/index.html", "anchor_text": "Stanford\u2019s SNAP"}, {"url": "https://arxiv.org/abs/1707.05005", "anchor_text": "Graph2vec \u2014 Narayanan et al"}, {"url": "https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf", "anchor_text": "Structural Deep Network embedding (SDNE) \u2014 Wang et al"}, {"url": "https://arxiv.org/abs/1503.03578", "anchor_text": "Large-scale Information Network Embedding (LINE) \u2014 Tang et al"}, {"url": "https://medium.com/u/370a562c29b0?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Jian Tang"}, {"url": "https://medium.com/u/c939be75faee?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Aur\u00e9lien G\u00e9ron"}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "video on the subject."}, {"url": "https://arxiv.org/abs/1706.07845", "anchor_text": "Hierarchical Representation Learning for Networks \u2014 Chen et al"}, {"url": "http://www.linkedin.com/in/flawnson", "anchor_text": "LinkedIn"}, {"url": "https://www.facebook.com/flawnson", "anchor_text": "Facebook"}, {"url": "https://www.instagram.com/flaws.non/?hl=en", "anchor_text": "Instagram"}, {"url": "https://medium.com/@flawnsontong1", "anchor_text": "Medium"}, {"url": "http://www.flawnson.com", "anchor_text": "my website"}, {"url": "https://github.com/flawnson", "anchor_text": "GitHub"}, {"url": "http://mail.google.com", "anchor_text": "flawnsontong1@gmail.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4305c10ad4a4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4305c10ad4a4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/entrepreneurship?source=post_page-----4305c10ad4a4---------------entrepreneurship-----------------", "anchor_text": "Entrepreneurship"}, {"url": "https://medium.com/tag/startup?source=post_page-----4305c10ad4a4---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/tag/technology?source=post_page-----4305c10ad4a4---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&user=Flawnson+Tong&userId=a1dcfdd487e6&source=-----4305c10ad4a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&user=Flawnson+Tong&userId=a1dcfdd487e6&source=-----4305c10ad4a4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4305c10ad4a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4305c10ad4a4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4305c10ad4a4--------------------------------", "anchor_text": ""}, {"url": "https://flawnsontong.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://flawnsontong.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Flawnson Tong"}, {"url": "https://flawnsontong.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1dcfdd487e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&user=Flawnson+Tong&userId=a1dcfdd487e6&source=post_page-a1dcfdd487e6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8d63c32fc3a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-deep-learning-on-graph-embeddings-4305c10ad4a4&newsletterV3=a1dcfdd487e6&newsletterV3Id=8d63c32fc3a0&user=Flawnson+Tong&userId=a1dcfdd487e6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}