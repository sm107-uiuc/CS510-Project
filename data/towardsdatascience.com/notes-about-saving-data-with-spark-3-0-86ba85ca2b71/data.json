{"url": "https://towardsdatascience.com/notes-about-saving-data-with-spark-3-0-86ba85ca2b71", "time": 1683014644.523328, "path": "towardsdatascience.com/notes-about-saving-data-with-spark-3-0-86ba85ca2b71/", "webpage": {"metadata": {"title": "Notes about saving data with Spark 3.0 | by David Vrba | Towards Data Science", "h1": "Notes about saving data with Spark 3.0", "description": "Apache Spark is a computational engine frequently used in a big data environment for data processing but it doesn\u2019t provide storage so in a typical scenario the output of the data processing has to\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Apache Spark is a computational engine frequently used in a big data environment for data processing but it doesn\u2019t provide storage so in a typical scenario the output of the data processing has to be stored in an external storage system. Spark SQL provides a couple of connectors for datasources such as file format (CSV, JSON, text, Parquet, ORC) or JDBC. Since 2.4 there is also support for Apache Avro and since 3.0 also read support for binary files. In addition, there are several libraries that allow connecting to other datasources, for example, MongoDB, ElasticSearch, or DynamoDB to name some.", "In this article, we will go over some possibilities that we have when saving data to a file format such as Apache Parquet, because the combination of Spark with Parquet provides a very nice experience, especially for analytical queries. We will first explain why Parquet is such a popular format for storing data in the Hadoop ecosystem and then we will describe different ways how to save the data with Spark while focussing especially on features that are missing in the official documentation or are difficult to find. One of these non-trivial features that we will look at is how to save the data in a pre-sorted state so it can be leveraged for data skipping with appropriate filters.", "The provided code snippets are using the Python API and are checked against Spark 3.0 and 2.4.", "Apache Parquet is an open-source file format originally developed at Twitter and Cloudera. It is a self-describing format, which means that it contains metadata with information about the schema (among other things). It is using a hybrid model for storing the data on the disk and by hybrid, we mean that it is not strictly row-oriented (such as JSON or CSV) neither strictly columnar but instead the data is first divided vertically into so-called row-groups and then each row-group is stored in a columnar representation. These row-groups allow for splitting the dataset into multiple files (each file can have one or more row-groups) so we don\u2019t end up with one huge file which is important especially in the big data environment.", "The columnar representation of each row-group is also a very important feature because it allows for a so-called column pruning. In analytical queries, we are not usually interested in all columns of the dataset but rather we select a few columns and do some aggregation on it. This column pruning lets us skip all other columns and scan only those selected in the query so it makes the reading much more efficient as compared to row-oriented formats where all columns have to be scanned. Another advantage of columnar formats is compression \u2014 each value in the column has the same data type and the values may even get repeated which can be used to store the data more efficiently using various encoding and compression techniques (that Parquet supports under the hood).", "Parquet files support data skipping on different levels, namely on partition level and row-group level. So a dataset can be partitioned by some (usually low-cardinality) key and this means that for each distinct value of this field there is going to be a subfolder in the root directory that will contain all records with that particular value of the key (each partition can still be divided into multiple files). The use case for the partitioning is to reduce the volume of the data when reading because when the partition key is used in a filter, Spark will apply partition pruning and it will skip all partitions that are not relevant for the query (this is supported also for other file formats such as JSON or CSV).", "The row-group level data skipping is based on parquet metadata because each parquet file has a footer that contains metadata about each row-group and this metadata contains statistical information such as min and max value for each column in the row-group. When reading the parquet file, Spark will first read the footer and use these statistics to check whether a given row-group can potentially contain relevant data for the query. This will be useful especially if the parquet file is sorted by the column that we use for filtering. Because, if the file is not sorted, then small and large values can be scattered across all row-groups and thus each row-group will have to be scanned because it can potentially contain some rows satisfied by the filter. The sort here is crucial and as we will see later on, it is not trivial to save the data in the sorted state.", "One of the options for saving the output of computation in Spark to a file format is using the save method", "As you can see it allows you to specify partition columns if you want the data to be partitioned in the file system where you save it. The default format is parquet so if you don\u2019t specify it, it will be assumed.", "The data analyst who will be using the data will probably more appreciate if you save the data with the saveAsTable method because it will allow him/her to access the data using", "The saveAsTable function allows also for using bucketing where each bucket can be also (optionally) sorted:", "We will not dive into bucketing here, but in general, it is a technique on how to pre-shuffle the data and save it in this state so it can be leveraged by follow-up queries to avoid shuffle. This is going to work if the metastore is properly set up with Spark because the information about bucketing will be saved here. This approach has also other benefits since the metastore can keep also other information about your dataset such as statistics when you call the ANALYZE TABLE command later on. The sortBy can be used only after bucketBy, because what will be sorted are the created buckets. Both modes overwrite and append will work here even if the table doesn\u2019t exist yet because the function will simply create it.", "Yet another possibility of how to save data is using the insertInto function. Unlike in the previous case, the table has to exist first, because if it doesn\u2019t, you will get an error: AnalysisException: Table not found. The syntax for using this function is rather simple", "but there are some cautions that are good to be aware of:", "The function insertInto has one big advantage over saveAsTable because it allows for a so-called dynamic overwrite. This feature lets you overwrite a specific partition in a partitioned table. For example, if your table is partitioned by year and you want to update only one year, then with saveAsTable you would have to overwrite the entire table, but with insertInto, you can overwrite only this single partition so it will be a much cheaper operation especially if there are lots of big partitions. To use this feature you have to set a corresponding configuration setting:", "Here again, it is good to be careful, because if the partitionOverwriteMode is set to static (which is the default value) it would overwrite the entire table and so all other partitions would become empty. The dynamic value makes sure that Spark will overwrite only partitions that we have data for in our DataFrame.", "As we mentioned above, sometimes it is desirable to save the data in the sorted state according to some column so it could be used for data skipping in analytical queries that use this column as a filter. In Spark SQL there are three functions when it comes to sorting. There is orderBy (or equivalently sort), sortWithinPartitions, and sortBy. Let\u2019s see what is the difference between them and how they can be used:", "Using the first two functions has one big catch that you would not find in documentation (written in October 2020). Let\u2019s see it on a simple example. For the sake of simplicity we want to partition the data by year and have each partition sorted by user_id, and for saving we use saveAsTable(). If you just call", "It is not going to work! It will not raise any error and it will save the data but the order will not be preserved! The point is that when writing data to a file format, Spark requires this ordering:", "here partitionColumns are columns by which we partition the data to the file system, bucketingIdExpression is derived from the bucketing column (not relevant in our query because we are not using bucketing here) and sortColumns are columns used in sortBy with bucketing (again not relevant for our query). If the data is sorted by these columns (and possibly some more), the requirement will be satisfied and Spark will preserve the order. If however, this requirement is not satisfied, Spark will forget the previous order and will sort the data again according to this requirement while writing the data. In our example, the required ordering is (year) which is the partition column and we don\u2019t have any bucketing here. This requirement is however not satisfied, because the actual ordering is (user_id), which is the column by which we sorted the data and this is the reason why Spark will not preserve our order and will sort the data again by the year column.", "To achieve our goal and save the data (each partition) sorted by user_id we can instead do this:", "Notice the difference now, we sort explicitly each partition by year and user_id, but since the data will be partitioned by year the sort by this column doesn\u2019t really matter. What matters is that now the Spark\u2019s requirement will be satisfied, so Spark will preserve our order and save the data sorted by year and user_id and since we partition by year, it basically means that each partition will be sorted by user_id, which is exactly what we want.", "Saving the data partitioned where each partition is sorted, will allow for efficient analytical queries with filters that will skip data on two levels \u2014 partition level and row-group level. Imagine a query like this:", "This will first use the partition filter to prune the partitions and inside this single partition 2020 it will check the metadata from the parquet footers for each row-group. Based on the statistics in the metadata Spark will pick the row-groups with min\u22641 and max\u22651 and only these row-groups will be scanned, so this will speed-up the query especially if there are lots of row-groups that can be skipped.", "In this article, we described three different ways how to save data to a file format (specifically Apache Parquet) using Spark and we also explained why Parquet is such a popular data format especially when it comes to analytical queries. We also pointed out some Spark features that are not so obvious such as the behavior of the insertInto function or how to save data in the sorted state.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior ML Engineer at Sociabakers and Apache Spark trainer and consultant. I lecture Spark trainings, workshops and give public talks related to Spark."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F86ba85ca2b71&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vrba.dave?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----86ba85ca2b71---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.save", "anchor_text": "save"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.saveAsTable", "anchor_text": "saveAsTable"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.insertInto", "anchor_text": "insertInto"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy", "anchor_text": "orderBy"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sortWithinPartitions", "anchor_text": "sortWithinPartitions"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.sortBy", "anchor_text": "sortBy"}, {"url": "https://medium.com/tag/spark-sql?source=post_page-----86ba85ca2b71---------------spark_sql-----------------", "anchor_text": "Spark Sql"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----86ba85ca2b71---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/query-optimization?source=post_page-----86ba85ca2b71---------------query_optimization-----------------", "anchor_text": "Query Optimization"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----86ba85ca2b71---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----86ba85ca2b71---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&user=David+Vrba&userId=b7f216c64e33&source=-----86ba85ca2b71---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&user=David+Vrba&userId=b7f216c64e33&source=-----86ba85ca2b71---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F86ba85ca2b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----86ba85ca2b71---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----86ba85ca2b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/@vrba.dave/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83cdb92c0d8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnotes-about-saving-data-with-spark-3-0-86ba85ca2b71&newsletterV3=b7f216c64e33&newsletterV3Id=83cdb92c0d8c&user=David+Vrba&userId=b7f216c64e33&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}