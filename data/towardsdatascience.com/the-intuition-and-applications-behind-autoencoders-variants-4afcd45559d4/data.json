{"url": "https://towardsdatascience.com/the-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4", "time": 1683014197.9215622, "path": "towardsdatascience.com/the-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4/", "webpage": {"metadata": {"title": "The Intuition and Applications Behind Autoencoders & Variants | by Andre Ye | Towards Data Science", "h1": "The Intuition and Applications Behind Autoencoders & Variants", "description": "Neural networks are fundamentally supervised \u2014 they take in a set of inputs, perform a series of complex matrix operations, and return a set of outputs. As the world is increasingly populated with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-future-of-deep-learning-can-be-broken-down-into-these-3-learning-paradigms-e7970dec5502", "anchor_text": "populated with unsupervised data", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/the-fascinating-blueprint-for-efficient-ai-self-supervised-learning-954f919f0d5d", "anchor_text": "self-supervised learning", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/exploring-the-simple-satisfying-math-behind-regularization-2c947755d19f", "anchor_text": "read this", "paragraph_index": 10}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 19}], "all_paragraphs": ["Neural networks are fundamentally supervised \u2014 they take in a set of inputs, perform a series of complex matrix operations, and return a set of outputs. As the world is increasingly populated with unsupervised data, simple and standard unsupervised algorithms can no longer suffice. We need to somehow apply the deep power of neural networks to unsupervised data.", "Luckily, creative applications of self-supervised learning \u2014 artificially creating labels from data that is unsupervised by nature, like tilting an image and training a network to determine the degree of rotation \u2014 have been a huge part in the application of unsupervised deep learning.", "Autoencoders are trained to recreate the input; in other words, the y label is the x input. Because autoencoders are built to have bottlenecks \u2014 the middle part of the network \u2014 which have less neurons than the input/output, the network must find a method to compress the information (encoding), which needs to be reconstructed (decoding).", "These types of models can have multiple hidden layers that seek to carry and transform the compressed information. Because there is a limited amount of space in these nodes, they are often known as \u2018latent representations\u2019.", "The word \u2018latent\u2019 comes from Latin, meaning \u2018lay hidden\u2019. Latent variables and representations are just that \u2014 they carry indirect, encoded information that can be decoded and used later. One can think of transfer learning as utilizing latent variables: although a pretrained model like Inception on ImageNet may not directly perform well on the dataset, it has established certain rules and knowledge about the dynamics of image recognition that makes further training much easier.", "When creating autoencoders, there a few components to take note of:", "One application of vanilla autoencoders is with anomaly detection. For instance, I may construct a one-dimensional convolutional autoencoder that uses 1-d conv. layers (with architectural bottlenecks) and train it to reconstruct input sequences.", "After being trained for a substantial period of time, the autoencoder learns latent representations of the sequences \u2014 it is able to pick up on important discriminatory aspects (which parts of the series are more valuable towards accurate reconstruction) and can assume certain features that are universal across the series.", "When it predicts on a test sequence, the reconstruction loss determines how similar it is to previous sequences. If the autoencoder can reconstruct the sequence properly, then its fundamental structure is very similar to previously seen data. On the other hand, if the network cannot recreate the input well, it does not abide by known patterns.", "Another application of autoencoders is in image denoising. Images are corrupted artificially by adding noise and are fed into an autoencoder, which attempts to replicate the original uncorrupted image. Autoencoders are best at the task of denoising because the network learns only to pass structural elements of the image \u2014 not useless noise \u2014 through the bottleneck.", "Sparse autoencoders are similar to autoencoders, but the hidden layer has at least the same number of nodes as the input and output layers (if not much more). However, L1 regularization is used on the hidden layers, which causes unnecessary nodes to de-activate. To get an intuition for why this happens, read this.", "Hence, in a sense the architecture is chosen \u2018by the model\u2019. It is not always good to let the model choose by itself, however; generally L1 regularization has the tendency to eliminate more neurons than may be necessary.", "Remember that the goal of regularization is not to find the best architecture for performance, but primarily to reduce the number of parameters, even at the cost of some performance. In the case where sparse architectures are desired, however, sparse autoencoders are a good choice.", "Variational Autoencoders, commonly abbreviated as VAEs, are extensions of autoencoders to generate content. As seen before with anomaly detection, the one thing autoencoders are good at is picking up patterns, essentially by mapping inputs to a reduced latent space. This doesn\u2019t result in a lot of originality.", "A component of any generative model is randomness. Variational Autoencoders map inputs to multidimensional Gaussian distributions instead of points in the latent space. Then, the decoder randomly samples a vector from this distribution to produce an output.", "The primary difference between variational autoencoders and autoencoders is that VAEs are fundamentally probabilistic. They build general rules shaped by probability distributions to interpret inputs and to produce outputs.", "When VAEs encoder an input, it is mapped to a distribution; thus there is room for randomness and \u2018creativity\u2019. On the other hand, autoencoders, which must recognize often intricate patterns, must approach latent spaces deterministically to achieve good results.", "In the end, autoencoders are really more a concept than any one algorithm. It\u2019s an architectural decision characterized by a bottleneck & reconstruction, driven by the intent to force the model to compress information into and interpret latent spaces. The encoder-decoder mindset can be further applied in creative fashions to several supervised problems, which has seen a substantial amount of success.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4afcd45559d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----4afcd45559d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/dOhJtfXJZfw", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/the-future-of-deep-learning-can-be-broken-down-into-these-3-learning-paradigms-e7970dec5502", "anchor_text": "populated with unsupervised data"}, {"url": "https://towardsdatascience.com/the-fascinating-blueprint-for-efficient-ai-self-supervised-learning-954f919f0d5d", "anchor_text": "self-supervised learning"}, {"url": "https://towardsdatascience.com/exploring-the-simple-satisfying-math-behind-regularization-2c947755d19f", "anchor_text": "read this"}, {"url": "https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.f8AX~1c8f3/w:1200/h:226/q:auto/https://www.unite.ai/wp-content/uploads/2020/09/variational.png", "anchor_text": "Unite AI"}, {"url": "https://towardsdatascience.com/obtaining-top-neural-network-performance-without-any-training-5af0af464c59", "anchor_text": "Obtaining Top Neural Network Performance Without Any TrainingWe know less about NNs than we thoughttowardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4afcd45559d4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4afcd45559d4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4afcd45559d4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----4afcd45559d4---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4afcd45559d4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&user=Andre+Ye&userId=be743a65b006&source=-----4afcd45559d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&user=Andre+Ye&userId=be743a65b006&source=-----4afcd45559d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4afcd45559d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4afcd45559d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4afcd45559d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4afcd45559d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4afcd45559d4--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}