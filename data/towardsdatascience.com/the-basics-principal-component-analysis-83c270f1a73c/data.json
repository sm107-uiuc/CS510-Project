{"url": "https://towardsdatascience.com/the-basics-principal-component-analysis-83c270f1a73c", "time": 1683003262.4167, "path": "towardsdatascience.com/the-basics-principal-component-analysis-83c270f1a73c/", "webpage": {"metadata": {"title": "The Basics: Principal Component Analysis | by Max Miller | Towards Data Science", "h1": "The Basics: Principal Component Analysis", "description": "Principle Component Analysis sits somewhere between unsupervised learning and data processing. On the one hand, it\u2019s an unsupervised method, but one that groups features together rather than points\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/unsupervised-learning-clustering-algorithms-5b290967f746", "anchor_text": "a clustering algorithm", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/the-basics-support-vector-machines-219585f1a107", "anchor_text": "support vector machine", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST handwritten digits", "paragraph_index": 14}], "all_paragraphs": ["Principle Component Analysis sits somewhere between unsupervised learning and data processing. On the one hand, it\u2019s an unsupervised method, but one that groups features together rather than points as in a clustering algorithm. But principal component analysis ends up being most useful, perhaps, when used in conjunction with a supervised model, where it can be used for dimensionality reduction \u2014 reducing the number of feature variables. PCA is also useful in a few other situations, such as a way to filter out random noise from data, but it\u2019s with an eye towards dimensionality reduction that we\u2019ll consider it here.", "What to do when you have too many dimensions", "Imagine you\u2019re conducting some sort of social science research which involves surveying people and recording various things about them. There are all sorts of things you might conceivably want in your dataset to aid your research. You\u2019ll definitely want demographic information, for instance: age, gender, where they live or where they\u2019re from. Socio-economic information is also frequently helpful. Maybe you\u2019re doing research on some sort of public health question and would like some medical information as well, maybe height, weight, and blood pressure. The number of variables you might collect information on is almost endless, so while you\u2019re at it, why not collect information on everything you can think of? Before you get too excited, consider that it\u2019s possible to have too many feature variables.", "How is that possible, to have too many features? Isn\u2019t more data better? As it turns out, having many different features brings its own set of challenges. A data set with many feature variables will tend to be sparser, that is, that the points in it will be farther from each other on average. Let\u2019s consider a simple case to get a feel for why this should be.", "Let\u2019s say you\u2019re doing research and you only have one independent feature variable, and for the sake of simplicity it\u2019s a categorical variable with only two options, say whether the subject is male or female. There are, so far, only two categories that respondents can fall into. What happens if you add another category? Again for the sake of simplicity, let\u2019s say you ask after a second feature that only has two options, how about whether the respondent has a college degree or not. Now, the number of categories has grown to four: college educated men, college educated women, and men and women without college degrees. If you add another variable with two possible values, the number of combinations jumps up to 8. As you add variables in this way, the number of possible combinations, and therefore the number of distinct categories grows exponentially. And of course, if you add a variable with more than two possible values, say which state the respondent lives in, the number of combinations will grow faster still.", "The exponential growth of combinations brings with it a number of challenges. For one thing, the amount of respondents you need essentially grows exponentially as well. Back when you only had one variable, you could get at least one example from each possible category with only two respondents. When you increased the number of variables to 3, suddenly you needed a minimum of 8 respondents to cover each possible combination (to say nothing about how many respondents you would need to sample before you could be sure you had randomly collected one from each combination). A data set with 100 such variables, even if each only had two possible options, would have an enormous number of possible combinations, and even a reasonably large data set might not have examples from every combination.", "For another, recall that a model like a support vector machine works by trying to divide data points with a hyper-plane that will neatly separate points of different classes. As you add features, the distance between points increases as the space becomes sparser and it becomes easier to fit a hyperplane that separates the points. Even if you\u2019re adding variables that have no real causal or explanatory value, you may run into this problem unless you increase the size of your sample commensurately, at which point you may start to run into the limitations of your computing power.", "So, we need to limit the number of variables our models are using, but we don\u2019t necessarily want to simply drop features or even know ahead of time which features we should drop. PCA allows us to decrease the number of variables our model uses while implicitly considering as much of the variance from all of the features as possible. The insight is that, particularly when you have many variables to work with, invariably, some of them will be correlated with each other, allowing you to essentially consider multiple features at once. Consider the following two correlated variables:", "These two variables are correlated, which means they aren\u2019t completely independent. If you know the value of one, you can make a decent guess at the value of the other. Perhaps these variables are for height and weight. You know that some people are slimmer than others, but in general tall people will weigh more than short ones. With this in mind, let\u2019s reframe the issue here. Let\u2019s start by tracing a line that describes the relationship between the two variables:", "The insight here is that we can use distance along this line as its own variable. Instead of saying that a point has a high x-value and a high y-value we can describe a point as being far along the line to the right, implicitly giving you information about both the x and y coordinates. In fact, let\u2019s re-plot our points using this line as a new axis against which the points are plotted:", "You may wonder what we\u2019ve achieved in all this. We used to have two dimensions, x and y, and we still have two dimensions, distance along the line and distance above/below the line. The key is that now most of our variance falls along only one dimension. Before both our x and y values ranged from -2 or -3 to 2 or 3. Now, the \u2018distance-along-line\u2019 variable ranges from -4 to 4, but \u2018distance-above-or-below-the-line\u2019 only ranges from around -1 to 1. There\u2019s less information in the distance above/below variable, so when we come to actually fit a model, we may choose to use the distance along the line variable on its own and leave the above/below variable out altogether.", "Principal component analysis generalizes this process and allows you to perform it on a data set with many different dimensions. It essentially creates a new set of axes, rotated such that the first axis falls along the line of greatest variance in the data, the second axis falls along the line of second greatest variance, and so on. Just like the ordinary x and y axes of the Cartesian plane, the new axes will be at right angles to each other, but they will be rotated relative to whatever coordinates you started with. These new axes are called the \u2018principal components\u2019 of the dataset. With only two variables it is easy to visualize what this looks like:", "But the principle is the same for any number of starting dimensions. By using only the first few of these principal components from a data set that originally had many different dimensions, you can capture most of the variance in the original dataset even while your model considers substantially fewer dimensions at one time. How exactly PCA finds these appropriately rotated new axes requires some linear algebra that is a little technical for this venue, but you can still build an intuition for what is happening under the hood without the complicated math.", "Bear in mind that when you leave out one of the lower ranked principal components, you are still losing information. In our example, not every point falls exactly on the line we found, so if we disregard the above/below the line variable we are absolutely losing precision. But because PCA has grouped as much of the variance in the data into one dimension as possible, we lose less information by dropping the second principal component than if we had dropped one of the original variables.", "To give a sense of how incredibly useful PCA can be, let\u2019s consider one of the most famous machine learning training data sets, the MNIST handwritten digits. If you\u2019ve never encountered it before, it\u2019s an enormous set of examples of handwritten digits (60,000 rows in the training set):", "The handwritten numbers were digitized and the data is represented as a series of grayscale values for each pixel in a 28 by 28 box. So this data set has 784 dimensions overall! With so many dimensions it\u2019s impossible to visualize the dataset altogether. And, for that matter, it\u2019s impossible (or at least impracticable) to use certain model types on it. A support vector machine might work, but fitting a SVM to a dataset of this size with this many dimensions takes an unworkable amount of time. Actually, some simpler models also become cumbersome when you have such a large, multi-dimensional dataset. KNN models require no real time to fit at all (the \u2018model\u2019 is essentially just the training dataset), but KNN models can take a long time to predict, because when you present it with a new data point to consider, the model needs to loop through all the training points to find the ones closest to the new point.", "With PCA, we don\u2019t need to consider that many dimensions at once! As it turns out, most of the variance in the dataset is described in the first few principal components. Consider how much you can you can see just in the first two principal components of the MNIST data:", "Certainly, the graph is a little messy, but you can already start to visually separate out the different digits: the zeros are on the right side, while the ones are on the left. As it turns out, with only a handful more principal components you can get to a respectable accuracy rate, even with a model as simple as a KNN:", "If you include the first twenty principal components, the accuracy of a simple KNN model with K set to 5 is around 97%, not bad for a complicated machine vision problem being tackled by 4 lines of code! And the model still predicts around 100 times faster than a similar KNN model trained on the complete raw dataset. PCA has allowed us to decrease the number of dimensions the model needs to tackle from 784 to just 20, sacrificing very little in the way of accuracy while gaining enormously in performance.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F83c270f1a73c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----83c270f1a73c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/unsupervised-learning-clustering-algorithms-5b290967f746", "anchor_text": "a clustering algorithm"}, {"url": "https://towardsdatascience.com/the-basics-support-vector-machines-219585f1a107", "anchor_text": "support vector machine"}, {"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST handwritten digits"}, {"url": "https://medium.com/tag/data-science?source=post_page-----83c270f1a73c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----83c270f1a73c---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/pca?source=post_page-----83c270f1a73c---------------pca-----------------", "anchor_text": "Pca"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----83c270f1a73c---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----83c270f1a73c---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&user=Max+Miller&userId=dfd5ba1a8332&source=-----83c270f1a73c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&user=Max+Miller&userId=dfd5ba1a8332&source=-----83c270f1a73c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F83c270f1a73c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----83c270f1a73c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----83c270f1a73c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----83c270f1a73c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----83c270f1a73c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-principal-component-analysis-83c270f1a73c&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}