{"url": "https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939", "time": 1683008086.305512, "path": "towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939/", "webpage": {"metadata": {"title": "Improving upon Rosenblatt\u2019s perceptron | by Jean-Christophe B. Loiseau | Towards Data Science", "h1": "Improving upon Rosenblatt\u2019s perceptron", "description": "ADALINE is closer to modern machine learning methods than Rosenblatt's perceptron is. We here briefly review the underlying mathematics and provide a Python implementation."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1811.10052", "anchor_text": "imaging and MRI", "paragraph_index": 0}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "StarCraft 2", "paragraph_index": 0}, {"url": "https://psycnet.apa.org/record/1959-09865-001", "anchor_text": "his perceptron", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Bernard_Widrow", "anchor_text": "Bernard Widrow", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Marcian_Hoff", "anchor_text": "Ted Hoff", "paragraph_index": 0}, {"url": "https://mitpress.mit.edu/books/perceptrons", "anchor_text": "a notorious book by Minsky & Papert", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a", "anchor_text": "Rosenblatt\u2019s perceptron", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "convex", "paragraph_index": 13}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Ordinary_least_squares", "anchor_text": "ordinary least squares", "paragraph_index": 25}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons", "anchor_text": "GitHub repo", "paragraph_index": 31}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling", "paragraph_index": 35}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics", "paragraph_index": 35}], "all_paragraphs": ["Machine learning and artificial intelligence have been having a transformative impact in numerous fields, from medical sciences (e.g. imaging and MRI) to real-time strategy video games (e.g. StarCraft 2). Key enablers for these successes have been deep neural networks characterized by an ever-increasing number of so-called hidden layers and artificial neurons. It must be emphasized however that neural networks initially had humble beginnings: when Frank Rosenblatt (1928\u20131971) introduced his perceptron in 1957 it had only one layer made of a single computational neuron, a far cry from today\u2019s neural networks with possibly hundreds of layers and thousands of neurons. Even though a lot has changed since the late 1950s, these series aim to walk students through the beginnings of neural networks. Doing so will make it easier to grasp eventually the mathematics deep learning relies on and how modern neural networks came to be what they are today. To do so, today\u2019s subject is the Adaptive Linear Neuron and the Delta rule introduced in 1960 by Bernard Widrow and his student Ted Hoff. Before diving in, let us however do a quick recap\u2019 about Rosenblatt\u2019s perceptron.", "When he first introduced his perceptron for binary classification problems back in 1957, Rosenblatt\u2019s machine used a single computational neuron as shown in the figure below.", "the perceptron first computes a weighted sum", "and then passes it to a Heaviside function to generate its output", "The final output is the predicted class (either 0 or 1) x belongs to. Despite the mathematical simplicity of this artificial neuron, Rosenblatt\u2019s main achievement has been to devise an algorithm enabling it to actually learn a set of weights w and bias b directly from training data. Today, this supervised learning algorithm is known as the Perceptron learning algorithm.", "Rosenblatt\u2019s perceptron is not free from limitations. Some of them, rooted in the mathematical model itself, have been pointed out in a notorious book by Minsky & Papert in 1969.", "To illustrate our point, let us consider the binary classification toy problem in figure 1. It is the same as the one considered when introducing Rosenblatt\u2019s perceptron. A single training example has however been mislabeled on purpose. Using standard machine learning tools, one would expect this one mislabeled training example not to deteriorate significantly the performances of the classification model. While this intuition may be correct for more recent models, we\u2019ll see it does not apply to Rosenblatt\u2019s perceptron.", "Figure 2 depicts the evolution of the perceptron\u2019s decision boundary as the number of epochs varies from 1 to 100 (i.e. the number of times we looped through the whole training dataset to learn the weights w and bias b). Quite clearly, the decision boundary jumps all over the place. Coincidentally, the number of misclassified points ranges from 1 all the way up to 69 out of 100 data points. This erratic behavior cannot be mitigated by reducing the learning rate (set to 1 by default) as one would classically recommend. Indeed, looking back at the perceptron learning algorithm (see Algorithm 1), the weights w and bias b are continuously updated as long as the perceptron misclassifies even a single point. Given our training dataset, there is however no chance that the perceptron can actually correctly classify all of the points and so the learning process goes on forever without ever converging! This fundamental limitation and lack of robustness of the perceptron learning algorithm can however be lifted by modifying slightly the learning process. The small modification we will hereafter discuss gave rise to the single-layer perceptron now known as the ADALINE.", "Like Rosenblatt\u2019s perceptron, ADALINE (aka Adaptive Linear Element or the Widrow-Hoff rule) is a single-layer perceptron. It however differs from it in how it learns the weights w and bias b from data.", "The main difference comes from the feedback error used to adapt the weights and bias of the two perceptrons. While Rosenblatt uses the classification error (i.e. a binary value), ADALINE introduces the concept of a so-called loss function (also sometimes call cost function or objective function) relying on the output of the artificial neuron before the quantization (i.e. on a continuous value). Albeit this might seem like a minor difference, we\u2019ll see shortly that it actually makes a big difference when it comes to the final model!", "The introduction of a loss function makes ADALINE much closer to modern machine learning methods than Rosenblatt\u2019s perceptron is. Given our set of m examples (x\u2098, y\u2098) with y\u2098 \u2208 {0, 1} denoting the class x\u2098 belongs to, the loss function for ADALINE is defined as", "where \u03c6(z) is the activation function (i.e. the identity function here). Based on this definition, the weights w and bias b are obtained as the solution to the minimization problem", "i.e. they are chosen such that the sum of squares errors on our set of examples is as small as possible.", "Let us see how to find the set of weights and bias minimizing our loss function. To do so, we\u2019ll rely on the fact that the loss function is quadratic and the associated minimization problem is convex. If you don\u2019t know what a convex optimization problem is, it is, in a nutshell, an optimization problem we know how to solve efficiently. We\u2019ll also require some basic high-school calculus.", "A convex function \u2112(z) has only one minimum. It is located at the point z where the gradient of \u2112(z) is zero, i.e.", "Our goal is to find the set of weights w and bias b satisfying this condition. These are the weights and bias resulting in the sum of squared errors being as small as it can get (given our data).", "The Delta rule: Remember that the gradient of a function indicates the direction with the maximum positive slope. A simple heuristic to find the point where the function is minimum is thus to move in the direction opposite to the gradient. Hence, let us first compute the gradient of our loss function. For a general activation function \u03c6(z), it is given by", "with \u03c6\u2019(z) the derivative of the activation function with respect to z. Note that ADALINE stands for the ADAptive LInear NEuron. Its activation is the identity, i.e. \u03c6(z) = z and thus \u03c6\u2019(z) = 1. The gradient of the loss function thus simplifies to", "Starting from a given set of weights w and bias b, the delta rule states that, to reduce the sum of squares errors, these weights and bias need to be updated as follows", "where the updates \u2206w and \u2206b are given by", "Here, \u03b1 is a scalar commonly called the learning rate or step size. In its simplest form, the delta rule assumes \u03b1 constant. The variables w and b are continuously updated until a prescribed number of iterations has been performed or when the norm of the update is smaller than a user-defined tolerance. That is all there is to it.", "As you can see, both the mathematics and the philosophy behind the delta rule are simple. It is nonetheless a special case of the more general backpropagation algorithm used to train deeper neural networks, hence its importance in this series. Do not hesitate to re-derive all of the math as an exercise. In the meantime, let us now move to the fun stuff and implement ADALINE in Python. Assuming you are already familiar with Python, the following code should be quite self-explanatory.", "For the sake of clarity and usability, we will try throughout this series to stick to the scikit-learn API. An extended version of this code is also available on my TowardsDataScience Github repo (here).", "Figure 4 shows the number of misclassified points as the training of both Rosenblatt\u2019s perceptron and ADALINE proceeds while figure 5 depicts the evolution of the decision boundaries for both models. Rosenblatt\u2019s perceptron is all over the place, having an accuracy close to 99% at a given epoch and less than 50% at the next one. This lack of robustness comes from the fact that one mislabeled data point prevents Rosenblatt\u2019s perceptron from learning anything.", "Comparatively, the number of misclassified points for ADALINE decreases monotonically as the training proceeds. Looking at the evolution of ADALINE\u2019s decision boundary, it can be seen that it is much smoother than that of Rosenblatt\u2019s perceptron. This smoother evolution results from the definition of the cost function, enabling the training procedure to perform small adjustments to the weights w and bias b at each step based on some evaluation of how much wrong the current prediction is rather than simply on whether it is or not (as would be the case for Rosenblatt\u2019s perceptron). The information encoded by the loss function thus makes the training of ADALINE more robust and well-behaved. Note finally that, even though the two classes are not linearly separable (due to the one mislabeled data point), the final linear decision boundary of ADALINE has nonetheless been chosen in a principled way based on the formulation of a minimization problem, thus ensuring it is optimal in some sense. No such things can be said about the decision boundary of Rosenblatt\u2019s perceptron as it simply does not exist for the problem considered herein, the perceptron learning algorithm being unable to converge. Here lies the major difference between these two single-layer perceptrons\u2026", "Before concluding, let us try to get a better understanding of what ADALINE is actually learning. Although it is used for classification purposes, it is closely related to ordinary least squares (OLS), one of the simplest regression models. This close connection is clearly visible when looking at the loss function", "Using a simple change of variable,", "this loss function can be re-written as", "where each row of X is a given training example. This loss function is nothing but the quadratic loss function minimized for ordinary least squares. The closed-form of the optimal solution is", "This equivalence between the two models thus allows us to understand the weights w and bias b of ADALINE as the coefficients appearing in the equation of the hyperplane that best separates the two classes in the least-squares sense.", "Although ADALINE improves upon Rosenblatt\u2019s perceptron, we\u2019ll see in coming posts that solving the problem in the least-squares sense is not the most appropriate thing one could do for a classification problem\u2026", "This post is the second from my beginner series on deep learning. It should have been published way earlier but life decided otherwise (newborn, new house, work, etc). As for the first post on Rosenblatt\u2019s perceptron, I know that tagging a post on Adaptive Linear Neurons as being deep learning may be far-fetched but trust me, we\u2019ll get there. In the meantime, do not hesitate to play with the code (see the GitHub repo) and, if interested, to re-derive all of the math yourself. It ain\u2019t that complicated. As stated previously, ADALINE is much closer to modern machine learning methods than Rosenblatt\u2019s perceptron is. As such, gaining a good understanding of the mathematics it relies on will be extremely valuable later on when we\u2019ll move to modern neural networks!", "In the next few posts, the following subjects will be discussed :", "Finally, you will find below a list of additional online resources on ADALINE-related topics. Do not hesitate to check these out as they probably treat some aspects we might have glossed over!", "PS: If you know any other relevant link, do not hesitate to message me and I\u2019ll edit the post to add it :]", "Want to read more of this content ? Check out my other articles on low-rank structure and data-driven modeling or simply my Machine learning basics!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Assistant Professor in Fluid Mechanics and Applied Mathematics. Passionate about machine learning, physics and science outreach."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd0517d3c5939&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857----d0517d3c5939---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1811.10052", "anchor_text": "imaging and MRI"}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "StarCraft 2"}, {"url": "https://psycnet.apa.org/record/1959-09865-001", "anchor_text": "his perceptron"}, {"url": "https://en.wikipedia.org/wiki/Bernard_Widrow", "anchor_text": "Bernard Widrow"}, {"url": "https://en.wikipedia.org/wiki/Marcian_Hoff", "anchor_text": "Ted Hoff"}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a", "anchor_text": "Rosenblatt\u2019s perceptron, the very first neural networkA quick introduction to deep learning.towardsdatascience.com"}, {"url": "https://mitpress.mit.edu/books/perceptrons", "anchor_text": "a notorious book by Minsky & Papert"}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a", "anchor_text": "Rosenblatt\u2019s perceptron"}, {"url": "https://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "convex"}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Ordinary_least_squares", "anchor_text": "ordinary least squares"}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master/A%20quick%20introduction%20to%20deep%20learning%20for%20beginners/Adaptative_Linear_Neurons", "anchor_text": "GitHub repo"}, {"url": "https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#references", "anchor_text": "Single-Layer Neural Networks and Gradient Descent"}, {"url": "https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html", "anchor_text": "Machine Learning FAQ"}, {"url": "https://computerhistory.org/profile/ted-hoff/", "anchor_text": "https://computerhistory.org/profile/ted-hoff/"}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling"}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics"}, {"url": "https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559", "anchor_text": "Binary cross-entropy and logistic regressionEver wondered why we use it, where it comes from and how to optimize it efficiently? Here is one explanation (code\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d0517d3c5939---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d0517d3c5939---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/machine-learning-python?source=post_page-----d0517d3c5939---------------machine_learning_python-----------------", "anchor_text": "Machine Learning Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d0517d3c5939---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----d0517d3c5939---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----d0517d3c5939---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----d0517d3c5939---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd0517d3c5939&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d0517d3c5939---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d0517d3c5939--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d0517d3c5939--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d0517d3c5939--------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://loiseau-jc.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "280 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F961520fda2b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-upon-rosenblatts-perceptron-d0517d3c5939&newsletterV3=147ab927857&newsletterV3Id=961520fda2b6&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}