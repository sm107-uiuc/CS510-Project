{"url": "https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc", "time": 1683011992.34093, "path": "towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc/", "webpage": {"metadata": {"title": "Understanding Automatic Text Summarization-1: Extractive Methods | by Abhijit Roy | Towards Data Science", "h1": "Understanding Automatic Text Summarization-1: Extractive Methods", "description": "Text summarization is commonly used by several websites and applications to create news feed and article summaries. It has become very essential for us due to our busy schedules. We prefer short\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source", "paragraph_index": 17}, {"url": "https://pypi.org/project/pytextrank/", "anchor_text": "Pytextrank", "paragraph_index": 41}, {"url": "https://pypi.org/project/pytextrank/", "anchor_text": "Source", "paragraph_index": 42}, {"url": "https://github.com/abr-98/Automatic-Text-Summarizations", "anchor_text": "GitHub link.", "paragraph_index": 43}, {"url": "https://medium.com/@myac.abhijit/understanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe", "anchor_text": "Part 2", "paragraph_index": 44}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 46}], "all_paragraphs": ["Text summarization is commonly used by several websites and applications to create news feed and article summaries. It has become very essential for us due to our busy schedules. We prefer short summaries with all the important points over reading a whole report and summarizing it ourselves. So, several attempts had been made to automate the summarizing process. In this article, we will talk about some of them and see how they work.", "Summarization is a technique to shorten long texts such that the summary has all the important points of the actual document.", "There are mainly four types of summaries:", "There are mainly two types of summarization:", "Extraction-based Summarization: The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.", "Abstraction-based Summarization: The abstractive approach involves summarization based on deep learning. So, it uses new phrases and terms, different from the actual document, keeping the points the same, just like how we actually summarize. So, it is much harder than the extractive approach.", "It has been observed that extractive summaries sometimes work better than the abstractive ones probably because extractive ones don\u2019t require natural language generations and semantic representations.", "There are two types of evaluations:", "Human Evaluation: Scores are assigned by human experts based on how well the summary covers the points, answer the queries, and other factors like grammaticality and non-redundancy.", "ROUGE: ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is the method that determines the quality of the summary by comparing it to other summaries made by humans as a reference. To evaluate the model, there are a number of references created by humans and the generated candidate summary by machine. The intuition behind this is if a model creates a good summary, then it must have common overlapping portions with the human references. It was proposed by Chin-Yew Lin, University of California.", "ROUGE-n: It is measure on the comparison between the machine-generated output and the reference output based on n-grams. An n-gram is a contiguous sequence of n items from a given sample of text or speech, i.e, it is simply a sequence of words. Bigrams mean two words, Trigrams mean 3 words and so on. We normally use Bigrams.", "\u201cWhere p is \u201cthe number of common n-grams between candidate and reference summary\u201d, and q is \u201cthe number of n-grams extracted from the reference summary only\u201d. -Source", "ROUGE-L: It states that the longer the longest common subsequence in two texts, the similar they are. So, it is flexible than n-gram. It assigns scores based on how long can be a sequence, which is common to the machine-generated candidate and the human reference.", "ROUGE-SU: It brings a concept of skip bi-grams and unigrams. Basically it allows or considers a bigram if there are some other words between the two words, i.e, the bigrams don\u2019t need to be consecutive words.", "ROUGE-2 is most popular and given by:", "Where for every bigram \u2018i\u2019 we calculate the minimum of the number of times it occurred in the generated document X and the reference document S, for all reference documents give, divided by the total number of times each bigram appears in all of the reference documents. It is based on BLEU scores.", "Feature-Based Summarization: Developed by H. P Luhan at IBM in 1958. The paper proposed that the importance of a sentence is a function of the high-frequency words in the document. Elaborately speaking, the algorithm measures the frequency of words and phrases in the document and decides the importance of the sentence considering the words in the sentence and their frequencies. It states if the sentence has words with higher frequencies, It is important, but here we do not include the common words like \u201ca\u201d,\u201d the\u201d. Etc.", "Extractive Summarizations: \u201cExtractive summarization techniques produce summaries by choosing a subset of the sentences in the original text\u201d.-Source", "The Extractive Summarizers first create an intermediate representation that has the main task of highlighting or taking out the most important information of the text to be summarized based on the representations. There are two main types of representations:", "For sentence importance using the word probabilities, the importance of a sentence is given by the average importance of the words in the sentence.", "Now, once we get the intermediate representations, we move to assign some scores to each sentence to specify their importance. For topic representations, a score to a sentence depends on the topic words it contains, and for an indicator representation, the score depends on the features of the sentences. Finally, the sentences having top scores, are picked and used to generate a summary.", "The graph-based methods were first introduced by a paper by Rada Mihalcea and Paul Tarau, University of North Texas. The method is called the Text Rank algorithm and is influenced by Google\u2019s Page Rank Algorithm. This algorithm primarily tries to find the importance of a vertex in a given graph.", "We have learned in this algorithm, each sentence is represented as a vertex. An edge joining two vertices or two sentences denotes that the two sentences are similar. If the similarity of any two sentences is greater than a particular threshold, the nodes representing the sentences are joined by an edge.", "When two vertices are joined, it portrays that, one vertex is casting a vote to the other one. More the number of votes to a particular node( vertex or sentence), more important is that node and apparently the sentence represented. Now, the votes are also kind of weighted, each vote is not of the same weight or importance. The importance of the vote also depends on the importance of the node or sentence casting the vote, higher the importance of the node casting the vote higher is the importance of the vote. So, the number of votes cast to a sentence and the importance of those votes determine the importance of the sentence. This is the same idea behind the google page rank algorithm, and how it decides and ranks webpages, just that the nodes represent the webpages.", "If we have a paragraph, we will decompose it into a set of sentences. Now,say we represent each sentence as a vertex \u2018vi\u2019 so, we obtain a set of vertices V. As discussed, an edge joins a vertex with another vertex of the same set, so an edge E can be represented as a subset of (V x V). In the case of a directed graph say, In(V{i}) is the number of incoming edges to a node and the Out(v{j}) is the number of outgoing edges from a given node, and the importance score of a vertex is given by S{j}.", "According to the google page rank algorithm,", "Where S(V{i}) is the score of the subject node under consideration, and S(V(j)) represents all the nodes that have outgoing edges to V{i}. Now, the score of V{j} is divided by the out-degree of V{j}, which is the consideration of the probability that the user will choose that particular webpage.", "Elaborately, if this is the graph, standing at A, as a user, I can go to both B and C, so the chance of me going to C is \u00bd, i.e, 1/( outdegree of A). The factor d is called the damping factor. In the original page rank algorithm, factor d incorporates randomness. 1-d denotes that the user will move to a random webpage, not going to the connected ones. The factor is generally set to 0.85. The same algorithm is implemented in the text-rank algorithm.", "Now, the question arises, how we obtain the scores?", "Let\u2019s check for the page rank algorithm first, then transform it for text rank. As we can see above there are 4 vertices, first, we assign random scores to all the vertices, say, [0.8,0.9,0.9,0.9]. Then, probability scores are assigned to the edges.", "The matrix is the adjacent matrix of the graph. It can be observed that the values of the adjacent matrices are the probability values i.e, 1/outdegree of that node or vertex. So, actually the page rank graph becomes unweighted as the equation only contains the term that gives the weight.", "We can see, the old score matrix is multiplied using the adjacency matrix to get the new score matrix. We will continue this until the L2 norm of the new score matrix and the old score matrix becomes less than a given constant mostly 1 x10^-8. This is a convergence property based on linear algebra and the theory of eigenvalues and vectors. We will skip the maths to keep it simple. Once the convergence is achieved we obtain the final importance scores from the score matrix.", "For the text rank algorithm, the equation and the graph are modified to a weighted graph, because here, just dividing with the out-degree won\u2019t convey the full importance. As a result, the equation becomes:", "The implementation of text rank consists of two different natural language processes:", "Previously this was done using the frequency factor, which gave poor results comparatively. The text rank paper introduced a fully unsupervised algorithm. According to the algorithm, the natural language text is tokenized and parts of speech are tagged, and single words are added to the word graph as nodes. Now, if two words are similar, the corresponding nodes are connected using an edge. The similarity is measured using the co-occurrences of words. If two words occur in a window of N words, N varying from 2 to 10, the two words are considered similar. The words with the maximum number of important incident edges are selected as the most important keywords.", "It also works similar to keyword extraction, the only difference is in keyword extraction, the nodes represented keywords, here they represent entire sentences. Now, for the formation of the graph for sentence ranking, the algorithm creates a vertex for each sentence in the text and adds to the graph. The sentences are too large so, co-occurrence measures can not be applied. So, the paper uses a \u201csimilarity\u201d between two sentences using the content overlap between two sentences, in simpler words, the similarity depends on the number of common word tokens present in the two sentences. The authors propose a very interesting \u201crecommendation\u201d insight here. They denote the joining of the edge between two similar sentences or vertices as if it is recommending the reader to read another line, which is similar to the current line he/she is reading. The similarity I feel therefore denotes the similar content or interest among the two sentences. To prevent long sentences from getting recommended the importances are multiplied with a normalizing factor.", "The similarity between two sentences is given by:", "Where given two sentences Si and Sj, with a sentence being represented by the set of Ni words that appear in the sentence:", "The most important sentences are obtained in the same way we did for Keyword extraction.", "This is an overall view of how text rank operates please go through the original paper to explore more.", "In practice, for summary extraction, we use cosine similarity, to decide the similarity between two sentences. Using this method, we may obtain several connected subgraphs that denote the number of important topics in the whole document. The connected component of the subgraphs gives the sentences important for the corresponding topics.", "The \u201cPytextrank\u201d library allows applying the text rank algorithm directly on python.", "Implementation of Pytextrank library by Source.", "For application details, please refer to the GitHub link.", "In this article, we have seen basic extractive summarization approaches and the details of the Textrank algorithm. For abstractive methods, feel free to go through Part 2 of the article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8eb512b21ecc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----8eb512b21ecc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Brett Jordan"}, {"url": "https://unsplash.com/s/photos/page?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "Source"}, {"url": "https://pypi.org/project/pytextrank/", "anchor_text": "Pytextrank"}, {"url": "https://pypi.org/project/pytextrank/", "anchor_text": "Source"}, {"url": "https://github.com/abr-98/Automatic-Text-Summarizations", "anchor_text": "GitHub link."}, {"url": "https://medium.com/@myac.abhijit/understanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe", "anchor_text": "Part 2"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"}, {"url": "https://arxiv.org/abs/1707.02268v3", "anchor_text": "https://arxiv.org/abs/1707.02268v3"}, {"url": "https://www.microsoft.com/en-us/research/publication/rouge-a-package-for-automatic-evaluation-of-summaries/", "anchor_text": "https://www.microsoft.com/en-us/research/publication/rouge-a-package-for-automatic-evaluation-of-summaries/"}, {"url": "http://www.csie.ntnu.edu.tw/~g96470318/A_trainable_document_summarizer_.pdf", "anchor_text": "http://www.csie.ntnu.edu.tw/~g96470318/A_trainable_document_summarizer_.pdf"}, {"url": "https://www.semanticscholar.org/paper/The-Automatic-Creation-of-Literature-Abstracts-Luhn/a6dcc17c6f3dbc2d203ade9ff671a895a9dead7c", "anchor_text": "https://www.semanticscholar.org/paper/The-Automatic-Creation-of-Literature-Abstracts-Luhn/a6dcc17c6f3dbc2d203ade9ff671a895a9dead7c"}, {"url": "https://medium.com/tag/summarization?source=post_page-----8eb512b21ecc---------------summarization-----------------", "anchor_text": "Summarization"}, {"url": "https://medium.com/tag/pagerank?source=post_page-----8eb512b21ecc---------------pagerank-----------------", "anchor_text": "Pagerank"}, {"url": "https://medium.com/tag/textrank?source=post_page-----8eb512b21ecc---------------textrank-----------------", "anchor_text": "Textrank"}, {"url": "https://medium.com/tag/extractive-summarization?source=post_page-----8eb512b21ecc---------------extractive_summarization-----------------", "anchor_text": "Extractive Summarization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----8eb512b21ecc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----8eb512b21ecc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8eb512b21ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8eb512b21ecc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8eb512b21ecc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "458 Followers"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}