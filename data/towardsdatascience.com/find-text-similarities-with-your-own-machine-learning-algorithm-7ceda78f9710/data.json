{"url": "https://towardsdatascience.com/find-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710", "time": 1683009663.0327759, "path": "towardsdatascience.com/find-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710/", "webpage": {"metadata": {"title": "Find Text Similarities with your own Machine Learning Algorithm | by G\u00fcnter R\u00f6hrich | Towards Data Science", "h1": "Find Text Similarities with your own Machine Learning Algorithm", "description": "First of all, there are tons of material focusing on processing text data and using various kind of classification algorithms to provide similarities and/or clusters. We will use an approach that is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@adriensieg/text-similarities-da019229c894", "anchor_text": "medium.com", "paragraph_index": 2}, {"url": "http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection", "anchor_text": "Reuters", "paragraph_index": 6}, {"url": "https://github.com/guenter-r/medium/blob/master/tf_idf.py", "anchor_text": "Github", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://www.nltk.org/api/nltk.tokenize.html", "anchor_text": "tokenizer (NLTK)", "paragraph_index": 24}, {"url": "https://www.wordclouds.com/", "anchor_text": "wordclouds.com", "paragraph_index": 50}, {"url": "http://linkedin.com/in/groehrich", "anchor_text": "linkedin.com/in/groehrich", "paragraph_index": 53}], "all_paragraphs": ["First of all, there are tons of material focusing on processing text data and using various kind of classification algorithms to provide similarities and/or clusters. We will use an approach that is exactly calculating similar documents to the one we select arbitrarily. The similarity evaluation based on the \u201cmost important words\u201d the documents and their related corpus contain.", "Why? Have you ever used a simple classification or regression implementation to classify data \u2014 pretty sure you have. Today we look at very well documented approaches with high accuracy and solid performance. Though, what I found essential for my personal learning curve is, implementing similar algorithms and concepts to understand the basic idea. Having this knowledge in mind, you will hardly struggle to understand further reaching concepts. One step after another.", "If this is one of your first adventures in the \u201cworking with text universe\u201d, I assume these words here will be quite a good start to see what could be done with the power of just a few lines of code. I personally recommend my favorite publication \u201cText Similarities\u00a0: Estimate the degree of similarity between two texts\u201d regarding text similarities which can be found right here on medium.com. If you wanted to dig deeper into the text processing procedures.", "Now let us focus on a quite straightforward implementation to characterize a document by just a few words and check if this could be an appropriate measure to compare documents / texts among each other.", "For our experiment we will tackle a variety of areas with regard to computer and data science , in other words, expect great fun! In the course of the outlined steps we will use several libraries as shortcuts, e.g. we will use predefined English stop words in order to \u201cfilter\u201d words we cannot use for further analysis, we will use libraries to process HTML and we will standard math libraries.", "In order to determine similarities among text elements we will go through the following steps:", "We will test our approach through analyzing two different datasets: (1) IMDB\u2019s review collection as well as (2) Reuters-21578 dataset [1] which can both be found easily on the web and downloaded onto your machine. For the sake of correctness, I am not at all affiliated with IMDB or Reuters.", "Intuitively, we could assume that important words are some sort of \u201cspecial\u201d to a document, so if we counted all words in a document and neglect words that won\u2019t contribute to the documents value like (is, has, will, etc. \u2014 so called stop-words ) we may find important words in a document.", "What I initially tried was to count words and consider the most frequent words to be most important (this approach is at most okay). Why this works to some extend is quite simple, we cleaned the text and removed (very frequently occurring) stop words, so naturally we will end up with frequent words that may be important. This idea is flawed for one good reason: Words that are absolutely not important, however not filtered through previous procedures (stopwords cleaning) may remain in the text and then be considered precious for our analysis.", "We will proceed and cover essential steps with code snippets, the whole code can be found on Github.", "0. The basic idea, \u201cInverse Text Frequency\u201d", "As an example, movie may occur often in case we talk about movie reviews (fun fact: we are about to analyze movie reviews) \u2014 however, the occurrence of \u2018movie\u2019 is not very likely to provide us with any solid information whether two text snippets may be similar. In other words, if we rely on \u2018movies\u2019, we will end up with far too many similarities.", "Term frequency\u2013inverse document frequency, short tf-idf is a common method to evaluate how important a single word is to a corpus. In general, this can be outlined as three calculations [2,3]. Please note that there are quite a few variations in the tf-idf space (especially with regard to counting normalization) \u2014 I played around with a few variations and chose the method that provided good overall results.", "How often can we find term t in document D? As long documents may lead to distortions of this question (as terms may occur more often), we consider the maximum frequency of a term t\u2019 in the document in order to normalize #t in D.", "2. In how many documents of the overall corpus do we find the term t? (Terms that do occur in all considered documents are less important if the occur many times in those considered documents)", "3. In the last step, the weight of the term t in the given document D is calculated as the product of the two above calculations:", "Why do we need the logarithmic function for this? Well, this is a natural way that allows us to limit the importance of a large number of occurrences. The larger the number of occurrences gets, the less steep is the change of importance if calculated as logarithmic.", "We will use the ft-id case as described before. For this approach, we will consider all available documents instead of just counting words within as single document (which was our first thought on, how to address this topic) \u2014 which will of course need more CPU memory for computation, but at the prize of hopefully being more accurate.", "We\u2019ve made it through the math, most of it \u2014great! I think this is a great time to take a short break an recap what we have seen so far. At least this is what I did right here. Now let\u2019s talk a bit more code \ud83d\udc0d.", "Our start is simple, we will import our IMDB and Reuters data:", "In this case we cannot rely on solid pandas functionality, however bs4 allows us to extract data without any effort, just n\ud83c\udf66. One thing may immediately pop into your eyes, the \u201csoup-functions\u201c do often look like typical regex functions. This hint may help you browse through the documentation, if needed.", "What is essential to our definitions before is that we are able to clean data properly. This will further allow us to compress sentences, paragraphs and eventually docs to a set of unique words. (We do not cover stemming in particular, however this could at some point improve our approach \u2014 have a look here if interested [4] ).", "2. & 3. Clean data and pack words into their bag", "As derived from the tf, idf formulas, we are required to gather all (cleaned) words, identify how often the words occur in a document and the maximum number of how often words occur in all documents. In order to remove stop words I recommend nltk\u2019s stop_words which are available in many languages.", "I have not used the tokenizer (NLTK) as the results are fine with our basic implementation, but with just a couple lines of code, you\u2019d be good to go. If we chose all words we found (and filtered) wrote them down and scaled the latter size by the word\u2019s number of occurrences, we could generate a word cloud like this:", "As we used the above function to clean and filter data as well as setting up our counts we will proceed with the implementation of the core functionality of our experiment, the tf-idf algorithm:", "Taking advantage of our tf-idf algorithm we could now decide to only consider a limited number of words, only n words with the highest score calculated per document [3]. This n is arbitrary, the larger n is, the sparser our similarity matrix will be \u2014 keep that in mind.", "In order to calculate similarities, we need a reference vector. While iterating over all documents, cleaning them and counting words, we will", "What we will do next is calculate bag similarities where a bag is represented by its n top words. All top words are gathered together in a single words vector. We will create vectors for every bag (iterative process) and in a proceeding step, we will stack all vectors together to a single matrix.", "We have already done quite some work in order to process and organize data in a manner that allows us to get remarkably closer to our final steps of our analysis. After stacking our vectors, we find a sparse matrix that is only dense around its diagonal (which makes perfectly sense, but more under 7. Calculate Similarities). Let\u2019s do some sanity checks first.", "Lets check our matrices for Reuters and IMDB (plot is limited to 25000 reviews):", "Calculating similarity is a fun task, linear algebra provides a very straightforward means of calculating similarity: What we are interested in is the cosine angle between two vectors A and B, the closer these two point towards the same direction, the closer their values is to one:", "In our last step we will multiply our matrix values with all other values in the matrix (similarity is 1 if we multiply a vector with itself), we call this similarity matrix m_d_m. As we can see there is a clear pattern for which we can find obvious similarities \u2014 if we needed to fit them together, this would be a perfect clustering task.", "In the following illustration we will check Reuters data, for a simple reason: Reuters data is daily news. This means that we of course do expect certain patterns of similarity, however news cover many different areas, which naturally come with a wider set of vocabulary. If we used the same plot for movie reviews, we would basically get a huge blue square (due to density).", "In our task we aimed at finding a similar document to the one we selected. We use a simple iteration to find all related documents that are at least x% similar, where x is defined arbitrary again. I have come to the conclusion that similarities around .5 work quite good.", "IMDB \u2014 In contrast to the Reuters dataset we may expect quite dense results, if we only select n important words of a text. This can be explained because of the fact, that reviews, good or bad, are likely to contain many similar words \u2014 this results in massive dense matrix! This brings us to a very sensitive point: We could choose more words ( a larger n ), however calculations would then become considerably more memory consuming. Let\u2019s try a couple comparisons given different parameters:", "It seems there are users who have invented their own shortcuts on writing reviews \ud83d\ude0f. Maybe it is just me, but I really find this interesting:", "I earlier mentioned that \u201cstemming\u201d is an interesting option to find similar words, when it comes to our example here we can clearly see why:", "I still enjoy it | I still enjoyed it", "These two are not similar to our code, enjoy and enjoyed are two different terms, hence their importance might be entirely different. As we could see in the comparison among those three snippets, it is likely that \u201cenjoy[ed]\u201d is not in our n top words.", "If figured our next test scenario is quite impressive and shows that our algorithm must work at least at some point. As our IMDB dataset does not provide any reference to the movie itself (unless explicitly mentioned), I was hoping to find reviews that belong to each other ( a potential reference for successive experiments \u2014 clustering ). Also note, that even if the film is mentioned, it is only mentioned a couple times, hence it might not be regarded as an important word and not be considered in our n top words at all.", "In the following example, our algorithm matched together two reviews, whereas both mention the original Japanese Title of the movie, one mentions the English movie title.", "Well, let\u2019s check our findings in our sparse matrix that contains the Reuters news data.", "Reuters \u2014 Browsing through the results we could see in m_d_m that data is clustered around a variety of entries. When examining this in more detail we find, that this is due to the fact, that the data contains many brief stock price updates (that look very similar). For these updates, we will find many similar entries in our matrix. Here is an example, I assume that Shr, cts, Net, Revs might be regarded as \u2018important\u2019 to our algorithm:", "If we go for an even stricter approach, like a similarity of 99% we would hardly expect any result (as a document match with itself, which is always 1, is excluded), however we do find one. As it turns out, this are duplicates and very similar text snippets in our data. Fair enough.", "Reuters data seems to be more challenging in general. This is on the one hand due to a variety of financial terms and abbreviations (that will take a portion of our \u201cimportant\u201d words) on the other hand, similar looking text does not necessarily talk about similar things. In order to only get text that might be linked in terms of its content, I will neglect data that has a greater similarity value than .55 (as this will be the financials which look pretty much the same).", "Although these to messages share most of their words, their similarity score is much less than I would have expected \u2014 especially with regard to the scores we could achieve in the IMDB dataset.", "As we have already calculated a singularity matrix, it may be handy to identify the top documents that have the most other documents in common (hence, these should be in the same cluster if we had checked this too).", "This simple algorithm allows us to find similar text (e.g. as we could see, that a duplicate could be found easily), however the more complex the document becomes and the more documents differ among each other, the more expensive the calculation gets. As we need to loop through the entire dataset for both, the specification of important words and the counting procedures the algorithm\u2019s resource requirements expand extensively on every additional record.", "If you found this post helpful, I would appreciate a \u201cfollow\u201d \ud83e\udec0, until then:", "[ WordCloud image ] A \u201cStar Wars review word cloud\u201d, created at wordclouds.com", "#nlp #corpus #numpy #bs4 #Beautifulsoup #examples #github #tf-idf #dataScience #tokenize #sklearn #python #R #rlang #stemming #wordcloud", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ Mondi Group \u2022 Georgia Tech \ud83c\udf93 \u2022 linkedin.com/in/groehrich"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ceda78f9710&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://guenterroehrich.medium.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "G\u00fcnter R\u00f6hrich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3718a9423801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=post_page-3718a9423801----7ceda78f9710---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@adriensieg/text-similarities-da019229c894", "anchor_text": "medium.com"}, {"url": "https://unsplash.com/@denisseleon?utm_source=medium&utm_medium=referral", "anchor_text": "source"}, {"url": "http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection", "anchor_text": "Reuters"}, {"url": "https://github.com/guenter-r/medium/blob/master/tf_idf.py", "anchor_text": "Github"}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "here"}, {"url": "https://www.nltk.org/api/nltk.tokenize.html", "anchor_text": "tokenizer (NLTK)"}, {"url": "https://github.com/guenter-r/medium/blob/master/tf_idf.py", "anchor_text": "https://github.com/guenter-r/medium/blob/master/tf_idf.py"}, {"url": "https://stackoverflow.com/questions/25134160/whats-the-meaning-of-the-categories-in-the-corpus-reuters-of-nltk/25149714#25149714", "anchor_text": "https://stackoverflow.com/questions/25134160/whats-the-meaning-of-the-categories-in-the-corpus-reuters-of-nltk/25149714#25149714"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"}, {"url": "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183", "anchor_text": "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183"}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "https://en.wikipedia.org/wiki/Stemming"}, {"url": "https://www.researchgate.net/publication/320914786/figure/fig2/AS:558221849841664@1510101868614/The-difference-between-Euclidean-distance-and-cosine-similarity.png", "anchor_text": "osine Similarity \u2014 https://www.researchgate.net/publication/320914786/figure/fig2/AS:558221849841664@1510101868614/The-difference-between-Euclidean-distance-and-cosine-similarity.png"}, {"url": "https://www.wordclouds.com/", "anchor_text": "wordclouds.com"}, {"url": "https://medium.com/tag/text?source=post_page-----7ceda78f9710---------------text-----------------", "anchor_text": "Text"}, {"url": "https://medium.com/tag/similarity?source=post_page-----7ceda78f9710---------------similarity-----------------", "anchor_text": "Similarity"}, {"url": "https://medium.com/tag/text-similarity?source=post_page-----7ceda78f9710---------------text_similarity-----------------", "anchor_text": "Text Similarity"}, {"url": "https://medium.com/tag/tf-idf?source=post_page-----7ceda78f9710---------------tf_idf-----------------", "anchor_text": "Tf Idf"}, {"url": "https://medium.com/tag/python?source=post_page-----7ceda78f9710---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=-----7ceda78f9710---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=-----7ceda78f9710---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7ceda78f9710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7ceda78f9710---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7ceda78f9710--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7ceda78f9710--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7ceda78f9710--------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "G\u00fcnter R\u00f6hrich"}, {"url": "https://guenterroehrich.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "158 Followers"}, {"url": "http://linkedin.com/in/groehrich", "anchor_text": "linkedin.com/in/groehrich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3718a9423801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=post_page-3718a9423801--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F276478487b6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffind-text-similarities-with-your-own-machine-learning-algorithm-7ceda78f9710&newsletterV3=3718a9423801&newsletterV3Id=276478487b6b&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}