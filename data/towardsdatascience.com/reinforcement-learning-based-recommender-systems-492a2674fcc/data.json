{"url": "https://towardsdatascience.com/reinforcement-learning-based-recommender-systems-492a2674fcc", "time": 1683016559.4442189, "path": "towardsdatascience.com/reinforcement-learning-based-recommender-systems-492a2674fcc/", "webpage": {"metadata": {"title": "Reinforcement Learning based Recommender Systems | by Debmalya Biswas | Towards Data Science", "h1": "Reinforcement Learning based Recommender Systems", "description": "Design personalized and interactive recommendation apps using a combination of Reinforcement Learning and Natural Language Processing (NLP) /Chatbots"}, "outgoing_paragraph_urls": [{"url": "http://ceur-ws.org/Vol-2820/AAI4H-10.pdf", "anchor_text": "paper pdf", "paragraph_index": 1}, {"url": "https://www.slideshare.net/DebmalyaBiswas/delayed-rewards-in-the-context-of-reinforcement-learning-based-recommender-systems", "anchor_text": "ppt", "paragraph_index": 1}], "all_paragraphs": ["Abstract. We present a Reinforcement Learning (RL) based approach to implement Recommender Systems. The results are based on a real-life Wellness app that is able to provide personalized health / activity related content to users in an interactive fashion. Unfortunately, current recommender systems are unable to adapt to continuously evolving features, e.g. user sentiment, and scenarios where the RL reward needs to computed based on multiple and unreliable feedback channels (e.g., sensors, wearables). To overcome this, we propose three constructs: (i) weighted feedback channels, (ii) delayed rewards, and (iii) reward boosting, which we believe are essential for RL to be used in Recommender Systems.", "This paper appears in the proceedings of AAI4H \u2014 Advances in Artificial Intelligence for Healthcare Workshop, co-located with the 24th European Conference on Artificial Intelligence (ECAI 2020), Sep 2020 (paper pdf) (ppt)", "Health / Wellness apps have historically suffered from low adoption rates. Personalized recommendations have the potential of improving adoption, by making increasingly relevant and timely recommendations to users. While recommendation engines (and consequently, the apps based on them) have grown in maturity, they still suffer from the \u2018cold start\u2019 problem and the fact that it is basically a push-based mechanism lacking the level of interactivity needed to make such apps appealing to millennials.", "We present a Wellness app case-study where we applied a combination of Reinforcement Learning (RL) and Natural Language Processing (NLP) / Chatbots to provide a highly personalized and interactive experience to users. We focus on the interactive aspect of the app, where the app is able to profile and converse with users in real-time, providing relevant content adapted to the current sentiment and past preferences of the user.", "The core of such chatbots is an intent recognition Natural Language Understanding (NLU) engine, which is trained with hard-coded examples of question variations. When no intent is matched with a confidence level above 30%, the chatbot returns a fallback answer. The user sentiment is computed based on both the (explicit) user response and (implicit) environmental aspects, e.g. location (home, office, market, \u2026), temperature, lighting, time of the day, weather, other family members present in the vicinity, and so on; to further adapt the chatbot response.", "RL refers to a branch of Artificial Intelligence (AI), which is able to achieve complex goals by maximizing a reward function in real-time. The reward function works similar to incentivizing a child with candy and spankings, such that the algorithm is penalized when it takes a wrong decision and rewarded when it takes a right one \u2014 this is reinforcement. The reinforcement aspect also allows it to adapt faster to real-time changes in the user sentiment. For a detailed introduction to RL frameworks, the interested reader is referred to [1].", "Previous works have explored RL in the context of Recommender Systems [2, 3, 4, 5], and enterprise adoption also seems to be gaining momentum with the recent availability of Cloud APIs (e.g. Azure Personalizer [6, 7]) and Google\u2019s RecSim [8]. However, they still work like a typical Recommender System. Given a user profile and categorized recommendations, the system makes a recommendation based on popularity, interests, demographics, frequency and other features. The main novelty of these systems is that they are able to identify the features (or combination of features) of recommendations getting higher rewards for a specific user; which can then be customized for that user to provide better recommendations [9].", "Unfortunately, this is still inefficient for real-life systems which need to adapt to continuously evolving features, e.g. user sentiment, and where the reward needs to computed based on multiple and unreliable feedback channels (e.g., sensors, wearables).", "The rest of the paper is organized as follows: Section 2 outlines the problem scenario and formulates it as an RL problem. In Section 3, we propose", "three RL constructs needed to overcome the above limitations: (i) weighted feedback channels, (ii) delayed rewards, and (iii) reward boosting, which we believe are essential constructs for RL to be used in Recommender Systems.", "\u2018Delayed Rewards\u2019 in this context is different from the notion of Delayed RL [10], where rewards in the distant future are not considered as valuable as immediate rewards. This is very different from our notion of \u2018Delayed Rewards\u2019 where a received reward is only applied after its consistency has been validated by a subsequent action. Section 4 concludes the paper and provides directions for future research.", "The app supports both push based notifications, where personalized health, fitness, activity, etc. related recommendations are pushed to the user; as well as interactive chats where the app reacts in response to a user query. We assume the existence of a knowledgebase KB of articles, pictures and videos, with the artifacts ranked according to their relevance to different user profiles / sentiments.", "The Wellness app architecture is described in Fig. 1, which shows how the user and environmental conditions (comprising the user feedback) are:", "1. gathered using available sensors to compute the \u2018current\u2019 feedback, including environmental context (e.g. webcam pic of the user can be used to infer the user sentiment to a chatbot response / notification, the room lighting conditions and other user present in the vicinity),", "2. which is then combined with the user conversation history to quantify the user sentiment curve and discount any sudden changes in sentiment due to unrelated factors;", "3. leading to the aggregate reward value corresponding to the last chatbot response / app notification provided to the user.", "This reward value is then provided as feedback to the RL agent, to choose the next optimal chatbot response / app notification from the knowledgebase.", "We formulate the RL Engine for the above scenario as follows:", "Action (a): An action a in this case corresponds to a KB article which is delivered to the user either as a push notification, or in response to a user query, or as part of an ongoing conversation.", "Agent (A): is the one performing actions. In this case, the Agent is the App delivering actions to the users, where an action is selected based on its Policy (described below).", "Environment: refers to the world with which the agent interacts, and which responds to the agent\u2019s actions. In our case, the Environment corresponds to the User U interacting with the App. U responds to A\u2019s actions, by providing different types of feedback, both explicit (in the form of a chat response) and implicit (e.g., change in facial expression).", "Policy(\ud835\udf0b): is the strategy that the agent employs to select the next based action. Given a user profile Up, (current) sentiment Us, and query Uq; the Policy function computes the product of the article scores returned by the NLP and Recommendation Engines respectively, selecting the article with the highest score as the next best action: (a) The NLP Engine (NE) parses the query and outputs a score for each KB article, based on the \u201ctext similarity\u201d of the article to the user query. (b) Similarly, the Recommendation Engine (RE) provides a score for each article based on the reward associated with each article, with respect to the user profile and sentiment. The Policy function can be formalized as follows:", "Reward (r): refers the feedback by which we measure the success or failure of an agent\u2019s recommended action. The feedback can e.g. refer to the amount of time that a user spends reading a recommended article. We consider a 2-step reward function computation where the feedback fa received with respect to a recommended action is first mapped to a sentiment score, which is then mapped to a reward.", "where r and s refer to the reward and sentiment functions, respectively. The RL formulation described above is illustrated in Fig. 2.", "As described in Fig. 1, we consider a multi-feedback channel, with feedback captured from user (edge) devices / sensors, e.g. webcam, thermostat, smartwatch, or a camera, microphone, accelerometer embedded within the mobile device hosting the app. For instance, a webcam frame capturing the facial expression of the user, heart rate provided by the user smartwatch, can be considered together with the user provided text response \u201cThanks for the great suggestion\u201d; in computing the user sentiment to a recommended action.", "Let {fa1, fa2, \u2026 fan} denote the feedback received for action a. Recall that s(f) denotes the user sentiment computed independently based on the respective sensory feedback f. The user sentiment computation can be considered as a classifier outputting a value between 1\u201310. The reward can then be computed as a weighted average of the sentiment scores, denoted below:", "where the weights {wa1, wa2, \u2026 wan} allow the system to harmonize the received feedback, as some feedback channels may suffer from low reliability issues. For instance, if fi corresponds to a user typed response, fj corresponds to a webcam snapshot; then higher weightage is given to fi. The reasoning here is that the user might be \u2018smiling\u2019 in the snapshot, however the \u2018smile\u2019 is due to his kid entering the room (also captured in the frame), and not necessarily in response to the received recommendation / action. At the same time, if the sentiment computed based on the user text response indicates that he is \u2018stressed\u2019, then we give higher weightage to user explicit (text response) feedback in this case.", "A \u2018delayed rewards\u2019 strategy is applied in the case of reward inconsistency, where the (current) computed reward is \u2018negative\u2019 for an action to which the user has been known to react positively (to actions of the same type historically); and vice versa. Given such inconsistency, the delayed rewards strategy buffers the computed reward rat for action at at time t; and provides an indication to the RL Agent-Policy (\ud835\udf0b) to try another recommendation of the same type \u2014 to validate the user sentiment \u2014 before updating the rewards for actions at both time t and t+1.", "To accommodate the \u2018delayed rewards\u2019 strategy, the rewards function is extended with a memory buffer that allows the rewards of last m actions from time (t+m) to t to be aggregated and applied retroactively at time (t+m). The delayed rewards function dr is denoted as follows:", "where |\ud835\udc61+\ud835\udc5a implies that the reward for the actions actions from time (t+m) to t, although computed individually; can only be applied at time (t+m). As before, the respective weights wi allow us to harmonize the effect of an inconsistent feedback, where the reward for an action \ud835\udc4et time \ud835\udc61\ud835\udc56 is applied based on the reward computed for a later action at time (\ud835\udc61+1)\ud835\udc56.", "To effectively enforce the \u2018delayed rewards\u2019 strategy, the Policy \ud835\udf0b is also extended to recommend an action of the same type, as the previous recommended action; if the delay flag d is set (d = 1): The \u201cdelayed\u201d Policy \ud835\udf0b\ud835\udc51\ud835\udc61 is denoted below:", "The RL formulation extended with delayed reward / policy is illustrated in Fig. 3.", "Rewards boosting, or rather rewards normalization, applies mainly to continuous chat interactions. In such cases, if the user sentiment for a recommended action is \u2018negative\u2019; it might not be the fault of the last action only. It is possible that the conversation sentiment was already degrading, and the last recommended action is simply following the downward trend. On the other hand, given a worsening conversation sentiment, a \u2018positive\u2019 sentiment for a recommended action implies that it had a very positive impact on the user; and hence its corresponding reward should be boosted.", "The boosted reward \ud835\udc5f\ud835\udc4f\ud835\udc4e\ud835\udc61 for an action at at time t is computed as follows:", "We leave it as future work to extend the \u2018boost\u2019 function to last n actions (instead of just the last action above). In this extended scenario, the system maintains a sentiment curve of the last n actions, and the deviation is computed with respect to a curve, instead of a discrete value. The expected benefit here is that it should allow the system to react better to user sentiment trends.", "In this work, we considered the implementation of a RL based Recommender system, in the context of a real-life Wellness App. RL is a powerful primitive for such problems as it allows the app to learn and adapt to user preferences / sentiment in real-time. However, during the case-study, we realized that current RL frameworks lack certain constructs needed for them to be applied to such Recommender Systems. To overcome this limitation, we introduced three RL constructs that we had to implement for our Wellness app. The proposed RL constructs are fundamental in nature as they impact the interplay between Reward and Policy functions; and we hope that their addition to existing RL frameworks will lead to increased enterprise adoption.", "[5] Ricciardelli, E., Biswas, D.: Self-improving Chatbots based on Reinforcement Learning. In: 4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (2019).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F492a2674fcc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@debmalyabiswas", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Debmalya Biswas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad84805121fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&user=Debmalya+Biswas&userId=ad84805121fe&source=post_page-ad84805121fe----492a2674fcc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "http://ceur-ws.org/Vol-2820/AAI4H-10.pdf", "anchor_text": "paper pdf"}, {"url": "https://www.slideshare.net/DebmalyaBiswas/delayed-rewards-in-the-context-of-reinforcement-learning-based-recommender-systems", "anchor_text": "ppt"}, {"url": "https://www.pexels.com/@olly?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Andrea Piacquadio"}, {"url": "https://www.pexels.com/photo/woman-in-red-long-sleeve-writing-on-chalk-board-3769714/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "https://azure.microsoft.com/en-us/services/cognitive-services/personalizer/", "anchor_text": "https://azure.microsoft.com/en-us/services/cognitive-services/personalizer/"}, {"url": "https://ai.googleblog.com/2019/11/recsim-configurable-simulation-platform.html", "anchor_text": "https://ai.googleblog.com/2019/11/recsim-configurable-simulation-platform.html"}, {"url": "https://analyticsindiamag.com/reinforcement-learning-recommender-system-recsim-google-ai/", "anchor_text": "https://analyticsindiamag.com/reinforcement-learning-recommender-system-recsim-google-ai/"}, {"url": "http://heim.ifi.uio.no/~mes/inf1400/COOL/REF/Standford/ch11.pdf", "anchor_text": "http://heim.ifi.uio.no/~mes/inf1400/COOL/REF/Standford/ch11.pdf"}, {"url": "https://medium.com/tag/recommendation-system?source=post_page-----492a2674fcc---------------recommendation_system-----------------", "anchor_text": "Recommendation System"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----492a2674fcc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----492a2674fcc---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/data-science?source=post_page-----492a2674fcc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----492a2674fcc---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&user=Debmalya+Biswas&userId=ad84805121fe&source=-----492a2674fcc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&user=Debmalya+Biswas&userId=ad84805121fe&source=-----492a2674fcc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F492a2674fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----492a2674fcc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----492a2674fcc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----492a2674fcc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----492a2674fcc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----492a2674fcc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----492a2674fcc--------------------------------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Debmalya Biswas"}, {"url": "https://debmalyabiswas.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "237 Followers"}, {"url": "https://www.linkedin.com/in/debmalya-biswas-3975261/", "anchor_text": "https://www.linkedin.com/in/debmalya-biswas-3975261/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad84805121fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&user=Debmalya+Biswas&userId=ad84805121fe&source=post_page-ad84805121fe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1a52f34e1d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-based-recommender-systems-492a2674fcc&newsletterV3=ad84805121fe&newsletterV3Id=1a52f34e1d70&user=Debmalya+Biswas&userId=ad84805121fe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}