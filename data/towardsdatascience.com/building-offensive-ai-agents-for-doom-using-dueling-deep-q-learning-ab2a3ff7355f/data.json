{"url": "https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f", "time": 1683013784.241139, "path": "towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f/", "webpage": {"metadata": {"title": "Building Offensive AI Agents for Doom using Dueling Deep Q-learning. | by Adrian Yijie Xu | Towards Data Science", "h1": "Building Offensive AI Agents for Doom using Dueling Deep Q-learning.", "description": "Over the last few articles, we\u2019ve discussed and implemented Deep Q-learning (DQN)and Double Deep Q Learning (DDQN) in the VizDoom game environment and evaluated their performance. Deep Q-learning is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "discussed", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "implemented", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "Deep Q-learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "Double Deep Q Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "VizDoom game environment", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "implementations", "paragraph_index": 2}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 7}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "implementation", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "implementation", "paragraph_index": 22}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent", "paragraph_index": 27}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 27}, {"url": "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/", "anchor_text": "\u201cImprovements in Deep Q Learning*", "paragraph_index": 28}], "all_paragraphs": ["Over the last few articles, we\u2019ve discussed and implemented Deep Q-learning (DQN)and Double Deep Q Learning (DDQN) in the VizDoom game environment and evaluated their performance. Deep Q-learning is a highly flexible and responsive online learning approach that utilizes rapid intra-episodic updates to it\u2019s estimations of state-action (Q) values in an environment in order to maximize reward. Double Deep Q-Learning builds upon this by decoupling the networks responsible for action selection and TD-target calculation in order to minimize Q-value overestimation, a problem particularly evident when earlier on in the training process, when the agent has yet to fully explore the majority of possible states.", "Inherently, using a single state-action value in judging a situation demands exploring and learning the effects of an action for every single state, resulting in an inherent hindrance to the model\u2019s generalization capabilities. Moreover, not all states are equally relevant within the context of the environment.", "Recall our Pong environment from our earlier implementations. Immediately after our agent hits the ball, the value of moving left or right is negligible, as the ball must first travel to the opponent and be returned towards the player. Calculating state-action values at this point to use for training may disrupt the convergence of our agent as a result. Ideally, we would like to be able to identify the value of each action without learning its effects specific to each state, in order to encourage our agent to focus on selecting actions relevant to the environment.", "Dueling Deep Q-Learning (henceforth DuelDQN) addresses these shortcomings by splitting the DQN network output into two streams: a value stream and an advantage (or action) stream. In doing so, we partially decouple the overall state-action evaluation process. In their seminal paper, Van Hasselt et. al presented a visualization of how DuelDQN affected agent performance in the Atari game Enduro, demonstrating how the agent could learn to focus on separate objectives. Notice how the value stream has learned to focus on the direction of the road, while the advantage stream has learned to focus on the immediate obstacles in front of the agent. In essence, we have gained a level of short-term and medium-term foresight through this approach.", "To calculate the Q-value of a state-action, we then utilize the advantage function is to tell us the relative importance of an action. The subtraction of the average advantage, calculated across all possible actions in a state, is used to find the relative advantage of our interested action.", "Intuitively, we have partially decoupled the action and state-value estimation processes in order to gain a more reliable appraisal of the environment.", "We\u2019ll be implementing our approach in the same VizDoomgym scenario as in our last article, Defend The Line, with the same multi-objective conditions. Some characteristics of the environment include:", "Our Google Colaboratory implementation is written in Python utilizing Pytorch, and can be found on the GradientCrescent Github. Our approach is based on the approach detailed in Tabor\u2019s excellent Reinforcement Learning course. As our DuelDQN implementation is similar to our previous vanilla DQN implementation, the overall high-level workflow is shared, and won\u2019t be repeated here.", "Let\u2019s start by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments. We\u2019ll also install the AV package necessary for Torchvision, which we\u2019ll use for visualization. Note that the runtime must be restarted after installation is complete before the rest of the notebook can be executed.", "Next, we initialize our environment scenario, inspect the observation space and action space, and visualize our environment.", "Next, we\u2019ll define our preprocessing wrappers. These are classes that inherit from the OpenAI gym base class, overriding their methods and variables in order to implicitly provide all of our necessary preprocessing. We\u2019ll start defining a wrapper to repeat every action for a number of frames, and perform an element-wise maxima in order to increase the intensity of any actions. You\u2019ll notice a few tertiary arguments such as fire_first and no_ops \u2014 these are environment-specific, and of no consequence to us in Vizdoomgym.", "Next, we define the preprocessing function for our observations. We\u2019ll make our environment symmetrical by converting it into the standardized Box space, swapping the channel integer to the front of our tensor, and resizing it to an area of (84,84) from its original (320,480) resolution. We\u2019ll also greyscale our environment, and normalize the entire image by dividing by a constant.", "Next, we create a wrapper to handle frame-stacking. The objective here is to help capture motion and direction from stacking frames, by stacking several frames together as a single batch. In this way, we can capture position, translation, velocity, and acceleration of the elements in the environment. With stacking, our input adopts a shape of (4,84,84,1).", "Finally, we tie all of our wrappers together into a single make_env() method, before returning the final environment for use.", "Next, let\u2019s define our model, a deep Q-network featuring two outputs for the dueling architecture. This is essentially a three layer convolutional network that takes preprocessed input observations, with the generated flattened output fed to a fully-connected layer, after which the output is then split into the value stream (with a single node output), and the advantage stream (with a node output corresponding to the number of actions in the environment).", "Note there are no activation layers here, as the presence of one would result in a binary output distribution. Our loss is the squared difference of our estimated Q-value of our current state-action and our predicted state-action value. We then attach the RMSProp optimizer to minimize our loss during training.", "Recall that the update function for dueling deep Q-learning requires the following:", "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we\u2019ll draw data in minibatches during training. Hence, we need a replay memory buffer from which to store and draw observations from.", "Next, we\u2019ll define our agent, which differs form our vanilla DQN implementation. Our agent be using an epsilon greedy policy with a decaying exploration rate, in order to maximize exploitation over time. To learn to predict state and advantages values that maximize our cumulative reward, our agent will be using the discounted future rewards obtained by sampling the stored memory.", "You\u2019\u2019ll notice that we initialize two copies of our DQN as part of our agent, with methods to copy weight parameters of our original network into a target network. While our vanilla approach utilized this setup to generate stationary TD-targets, the presence of dual streams in our DuelDQN approach adds a layer of complexity to the process:", "With all of supporting code defined, let\u2019s run our main training loop. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "We\u2019ve graphed the average score of our agents together with our episodic epsilon value, across 500, 1000, and 2000 episodes below.", "Looking at the results and comparing them to our vanilla DQN implementation and Double DQN implementation, you\u2019ll notice a significantly improved improvement rate in distribution across 500, 1000, and 2000 episodes. moreover, with an even more constrained reward oscillation, suggesting improved convergence when compared either implementations.", "We can visualize the performance of our agent at 500 and 1000 episodes below.", "At 500 episodes, the agent has adapted the same strategy previously identified for DQN and DDQN at higher training times, attributed to a convergence at a local minima. Some offensive action is still taken but the primary strategy still relies on friendly fire between the monsters.", "Our agent has managed to break out of the localized minima, and discovered an alternative strategy oriented around a more offensive role. This is something neither our DQN and DDQN models were capable of, even at 2000 episodes \u2014 demonstrating the utility of the two-stream approach of a DuelDQN in identifying and prioritizing actions relevant to the environment.", "That wraps up this implementation on Double Deep Q-learning. In our next article, we\u2019ll finish our series on Q-learning approaches by combining all that we\u2019ve learned into a single method, and use it on a more dynamic finale.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository", "Simonini, \u201cImprovements in Deep Q Learning*", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fab2a3ff7355f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----ab2a3ff7355f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "discussed"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "implemented"}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "Deep Q-learning"}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "Double Deep Q Learning"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "VizDoom game environment"}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "previous Double DQN Agent"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "previous agent"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "implementations"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "here"}, {"url": "https://github.com/shakenes/vizdoomgym.git", "anchor_text": "https://github.com/shakenes/vizdoomgym.git"}, {"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "anchor_text": "implementation"}, {"url": "https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4", "anchor_text": "implementation"}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/", "anchor_text": "\u201cImprovements in Deep Q Learning*"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ab2a3ff7355f---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ab2a3ff7355f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/doom?source=post_page-----ab2a3ff7355f---------------doom-----------------", "anchor_text": "Doom"}, {"url": "https://medium.com/tag/openai?source=post_page-----ab2a3ff7355f---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/ai?source=post_page-----ab2a3ff7355f---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ab2a3ff7355f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ab2a3ff7355f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fab2a3ff7355f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ab2a3ff7355f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ab2a3ff7355f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}