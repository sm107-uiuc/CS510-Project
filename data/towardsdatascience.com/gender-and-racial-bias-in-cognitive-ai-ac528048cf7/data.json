{"url": "https://towardsdatascience.com/gender-and-racial-bias-in-cognitive-ai-ac528048cf7", "time": 1683014241.4630551, "path": "towardsdatascience.com/gender-and-racial-bias-in-cognitive-ai-ac528048cf7/", "webpage": {"metadata": {"title": "Gender And Racial Bias In Cognitive AI. | by Jerry Buaba | Towards Data Science", "h1": "Gender And Racial Bias In Cognitive AI.", "description": "We cannot understand the concept of bias in AI unless we first understand what the term \u2018bias\u2019 means. Bias, as defined by Wikipedia, is disproportionate weight in favor of or against an idea or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Bias", "anchor_text": "Wikipedia", "paragraph_index": 0}, {"url": "https://medium.com/u/ae8672bc4dec?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Derek Degbedzui", "paragraph_index": 2}, {"url": "https://twitter.com/colinmadland", "anchor_text": "Colin Madland", "paragraph_index": 5}, {"url": "https://www.linkedin.com/in/buabaj/", "anchor_text": "LinkedIn", "paragraph_index": 11}, {"url": "https://twitter.com/buabaj_", "anchor_text": "Twitter", "paragraph_index": 11}, {"url": "https://medium.com/u/ba2c585b06bd?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Anna Ayiku", "paragraph_index": 12}], "all_paragraphs": ["We cannot understand the concept of bias in AI unless we first understand what the term \u2018bias\u2019 means. Bias, as defined by Wikipedia, is disproportionate weight in favor of or against an idea or thing, usually in a way that is closed-minded, prejudicial, or unfair. Biases can be innate or learned. People may develop biases for or against an individual, a group, or a belief. In science and engineering, a bias is a systematic error. Statistical bias results from an unfair sampling of a population, or from an estimation process that does not give accurate results on average.", "Gender and racial bias concerning image AI, also known as cognitive AI bias, is a concept where models and algorithms built by engineers fail to provide optimal services to people of a specific gender or race. Most often in image AI, even recognition systems fail to detect and recognize underrepresented genders and races of people. This is a very serious issue especially for systems that are built to serve the general public equally and transparently. In this story, we are going to discuss some of the typical instances where models and algorithms portray gender and racial bias, discuss some effects caused by this and talk about how we can help fight racial and gender bias in image AI.", "An intelligent model is only as good as the data used to train it. \u2014 Derek Degbedzui", "Models and algorithms can only do what they have been trained to do; detect and recognize things (or people) they have been trained to recognize. Almost every image AI model or algorithm in production has been trained and tested with very big data. Such datasets are very time-consuming and expensive to make. It requires high-quality headshot photos of many different people varying in terms of gender and race if the model or algorithm is been developed to serve the public. Because of the stress involved in making such huge datasets, most engineers prefer to just use open or closed source datasets where much of the data collection has already been done for them. What they fail to recognize is that, even though their models and algorithms will pass with very high accuracy, there also has to be sufficient data of people of almost every gender and race. Your models and algorithms will fail against black people if it was trained with data of only white people and vice versa. The same applies to genders. Your organization has the sole responsibility of making sure your train and test data includes all possible races and genders of people who may use your service or product. When these things are taken care of, we can finally have unbiased systems, models or algorithms in cognitive AI.", "Over the past few weeks, lots of people have found out about the cognitive bias most AI systems portray and have set out to openly announce them so the general public knows and hence, force the companies involved to fix their AI systems. I will share a few of those tweets so anyone who wants to learn more about what went on during the public tests can find out.", "The same user in the above embed, Colin Madland noticed that after posting an image with both his face and the face of his black faculty member, the twitter algorithm cropped out the face of his black faculty member in the image preview on mobile devices.", "Many users also conducted social experiments to test if the claims against the twitter algorithm were true, the results remained the same and the general conclusion drawn was that the algorithm was indeed racially biased. Here are a few tweets from the social experiments performed.", "These are just a few out of the thousands of instances where the twitter algorithm has portrayed cognitive AI bias in cropping people of color out of images for previews.", "Over the years of actively incorporating AI into our everyday lives, the gender and racial bias it comes with has affected the lives of many people of color and underrepresented genders. There has been a lot of false predictions by these systems which have led to lots of people spending their lives in jail, not enjoying certain services and even death. Here are a few articles that point out some of the adverse consequences that cognitive bias in AI has brought about.", "The first and most important step in fighting gender and racial bias in cognitive AI is to correct the datasets we use in training and testing our systems, models and algorithms. We need to rethink our data collection and retention protocols and diversify our engineering teams. We need to critically ensure that the systems, models and algorithms we deploy to production have passed all tests we can conduct to ensure that the systems are robust even as they are considerate about every single gender or race of persons who are likely to use the systems or models.", "I hope I can raise awareness about the systemic gender and racial bias in cognitive AI and encourage engineers and developers to do better ad build better systems than we currently have. The adverse effects of racial and gender bias in cognitive AI are quite serious and we wouldn\u2019t want innocent people to go through some of the things they are currently going through.", "Feel free to connect with me on LinkedIn or Twitter if you want to talk about something relating to this topic or Data science, AI or ML in general. I am readily available to have a chat. Cheers to building systems and algorithms that make the world a better place for people of all races and genders. Happy Hacking! \ud83d\ude80", "God bless Anna Ayiku for her time and patience to proofread and correct the many mistakes I made writing this \u2764\ufe0f", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Developer and Machine Learning Engineer passionate about Data Science, Machine Learning and Analytics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac528048cf7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://buabajerry.medium.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": ""}, {"url": "https://buabajerry.medium.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Jerry Buaba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95976dd71ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&user=Jerry+Buaba&userId=95976dd71ae8&source=post_page-95976dd71ae8----ac528048cf7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@claybanks?utm_source=medium&utm_medium=referral", "anchor_text": "Clay Banks"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Bias", "anchor_text": "Wikipedia"}, {"url": "https://medium.com/u/ae8672bc4dec?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Derek Degbedzui"}, {"url": "https://twitter.com/colinmadland", "anchor_text": "Colin Madland"}, {"url": "https://twitter.com/colinmadland", "anchor_text": "Colin Madland"}, {"url": "https://twitter.com/kharijohnson", "anchor_text": "Khari johnso"}, {"url": "https://www.nature.com/articles/d41586-019-03228-6", "anchor_text": "Millions of black people affected by racial bias in health-care algorithmsAn algorithm widely used in US hospitals to allocate health care to patients has been systematically discriminating\u2026www.nature.com"}, {"url": "https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773", "anchor_text": "Why Can\u2019t This Soap Dispenser Identify Dark Skin?On Wednesday, a Facebook employee in Nigeria shared footage of a minor inconvenience that he says speaks to tech\u2019s\u2026gizmodo.com"}, {"url": "https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html", "anchor_text": "Wrongfully Accused by an AlgorithmThe Great Read In what may be the first known case of its kind, a faulty facial recognition match led to a Michigan\u2026www.nytimes.com"}, {"url": "https://www.rnz.co.nz/news/te-manu-korihi/425081/police-facial-recognition-discrimination-against-maori-a-matter-of-time-expert", "anchor_text": "Police facial recognition discrimination against M\u0101ori a matter of time \u2014 expertIt is only a matter of time before a M\u0101ori person is wrongfully arrested because of a false match on facial recognition\u2026www.rnz.co.nz"}, {"url": "https://www.nytimes.com/2019/04/25/lens/sarah-lewis-racial-bias-photography.html", "anchor_text": "The Racial Bias Built Into PhotographySarah Lewis explores the relationship between racism and the camera. This week, Harvard University\u2019s Radcliffe\u2026www.nytimes.com"}, {"url": "https://hbr.org/2019/05/voice-recognition-still-has-significant-race-and-gender-biases", "anchor_text": "Voice Recognition Still Has Significant Race and Gender BiasesExecutive Summary As with facial recognition, web searches, and even soap dispensers, speech recognition is another\u2026hbr.org"}, {"url": "https://unsplash.com/@wocintechchat?utm_source=medium&utm_medium=referral", "anchor_text": "Christina @ wocintechchat.com"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.linkedin.com/in/buabaj/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/buabaj_", "anchor_text": "Twitter"}, {"url": "https://medium.com/u/ba2c585b06bd?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Anna Ayiku"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ac528048cf7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac528048cf7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ac528048cf7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----ac528048cf7---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/racism?source=post_page-----ac528048cf7---------------racism-----------------", "anchor_text": "Racism"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&user=Jerry+Buaba&userId=95976dd71ae8&source=-----ac528048cf7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&user=Jerry+Buaba&userId=95976dd71ae8&source=-----ac528048cf7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac528048cf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac528048cf7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac528048cf7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac528048cf7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac528048cf7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac528048cf7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac528048cf7--------------------------------", "anchor_text": ""}, {"url": "https://buabajerry.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://buabajerry.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jerry Buaba"}, {"url": "https://buabajerry.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "364 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95976dd71ae8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&user=Jerry+Buaba&userId=95976dd71ae8&source=post_page-95976dd71ae8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F937e9073f61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgender-and-racial-bias-in-cognitive-ai-ac528048cf7&newsletterV3=95976dd71ae8&newsletterV3Id=937e9073f61a&user=Jerry+Buaba&userId=95976dd71ae8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}