{"url": "https://towardsdatascience.com/handcrafting-an-artificial-neural-network-e0b663e88a53", "time": 1682995138.257795, "path": "towardsdatascience.com/handcrafting-an-artificial-neural-network-e0b663e88a53/", "webpage": {"metadata": {"title": "HandCrafting an Artificial Neural Network | by Tirth Patel | Towards Data Science", "h1": "HandCrafting an Artificial Neural Network", "description": "This article contains the fully vectorized code of neural network with Dropout and L2 regularization"}, "outgoing_paragraph_urls": [{"url": "https://github.com/tirthasheshpatel/Neural-Network", "anchor_text": "entire code", "paragraph_index": 6}, {"url": "https://www.deeplearning.ai/", "anchor_text": "Andrew Ng", "paragraph_index": 7}, {"url": "http://coursera.org", "anchor_text": "Coursera", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Gaussian Normal Distributions", "paragraph_index": 20}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "[1]N.Shrivastava et. al., 2014", "paragraph_index": 23}, {"url": "https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb", "anchor_text": "vanishing/exploding gradient problem", "paragraph_index": 36}, {"url": "https://www.kaggle.com/c/digit-recognizer", "anchor_text": "Kaggle", "paragraph_index": 42}], "all_paragraphs": ["In this article, I have implemented a fully vectorized python code of an Artificial Neural Network tested on multiple datasets. Further, dropout and L2 regularization techniques are implemented and explained in detail.", "It is highly recommended to go through the basic working of an Artificial Neural Network, forward propagation, and backward propagation.", "This article is divided into 10 sections:", "Any sort of feedback is highly appreciated.", "Artificial Neural Network is one of the most beautiful and basic concepts of Supervised Deep Learning. It can be used to perform multiple tasks like binary or categorical classification. It seems pretty easy to understand and implement unless you start coding it up. During coding such a network, small problems pop up which lead to big mistakes and help you understand the concepts that you previously missed. So, in this article, I have tried to implement an Artificial Neural Network that would probably help you save days of work needed to properly code and understand each and every concept of the topic. I would be using standard notations and symbols during the article.", "This article is too dense so if you are not comfortable with neural networks and their notations, you would probably find it difficult to understand everything. So, I would suggest to give it some time and go slowly referring to the resources I have provided in the article.", "The entire code is available on Github.", "I would preassume that you know what neural networks are and how they learn. It would be pretty easy to follow if you are comfortable with Python and libraries like numpy. Also, good knowledge of linear algebra and calculus is needed to cakewalk through forward and backpropagation section. Moreover, I would highly suggest going through the videos of courses by Andrew Ng on Coursera.", "Now, we can start coding a neural network. The first thing is to import all the libraries that we will need to implement our network.", "We will use pandas to import and clean our dataset. Numpy is the most important library for performing matrix algebra and complex calculations. Libraries like warnings, time, sys and os are rarely used.", "We will need activation functions later in the article to perform forward propagation. Also, we need derivatives of the activation functions during backpropagation. So, let\u2019s code some activation functions up.", "We have coded the four most popular activation functions. First is the regular old sigmoid activation function.", "Then we have ReLU or \u201cRectified Linear Unit\u201d. We will be mostly using this activation function. Note that we will keep the derivative of ReLU 0 at point 0.", "We also have an extended version of ReLU called Leaky ReLU. It works just like ReLU and can provide better results on some datasets (not necessarily all).", "Then we have tanh (hyperbolic tangent) activation function. It is also widely used and almost always superior to sigmoid.", "Also, PHI and PHI_PRIME are the python dictionaries containing the activation functions and their derivatives respectively.", "In this section, we will create and initialize our Neural Network class. Firstly, we will decide which parameters to use during initialization. We need:", "Keeping this in mind, let us start coding the class of our Neural Network:", "Now we have a properly documented class of Neural network and we can proceed to initialize other variables of the network.", "As shown, we will use \u2018self.m\u2019 to store the number of examples in our dataset. \u2018self.n\u2019 will store the information of the number of neurons in each layer. \u2018self.ac_funcs\u2019 is the python list of activation functions of each layer. \u2018self.cost\u2019 will store the logged values of the cost function as we train our network. \u2018self.acc\u2019 will store the logged accuracy achieved on the dataset after training. Having initialized all the variables of our network, let\u2019s move further to initialize the weights and biases of our network.", "The interesting part starts now. We know that weights cannot be initialized to zeros as the hypothesis of each neuron becomes the same and the network never learns. So we have to have some way to break the symmetry and make our neural network learn. We can use the Gaussian Normal Distributions to get our random values. As these distributions have a mean of zero, the weights get centered to zero and are very small. Hence, the network starts learning very quickly and efficiently. We can use np.random.randn() function to generate random values from the normal distribution. The following two lines of code are enough to initialize our weights and biases.", "We have initialized our weights to random values from the Normal Distributions. The biases have been initialized to zeros.", "First, let\u2019s understand forward propagation without any regularization.", "We have Z as the hypothesis of each neuron connection from one layer to other. Once we calculate Z, we apply activation function f to the Z values to get activations y of each neuron in each layer. This is the \u2018pure vanilla\u2019 forward propagation. But as stated in the paper by [1]N.Shrivastava et. al., 2014, Dropout is an amazing technique to improve the generalization of the neural network and make it more robust. So, let\u2019s first get some intuition of Dropout regularization.", "Dropout, as its name suggests, refers to \u201cdeactivating\u201d some neurons in our neural network and training the rest of the neurons.", "To improve the performance, we can train tens and hundreds of neural networks with different values of hyperparameters, get the output of all the networks and take their mean to get our final results. This process is computationally very expensive and cannot be implemented practically. Hence, we need a way to do something similar in a more optimized and computationally inexpensive way. Dropout regularization does something exactly similar in a very inexpensive and simple way. In fact, Dropout is so easy and simple way to optimize the performance, that it gained a lot of attention recently, and is used almost everywhere in numerous other models of Deep Learning.", "To implement dropout, we will use the following approach:", "We will first pull out random value from Bernoulli\u2019s distribution, keep the neuron if the probability is above a particular threshold, and then perform regular forward propagation. Note that we don\u2019t apply dropout during predicting the values on a new dataset or during test time.", "We will have keep_prob as the probability of survival of neurons per layer. We will only keep the neurons with a probability higher than the probability of survival or keep_prob. Suppose, its value is 0.8. It means that we will deactivate 20% of the neurons in each layer and train the rest 80% of the neurons. Note that we deactivate randomly chosen neurons after each iteration. This helps the neurons to learn features that generalize over a larger dataset. A really intuitive proof is given in the paper [1].", "We first initialize the list that will store the Z and A values. We first append the linear values of the first layer in Z and then append the activation of neurons of the first layer in A. Here, PHI is a python dictionary containing the activation functions that we coded earlier. We similarly calculate the values of Z and A for all other layers using a for loop. Note that we don\u2019t apply dropout in the input layer. We finally return the calculated values of Z and A.", "We will use the standard binary/categorical cross entropy cost function.", "We have coded our cost function with L2 Regularization. The parameter lambda is known as \u201cpenalization parameter\u201d. It helps the values of weights to not increase rapidly and hence generalize better. Here, \u2018a\u2019 contains the activation values of the output layer. We also have the function _cost_derivative to calculate the derivative of the cost function with respect to the activations of the output layer. We would need that later during backpropagation.", "Here are some formulas that we would need to perform backpropagation.", "We will implement this on a deep neural network. The formulas on the right are fully vectorized and so we will be using them. Once you understand these formulas, we can go ahead to code them.", "We take epochs, alpha (learning rate), _lambda, keep_prob, and interval as parameters of our function to implement backpropagation. Description of each of them is given in the documentation comment.", "We start with forward propagation. Then we calculate the derivative of our cost function as delta. Now, for each layer, we calculate delta_w and delta_b containing the derivative of the cost function with respect to the weights and biases of our network. Then we update delta, weights, and biases according to their respective formulas. After updating the weights and biases starting from the last layer to the second layer, we update the weights and biases of the first layer. We do this for several iterations until the values of weights and biases converge.", "Important Note: A big mistake possible here is updating delta after updating the weights and biases. Doing so can lead to a very bad case of vanishing/exploding gradient problem.", "Most of our work is done here but we still need to code function that can predict results on a new dataset. Hence, as our last step, we will code a function to predict labels of a new dataset.", "This step is pretty straightforward. We just need to perform forward propagation but without Dropout Regularization. We do not apply dropout regularization during test time as we need all the neurons of all the layers to provide us with proper results and not just some random values.", "As shown, we will return the activations of the output layer as the result.", "Here is the entire code to implement an Artificial Neural Network yourself. I have added certain pieces of code for printing the cost and accuracy of our network as we train it. Except that, everything is the same.", "Congratulations! We have finally finished coding our neural network. Now, we can test our network on different datasets.", "We will test our network on the famous MNIST dataset for digit classification. We will only use 8000 images to train our network and predict on 2000 other images. You can get the dataset on Kaggle.", "I have trained two hidden layered neural network with 32 and 16 neurons. I have used ReLU activation function in both the layers. After training the network for 2500 epochs with penalization parameter 1.0, and learning rate 0.1 we have:", "The graph of Cost vs Epochs looks like:", "We achieve a pretty good accuracy over both training and test set. We can achieve even more accuracy by tuning the hyperparameters by using techniques like Grid Search, Randomized Grid Search, etc.", "Also, feel free to try for different values of hyperparameters, activation functions, and datasets. If you think that there could be any improvements in the code then do share on GitHub or here in the comment section. Any sort of feedback is highly appreciated.", "If you have understood the code for the neural network that I have provided above, then here are a few more changes you can do to make it better.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe0b663e88a53&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tirthasheshpatel?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tirthasheshpatel?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Tirth Patel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2d6baa0039d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&user=Tirth+Patel&userId=2d6baa0039d2&source=post_page-2d6baa0039d2----e0b663e88a53---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral", "anchor_text": "JJ Ying"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/tirthasheshpatel/Neural-Network", "anchor_text": "entire code"}, {"url": "https://github.com/tirthasheshpatel/Neural-Network", "anchor_text": "tirthasheshpatel/Neural-NetworkHand made Neural Network for demonstration and teaching purposes - tirthasheshpatel/Neural-Networkgithub.com"}, {"url": "https://www.deeplearning.ai/", "anchor_text": "Andrew Ng"}, {"url": "http://coursera.org", "anchor_text": "Coursera"}, {"url": "https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e", "anchor_text": "Source"}, {"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "anchor_text": "Source"}, {"url": "https://isaacchanghau.github.io/post/activation_functions/", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Gaussian Normal Distributions"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "Source"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "[1]N.Shrivastava et. al., 2014"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "Source"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "Source"}, {"url": "https://www.deeplearning.ai/", "anchor_text": "Source"}, {"url": "https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb", "anchor_text": "vanishing/exploding gradient problem"}, {"url": "https://www.kaggle.com/c/digit-recognizer", "anchor_text": "Kaggle"}, {"url": "https://link.medium.com/oBUOo6EGsU", "anchor_text": "softmax activation function"}, {"url": "https://en.m.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "mini-batch gradient descent"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e0b663e88a53---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e0b663e88a53---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e0b663e88a53---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&user=Tirth+Patel&userId=2d6baa0039d2&source=-----e0b663e88a53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&user=Tirth+Patel&userId=2d6baa0039d2&source=-----e0b663e88a53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe0b663e88a53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e0b663e88a53---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e0b663e88a53--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e0b663e88a53--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e0b663e88a53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tirthasheshpatel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tirthasheshpatel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tirth Patel"}, {"url": "https://medium.com/@tirthasheshpatel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2d6baa0039d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&user=Tirth+Patel&userId=2d6baa0039d2&source=post_page-2d6baa0039d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2d6baa0039d2%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandcrafting-an-artificial-neural-network-e0b663e88a53&user=Tirth+Patel&userId=2d6baa0039d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}