{"url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "time": 1683002413.364053, "path": "towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4/", "webpage": {"metadata": {"title": "Word Embeddings for NLP. Understanding word embeddings and their\u2026 | by Renu Khandelwal | Towards Data Science", "h1": "Word Embeddings for NLP", "description": "In this article, we will understand how to process text for usage in machine learning algorithms. What are embeddings and why are they used for text processing? Natural Language Processing(NLP)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient estimation of Word Representations in vector space \u2014 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean", "paragraph_index": 42}], "all_paragraphs": ["In this article, we will understand how to process text for usage in machine learning algorithms. What are embeddings and why are they used for text processing?", "Natural Language Processing(NLP) refers to computer systems designed to understand human language. Human language, like English or Hindi consists of words and sentences, and NLP attempts to extract information from these sentences.", "A few of the tasks that NLP is used for", "Machine learning and deep learning algorithms only take numeric input so how do we convert text to numbers?", "Bag of words is a simple and popular technique for feature extraction from text. Bag of word model processes the text to find how many times each word appeared in the sentence. This is also called as vectorization.", "In the code below, we use CountVectorizer, it tokenizes a collection of text documents, builds a vocabulary of known words, and encodes new documents using that vocabulary.", "In the text classification problem, we have a set of texts and their respective labels. We use the Bag of Words model to extract features from the text and we do this by converting the text into a matrix of occurrence of words within a document.", "What is the problem with bag of words?", "In the bag of words model, each document is represented as a word-count vector. These counts can be binary counts, a word may occur in the text or not or will have absolute counts. The size of the vector is equal to the number of elements in the vocabulary. If most of the elements are zero then the bag of words will be a sparse matrix.", "In deep learning, we would have sparse matrix as we will be working with huge amount of training data. Sparse representations are harder to model both for computational reasons as well as for informational reasons.", "Huge amount of weights: Huge input vectors means a huge number of weights for a neural network.", "Computationally intensive: More weights means more computation required to train and predict.", "Lack of meaningful relations and no consideration for order of words: BOW is a collection of words that appear in the text or sentences with the word counts. Bag of words does not take into consideration the order in which they appear.", "Word Embedding is solution to these problems", "Embeddings translate large sparse vectors into a lower-dimensional space that preserves semantic relationships.", "Word embeddings is a technique where individual words of a domain or language are represented as real-valued vectors in a lower dimensional space.", "Sparse Matrix problem with BOW is solved by mapping high-dimensional data into a lower-dimensional space.", "Lack of meaningful relationship issue of BOW is solved by placing vectors of semantically similar items close to each other. This way words that have similar meaning have similar distances in the vector space as shown below.", "\u201cking is to queen as man is to woman\u201d encoded in the vector space as well as verb Tense and Country and their capitals are encoded in low dimensional space preserving the semantic relationships.", "How are semantically similar items placed close to each other?", "Let\u2019s explain this using collaborative filtering used in recommendation engines.", "Recommendation engines predict what a user would purchase based on historical purchases of other users with similar interests. using collaborative filtering", "Amazon and Netflix use recommendation engines for suggesting products or movies to their users", "Collaborative filtering is a method where all the similar products bought by multiple customers are embedded into a low dimensional space. This low dimensional space will contain similar products close to each other, hence, it is also called as nearest neighborhood algorithm.", "This technique of nearest neighborhood is used for placing semantically similar items close to each other", "How do we map high-dimensional data into a lower-dimensional space?", "Standard dimensionality reduction techniques like Principal Component Analysis(PCA) can be used to create word embeddings. PCA tries to find highly correlated dimensions that can be collapsed into a single dimension using the BOW.", "Word2vec is an algorithm invented at Google for training word embeddings. word2vec relies on the distributional hypothesis. The distributional hypothesis states that words which, often have the same neighboring words tend to be semantically similar. This helps to map semantically similar words to geometrically close embedding vectors.", "Distributional hypothesis uses continuous bag of words(CBOW) or skip grams.", "word2vec models are shallow neural network with an input layer, a projection layer and an output layer. It is trained to reconstruct linguistic contexts of words. Input layer for Word2vec neural network takes a larger corpus of text to produce a vector space, typically of several hundred dimensions. Every unique word in the text corpus is assigned a corresponding vector in the space.", "This architecture is called continuous bag of words CBOW as it uses continuous distributed representation of the context. It considers both the order of words in the history as well as future.", "This helps common context word vectors in the corpus to be located close to one another in the vector space.", "Skip gram does not predict the current word based on the context instead it uses each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.", "GloVe was developed by Pennington, et al. at Stanford. It is called Global Vectors as the global corpus statistics are captured directly by the model.", "LSA efficiently leverages statistical information but does not do well on word analogy, thus indicating a sub-optimal vector space structure.", "Methods like skip-gram perform better on the analogy task, but poorly utilize the statistics of the corpus as they are not trained on global co-occurrence counts. GloVe uses a specific weighted least squares model to train on global word co-occurrence counts to make efficient use of statistics.", "Consider two words i=ice and j=steam in the context of Thermodynamics domain. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words k.", "Probe words like water or fashion that are either related to both ice and steam, or to neither, the ratio should be close to one. Probe words like solid related to ice but not to steam will have large value for the ratio", "Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.", "It is the gender that distinguishes man from woman, similar to word pairs, such as king and queen or brother and sister. To state this observation mathematically, we might expect that the vector differences man: woman, king: queen, and brother: sister might all be roughly equal. This property and other interesting patterns can be observed in the above set of visualizations using GloVe.", "Word embeddings are considered to be one of the successful applications of unsupervised learning at present. They do not require any annotated corpora. Embeddings use a lower-dimensional space while preserving semantic relationships.", "GloVe: Global Vectors for Word Representation", "Efficient estimation of Word Representations in vector space \u2014 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A Technology Enthusiast who constantly seeks out new challenges by exploring cutting-edge technologies to make the world a better place!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b72991e01d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://arshren.medium.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----5b72991e01d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/datadriveninvestor/simple-text-summarizer-using-nlp-d8aaf5828e68", "anchor_text": "ode for simple text summarizer"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "anchor_text": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Source"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient estimation of Word Representations in vector space \u2014 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data", "anchor_text": "Embeddings: Categorical Input Data | Machine Learning Crash CourseEstimated Time: 10 minutes Categorical data refers to input features that represent one or more discrete items from a\u2026developers.google.com"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2vecWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer\u2026en.wikipedia.org"}, {"url": "https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/", "anchor_text": "https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5b72991e01d4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5b72991e01d4---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----5b72991e01d4---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----5b72991e01d4---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/bag-of-words?source=post_page-----5b72991e01d4---------------bag_of_words-----------------", "anchor_text": "Bag Of Words"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&user=Renu+Khandelwal&userId=31b07253bc35&source=-----5b72991e01d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&user=Renu+Khandelwal&userId=31b07253bc35&source=-----5b72991e01d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b72991e01d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b72991e01d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b72991e01d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b72991e01d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b72991e01d4--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://arshren.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.9K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1cb44d62203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-for-nlp-5b72991e01d4&newsletterV3=31b07253bc35&newsletterV3Id=b1cb44d62203&user=Renu+Khandelwal&userId=31b07253bc35&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}