{"url": "https://towardsdatascience.com/linguistics-wisdom-of-nlp-models-8c8554bc8c66", "time": 1683016880.047557, "path": "towardsdatascience.com/linguistics-wisdom-of-nlp-models-8c8554bc8c66/", "webpage": {"metadata": {"title": "Linguistics Wisdom of NLP Models. Analyzing, Designing, and Evaluating\u2026 | by Keyur Faldu | Towards Data Science", "h1": "Linguistics Wisdom of NLP Models", "description": "This article is authored by Keyur Faldu and Dr. Amit Sheth. This article elaborates on a niche aspect of the broader cover story on \u201cRise of Modern NLP and the Need of Interpretability!\u201d At Embibe\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@keyurfaldu", "anchor_text": "Keyur Faldu", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/amitsheth/", "anchor_text": "Dr. Amit Sheth", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/rise-of-modern-nlp-and-the-need-of-interpretability-97dd4a655ac3", "anchor_text": "\u201cRise of Modern NLP and the Need of Interpretability!\u201d", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/encoded-linguistic-knowledge-in-nlp-models-b9558ba90943", "anchor_text": "Discovering the Encoded Linguistic Knowledge in NLP models", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Lin et al.", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Open Sesame: Getting Inside BERT\u2019s Linguistic Knowledge", "paragraph_index": 10}], "all_paragraphs": ["This article is authored by Keyur Faldu and Dr. Amit Sheth. This article elaborates on a niche aspect of the broader cover story on \u201cRise of Modern NLP and the Need of Interpretability!\u201d At Embibe, we focus on developing interpretable and explainable Deep Learning systems, and we survey the current state of the art techniques to answer some open questions on linguistic wisdom acquired by NLP models.", "This article is in continuation of the previous article (Discovering the Encoded Linguistic Knowledge in NLP models) to understand what linguistic knowledge is encoded in NLP models. The previous article covers what is probing, how it is different from multi-task learning, and two types of probes \u2014 representation based probes and attention weights based probes. It also shed light on how a probe task (or auxiliary task) is used to assess the linguistic ability of NLP models trained on some other primary task(s).", "Naturally, the prediction performance of probes on linguistic tasks, or supporting patterns to correlate or compare neural network mechanics with linguistic phenomenon gives insights on whats and hows of encoded linguistic knowledge. Prediction performance could be classification accuracy, correlation coefficients, or mean reciprocal rank of predicting the gold label. Note that the prediction performance of the model on the probe task can be compared with the state of the art performance of an explicitly trained model for the same task as the primary task to understand the extent of encoded linguistic knowledge. However, there are other aspects to dive deeper to analyze such probes, including the following.", "The above considerations help us elaborate more to understand probes better. We can also draw meaningful conclusions on encoded linguistic knowledge in NLP models. Let us dive deeper into examples and surveys of research papers on these topics.", "One of the early research to formally investigate the problem of probing encoded linguistic knowledge is \u201cFINE-GRAINED ANALYSIS OF SENTENCE EMBEDDINGS USING AUXILIARY PREDICTION TASKS\u201d, where, Adi et al. [11] aims for a better understanding of encoded sentence representations.", "Three auxiliary tasks related to sentence structures were considered:", "These probes are based on the sentence embedding which is computed as the average of final representations produced by the encoder-decoder model and the CBOW (continuous bag of words) model. Key findings in the paper to understand \u201care bigger models better at encoding above linguistic knowledge\u201d as follow:", "Increasing the number of dimensions benefits some tasks more than others. As shown in figure 2, the (a) length and \u00a9 order tests get the benefit of bigger representation dimensions, whereas the content test peaks at representation with 750 dimensions.", "Models can be tested on generalization data to verify the extent of model learning. And, deliberately designed complex generalization data can test the limit of linguistic wisdom learned by NLP models. Generalization over such complex data shows the real linguistic ability as opposed to memorizing surface-level patterns.", "Figure 3. The examples of training and development data, which are simpler in nature. Generalization data are more complex with the presence of distractors. (i) Main auxiliary task: \u201cWill\u201d is the target word, and \u201ccan\u201d is a distractor added in the generalization data (ii) Subject noun task: \u201cbee\u201d is a target word, and \u201cqueen\u201d is a distractor added in the generalization data. (Lin et al.[15], ACL 2019)", "Lin et al. [15] carried out such experiments in the paper, \u201cOpen Sesame: Getting Inside BERT\u2019s Linguistic Knowledge\u201d. Figure 3 shows how generalized data can contain deliberate distractors to the stress-test model\u2019s encoded linguistic knowledge.", "This paper further investigates the \u2018attention mechanism\u2019 of the model and how much it is sensitive to such distractions. It proposes \u2018Confusion score\u2019 which is the binary cross-entropy of attention of candidate tokens to the target token.", "We can see how confusion drops when the complexity of the distractor becomes lesser in the cases below.", "As classifier probes are of comparatively lesser complexity, it is interesting to investigate if we can decode encoded linguistic knowledge in totality. Let\u2019s say, can we build dependency parse trees altogether relying on encoded representations?", "Hewitt and Manning [5] propose \u201cStructural Probe\u201d in the paper \u201cA Structural Probe for Finding Syntax in Word Representations\u201d, where it can be empirically concluded that it is possible to transform the space of internal representations to the space of linguistic knowledge. The probe identifies a linear transformation under which the squared L2 distance of transformed representations encodes the distance between words in the parse tree, and one in which the squared L2 norm of transformed representations encodes depth in the parse tree.", "As can be seen, linguistic knowledge was learned by model layer after layer, and it fades in top layers because these layers are more tuned towards the primary objective function. It was also studied if increasing dimensions of transformed space help in expressing linguistic knowledge, where experiments convey that linguistic knowledge for a parse dependency tree can be expressed in about 32 or 64 dimensions, adding further dimensions does not add further value.", "Probes, supervised models trained to predict linguistic properties have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? Can we meaningfully compare the linguistic properties of layers of a model using linguistic task accuracy? A sufficiently deep probe model can memorize the linguistic information. So how can we address this limitation?", "Hewitt and Liang propose \u201cSelectivity\u201d as a measure to show the effectiveness of probes in the paper \u201cDesigning and Interpreting Probes with Control Tasks\u201d. Control tasks are designed to know how a probe can learn linguistic information independent of encoded representations. Selectivity is defined as the difference between linguistic task accuracy and control task accuracy.", "As can be seen in the above figure 9, a control task for part of speech prediction would assign some word type (or identity) to a set of words independently, and a POS tag would be predicted based on word types (ignoring encoded representations altogether). So, if a deep probe is able to memorize it should be able to perform well for a control task as well. Probe model complexity and accuracy achieved for the auxiliary task of part-of-speech and its control task can be seen above in the right figure. It is of utmost importance to choose a probe with high selectivity and high accuracy to draw out conclusions.", "Adi et al investigate the source of sentence structure knowledge in the paper \u201cFINE-GRAINED ANALYSIS OF SENTENCE EMBEDDINGS USING AUXILIARY PREDICTION TASKS\u201d. Inspite of the CBOW model being oblivious to the context around, Probe was able to give high accuracy on the auxiliary task to predict the sentence length. However, it was found that just the norm of sentence embedding was indicative of sentence length (figure 10 (right)), so the source of information was not from the encoded representations of a token. However, when these representations were aggregated, the norm tends to move towards 0, as established by the central limit theorem and Hoeffding\u2018s inequality. It can be noticed in figure 10 (left) that the length prediction accuracy for synthetic sentences (random words chose to form a synthetic sentence) was also close to legitimate sentences. So, the actual source of knowledge to determine the sentence length was just the statistical property to the aggregation of random variables.", "Hence, it requires in-depth study and analysis to drive inference from outcomes of probes.", "Now that we have surveyed techniques to analyze probes for encoded linguistic knowledge, a follow-up question is \u201ccan we infuse explicit linguistic knowledge for desired outcomes?\u201d. There is an interesting study about paraphrase generation, \u201cSyntax-guided Controlled Generation of Paraphrases\u201d. Kumar et al [a] have shown that to paraphrase a source sentence, how can the syntax of an exemplar sentence be leveraged. A generated paraphrase should preserve the meaning of the source sentence but syntactic sentence structure should be similar to an exemplar sentence.", "The above figure 11 shows generated paraphrases with guidance from the syntax of different exemplar sentences. We can observe how the model is able to get guidance from the syntax of exemplar sentences. Note that only the syntax of exemplar sentences is given as an input, actual individual tokens are not fed to the model. A syntax tree of an exemplar sentence can be extracted at different height H, and it can be fed as an input to the encoder-decoder model. Lesser height gives more flexibility of paraphrasing, while deeper height would try to explicitly control the syntax structure of paraphrase.", "The encoded linguistic knowledge is essential to understanding the meaning of natural language. Most of the probes we have seen deals with syntactic linguistic knowledge. Semantic meaning captured in the text needs to be understood. We need to develop frameworks to assess the capabilities of NLP models like BERT for the same. Reading comprehension, text similarity, question answering, neural machine translation, etc are some of the examples where the true performance of the model would be based on its ability to encode semantic meaning.", "Benchmarks like GLUE and SuperGLUE are developed to assess the abilities of fine-tuned NLP models to perform the tasks based on natural language understanding. Generally, the performance of NLP models is compared with validation accuracy. There are inherent limitations in using validation accuracy like overfitting, data distribution of validation set, etc. The paper \u201cBeyond Accuracy: Behavioral Testing of NLP Models with CheckList\u201d, presents a framework for assessing the model\u2019s performance beyond validation accuracy.", "\u201cCHECKLIST\u201d suggests three different test types, Minimum Functionality tests (MFT) where examples are generated with the expected gold labels., Invariance (INV) where, from given examples, it creates new examples where the gold labels get flipped., and Directional Expectation tests (DIR) changes the gold labels in a positive or negative direction. Examples of each are given below:", "It was surprising to notice that while models like Roberta and BERT surpass human baselines (with the accuracies of 91.1% and 91.3%) are failing badly on simple rule-based generalizations of validation dataset. That said, there is a long road map ahead to achieve human-level natural language understanding.", "We have gone through Probes to assess encoded linguistic knowledge in NLP models. We have found that", "The encoded linguistic knowledge is primarily syntactic in nature, and as demonstrated by \u201cCHECKLIST\u201d, models fail on generalization which is semantic in nature. State of the art NLP models is primarily pre-trained in self-supervised fashion on unlabelled data, and fine-tuned on limited labeled data for the downstream tasks. It is certainly difficult to acquire semantic knowledge related to tasks or domains from unlabelled data or limited labeled data.", "Infusing semantic knowledge and domain knowledge improves the ability of the NLP model to encode semantic and domain knowledge. As a result, it inherently develops the ability to reason and generate plausible and faithful explanations. Guar et al [19] describe how knowledge graphs can help make deep learning systems more interpretable and explainable.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c8554bc8c66&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://keyurfaldu.medium.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": ""}, {"url": "https://keyurfaldu.medium.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Keyur Faldu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87b1e5f512f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&user=Keyur+Faldu&userId=87b1e5f512f3&source=post_page-87b1e5f512f3----8c8554bc8c66---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@keyurfaldu", "anchor_text": "Keyur Faldu"}, {"url": "https://www.linkedin.com/in/amitsheth/", "anchor_text": "Dr. Amit Sheth"}, {"url": "https://towardsdatascience.com/rise-of-modern-nlp-and-the-need-of-interpretability-97dd4a655ac3", "anchor_text": "\u201cRise of Modern NLP and the Need of Interpretability!\u201d"}, {"url": "https://towardsdatascience.com/encoded-linguistic-knowledge-in-nlp-models-b9558ba90943", "anchor_text": "Discovering the Encoded Linguistic Knowledge in NLP models"}, {"url": "https://arxiv.org/pdf/1608.04207.pdf", "anchor_text": "Adi et al."}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Lin et al."}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Open Sesame: Getting Inside BERT\u2019s Linguistic Knowledge"}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Lin et al."}, {"url": "https://arxiv.org/pdf/1906.01698.pdf", "anchor_text": "Lin et al."}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "Hewitt et al."}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "Hewitt et al."}, {"url": "https://www.aclweb.org/anthology/D19-1275.pdf", "anchor_text": "Hewitt et al."}, {"url": "https://arxiv.org/pdf/1608.04207.pdf", "anchor_text": "Adi et al."}, {"url": "https://arxiv.org/abs/2005.08417", "anchor_text": "Kumar et al."}, {"url": "https://arxiv.org/abs/2005.08417", "anchor_text": "Kumar et al."}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.442.pdf", "anchor_text": "Ribeiro et al."}, {"url": "https://arxiv.org/pdf/1901.05287.pdf", "anchor_text": "\u201cAssessing BERT\u2019s Syntactic Abilities\u201d"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8c8554bc8c66---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8c8554bc8c66---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8c8554bc8c66---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c8554bc8c66---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----8c8554bc8c66---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&user=Keyur+Faldu&userId=87b1e5f512f3&source=-----8c8554bc8c66---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&user=Keyur+Faldu&userId=87b1e5f512f3&source=-----8c8554bc8c66---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c8554bc8c66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c8554bc8c66---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c8554bc8c66--------------------------------", "anchor_text": ""}, {"url": "https://keyurfaldu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://keyurfaldu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Keyur Faldu"}, {"url": "https://keyurfaldu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "82 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F87b1e5f512f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&user=Keyur+Faldu&userId=87b1e5f512f3&source=post_page-87b1e5f512f3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5d2341f2b0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistics-wisdom-of-nlp-models-8c8554bc8c66&newsletterV3=87b1e5f512f3&newsletterV3Id=a5d2341f2b0d&user=Keyur+Faldu&userId=87b1e5f512f3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}