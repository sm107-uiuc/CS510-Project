{"url": "https://towardsdatascience.com/got-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0", "time": 1683013097.835604, "path": "towardsdatascience.com/got-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0/", "webpage": {"metadata": {"title": "Got data? How I taught myself how to scrape websites in a few hours (and you can, too) | by JP Hwang | Towards Data Science", "h1": "Got data? How I taught myself how to scrape websites in a few hours (and you can, too)", "description": "I had a shameful secret. It is one that affects a surprising number of people in the data science community. And I was too lazy to face the problem and tackle it head-on. For the majority of the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "BeautifulSoup", "paragraph_index": 4}, {"url": "https://requests.readthedocs.io/en/master/", "anchor_text": "requests", "paragraph_index": 4}, {"url": "https://docs.python.org/3/library/re.html", "anchor_text": "regular expressions", "paragraph_index": 4}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "git repo here", "paragraph_index": 7}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "https://github.com/databyjp/beginner_scraping", "paragraph_index": 10}, {"url": "https://scrapethissite.com/faq/", "anchor_text": "ScrapeThisSite.com", "paragraph_index": 12}, {"url": "https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction", "anchor_text": "DOM (Document Object Model)", "paragraph_index": 23}, {"url": "https://docs.python.org/3/library/re.html", "anchor_text": "regex", "paragraph_index": 36}, {"url": "https://github.com/plotly/plotly.py", "anchor_text": "plotly", "paragraph_index": 53}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "GitHub repo", "paragraph_index": 56}, {"url": "https://github.com/SeleniumHQ/selenium", "anchor_text": "selenium", "paragraph_index": 63}, {"url": "https://github.com/scrapy/scrapy", "anchor_text": "scrapy", "paragraph_index": 63}, {"url": "https://www.theverge.com/2019/9/10/20859399/linkedin-hiq-data-scraping-cfaa-lawsuit-ninth-circuit-ruling", "anchor_text": "going to court", "paragraph_index": 65}, {"url": "https://twitter.com/_jphwang", "anchor_text": "twitte", "paragraph_index": 66}], "all_paragraphs": ["I had a shameful secret. It is one that affects a surprising number of people in the data science community. And I was too lazy to face the problem and tackle it head-on.", "I didn\u2019t know how to scrape data.", "For the majority of the time, it didn\u2019t impact my life \u2014 I had access to datasets, or other people had developed custom scrapers / APIs for what I needed.", "But every so often I would look at a website and wish that I could grab some of that sweet, original data to do some serious analysis.", "Relatively recently, I taught myself how to scrape websites with Python using a combination of BeautifulSoup, requests and regular expressions.", "I had a shameful secret\u2026 I didn\u2019t know how to scrape data.", "The whole process was far easier than I thought it was going to be, and as a result I am able to make my own data sets.", "So here, I wanted to share my experience so you can do it yourself as well. As with my other articles, I include the entire code in my git repo here so you can follow along or adapt the code for your own purposes.", "I assume you\u2019re familiar with python. Even if you\u2019re relatively new, this tutorial shouldn\u2019t be too tricky, though.", "You\u2019ll need BeautifulSoup, requests, and pandas. Install each (in your virtual environment) with a pip install [PACKAGE_NAME].", "You can find my code here: https://github.com/databyjp/beginner_scraping", "Once we learn how to scrape data, the skill can be applied to almost any site. But it is important to get the fundamentals right; so let\u2019s start somewhere that is easy, while being reflective of the real world.", "Many of you know that I\u2019m a sports fan \u2014 so let\u2019s get started by scraping our numerical data, which we will get from ScrapeThisSite.com.", "As the creative name suggests, this site is designed to practice scraping. Given that the data is in tables, it is also easy to check that the data has been scraped correctly.", "Before we do anything, we need the raw data. This is where the requests library comes in. Getting the data is straightforward, just taking a line of code as follows:", "It\u2019s that easy to get a copy of the web page. To check that the page has been loaded correctly, try:", "If you don\u2019t get an error, it should mean that the page has been downloaded correctly. How good is that? Now to the meat of the problem; getting data from our page.", "To scrape a site, we need to identify which part of the website holds the information that we are after. Although this is easy visually, it\u2019s annoyingly difficult to do in code.", "Your best friend in this task is the \u201cinspect element\u201d button on your browser. There are different ways to actually address the elements to be scraped, but that\u2019s secondary. First you need to identify the data being scraped.", "For instance, let\u2019s say that I would like to scrape this page.", "Before we go any further, take a look at the underlying code. Here\u2019s a small sample of it.", "Given that it\u2019s designed to be used by folks learning scraping, it\u2019s not all that difficult to read. Still, it\u2019s a pain correlating what you see here with what you see rendered.", "What you should be doing is to highlight the relevant element on the page, right-click and choose \u201cinspect element\u201d. This will bring up a layout similar to the below, although it will vary by browser.", "The code that will be brought up is the DOM (Document Object Model). Without getting too technical, this allows the code to be matched with the rendered end result.", "I highly recommend scrolling through the various elements here, selecting them, and generally observing the DOM\u2019s structure.", "More concretely, let\u2019s see what we would do to scrape the table showing the conference standings as below.", "An inspection of the elements reveal that we would like to obtain the contents of this table. Although it doesn\u2019t have a unique id, it does reside in a section with id value hockey.", "BeautifulSoup finds all of this easy, where parameter id can be passed on (id values in html are unique), or just a default parameter of tags.", "We can find all of this by:", "You can verify that there is only one table in that div, by running:", "We notice that the table includes header rows <th> as well as data rows <tr>. Let\u2019s scrape just the rows \u2014 and we also notice that each data row here has the class attribute with value team, so let\u2019s also filter by that attribute. This can be done by passing these through as a dictionary, allowing us to filter by whatever custom attributes people define!", "The element collected looks like this:", "What if we want to extract the team name from all this mess?", "I see that the name column is marked by its class attribute, so I can find the column by its tag (td) and the attribute.", "But since the results include a ton of whitespace:", "This is a job for regular expressions!", "If you\u2019re not sure what is going on there \u2014 the regex is substituting either the start of string ^ whitespace(s) \\s+ or | end of string $ whitespace(s), with nothing. You can throw in a re.M flag if it might go over multiple lines also.", "Fabulous. Now, how would we extend this to collect the entire table?", "To do this, we are going to identify the table, as we have done before, isolate each row of data, and then iterate through the row to collect the data elements.", "Upon review of the elements (see below screenshow), I notice that each column has a <td> tag with varying data-stat attributes, such as \u201cwins\u201d, \u201closses\u201d, \u201cwin_loss_pct\u201d, etc.", "We could hard-code these and loop through them manually. But, a more fun way is to actually grab a row, and generate a list of these column attributes, like below.", "This gets us a list of values:", "Using this snippet, we can just write a few lines of code to perform this task of grabbing the data. At a high level, the code loops through the rows, makes sure that the row is not a header row, loops through the columns to gather the data into a dictionary, and collates it all into a list.", "The entire resulting data is then put into a pandas DataFrame.", "That should be relatedly clear, but if that didn\u2019t make a lot of sense that\u2019s okay. Instead, take a look at the code below. The actual code is quite succinct and almost shorter than the my description!", "Let\u2019s wrap this up by scraping data from multiple pages. You might have noticed the links to various pages at the bottom.", "Once again, take a look at the element \u2014 and from what we find, we can decide on our strategy.", "We can deal with this in one of a number of ways, but this time, we will grab each of the links here and then scrape each resulting page.", "The way I chose to do this is to fine the ul tagged element, with class attribute value of pagination. Then I find each li element, grab the href targets (i.e. the links), and then convert it through a set to remove any duplicates.", "And all we need to do now is to create a function to generalise our task of scraping the page above, and concatenate the returned result from each page.", "Just for good measure, we can sort the results and save them as a file:", "Putting it all together, we get:", "And that\u2019s it! In just a few lines of code, we can scrape decades of data from this website to generate our own dataset.", "Just as a sanity check, let\u2019s plot some of this data with plotly to see the correlation between win percentage and a few different metrics. One really key thing to note is that before manipulating downloaded pages, we need to convert data types in our dataframes!", "Why? Well, as you might know already \u2014 more or less all website data is presented as text. And text, even if it is of numbers, cannot be manipulated. So, wins for example need to be manipulated to:", "Some visualisation libraries will make this conversion internally, but if you are manipulating the data in pandas you will certainly need to do this.", "(I won\u2019t show full the code here, but it is in my GitHub repo \u2014 in the scraper_mult_pages.py file.)", "So here\u2019s a graph comparing win percentages vs goals scored for the season:", "That\u2019s not terrible but doesn\u2019t look like that great a correlation. What about goals allowed (scored by opponent)?", "That\u2019s better! Maybe defence does win championships / games. We can combine these two to look at goal differentials:", "Woah! What\u2019s a great correlation if I do say so myself.", "Isn\u2019t that fantastic? Well, I think it is. You might not be that interested in this particular data, but the point is \u2014 you too now have all the tools you\u2019ll need to get data on whatever it is that holds your interest.", "I hope that was useful. I for one am glad that I bothered to learn how to do this.", "There\u2019s a lot more to scraping when you get down to it \u2014 using packages like selenium might become necessary for certain websites, and a more fully featured frameworks like scrapy might save you time on big projects. But there\u2019s a lot that we can already do by adapting these techniques.", "Go ahead \u2014 try it out!", "With one note: This site that I have used explicitly allows scraping, but not everybody does. Web scraping has been a somewhat controversial topic, one recent case even going to court. As always, be sensible and respectful of other people\u2019s sites and potential intellectual property.", "But just before you go \u2014 if you liked this, say hi / follow on twitter, or follow here for updates. ICYMI: I also wrote this article on visualising hidden information, using NBA assists data as an example:", "and more recently, this one showcasing some amazing data science portfolios that I\u2019ve found across the Internet.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Tech / Data science writer & educator; Python dev; sports analytics enthusiast. \ud83c\udde6\ud83c\uddfa \ud83d\udc26: @_jphwang"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2fe19889d6b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@_jphwang?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@_jphwang?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "JP Hwang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F964fe0870229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&user=JP+Hwang&userId=964fe0870229&source=post_page-964fe0870229----2fe19889d6b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "code"}, {"url": "https://scrapethissite.com/pages/", "anchor_text": "ScrapeThisSite"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "BeautifulSoup"}, {"url": "https://requests.readthedocs.io/en/master/", "anchor_text": "requests"}, {"url": "https://docs.python.org/3/library/re.html", "anchor_text": "regular expressions"}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "git repo here"}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "https://github.com/databyjp/beginner_scraping"}, {"url": "https://scrapethissite.com/faq/", "anchor_text": "ScrapeThisSite.com"}, {"url": "https://scrapethissite.com/pages/forms/", "anchor_text": "https://scrapethissite.com/pages/forms/"}, {"url": "https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction", "anchor_text": "DOM (Document Object Model)"}, {"url": "https://docs.python.org/3/library/re.html", "anchor_text": "regex"}, {"url": "https://github.com/plotly/plotly.py", "anchor_text": "plotly"}, {"url": "https://github.com/databyjp/beginner_scraping", "anchor_text": "GitHub repo"}, {"url": "https://github.com/SeleniumHQ/selenium", "anchor_text": "selenium"}, {"url": "https://github.com/scrapy/scrapy", "anchor_text": "scrapy"}, {"url": "https://www.theverge.com/2019/9/10/20859399/linkedin-hiq-data-scraping-cfaa-lawsuit-ninth-circuit-ruling", "anchor_text": "going to court"}, {"url": "https://www.scraperapi.com/blog/is-web-scraping-legal/", "anchor_text": "https://www.scraperapi.com/blog/is-web-scraping-legal/"}, {"url": "https://twitter.com/_jphwang", "anchor_text": "twitte"}, {"url": "https://towardsdatascience.com/how-to-visualize-hidden-relationships-in-data-with-python-analysing-nba-assists-e480de59db50", "anchor_text": "How to visualize hidden relationships in data with Python \u2014 analysing NBA assistsManipulating & visualising data with interactive shot, bubble & Sankey charts for insights with Plotly (code & data\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/these-data-science-portfolios-will-awe-and-inspire-you-mid-2020-edition-728e1021f60", "anchor_text": "These data science portfolios will awe and inspire you (mid-2020 edition)Use these to improve your own data science portfolio, learn new skills or discover new, interesting projects.towardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2fe19889d6b0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----2fe19889d6b0---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/programming?source=post_page-----2fe19889d6b0---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/python?source=post_page-----2fe19889d6b0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----2fe19889d6b0---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&user=JP+Hwang&userId=964fe0870229&source=-----2fe19889d6b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&user=JP+Hwang&userId=964fe0870229&source=-----2fe19889d6b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2fe19889d6b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2fe19889d6b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2fe19889d6b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@_jphwang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@_jphwang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "JP Hwang"}, {"url": "https://medium.com/@_jphwang/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F964fe0870229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&user=JP+Hwang&userId=964fe0870229&source=post_page-964fe0870229--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F743cf2a1623e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgot-data-how-i-taught-myself-how-to-scrape-websites-in-a-few-hours-and-you-can-too-2fe19889d6b0&newsletterV3=964fe0870229&newsletterV3Id=743cf2a1623e&user=JP+Hwang&userId=964fe0870229&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}