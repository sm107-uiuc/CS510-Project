{"url": "https://towardsdatascience.com/how-transformers-work-6cb4629506df", "time": 1683016077.1901639, "path": "towardsdatascience.com/how-transformers-work-6cb4629506df/", "webpage": {"metadata": {"title": "How do Transformers Work? An Introduction | Towards Data Science", "h1": "How Transformers Work", "description": "GPT-3, BERT, XLNet, all of these are the state of the art in natural language processing (NLP), all are transformers - we explain how they work here."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here", "paragraph_index": 46}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural Machine Translation by Jointly Learning to Align and Translate", "paragraph_index": 48}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer: A Novel Neural Network Architecture for Language Understanding", "paragraph_index": 49}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence Learning with Neural Networks", "paragraph_index": 51}, {"url": "https://arxiv.org/abs/1406.1078", "anchor_text": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "paragraph_index": 52}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs", "paragraph_index": 56}], "all_paragraphs": ["GPT-3, BERT, XLNet, all of these are the current state of the art in natural language processing (NLP) \u2014 and all of them use a special architecture component called a Transformer.", "Without a doubt, transformers are one of the biggest developments in AI in the past decade \u2014 bringing us ever closer to the unimaginable future of humanity.", "Nonetheless, despite their popularity, transformers can seem confusing at first. Part of this seems to be because many \u2018introductions\u2019 to transformers miss the context that is just as important as understanding the transformer model architecture itself.", "How can anyone be expected to grasp the concept of a transformer without first understanding attention and where attention came from? It\u2019s not possible.", "That is the motive for writing this article. We will intuitively grasp these systems' mechanics by visually exploring the flow of information through a Transformer model \u2014 touching on the math behind the magic.", "The attention mechanism predates transformers and was originally used to enhance the previous cutting-edge in NLP \u2014 recurrent neural nets (RNNs).", "Originally, these RNNs shared information through a single point of contact, which meant no direct connection between the majority of input and output units resulting in a bottleneck of information and limited performance for long-range dependencies.", "Two layers of simple, densely connected neural nets were added \u2014 one for the encoder consuming all of the input values \u2014 and the other for the decoder, which would pass the current timestep's hidden state to the attention operation [1].", "The attention operation produces an \u2018alignment\u2019 score, which is concatenated to the decoder hidden state \u2014 and a softmax operation performed to produce the output probabilities.", "Information is passed between the RNNs and encoder-decoders via the query, key, and value tensors. The query is delivered from the decoder hidden state, whereas the key-value tensors are delivered from the encoder hidden states.", "Where word representations were found to have high similarity (alignment), the attention operation would assign a high score \u2014 and those with low similarity a low score.", "This mechanism produced a behavior called attention, where units in the output layer could focus on particular, relevant units in the input layer.", "Before transformers and attention, we had RNNs. These networks were well suited to language tasks thanks to their recurrence.", "This recurrence allowed the neural nets to register whether a word was present in a sequence and consider the words surrounding it \u2014 and even words long preceding or following it. Because of this, the nuances of human language could be better represented.", "However, this new attention mechanism proved to be very powerful. Powerful enough for a 2017 paper called Attention Is All You Need to introduce a model without recurrence, replacing it entirely with attention [2] \u2014 the first transformer model.", "Three mechanisms made attention work without the need for recurrence \u2014 (1) positional encoding, (2) multi-head attention, and (3) self-attention.", "As we mentioned, before transformers, the cutting-edge in NLP were RNNs \u2014 in particular, RNNs using the long short-term memory (LSTM) unit.", "By nature, these networks were able to consider the order of words by passing the output states of one unit (at t-1) to the input states of the following unit (at t).", "Transformer models are not sequential like RNNs, and so a new method was required to save the now missing sequential semantics that previously allowed us to consider the order of words \u2014 and not just their presence.", "To keep the positional information of words, a positional encoding is added to the word embedding before entering the attention mechanism.", "The approach taken in the Attention Is All You Need paper was to produce a different sinusoidal function for every dimension in the embedding dimension.", "Remember before we said word2vec introduced to the concept of representing a word as many numbers in a 50 to 100-dimensional vector? Here, in the Vaswani et al. paper, they use the same idea but to represent a word's position.", "However, this time \u2014 rather than calculating the vector values using an ML model, the values are calculated using modified sinusoidal functions.", "Each vector index is assigned an alternating sine-cosine-sine function (index 0 is sine; index 1 is cosine). Next, as the index value increases away from zero towards d (the embedding dimensionality), the sinusoidal function frequency decreases.", "The final effect of this is like a clock with many hands. Each token we input is one time-step forward. The first hand (index 0, sine) begins pointing at 12 and rotates around the clock eight times in the 100 time-steps to reach the end of our sequence.", "The next hand (index 1, cosine) begins pointing at 6 and rotates around the clock slightly less than eight times (say 7.98 times) during the 100 time-steps.", "This rotation (the frequency) decreases as we increase the index number of our embedding dimension. On reaching the final index (512th in A. Vaswani et al. paper), the hand will have moved from 12 o\u2019clock to 0.001 seconds past 12 during the 100 time-steps.", "Plotting all 512 of these clock hands as sinusoidal looks like this:", "If we stop the clock at any one time-step, position 20, our positional encoding for that time-step is a vector of all the values being pointed at by the clock hands.", "We can take the same unruly sinusoidal plot from above onto an easier to understand heatmap:", "We can see the higher frequency in the lower embedding dimensions (left) , decreasing as the embedding dimension increases. Around dimension 24, the frequency has decreased so much that we no longer see any change in the remaining (alternating) sine-cosine waves.", "Finally, these positional encodings are added to the word embeddings.", "To add these two vectors together, they need to have an equal number of dimensions. Vaswani et al. used a 512-embedding vector for both the word and positional embeddings [2].", "Previously we were computing the attention using both the model input and output layers \u2014 self-attention allows us to use just one of these.", "So, where before the Key and Value were extracted from the input encoder and the Query from the output hidden state \u2014 we now get all three from a single input embedding array.", "Self-attention allows the inputs to interact with each other \u2014 meaning we can encode the \u2018alignment\u2019 between words within the same input. For example:", "The first image (left) shows the attention of the word \u2018it\u2019 is focused on \u2018animal\u2019 when the final word is \u2018tired\u2019. When the final word is changed to \u2018wide\u2019, the attention of \u2018it\u2019 shifts focus to \u2018street\u2019.", "This relationship between the words is encoding into the tensors when passed through the self-attention unit.", "Normal attention would only perform this operation between the input and output sequences \u2014 as represented in the above heatmap.", "Multi-head attention was a crucial factor in the success of the first transformer. Without it, results showed it to perform worse than it\u2019s recurrent predecessors [2].", "After we have produced our position encoding word embedding tensors \u2014 we pass them into multi-head attention units. The \u2018multi-head\u2019 part means that we are actually running multiple self-attention units in parallel.", "Once the multiple attention units have processed our tensors in parallel, the results are concatenated together, resized through the linear unit \u2014 and passed on to the next step.", "Now, the power in using multi-head attention comes from including several representative subspaces at once. This allows one subspace to identify the relationship between it and tired, and another on the relationship between it and dog.", "Multi-head attention is not a requirement in transformer models. BERT \u2014 one of the most powerful NLP models in the world, stacks single head attention units and feed-forward units in series \u2014 but does not make use of multi-head attention [4].", "That\u2019s all for this introduction to transformers. We\u2019ve covered:", "All of these are very abstract concepts and can be difficult to wrap our heads around. However, they form the foundation of the modern-day standard in NLP and produce truly next-level results so they are worth committing the time to.", "I hope this article has helped you better understand these concepts and how they fit together in a Transformer. If you\u2019d like more, I post programming/ML tutorials on YouTube here!", "If you have any questions, ideas, or suggestions \u2014 get in touch via Twitter or in the comments below.", "[1] D. Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate (2015), ICLR", "[3] J. Uszkoreit, Transformer: A Novel Neural Network Architecture for Language Understanding (2017), Google AI Blog", "[4] J. Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019), ACL", "[6] I. Sutskever et al., Sequence to Sequence Learning with Neural Networks (2014), NeurIPS", "[7] K. Cho et al., Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014), EMNLP", "If you\u2019d like to better understand the origins of transformers throughout the past decade of NLP, I cover exactly that in this article:", "*All images by the author except where stated otherwise", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance ML engineer learning and writing about everything. I post a lot on YT https://www.youtube.com/c/jamesbriggs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6cb4629506df&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jamescalam.medium.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "James Briggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----6cb4629506df---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jplenio?utm_source=medium&utm_medium=referral", "anchor_text": "Johannes Plenio"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here"}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer: A Novel Neural Network Architecture for Language Understanding"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence Learning with Neural Networks"}, {"url": "https://arxiv.org/abs/1406.1078", "anchor_text": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"}, {"url": "https://bit.ly/nlp-transformers", "anchor_text": "\ud83e\udd16 70% Discount on the NLP With Transformers Course"}, {"url": "https://towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe", "anchor_text": "Evolution of Natural Language ProcessingIntuitive visual explanations of the past decade of NLPtowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6cb4629506df---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6cb4629506df---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----6cb4629506df---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----6cb4629506df---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/software-development?source=post_page-----6cb4629506df---------------software_development-----------------", "anchor_text": "Software Development"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&user=James+Briggs&userId=b9d77a4ca1d1&source=-----6cb4629506df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&user=James+Briggs&userId=b9d77a4ca1d1&source=-----6cb4629506df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6cb4629506df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6cb4629506df---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6cb4629506df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6cb4629506df--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6cb4629506df--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6cb4629506df--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6cb4629506df--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Briggs"}, {"url": "https://jamescalam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.6K Followers"}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F75e31c56d187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-transformers-work-6cb4629506df&newsletterV3=b9d77a4ca1d1&newsletterV3Id=75e31c56d187&user=James+Briggs&userId=b9d77a4ca1d1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}