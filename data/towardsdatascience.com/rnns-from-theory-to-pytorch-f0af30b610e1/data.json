{"url": "https://towardsdatascience.com/rnns-from-theory-to-pytorch-f0af30b610e1", "time": 1683017983.506785, "path": "towardsdatascience.com/rnns-from-theory-to-pytorch-f0af30b610e1/", "webpage": {"metadata": {"title": "RNNs from theory to PyTorch. Let me show you what RNNs are, where\u2026 | by Tiago Miguel | Towards Data Science", "h1": "RNNs from theory to PyTorch", "description": "Most types of neural networks are built to make predictions on samples whose targets they have been trained on. A prime example is the MNIST dataset. A regular neural network like the MLP knows there\u2026"}, "outgoing_paragraph_urls": [{"url": "https://dictionary.cambridge.org/us/dictionary/english/sequence", "anchor_text": "Cambridge University Press", "paragraph_index": 3}, {"url": "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/?source=post_page---------------------------", "anchor_text": "this website", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN", "anchor_text": "documentation", "paragraph_index": 48}], "all_paragraphs": ["Most types of neural networks are built to make predictions on samples whose targets they have been trained on. A prime example is the MNIST dataset. A regular neural network like the MLP knows there are 10 digits and it is only based on them that it makes predictions even if the images are very different from those the network has been trained on.", "Now, imagine we could leverage such a network with a sequential analysis by providing a sequence of 9 ordered digits and let the network guess the 10th. The network would not only know how to discriminate the 10 digits but would also know that, in a sequence of 0 to 8, the next digit is most likely a 9.", "When analyzing sequenced data, we understand that most often than not, the elements within that sequence are somehow related, meaning they depend on one another. Because of this, we need to take into account each element to understand the sequence\u2019s idea.", "The Cambridge University Press defines a sequence as either \u201cthe order in which things or events follow each other\u201d or, and most importantly, \u201ca series of related things or events\u201d. To adjust this definition into the scope of Deep Learning, sequence is a set of data that contains trainable context, and removing some elements may render it useless.", "But what does a sequence contain? What pieces of grouped data could have context? And how can we extract context to leverage the power of neural networks? Before getting into the neural network itself, let me show you two types of problems frequently solved with Recurrent Neural Networks (RNNs).", "This first example is a Timeseries Forecasting problem, where we train a neural net with a sequence of existing numerical values (blue) in order to predict future time steps (red).", "If we make a sequence out of the monthly energetic expenditure of a household throughout the years, we can see that there is a rising sinusoidal trend with a sudden fall.", "The sinusoidal part\u2019s context may be the different energetic requirements throughout the year from summer to winter and back to summer again. The rising energetic expenditure could come from using more electrical appliances and devices or switching to more powerful ones that may require more energy. The sudden fall\u2019s context could mean one person grew old enough to leave home and the energy required by that person is no longer there.", "The better you understand the context, the more information you can feed the network with to help it understand the data, generally through concatenation of the input vector. In this case, for every month, we could concatenate three more values to the energy that would be the number of electric appliances and devices, their energetic efficiency and the number of people the household contains.", "Mary rides the bycicle, the bycicle is ____.", "A second example is a Natural Language Processing problem. This is also a good example because the neural net must take into consideration the context provided by the existing sentence to complete it.", "Let\u2019s say our network is trained to complete sentences with possessive pronouns. A well trained network would understand the sentence is built in the third person singular and that Mary is most likely a feminine name. So, the predicted pronoun should be \u201chers\u201d instead of the masculine \u201chis\u201d or the plural \u201ctheirs\u201d.", "Now that we have seen two examples of sequenced data, let\u2019s explore the network going about its forward and backward propagation.", "As we have seen, RNNs extract information out of sequences to improve their predictive power.", "A simple RNN diagram is represented above. The green node lets in some input x^t and outputs some value h^t that is also fed to the node again containing information gathered from the input. Whatever pattern there is in what is being fed to the node, it learns it and keeps that information for the next input. The superscript t stands for a time step.", "There are some variations to the neural network\u2019s configuration based on the shape of the input or output, we\u2019ll understand what happens inside the nodes later.", "Many-to-one configuration is when we feed several inputs in different time steps to get one output, which could be the sentiment analysis captured in various frames of a movie scene.", "One-to-many uses one input to obtain several outputs. For instance, we could encode a poem conveying a certain emotion using a many-to-one configuration and use the one-to-many configuration to create new lines of the poem with that same emotion.", "Many-to-many uses several inputs to obtain several outputs like using a sequence of values like in the energetic usage and forecast twelve months in the future instead of only one.", "The stacked configuration is just a network with more than one hidden layer of nodes.", "To understand what\u2019s happening inside the nodes of the neural network, we\u2019ll use a simple dataset as a Timeseries Forecasting example. Bellow is the full sequence of values and its restructuring as a training and a testing dataset.", "I took this example from this website which is a great resource for Deep Learning in general. Now, let\u2019s separate the datasets into batches.", "I don\u2019t show it here, but don\u2019t forget the dataset should be normalized. It is important since neural networks are sensitive to the magnitude of the dataset values.", "The idea is to predict one value into the future. So, say we pick the first row of the batch: [10 20 30], after training our network we should get the value 40. To test the neural net, one could feed the vector [70 80 90] and expect to obtain a value close 100 if the network is well trained.", "We\u2019ll use the many-to-one configuration feeding the three time steps of each sequence separately. When using recurrent networks, the input is not the only value that goes into the network, there is also a hidden array that is the structure that will transport the context of the sequence from node to node. We initialise it as an array of zeros and it is concatenated to the input. Its dimensionality (1 x 2) is a personal choice, just to use a different dimension from the step input of 1 x 1.", "Taking a closer look, we can see that the weight matrix is separated into two parts. The first deals with the input creating two outputs and the second deals with the hidden array creating also two outputs. These two sets of outputs are then added together and a new hidden array is obtained, containing information from the first input (10), and is fed to the next time step input (20). It should be noted that the weight and bias matrices are the same from time step to time step.", "The global input vector X^t, the weight matrix W and the bias matrix B and the calculation of the hidden arrays are represented above. Only one step missing to finish the forward pass and that is using the last hidden array to predict the next time step into the future with a linear layer to calculate the final result. The full network has the following form:", "Can you see the many-to-one configuration? We feed three inputs from a sequence, their context is captured by the weight and bias matrices and stored in a hidden array that is updated with new information at every time step. Finally, that context stored in the final hidden array goes through another set of weights and biases and one value is outputed after all time steps of the sequence were inputed into the network.", "We can see the linear form of the final hidden state and the weight and bias matrices of the linear layer as well as the prediction value calculation (y_hat).", "Now, this was the forward propagation of the RNN but we still haven\u2019t seen the backward pass.", "The backward propagation is a very important step of the training of every neural network. Here, the error between the predicted output and the real value is propagated towards the neural net with the goal of improving the weights and biases in order to obtain better predictions with every iteration.", "Most times, this step gets overlooked because of its complexity. I\u2019ll present you an explanation as simple as I can make it while mentioning the important bits.", "The backward pass is a series of derivations using the chain rule of calculus from the loss until all the weight and bias parameters. This means that we ultimately want the following values (or arrays if they are multi dimensional as is the case):", "I recommend reading about Gradient Descent if you are not familiar with the mathematical meaning of first derivatives but in essence, when the first derivative is a zero, generally it means we found a minimum value in our system and ideally we wouldn\u2019t be able to improve it any further.", "A little side note here: the zero could also be maximum values, which are unstable and an optimization shouldn\u2019t go there, or a saddle point which in itself isn\u2019t very stable position as well. And the minimum may be global (lowest minimum of the function) or local. This doesn\u2019t really matter to my explanation, but if you want to know more about it you can look it up!", "What you see in the picture are two balls rolling down the valley. Visually, the first derivative gives us a magnitude of the inclination of the hills. If we travel the direction in which the W axis increases (left to right), the inclination will be negative (going down) for the green ball and positive (going up) for the red ball.", "Read the next paragraph carefully and go back to the figure as needed.", "If we want the loss to be minimal, we want the balls to go to the lowest point in the valley. W represents the values of the weights and biases so if we are in the position of the green ball we will subtract some portion of the negative derivative (making it positive) to the W position of the green ball moving it right and subtract some portion of the positive derivative (making it negative) to the W position of the red ball moving it left in order to approach both balls to the minimum.", "Mathematically what we have is the following:", "The \u03b7 adjusts the proportion of the derivative we use to update the weights and biases.", "Now, to get on with the backward pass issue. I\u2019ll present the chained derivatives from the loss to all the parameters and we\u2019ll see what each derivative represents. It is important to have the equations from the layers presented above along with their parameter\u2019s matrices in mind.", "One thing to bear in mind is that the shapes of the four first derivative arrays we are looking for need to be the same as the parameters we will update. For example, the shape of the array dL/dW_h must be the same as the weight array W_h. The superscripted T means the matrix is transposed.", "We have found the derivatives of the final linear layer\u2019s parameters. Knowing that dL/dy_hat has shape 1 x 1 and h\u00b3 2 x 1 and that for matrix multiplication to happen the inner dimensions must be equal, h\u00b3 needs to be transposed. We obtain a matrix that is 1 x 2 and since W_h is 2 x 1, we need a second transposition.", "Now, the recurrent backward pass is slightly different. Despite the parameters are the same in all the three time steps, we still back propagate through all of them and for each hidden node, we calculate a new dL/dW_x and dL/dB_x and add to them together.", "The numbered superscript in the derivatives\u2019 denominator relates to the time step. We calculated the parameters for the third time step and now we need to go back to the second time step.", "We have gone back to dL/dh\u00b2, this means we back propagated an entire recurrent hidden node and for that we needed to deconcatenate the input and the hidden state from the global input derivative. The next two hidden states\u2019 calculations are the same so I won\u2019t go over them.", "Now we add the derivatives together and apply the Gradient Descent equation we saw earlier to update the parameters and the model is ready for another iteration. Let\u2019s see how to build a simple RNN with PyTorch.", "Using PyTorch makes it very simple since we don\u2019t really need to worry about the backward pass. However, I still believe it is important to know how it works even if we don\u2019t use it directly.", "Moving on, if we refer to PyTorch\u2019s documentation, we can see they already have an RNN object ready to be used. When defining it, there are two essential parameters:", "input_size \u2014 The number of expected features in the input x", "hidden_size \u2014 The number of features in the hidden state h", "The input_size is 1 since we are using one time step of each sequence (eg. 10 from the sequence 10, 20, 30) at a time and the hidden_size is 2 since we obtain a hidden state containing two values.", "Defining the n_layers parameter to 2 would mean we have a stacked RNN with two hidden layers.", "Also we are going to define the parameter batch_first to True. This means the batch dimension in input and output comes first (don\u2019t mistake with Input and Output)", "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence.", "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch.", "The inputs of the RNN should be an input array of shape 1 x 3 x 1. The sequence contains three time steps that are 10, 20 and 30 for the first batch of the dataset. From each batch, an input of size one will be fed three times to the network being the three time steps of the sequence.", "The hidden state h_0 is our first hidden array that we feed the network along with the first time step input of shape 1 x 1 x 2.", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.", "The output contains all the hidden states calculated by the neural network at each time step of shape 1 x 3 x 2 and h_n is the hidden state from the last time step. This is useful to keep because, if we choose to use a stacked recurrent network, this will be the hidden state that will be fed at the first time step and has shape 1 x 1 x 2.", "All these arrays are represented in the example above and can be seen in the RNN diagram. One more thing to note is, using recurrent networks and the specific example of Timeseries Forecasting, setting the num_directions to 2 would mean predicting into both the future and the past. That type of configuration will not be considered here.", "I\u2019ll leave a piece of code where I implement the RNN and how to train it. I\u2019ll also leave it to you to use it as you need with the dataset you want. Don\u2019t forget to normalise your data and to create a dataset and a dataloader before using the network.", "To end this story with a brief summary of what was discussed here, we started by seeing two types of problems commonly solved with recurrent networks being Timeseries Forecasting and Natural Language Processing.", "Later we saw examples of some typical configurations and an actual example whose objective was to predict one time step into the future using the many-to-one configuration.", "In the forward pass we understood how the inputs and the hidden states interact with the weights and biases of the recurrent layers and how to use the information contained in the last hidden state to predict the next time step value.", "The backward pass is just the application of the chain rule from the loss gradient with respect to the predictions until it becomes with respect to the parameters we want to optimize.", "Finally, we went through part of the documentaion of PyTorch on RNNs and discussed the most important bits in order to build a basic recurrent network.", "Thank you for reading! Maybe you got something out of this lengthy story. I write them to help me understand new concepts and hopefully help others too.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Chemical Engineer enthusiast in learning programming with python, data science and machine/deep learning skills"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff0af30b610e1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tiagomiguelrs.medium.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": ""}, {"url": "https://tiagomiguelrs.medium.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Tiago Miguel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fafc1718125af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&user=Tiago+Miguel&userId=afc1718125af&source=post_page-afc1718125af----f0af30b610e1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sherihoo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Sheri Hooley"}, {"url": "https://unsplash.com/s/photos/connect-with?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://dictionary.cambridge.org/us/dictionary/english/sequence", "anchor_text": "Cambridge University Press"}, {"url": "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/?source=post_page---------------------------", "anchor_text": "this website"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN", "anchor_text": "documentation"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----f0af30b610e1---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/theory?source=post_page-----f0af30b610e1---------------theory-----------------", "anchor_text": "Theory"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----f0af30b610e1---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/rnn?source=post_page-----f0af30b610e1---------------rnn-----------------", "anchor_text": "Rnn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&user=Tiago+Miguel&userId=afc1718125af&source=-----f0af30b610e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&user=Tiago+Miguel&userId=afc1718125af&source=-----f0af30b610e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff0af30b610e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f0af30b610e1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f0af30b610e1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f0af30b610e1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f0af30b610e1--------------------------------", "anchor_text": ""}, {"url": "https://tiagomiguelrs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tiagomiguelrs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tiago Miguel"}, {"url": "https://tiagomiguelrs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "31 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fafc1718125af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&user=Tiago+Miguel&userId=afc1718125af&source=post_page-afc1718125af--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fafc1718125af%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frnns-from-theory-to-pytorch-f0af30b610e1&user=Tiago+Miguel&userId=afc1718125af&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}