{"url": "https://towardsdatascience.com/advanced-use-cases-for-recommendation-engines-4a420b14ab4e", "time": 1683001864.622473, "path": "towardsdatascience.com/advanced-use-cases-for-recommendation-engines-4a420b14ab4e/", "webpage": {"metadata": {"title": "How to build an Advanced Recommendation Engine | by Jan Teichmann | Towards Data Science", "h1": "How to build an Advanced Recommendation Engine", "description": "It\u2019s important to remember, you should not start with the advanced use-cases for recommendation engines. After the quick and simple implementation of version 1, your recommendation engine becomes\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/how-to-build-a-recommendation-engine-quick-and-simple-aec8c71a823e", "anchor_text": "Part 1", "paragraph_index": 8}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html#_parameters", "anchor_text": "documentation", "paragraph_index": 16}, {"url": "https://mahout.apache.org/users/algorithms/intro-cooccurrence-spark.html", "anchor_text": "Mahout website", "paragraph_index": 31}, {"url": "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html", "anchor_text": "documentation", "paragraph_index": 37}, {"url": "https://spark.apache.org/docs/latest/ml-features#locality-sensitive-hashing", "anchor_text": "LSH", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "Random Projection", "paragraph_index": 38}, {"url": "https://gombru.github.io/2019/04/03/ranking_loss/", "anchor_text": "triplet loss", "paragraph_index": 60}, {"url": "https://www.linkedin.com/in/janteichmann/", "anchor_text": "https://www.linkedin.com/in/janteichmann/", "paragraph_index": 63}], "all_paragraphs": ["It\u2019s important to remember, you should not start with the advanced use-cases for recommendation engines. After the quick and simple implementation of version 1, your recommendation engine becomes very bespoke towards your business problem, products and users. To build such a bespoke model, you will need the feedback loop and baseline from version 1 to decide on the directions of improvements for your recommendation engine. With bespoke solutions, agile is the best methodology to get there successfully. In practice this means you build version 1.1 and 1.2 and 1.3 long before you start any work on version 2.", "For your version 1 recommendation engine you have multiple options depending on your starting point:", "Congratulations, after you deployed version 1 of your recommendation engine it\u2019s time to celebrate your achievement! You have your first recommendation engine live and it\u2019s performing good enough that stakeholders want you to improve it rather than take it offline. \ud83c\udf89", "Unfortunately, there are no silver bullets for how you improve your recommendation engine from here. You are going to slowly build a highly bespoke solution and the path to success in reality looks much more like a baptism of fire. You will deliver that solution via a lot of experimentation and learning from what did not work (failure?), probably under growing pressure from stakeholders, your boss and angry users.", "The issues you will face with version 1:", "Let\u2019s have a look at what we actually built in Part 1 and how it compares to more sophisticated recommendation systems:", "We started with the simplest type of models: a recommendation engine based on item-to-item co-occurrence. This model is very similar to a classifier in terms of how to interpret the scores and we are still a long way from personalised ranking of items. This is a common misunderstanding which can harm the user experience when integrating recommendations into products assuming that the scores reflect a personalised ranking or consideration/relevance score.", "If you followed the previous part of building a quick and simple collaborative filtering recommender system with Elasticsearch than your version 1 model might not be perfect but it is working to some degree. Before changing your entire implementation it might be worth trying to simply improve the scoring requests for the current version.", "What you have build in Part 1 is a stateless model. Stateless models do not remember or care about preceding events. Every user interaction and scoring request is fully independent. Therefore, stateless models are really easy to manage and allow, for example, horizontal scaling behind a simple load balancer.", "This provides an obvious first improvement: incorporate a state into your model. Users interact with a series of products and content on your site. Adding a state memory to your model allows you to query Elasticsearch with an interaction series for a user which expands the possibilities of matches to work with. One the one hand, you could simply use this to expand the amount of potential co-occurrence in your foreground data set. On the other hand, you can use this in more complex rules to filter relevant recommendations, e.g. remove recommendations a user already saw previously.", "However, you want to avoid adding the state to your model itself. Ideally, you always decouple the state manager from the rest of the model. The state manager should be an independent micro-service which enriches a stateless incoming request from, i.e. your website or app, with the previous interactions of that user using some form of cache. Managing the state as an independent micro-service is a great design principle and is also part of the Rendezvous Architecture design which I blogged about previously (I called it the Model Data Enricher in that blog post):", "Redis is a great solution for handling the state for a recommendation system. It\u2019s fast, lightweight and exists as a fully managed service in any cloud, e.g. AWS ElasticCache for Redis. The below code uses a sliding window TTL to store user interactions and automatically deletes old interactions from the cache. Feel free to adapt it towards your needs:", "Upside of this option: You get higher volumes of co-occurrence signals and have the opportunity to filter out recommendations a user has already seen. This can boost quantity and quality without much additional work.", "Downside of this option: Don\u2019t expect miracles: you still just work with the same original data. There are diminishing returns and only so much additional value you can extract from the same data.", "Because there is only so much additional value you can squeeze from the same data we have another obvious way to improve our recommendation system. Your users probably interact with your websites, apps and products in many different ways:", "Version 1 of your recommendation engine used just one user interaction and is based on implicit feedback to maximise the volume of signals over the quality of recommendations. It\u2019s worth incorporating additional data in particular to improve the quality of recommendations and to capture desired business logic.", "The easiest option is to add an additional document type to your existing Elasticsearch cluster following exactly the same logic as you did with your first metric. Document types in Elasticsearch are comparable to tables in a RDBs. You then run 2 parallel queries against each document type respectively which can be independently tweaked reflecting the quality of the metrics. You achieve that by adjusting the \u201cmin_doc_count\u201d setting in the significant_terms aggregation of Elasticsearch or by switching the statistic used to define a significant recommendation. You can read about the available options in the documentation and check my other article for the code examples:", "Upside of this option: Easy to implement as you just re-purpose existing solutions and code to ingest additional data as an independent document type. This allows for a wide range of options to build a complex rules engine on top of the Elasticsearch recommendations to incorporate desired business logic, e.g. bias towards promotions.", "Downside of this option: Increasing cost of the Elasticsearch cluster.", "As you add more and more metrics to your model you will notice that not only the cost of your Elasticsearch cluster spirals out of control, the time and CPU resources to process signals in your rules engine are equally on a steep increase. Your rules engine translating signals into final recommendations starts to get significantly complex. A review of the rules engine will probably conclude that you are processing complex relations between your data and there are much better solutions for these use-cases: It is probably time to consider a Graph Database!", "A Graph Database is the perfect solution for highly relational data, e.g. high volumes of relations as well as complex relations. Instead of resolving these relations on the fly with every single query, a graph DB materialises these relations as edges in your graph to significantly speed up queries based on the relations between data (nodes). There are many vendors for graph DBs and it\u2019s beyond the scope of this blog post to make any recommendations. This might be the topic of an upcoming blog post.", "Upside of this option: Can handle complex relations at speed opening up complex use-cases with complex business logic.", "Downside of this option: You have to translate your recommendation logic into graph queries.", "Collaborative filtering is build on top of co-occurrences within user interaction data. If your business has no robust definition of a single user then it\u2019s difficult to create a good history matrix capturing long term user interactions.", "As a consequence, there will be much fewer signals in the co-occurrence matrix which is built from that history of user interactions.", "This could be the case when your users use multiple devices and have little incentives to create an account or log-in to use your services. If you have a big proportion of anonymous traffic on your site it\u2019s worthwhile to look at a probabilistic model to identify a single user over time and across devices.", "Gartner termed cross-device identification (XDID) the emerging technology to watch and data science is there to help. The different options to identify users are: Account Sign-ins, Cookie Identification, Device Fingerprinting and IP Matching.", "IP Matching is a probabilistic option of lower quality to identify single users. However, IPs are cross-device available. There are problems with dynamic and shared IPs like in the case of 4G networks or public Wifi. Regardless, IPs are the most viable option. I wrote an entire article about this topic of using a Graph of IPs and Timestamps to match single users with Spark\u2019s GraphX engine:", "Upside of this option: Matching single users across devices can improve the quality of co-occurrence signals significantly and has use-cases way beyond recommendations alone.", "Downside of this option: Single user matching is a complex topic in its own right. It is as much an interesting data science problem as it is a difficult data engineering challenge to solve.", "So far we have always used Elasticsearch as the backend to compute the signals for recommendations on the fly. This is valuable in the early stages of a recommendation road map to allow flexible experimentation e.g. with the quantity vs quality trade-off. The downside of using Elasticsearch in that way is that every query has a significant CPU cost. If collaborative filtering works well for your use-cases then it\u2019s probably best to choose a better collaborative filtering algorithm and compute the recommendation signals more efficiently in batch rather than on the fly. This way, we keep Elasticsearch only as our presentation layer for serving recommendations to users.", "We can simply calculate the item-item co-occurrence signals in batch. This can be done with Mahout\u2019s spark-itemsimilarity implementation which exists with a CLI. This makes it as simple as calling the spark programme in the Mahout library and provide it with the paths to the relevant data. You can find detailed documentation on the Mahout website. You also see the similarity of Mahout\u2019s solution architecture and ours. We use Elasticsearch rather than Solr and Redis as our cache for user interactions. The rest follows the very same solution architecture.", "Upside of this option: Cheaper batch scoring version while still simple to use and operate.", "Downside of this option: Not much flexibility and sophistication. This will bring your Elasticsearch costs under control but won\u2019t be improving the performance of your recommendations if that was a concern of your version 1 implementation.", "After we decided to create our recommendations in batch we decoupled the training, scoring and serving of our model into multiple steps. This also opens the door for us to look at more sophisticated recommendation algorithms rather than just co-occurrence based models we used so far. The next step up would be a recommendation system based on a latent factor model using matrix factorisation.", "A latent factor model would allow us to create both (1) an item-item model using a similarity of product factors and (2) an item-user model.", "A good solution for a batch latent factor model is the ALS (alternating least squares) implementation in Spark, a matrix factorisation technique for recommendation engines. It\u2019s called alternating least squares because the algorithm alternates between improving the item and the user factors respectively: The algorithm first fixes the user factors and runs gradient descent on the item factors. It then switches and fixes the item factors to improve the user factors. The ALS algorithm is very scalable and can run in a distributed fashion in parallel and processes large datasets with Spark ML very efficiently.", "You can find an example for implementing ALS with Spark ML in the documentation. We still use the same solution architecture as discussed before and Spark ML provides the methods for creating recommendations for the user-item model.", "However, we have to write some custom code to calculate the similarity of product factors for an item-item model. Calculating an all-pairs similarity between products is not scaling well to big product catalogues. The growing number of combinations O(n^2) results very quickly in overly costly shuffle operations and infeasible compute times. Spark offers a solution with Locality Sensitive Hashing (LSH), a much more efficient approach to identifying approximate nearest neighbours. LSH uses a special hash function to reduce the dimensionality of the data while increasing the probability of a hash collision the more similar the data is. This means that similar data is likely to end up in the same bucket and not in buckets with dissimilar data. LSH is a probabilistic approximation and provides a trade-off between speed and accuracy. For our problem of clustering item latent factors for an item-item recommendation model we use the Random Projection as our hashing function which approximates the cosine similarity for bucketing of our product vectors.", "The following code is an example of using Spark ML ALS for collaborative filtering and LSH to create an item-item model based on item similarity:", "The full code example can be found here:", "Upside of this option: Cheaper and scalable batch model providing both user-item and item-item models.", "Downside of this option: More complex model with many more hyper parameters to tune for ALS and LSH.", "So far your models produce recommendations but this is not necessarily the only way to use your behavioural data and latent factor models. Your company probably has a CRM or Marketing team which would love some segmentation for their targeting. Behavioural data can be a real asset for both product and user segmentation.", "You can read about using the data and algorithms of your current recommendation engine for segmentation in my previous article dedicated to the topic:", "Upside of this option: you get more stakeholders involved and create a bigger business impact with your model. This makes it less likely that the business shelves the project.", "Downside of this option: you have more stakeholders involved with their unique requirements and growing pressure for improvements. The data science team needs to be able to scale with that growing demand.", "Collaborative filtering has been a powerful recommendation algorithm because we don\u2019t need to know our products or users. On the one hand, we learn recommendations based on explicit or implicit feedback alone. But on the other hand, each recommendation is tied to that specific product or user. This has some downsides:", "To overcome these problems we have to tie the signals and indicators for our recommendations against features which can outlive a product or user itself and generalise across a wider range of similar products and users. We need to build up features and knowledge about our products and users:", "It\u2019s usually easier to build features describing products than users:", "I published a previous blog post about a great example of creating geographic area embeddings using search embeddings:", "After we have created a wide range of features to describe products and potentially users we can use a Neural Network to learn signals between our recommendations based on collaborative filtering and our new product features.", "We use a Siamese neural network to learn a mapping of product or user features from an item embedding space to a recommendation embedding space. The recommendation embedding of items or users have much lower dimensionality than the NN input and a desired behaviour of being similar as measured by their cosine similarity in the recommendation embedding space. The twist in Siamese neural networks is that the weights of both networks are identical and the gradient descent of both neural networks is coupled together. In practice, this is achieved by using a single neural network and scoring two inputs in sequence:", "We use the cosine similarity as the output of our Siamese neural network model:", "Otherwise, our neural network is not much different from any other neural network. To train our Siamese neural network we need to define a loss function: the contrastive loss optimises the absolute cosine similarity between a pair of inputs to maximise the cosine similarity for positive pairs, but minimise the similarity for negative pairs.", "The margin (between 0 and 2) can be used as a weight for how much the training should focus on improving dissimilarity over similarity of pairs.", "We also need labelled data for the training. We can obtain labels from our Elasticsearch cluster we used for version 1. Items in the foreground meeting our quality requirements are positive pairs while other items can act as negative pairs.", "The training loop for the Siamese Neural Network is straight forward:", "The final plot shows the trained 2D recommendation embedding space of similar films from the Spark example movie lens dataset:", "Upside of this option: No more cold start issue!", "Downside of this option: You need a wide range of useful item embeddings as input to your Siamese Neural Network. Defining the training labels requires experimentation: for your business problem, what defines a good positive or negative item pair? The contrastive loss function is simple to implement but an imbalance of positive and negative item pairs might require a switch to a more complex loss function, e.g. triplet loss.", "Jan is a successful thought leader and consultant in the data transformation of companies and has a track record of bringing data science into commercial production usage at scale. He has recently been recognised by dataIQ as one of the 100 most influential data and analytics practitioners in the UK.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m a Data Scientist by Profession; Techie, Geek and Innovator with Passion. https://www.linkedin.com/in/janteichmann/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4a420b14ab4e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jan.teichmann?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jan.teichmann?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Jan Teichmann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F941c2fa7cfd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&user=Jan+Teichmann&userId=941c2fa7cfd&source=post_page-941c2fa7cfd----4a420b14ab4e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/how-to-build-a-recommendation-engine-quick-and-simple-aec8c71a823e", "anchor_text": "How to build a Recommendation Engine quick and simplePart 1: an introduction, how to get to production in a week and where to go after thattowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-to-build-a-recommendation-engine-quick-and-simple-aec8c71a823e", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/rendezvous-architecture-for-data-science-in-production-79c4d48f12b", "anchor_text": "Rendezvous Architecture for Data Science in ProductionHow to build a cutting edge data science platform to solve the real challenge in Data Science: Productionisation.towardsdatascience.com"}, {"url": "https://gitlab.com/jan-teichmann/redis_user_interaction_cache", "anchor_text": "Jan Teichmann / redis_user_interaction_cacheGitLab.comgitlab.com"}, {"url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html#_parameters", "anchor_text": "documentation"}, {"url": "https://towardsdatascience.com/how-to-build-a-recommendation-engine-quick-and-simple-aec8c71a823e", "anchor_text": "How to build a Recommendation Engine quick and simplePart 1: an introduction, how to get to production in a week and where to go after thattowardsdatascience.com"}, {"url": "https://tinkerpop.apache.org/docs/current/tutorials/the-gremlin-console/", "anchor_text": "https://tinkerpop.apache.org/docs/current/tutorials/the-gremlin-console/"}, {"url": "https://pxhere.com/en/photo/1444327", "anchor_text": "Pxhere"}, {"url": "https://towardsdatascience.com/single-userid-matching-for-anonymous-users-across-devices-with-graphx-72fe111ac44b", "anchor_text": "Single UserID Matching for Anonymous Users Across Devices with GraphXHow to implement a probabilistic session similarity to create a single user ID for cross-device and anonymous user\u2026towardsdatascience.com"}, {"url": "https://mahout.apache.org/users/algorithms/intro-cooccurrence-spark.html", "anchor_text": "Mahout website"}, {"url": "https://mahout.apache.org/docs/latest/algorithms/recommenders/", "anchor_text": "https://mahout.apache.org/docs/latest/algorithms/recommenders/"}, {"url": "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html", "anchor_text": "documentation"}, {"url": "https://spark.apache.org/docs/latest/ml-features#locality-sensitive-hashing", "anchor_text": "LSH"}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "Random Projection"}, {"url": "https://gitlab.com/snippets/1915919", "anchor_text": "Spark ALS item-item similarity ($1915919) \u00b7 SnippetsGitLab.comgitlab.com"}, {"url": "https://towardsdatascience.com/data-science-powered-segmentation-models-ae89f9bd405f", "anchor_text": "AI meets marketing segmentation modelsNew gold standard: using Machine Learning to derive a user and product segmentation from behavioural data for a\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/location-location-location-ec2cc8a223b1", "anchor_text": "Location Location LocationGeographic area embeddings at Zoopla.co.uk using Machine Learning (Skip-Gram) and a little black magic wizardry.towardsdatascience.com"}, {"url": "http://twitter.com/staticmethod", "anchor_text": "@staticmethod"}, {"url": "https://gombru.github.io/2019/04/03/ranking_loss/", "anchor_text": "triplet loss"}, {"url": "https://www.linkedin.com/in/janteichmann/", "anchor_text": "https://www.linkedin.com/in/janteichmann/"}, {"url": "https://medium.com/@jan.teichmann", "anchor_text": "https://medium.com/@jan.teichmann"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4a420b14ab4e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4a420b14ab4e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/recommendations?source=post_page-----4a420b14ab4e---------------recommendations-----------------", "anchor_text": "Recommendations"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4a420b14ab4e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----4a420b14ab4e---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&user=Jan+Teichmann&userId=941c2fa7cfd&source=-----4a420b14ab4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&user=Jan+Teichmann&userId=941c2fa7cfd&source=-----4a420b14ab4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4a420b14ab4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4a420b14ab4e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4a420b14ab4e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jan.teichmann?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jan.teichmann?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jan Teichmann"}, {"url": "https://medium.com/@jan.teichmann/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.2K Followers"}, {"url": "https://www.linkedin.com/in/janteichmann/", "anchor_text": "https://www.linkedin.com/in/janteichmann/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F941c2fa7cfd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&user=Jan+Teichmann&userId=941c2fa7cfd&source=post_page-941c2fa7cfd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F463734a42bc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-use-cases-for-recommendation-engines-4a420b14ab4e&newsletterV3=941c2fa7cfd&newsletterV3Id=463734a42bc7&user=Jan+Teichmann&userId=941c2fa7cfd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}