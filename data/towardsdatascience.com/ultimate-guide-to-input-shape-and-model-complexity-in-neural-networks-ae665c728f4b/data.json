{"url": "https://towardsdatascience.com/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b", "time": 1683009399.064876, "path": "towardsdatascience.com/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b/", "webpage": {"metadata": {"title": "Ultimate Guide to Input shape and Model Complexity in Neural Networks | by Chetana Didugu | Towards Data Science", "h1": "Ultimate Guide to Input shape and Model Complexity in Neural Networks", "description": "While building neural networks, a lot of beginners and non-beginners alike, seem to get caught up in figuring out the input shape that needs to be fed into the neural network. But why should we know\u2026"}, "outgoing_paragraph_urls": [{"url": "https://iq.opengenus.org/resnet50-architecture/", "anchor_text": "ResNet50", "paragraph_index": 4}, {"url": "https://neurohive.io/en/popular-networks/vgg16/", "anchor_text": "VGG16", "paragraph_index": 4}, {"url": "https://www.learnopencv.com/understanding-alexnet/#:~:text=AlexNet%20Architecture,two%20GTX%20580%203GB%20GPUs.&text=AlexNet%20consists%20of%205%20Convolutional%20Layers%20and%203%20Fully%20Connected%20Layers.", "anchor_text": "AlexNet", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here", "paragraph_index": 27}], "all_paragraphs": ["While building neural networks, a lot of beginners and non-beginners alike, seem to get caught up in figuring out the input shape that needs to be fed into the neural network.", "But why should we know the input shape, and why should we feed it? Can\u2019t the Neural Network figure it out on its own?The answer to this question lies in the basics of matrix multiplication.", "Suppose we have two matrices A and B. Let the dimensions of B be m rows x n columns. Now, for the two matrices to be compatible for multiplication, the column dimension of A should be the same as the row dimension of B. which means A should be of dimensions k x m, where k can be any number.", "Now picture A to be the input tensor (a set of images, a sample set of input features, text data of a particular vocabulary size, etc.) and B to be the first hidden layer in the neural network. k will be the number of input samples, and m is the dimension of each input sample. The shape of m depends on the type of input and the type of hidden layer.", "These are the fully connected neural networks that are used for classification and regression tasks. These are also sometimes attached to the end of certain more advance architectures (ResNet50, VGG16, AlexNet, etc.)", "Let us look at one such neural network:", "This model consists of three hidden layers and an input layer. Dropout layers are added in between each pair of dense layers for regularisation. The Dropout layer takes and argument \u201crate\u201d, which specifies the proportion of neurons in the preceding dense layer that should take a value of zero. In this model, the rate is set to 0.5, which means 50% of the neurons in the hidden layers are given a weight of 0.", "In Keras, the input dimension needs to be given excluding the batch-size (number of samples). In this neural network, the input shape is given as (32, ). 32 refers to the number of features in each input sample. Instead of not mentioning the batch-size, even a placeholder can be given. Another way to give the input dimension in the above model is (None, 32, ).", "If the data is multi-dimensional, like image data, then the input data must be given as (m, n) where m is the height-dimension and n is the width-dimension.", "Since 32 is the feature size, it is the column dimension of the input matrix. This means that the row dimension of the hidden layer is also 32. Now that we have the input game sorted, let us look at the model and understand its complexity. Here, by complexity we mean the number of trainable parameters (weight and bias parameters). Higher the number of trainable parameters, more the complexity of the model.", "Since each input feature is connected to each neuron in the hidden layer, the total number of connections is the product of the input feature size (m) and the hidden layer size (n). Since each connection is associated with a weight parameter, the number of weight parameters is m x n. Each output neuron is associated with one bias parameter, hence the number of bias parameters is n. The total number of trainable parameters = m x n + n", "The first dense/hidden layer has 12 neurons, which is its output dimension. This appears as the second output argument in the model summary, against the first hidden layer. The subsequent dropout layer does not alter the dimension of the output. It only alters the weights of the neurons. The second hidden layer has 8 output neurons, and the next one has 6. The final output layer has 1 neuron. Let us verify the total trainable parameters of this model.", "2. Convolution Neural Networks (CNN): These are mostly used to process image data for various computer vision applications such as image detection, image classification, semantic segmentation, etc. Since image data is a multi-dimensional data, it requires different types of processing layers that can detect the most important features of the image. This is what convolutional neural networks do.", "Images are represented as a tuple of three dimensions \u2014 horizontal dimension (width), vertical dimension (height) and the number of channels. If the image is grey-scale, then the channel argument takes a value of 1, and if coloured, then it takes a value of 3, one for each of Red, Green and Blue channels.", "Convolutional neural networks have two special types of layers. A convolution layer (Conv2D in the model), and a pooling layer (MaxPooling2D). A 2-D convolution layer of dimension k consists of a k x k filter that is passed over each pixel in the image. Since the k x k filter covers k x k pixels, when it passes over a pixel, k\u00b2-1 of its neighbours also get covered. An element-wise multiplication of the filter matrix and the covered image matrix is performed. These values are summed up and populated in the corresponding output pixel.", "For example, if the convolution filter of 2 x 2 dimension is passed over an image pixel at position (1, 1), then it covers (0, 0), (0,1) and (1,0) as well. The (0, 0) value of the filter is multiplied with (0, 0) value of the image and so on. We get four values, which are added up and populated into (1, 1) position of the output.", "Notice that this shrinks the size of the image. If the filter were to pass over an border pixel say (0, 1), then there would be no output, since this pixel would not have neighbours for the convolution to go through. This would appear as a blacked out border around the image of width k/2.", "When a convolution filter of size k x k is passed over an image of size n x n, then the output size becomes n-k+1.", "In order to prevent the shrinkage, a padding is added around the border. The argument padding is set to \u201csame\u201d, which means that a padding is added around the image in such a way that the original image size doesn\u2019t change.", "The pooling layer does just that; it pools a certain number of pixels in the image and captures the most prominent feature (max pooling) or an aggregate (average pooling) of the pixels as the output. The model contains a Max Pooling layer of size 2 x 2, which captures the maximum pixel value of each 4 pixel cluster. This reduces the size of the output to 1/4 of its original size (or by 1/k\u00b2 for a pooling layer of size k x k).", "Let us take a closer look at the model. The input dimension is 284 x 284 x 3. It is passed through the first convolution layer of sixteen 2 x 2 filters, with padding. Hence the output dimension of this layer is 284 x 284 x 16. The subsequent layer is a Max Pooling layer of dimension 4 x 4, which shrinks the image by 16 times, 4 times height-wise and 4 times width-wise. So the output dimension is 71 x 71 x 16. The next convolution layer, also with padding, and 32 filters gives an output of 71 x 71 x 32. The next pooling layer of dimension 2 x 2 shrinks the input to dimension 35 x 35 x 32.", "This output is fed into an output layer (fully connected/dense layer). However, the data needs to be reshaped into a single dimension before feeding it to the dense layer. This is achieved by the Flatten layer.", "For a convolutional layer of m input channels, and n filters (output channels) of filter/kernel size k x k, the kernel goes over each channel of an image individually and produces an output per output channel. Hence for each combination of input-output channel, we need to assign k x k weights. Hence the number of weight parameters is m x k x k x n and the number of biases is equal to the number of channels, n. The total number of parameters is m x k x k x n + n.", "A pooling layer does nothing more than aggregate pixel values. Hence there is not trainable parameter here.", "Let us verify the tally of model parameters for the convolution neural network.", "3. Recurrent Neural Networks (RNN): These neural networks are used to process sequential data, or data where the current output depends not only on current input, but also on the previous input. It is used for time series forecasting, natural language processing, etc. A unique feature of RNNs is that it contains gates, which allow or omit input from the previous hidden states to be added to the current input, completely configurable by the user.", "There are three main types of RNN. The most basic one is the \u201cvanilla\u201d RNN, which contains one gate. The other two are the Long Short-term Memory units (LSTM), and the Gated Recurrent Unit (GRU). While the LSTM has 4 gates, the GRU has 3, which makes it computationally faster than LSTM units.", "Above is an illustration distinguishing the architecture of an LSTM and a GRU, courtesy a detailed blog by Michael Phi, which you can refer to here.", "We have a simple LSTM model (4 gates) here, which feeds into a dense output layer. The model takes an input of three dimensions: batch size, time stamp and features. As is the case with all Keras layers, batch size is not a mandatory argument, but the other two need to be given. In the above example, the input contains 100 time steps and 2 features. Each time step is a sequence of observations (a sequence of words for example). Features are akin to channels in a convolutional neural network. In the model, there are 2 input channels.", "In a recurrent neural network with g gates, m input features and n output units, each gate has connections with the current input as well with the hidden state (output) of the previous unit. Hence for each gate, the number of weight parameters is n x n+ m x n. Each output unit has a bias parameter, so the number of bias parameters is n. Total parameter for a single gate is n x n + n x m + n. For g gates the total is g x (n x n + n x m + n).", "For the above model, let us verify the parameter tally.", "Knowing the input shape is very important to build a neural network because all the linear algebraic computations are based on matrix dimensions. However, this post attempts to solve the mystery around it. Also, Keras has very comprehensive documentation on each of these models and the input shape, which makes dealing with this challenge a tad bit easier.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fae665c728f4b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://cervio.medium.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": ""}, {"url": "https://cervio.medium.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Chetana Didugu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb513a31190a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&user=Chetana+Didugu&userId=b513a31190a0&source=post_page-b513a31190a0----ae665c728f4b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://alexlenail.me/NN-SVG/index.html", "anchor_text": "http://alexlenail.me/NN-SVG/index.html"}, {"url": "https://iq.opengenus.org/resnet50-architecture/", "anchor_text": "ResNet50"}, {"url": "https://neurohive.io/en/popular-networks/vgg16/", "anchor_text": "VGG16"}, {"url": "https://www.learnopencv.com/understanding-alexnet/#:~:text=AlexNet%20Architecture,two%20GTX%20580%203GB%20GPUs.&text=AlexNet%20consists%20of%205%20Convolutional%20Layers%20and%203%20Fully%20Connected%20Layers.", "anchor_text": "AlexNet"}, {"url": "http://alexlenail.me/NN-SVG/index.html", "anchor_text": "http://alexlenail.me/NN-SVG/index.html"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "Illustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanationHi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I\u2019m Michael\u2026towardsdatascience.com"}, {"url": "https://keras.io/api/layers/#:~:text=Layers%20are%20the%20basic%20building,variables%20(the%20layer's%20weights).", "anchor_text": "https://keras.io/api/layers/#:~:text=Layers%20are%20the%20basic%20building,variables%20(the%20layer's%20weights)."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ae665c728f4b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ae665c728f4b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/recurring-neural-networks?source=post_page-----ae665c728f4b---------------recurring_neural_networks-----------------", "anchor_text": "Recurring Neural Networks"}, {"url": "https://medium.com/tag/lstm?source=post_page-----ae665c728f4b---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/convolutional-neural-net?source=post_page-----ae665c728f4b---------------convolutional_neural_net-----------------", "anchor_text": "Convolutional Neural Net"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&user=Chetana+Didugu&userId=b513a31190a0&source=-----ae665c728f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&user=Chetana+Didugu&userId=b513a31190a0&source=-----ae665c728f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fae665c728f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ae665c728f4b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ae665c728f4b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ae665c728f4b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ae665c728f4b--------------------------------", "anchor_text": ""}, {"url": "https://cervio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://cervio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chetana Didugu"}, {"url": "https://cervio.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "168 Followers"}, {"url": "https://www.linkedin.com/in/kavitha-chetana-didugu/", "anchor_text": "https://www.linkedin.com/in/kavitha-chetana-didugu/"}, {"url": "https://github.com/kavithacd", "anchor_text": "https://github.com/kavithacd"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb513a31190a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&user=Chetana+Didugu&userId=b513a31190a0&source=post_page-b513a31190a0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffb4adc1375bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b&newsletterV3=b513a31190a0&newsletterV3Id=fb4adc1375bf&user=Chetana+Didugu&userId=b513a31190a0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}