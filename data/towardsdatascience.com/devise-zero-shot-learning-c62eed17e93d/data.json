{"url": "https://towardsdatascience.com/devise-zero-shot-learning-c62eed17e93d", "time": 1682995492.291166, "path": "towardsdatascience.com/devise-zero-shot-learning-c62eed17e93d/", "webpage": {"metadata": {"title": "DeViSE Zero-shot learning. DeViSE combines computer vision with\u2026 | by Fabio M. Graetz | Towards Data Science", "h1": "DeViSE Zero-shot learning", "description": "First, I will explain to you what the potential problem with \u201ctraditional\u201d image classifiers can be and how DeViSE addresses these issues by combining computer vision and language modeling. To\u2026"}, "outgoing_paragraph_urls": [{"url": "http://ec2-18-195-116-70.eu-central-1.compute.amazonaws.com:8000/apidocs/", "anchor_text": "deployed on AWS", "paragraph_index": 1}, {"url": "https://youtu.be/gbceqO8PpBg?t=5103", "anchor_text": "this", "paragraph_index": 25}, {"url": "https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip", "anchor_text": "here", "paragraph_index": 26}, {"url": "https://youtu.be/tY0n9OT5_nA?t=6940", "anchor_text": "this part", "paragraph_index": 34}, {"url": "https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb", "anchor_text": "notebook", "paragraph_index": 34}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification/blob/master/DeViSE%20-%20A%20Deep%20Visual-Semantic%20Embedding%20Model.ipynb", "anchor_text": "My notebook", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/1511.06434.pdf", "anchor_text": "Radford et al., 2016", "paragraph_index": 51}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification/blob/master/DeViSE%20-%20A%20Deep%20Visual-Semantic%20Embedding%20Model.ipynb", "anchor_text": "see notebook", "paragraph_index": 57}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "this file", "paragraph_index": 78}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification", "anchor_text": "repository", "paragraph_index": 79}, {"url": "https://www.udemy.com/deploy-data-science-nlp-models-with-docker-containers/", "anchor_text": "this course", "paragraph_index": 79}, {"url": "https://www.youtube.com/watch?v=9K3OEhm2lR0&list=WL&index=7&t=0s", "anchor_text": "this video", "paragraph_index": 80}], "all_paragraphs": ["This article is organized as follows:", "First, I will explain to you what the potential problem with \u201ctraditional\u201d image classifiers can be and how DeViSE addresses these issues by combining computer vision and language modeling. To visualize the model\u2019s capabilities I will then show you some cool examples of zero-shot predictions made by the model I deployed on AWS. Finally, I will guide you to great resources that you can follow if you would like to train and deploy such a model yourself.", "In classical programming, you might define a set of rules that allow you to calculate answers from your data. Rules and data in, answers out.", "If I challenged you to write down a set of rules to decide whether an arbitrary image contains a car, given only the pixel values, you were bound to fail. The task just is too complicated to manually express such rules. So what we do is \u201cparameterizing the rules\u201d and learning the parameters from data and answers. Data and answers in, rules out. This, in principle, is the idea behind of all (supervised) machine learning.", "\u201cTraditional\u201d image classifiers in the form of convolutional neural networks (traditional with respect to DeViSE) are parameterized functions that learn to calculate a meaningful probability for each of n possible categories for an input image. The probability distribution is usually calculated with a softmax layer at the top of the network.", "Such convnets have been shown to perform great on image recognition tasks, so why is there a need for improved methods?", "The big problem is that we are restricted to the n categories of the dataset that we trained our image classifier on. The trimmed version of ImageNet (1000 classes) contains more than 90 different breeds of dogs. A well-trained classifier trained on this dataset can, thus, successfully differentiate between many more dog breeds than I can, but it can never classify an image as a dog because the word \u201cdog\u201d in itself is not a category.", "The DeViSE model, on the other hand, predicts the following four pictures to be very \u201cdog\u201d-like, even though it was never shown a training pair of a category \u201cdog\u201d.", "Disclaimer: if you train a model for a restricted setting, i.e. medical diagnosis, classifiers with softmax output layers are of course a perfectly valid and good approach. Your model does not have to generalize to a myriad of different categories that were not seen during training.", "Back to our dogs. In principle, you might construct a hand-crafted hierarchy that determines that if an image contains a \u201cGerman shepherd dog\u201d it automatically must contain a dog. But constructing such a hierarchy by hand for all existing words is a lot of work which we desperately want to avoid in machine learning. After all, the more rigidly defined categories you choose to partition our world into, the more complicated and expensive it is to construct labeled datasets.", "Let\u2019s take a closer look at the class probabilities an image classifier returns:", "With a softmax output layer, each picture can belong to only one single category as softmax is designed to assign a high probability to one single class. This means that you should not introduce an additional category \u201cdog\u201d because the network would have a hard time deciding whether to categorize the image as \u201cdog\u201d or the specific dog breed.", "In principle, this problem may be addressed by using the sigmoid activation function in the output layer instead of softmax and allowing different labels to be true for a single image. You would then not predict n class probabilities for an image that sum up to 1 but one independent probability for each of the classes, saying whether an object of that class is likely contained in the image or not. This allows you to introduce additional categories such as \u201cdog\u201d without competing with the dog breeds.", "What do I mean by this?", "If you use a softmax layer at the top of your network you might get an answer like \u201cthe image contains a German shepherd dog with a probability of 87%\u201d. All other 999 classes combined would sum up to 13%.", "If you use sigmoid in the output layer and allow different categories to be contained in an image you might get an answer like \u201cthe image contains a German shepherd dog with a probability of 87%, a dog with a probability of 89%, a French bulldog with a probability of 67%, \u2026, a nuclear submarine with a probability of 0.13% and an airplane with a probability of 1%\u201d. Note, that the probabilities do not add up to 100% anymore.", "With this approach, the classifier does not have to decide whether it should assign a high probability to \u201cdog\u201d or to \u201cGerman shepherd dog\u201d as it would have to do with a softmax output layer. It decides for each category whether it is likely detected or not \u2014 regardless of the other categories.", "Let\u2019s say you found or created labels for ImageNet that contain many thousands of additional categories, i.e. \u201cdog\u201d, and actually managed to train a convolutional neural network that allows predicting those tens of thousands of class probabilities. The network might now say \u201cthis image contains a German shepherd dog and a dog\u201d. Can the network answer the question of whether this image contains a \u201cpuppy\u201d, \u201cdoggy\u201d or \u201chound\u201d? No, it can\u2019t because while you added categories like \u201cdog\u201d to the dataset you did not include the categories \u201cpuppy\u201d, \u201cdoggy\u201d or \u201chound\u201d. So what next, should we add those words as potential labels as well?", "The problem is that neither of these classifiers can predict classes they have not encountered during training, they cannot transfer semantic information about categories they have been trained on to unseen but similar categories.", "Models like DeViSE offer a truly elegant solution to this problem by combining the image recognition task with semantic information about the similarity of words/categories learned from language models trained on unannotated text.", "Basically, the goal is to develop a model that is able to infer that an image contains a \u201cdog\u201d, \u201cdoggy\u201d, \u201cpuppy\u201d or \u201chound\u201d because those words are similar to \u201cGerman shepherd dog\u201d. We want our model to generalize to categories that were never encountered during training, simply because those categories are semantically similar to categories that the model was trained on. This is called zero-shot classification.", "Building a DeViSE model starts with training a language model on an unannotated text corpus.", "Every word (or token) is represented as a point in a high dimensional space \u2014 or in plain English \u2014 as a vector of let\u2019s say 300 floating point numbers (could of course also be 400 or 500 or 1325 but let\u2019s say 300 in the rest of the article). Such a vector is called word vector and the numbers the word vectors contain are parameters of the model. This means, that the values can be changed by the optimizer as the model learns. Next, you need a neural network, a parameterized function, that predicts the next word of a sentence given the past m words. Simply put, this asks the question \u201cif you have this sentence of m words, what could the next word be?\u201d. The great thing about asking this question is that you do not have to do any labeling of your dataset, you simply go through any text corpus and repeatedly ask your neural network what the next word is. The network learns to predict the next word by adapting its parameters including the numbers in all of the word vectors that represent the words.", "The 300 element vectors point to a point in a high dimensional embedding space and similar words end up close to each other in that space because synonyms tend to appear in similar contexts. The point in this high dimensional space that represents \u201cdog\u201d will be close to the point that represents \u201cdoggy\u201d. (A possible interpretation of \u201cclose\u201d in this high dimensional space will be explained later.)", "I understand that all this might sound extremely intimidating to you but don\u2019t worry, what I want you to know at this point is that after training the language model, the 300 numbers that represent the word \u201cdog\u201d are closer to the 300 numbers that represent \u201cdoggy\u201d than they are to the 300 numbers that represent \u201ctrain\u201d.", "If you haven\u2019t trained a language model yourself but are interested in doing so, I highly recommend to code along to this, everything I wrote here will make a lot more sense to you after that, I promise.", "For now, all we need are semantically meaningful word vectors. Luckily for us, we can simply download trained embeddings/word vectors here.", "Now comes the second part: mapping images into this rich high dimensional embedding space. Sounds confusing? Don\u2019t worry, it\u2019s easier than it sounds.", "We take a pre-trained softmax output layer based image classifier, truncate the final classification layers at the top of the network and replace them with a few layers that do not predict n class probabilities for n classes but instead simply 300 numbers \u2014 which represent a word vector. We convert the classification problem to a regression problem.", "Mapping an image into the high dimensional word vector embedding space then allows us to find words that are close to it in this high dimensional embedding space \u2014 or in English: we predict a word vector for an image and find words that have similar word vectors.", "A distinct advantage of our model is its ability to make reasonable inferences about candidate labels it has never visually observed. For example, a DeViSE model trained on images labeled tiger shark, bull shark, and blue shark, but never with images labeled shark, would likely have the ability to generalize to this more coarse-grained descriptor because the language model has learned a repre- sentation of the general concept of shark which is similar to all of the specific sharks.", "For example, the 9 nearest terms to tiger shark using cosine distance are bull shark, blacktip shark, shark, oceanic whitetip shark, sandbar shark, dusky shark, blue shark, requiem shark, and great white shark. The 9 nearest terms to car are cars, muscle car, sports car, compact car, automobile, racing car, pickup truck, dealership, and sedans.", "The main steps to train a DeViSE model are:", "5. Define a loss function. You could i.e. use an adaptation of cosine similarity. Remember that our number representations for the words are vectors. In school, you might have been taught that you can think of vectors as arrows. When two arrows point in the same direction they are similar (we neglect the length of the arrow). The cosine similarity calculates the cosine of the angle between two vectors. If the angle is 0\u00b0, the cosine similarity is 1, if it is 90\u00b0, the cosine similarity is 0. For the loss, we want a smaller number when the vectors are more similar to each other, hence, when the angle between them is smaller. We, therefore, simply calculate the loss as 1 - cosine similarity of the two vectors:", "If you want to do this yourself, I suggest you code along to this part of fast.ai part 2. The official course notebook is here. My notebook is slightly less cluttered/more commented. It also builds the model from scratch using PyTorch instead of calling a fast.ai convenience function to build it. Feel free to use it for reference as well.", "Let\u2019s summarize what we discussed: Instead of building a model that returns n class probabilities for n classes, we build a model that predicts a word vector for a given image. We then simply find words with similar word vectors (approximate nearest neighbor search). This allows the model to utilize semantic knowledge to generalize and predict classes that were not seen during training (zero-shot learning).", "There is an additional, huge benefit of such a method:", "Perhaps more importantly, though here we trained on a curated academic image dataset, our model\u2019s architecture naturally lends itself to being trained on all available images that can be annotated with any text term contained in the (larger) vocabulary. We believe that training massive \u201copen\u201d image datasets of this form will dramatically improve the quality of visual object categorization systems.", "Ok, enough theory, let\u2019s have some fun with the model! I deployed it on AWS using Docker so that you can play with it as well. (Disclaimer: the app runs on a single t2.micro instance on AWS so that it stays within the AWS free-tier new customers get for one year. My stories usually have ~100 viewers per day so I do not expect too many readers trying to use it at the same time.)", "We will do three different kinds of zero-shot predictions with the model:", "I provided the app with ~5000 random pictures from the validation dataset I created during training. The app predicts a word vector for each of the stored pictures. This allows us to:", "2. Predict semantically similar images given an input image. This is done by predicting the word vector for the input image and then performing approximate nearest neighbor search on the word vectors predicted for the stored images.", "3. Predict semantically similar images given an input word. This is done by translating the input word to its corresponding word vector and then performing approximate nearest neighbor search on the predicted word vectors of the stored images.", "Let\u2019s start with number 3. and try the word \u201cbird\u201d. There are several kinds of birds in ImageNet, i.e. \u201calbatross\u201d, \u201csnowbird\u201d, \u201cindigo finch\u201d, \u201cAmerican eagle\u201d, \u201cvulture\u201d, or \u201costrich\u201d but not the category \u201cbird\u201d.", "The model predicts the following images to be most \u201cbird\u201d-like:", "Same for \u201cinsect\u201d (not a category in ImageNet):", "The category \u201ccurch\u201d is contained. Apparently, the model can generalize to the category \u201ccathedral\u201d:", "The word \u201cmeasure\u201d does not appear in any of the ImageNet categories (in fact, no verb is a category). Yet the model is able to predict the following images for \u201cmeasure\u201d:", "The word \u201cdress\u201d is not part of any category in ImageNet either, yet the model predicts the following pictures:", "\u201csports car, sport car\u201d is the only category containing the word \u201csport\u201d. The model predicts the following pictures for \u201csport\u201d:", "The function wordvec_2_image requires one but accepts two words as input. If you pass two words, the word vectors are averaged (added up and divided by 2). Let\u2019s see if the language model has learned smooth transitions, meaning that we can interpolate between word vectors and find something meaningful.", "(Mikolov et al., 2013) demonstrated that simple arithmetic operations revealed rich linear structure in representation space. One canonical example demonstrated that the vector(\u201dKing\u201d) \u2014 vector(\u201dMan\u201d) + vector(\u201dWoman\u201d) resulted in a vector whose nearest neighbor was the vector for Queen. (Citation from Radford et al., 2016)", "The top four images from the stored dataset predicted for the word \u201cship\u201d:", "There are two old sail ships and to modern vessels. Let\u2019s check what the model predicts for the words \u201cship\u201d + \u201csail\u201d:", "Ok, the modern ships are gone, all ships have sails now. Does this mean that meaningful interpolations are possible with this kind of model? Well, we only saw four pictures so I\u2019d be careful with such a statement. The fifth one could have been a modern ship. And sails only appear on boats or ships so the word vectors might be really close to each other in the first place. Averaging them would then return a word vector that is very similar to both \u201cship\u201d and \u201csail\u201d as well and the result we saw might be a coincidence rather than meaningful interpolation.", "\u201cSail\u201d alone, as expected, gives a similar selection:", "The last paragraph was quite vague. Let\u2019s try to quantify some of this:", "We calculate the Pearson correlation coefficient of two word vectors with np.corrcoef(wordvectors.get_word_vector(\"ship\"), wordvectors.get_word_vector(\"boat\"))[0,1] to estimate the similarity of the two words (see notebook). A higher number means they are likely more similar. The correlation coefficient of a vector with itself is 1.", "\u201dShip\u201d and \u201cbug\u201d, which obviously are very unsimilar, give a correlation coefficient of 0.08.", "\u201cShip\u201d and \u201cboat\u201d, which can be counted as synonyms, give 0.56, \u201cship\u201d and \u201csail\u201d give 0.59, and \u201cboat\u201d and \u201csail\u201d give 0.57. So \u201csail\u201d is indeed very similar to both \u201cship\u201d and \u201cboat\u201d. Let\u2019s try to interpolate with a vector that is less similar to \u201cship\u201d, i.e. \u201cengine\u201d.", "There are ships with engines but not all engines are on ships. The correlation coefficient for \u201cship\u201d and \u201cengine\u201d is 0.31.", "Ok, now the ships with sails are gone. Let\u2019s crosscheck this. The results for only \u201cengine\u201d are:", "All of those things have engines but apparently \u201cengine\u201d does not automatically mean \u201cship with engine\u201d to the model. It appears that meaningful interpolation might in principle be possible with this kind of visual semantic embedding model. Notice that I write appears and might. Keep in mind that we only saw a few pictures.", "I have to admit though, that most combinations of two words I tried did not yield any meaningful results. This could be attributed to the fact, that I provided the model only with ~5000 images in order to stay within the AWS free-tier. Can you find other cool combinations? If you do, let me know in the comments!", "Let\u2019s try the other two functions of the API. We pass a picture and get the top-five zero-shot predictions and the top-four semantically most similar pictures from the stored dataset.", "Neither \u201cshrimp\u201d nor \u201cprawn\u201d are categories in ImageNet:", "The top-five zero-shot predictions for this picture are:", "Google \u201ccrayfish\u201d, I can understand why the model believes that they are semantically similar to shrimps. All stored images of the app are from ImageNet, the model, therefore, cannot return images of a shrimp. I would argue that three of the four are semantically similar. What the child is doing there, I have no clue\u2026", "The last try was an image of a category not contained in ImageNet. Let\u2019s try one that is contained to see if the model can in principle be exact and not only find similar classes. The following image shows a blue jay and the category \u201cjay\u201d exists in ImageNet,", "And the semantically most similar images from the stored dataset are:", "I\u2019m an astrophysicist, not an ornithologist but to me, three of the birds have blue and white feathers in the right spots so I count this as a success.", "I only showed you successful examples until now, but of course, I also found several examples where the predictions are wrong. Let\u2019s try this image:", "The model returns the following top-five predictions", "and the following four pictures even though \u201cpolar bear\u201d is an ImageNet category!", "And when asked to show pictures for the category \u201ccat\u201d, the model returns dogs as well.", "For this cat picture, the zero-shot predictions, however, are", "and the model returns the following four semantically similar pictures:", "When asking for pictures semantically similar to the word \u201ctabby\u201d, I get cat pictures as well\u2026", "If you try other words or pictures and want to find out, whether a specific word is part of any category of ImageNet, download this file and run cat imagenet1000_clsidx_to_labels.txt | grep word in the command line.", "In this repository, you can find a notebook to train the model (based on fastai), a pre-trained model, in case you don\u2019t want to train it yourself but would like to experiment with the app, the flask-app that exposes the model as an API, instructions on how to try the app using the flask development server and instructions on how to run the app in a Docker container. Consider this course if you are interested in learning Docker to deploy machine learning models. It costs a few dollars (I\u2019m not affiliated) and the English is not perfect but I learned exactly what I was looking for to get started.", "I found this video to be very helpful for deploying the container on AWS. Just a word of warning: if you try to deploy an app on AWS and are eligible for the AWS free-tier services (for new customers), make sure to use an ec2 t2.micro instance as other instances are not eligible to use for free. Also, keep in mind that there are no \u201cfree-tier only accounts\u201d and that you will be billed once you exceed the free-tier limits.", "If this paper would be published in 2019 as opposed to 2013, I could very well picture the headlines in some media: \u201cGoogle scientists create an AI that can see, speak and describe things it was never taught \u2026 robots are going to kill us all.\u201d Ok, I am exaggerating on purpose here, but with all this hype it is important to explain to people what machine learning is and what it isn\u2019t, especially if you belong to those who can understand. Ultimately it comes down to the following:", "[\u2026] the central problem in machine learning and deep learning is to meaningfully transform data: in other words, to learn useful representations of the input data at hand \u2014 representations that get us closer to the expected output. [\u2026] All machine-learning algorithms consist of automatically finding such transformations that turn data into more-useful representations for a given task. [\u2026] So that\u2019s what deep learning is, technically: a multistage way to learn data representations. It\u2019s a simple idea \u2014 but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic. (Citation from Chollet, Fran\u00e7ois, Deep Learning with Python)", "The idea to transform an input image into a word vector instead of n class probabilities is beautifully simple, yet it allows us to do amazing things as word embeddings are continuous and allow us to measure the similarity of images to unknown categories \u2014 only by transforming data into a more useful form! In addition, this algorithm is able to utilize much larger and less curated real-life datasets than \u201ctraditional\u201d deep learning image classifiers (as long as a word can be found that describes the image) which can dramatically improve the quality of visual object classification methods.", "I hope you had fun reading this article and that I was able to teach you something. In case you have questions, let me know in the comments, I\u2019d love to help you understand :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior MLOps engineer at Recogni | Machine Learning | Kubernetes | Theoretical Astrophysicist | Bespoke Shoemaking | Berlin"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc62eed17e93d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fabiograetz?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Fabio M. Graetz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb820388a7e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=post_page-fb820388a7e9----c62eed17e93d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/The_School_of_Athens", "anchor_text": "The School of Athens"}, {"url": "https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf", "anchor_text": "Fromme et al. (2013)"}, {"url": "http://ec2-18-195-116-70.eu-central-1.compute.amazonaws.com:8000/apidocs/", "anchor_text": "on AWS"}, {"url": "http://ec2-18-195-116-70.eu-central-1.compute.amazonaws.com:8000/apidocs/", "anchor_text": "deployed on AWS"}, {"url": "https://youtu.be/gbceqO8PpBg?t=5103", "anchor_text": "this"}, {"url": "https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip", "anchor_text": "here"}, {"url": "https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip", "anchor_text": "word vectors"}, {"url": "https://www.kaggle.com/c/imagenet-object-localization-challenge", "anchor_text": "here"}, {"url": "http://files.fast.ai/data/imagenet-sample-train.tar.gz", "anchor_text": "here"}, {"url": "https://youtu.be/tY0n9OT5_nA?t=6940", "anchor_text": "this part"}, {"url": "https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb", "anchor_text": "notebook"}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification/blob/master/DeViSE%20-%20A%20Deep%20Visual-Semantic%20Embedding%20Model.ipynb", "anchor_text": "My notebook"}, {"url": "http://ec2-18-195-116-70.eu-central-1.compute.amazonaws.com:8000/apidocs/", "anchor_text": "DeViSEDeep Visual-Semantic Embedding Model by Fromme et al. (2013)"}, {"url": "https://arxiv.org/pdf/1511.06434.pdf", "anchor_text": "Radford et al., 2016"}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification/blob/master/DeViSE%20-%20A%20Deep%20Visual-Semantic%20Embedding%20Model.ipynb", "anchor_text": "see notebook"}, {"url": "https://commons.wikimedia.org/wiki/File:Shrimp_on_cutting_board.jpg", "anchor_text": "Source"}, {"url": "https://commons.wikimedia.org/wiki/File:Cyanocitta-cristata-004.jpg", "anchor_text": "Source"}, {"url": "https://commons.wikimedia.org/wiki/File:Polar_bear_(Ursus_maritimus)_in_the_drift_ice_region_north_of_Svalbard.jpg", "anchor_text": "Source"}, {"url": "https://commons.wikimedia.org/wiki/File:Tabby_Pfaffengrund.JPG", "anchor_text": "Source"}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "this file"}, {"url": "https://github.com/fg91/DeViSE-zero-shot-classification", "anchor_text": "repository"}, {"url": "https://www.udemy.com/deploy-data-science-nlp-models-with-docker-containers/", "anchor_text": "this course"}, {"url": "https://www.youtube.com/watch?v=9K3OEhm2lR0&list=WL&index=7&t=0s", "anchor_text": "this video"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c62eed17e93d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c62eed17e93d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c62eed17e93d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----c62eed17e93d---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c62eed17e93d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=-----c62eed17e93d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=-----c62eed17e93d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc62eed17e93d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c62eed17e93d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c62eed17e93d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c62eed17e93d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c62eed17e93d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fabio M. Graetz"}, {"url": "https://medium.com/@fabiograetz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "849 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb820388a7e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=post_page-fb820388a7e9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc88e15df57a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevise-zero-shot-learning-c62eed17e93d&newsletterV3=fb820388a7e9&newsletterV3Id=c88e15df57a4&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}