{"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb", "time": 1682997138.336174, "path": "towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Model Based Planning Methods Extension | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Model Based Planning Methods Extension", "description": "In last article, we walked through how to model an environment in an reinforcement learning setting and how to leverage the model to accelerate the learning process. In this article, I would like to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8", "anchor_text": "article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8", "anchor_text": "article", "paragraph_index": 0}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaQ%2B.py", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/PrioritySweeping.py", "anchor_text": "here", "paragraph_index": 27}], "all_paragraphs": ["In last article, we walked through how to model an environment in an reinforcement learning setting and how to leverage the model to accelerate the learning process. In this article, I would like to further the topic and introduce 2 more algorithms, Dyna-Q+ and Priority Sweeping, both based on Dyna-Q method that we learnt in last article. (If you find some game settings confusing, please check my last article)", "In the following paragraphs, we will leverage these 2 algorithms to solve 2 problems:", "In last article, I introduced an example of Dyna-Maze, where the action is deterministic, and the agent learns the model, which is a mapping from (currentState, action) -> (nextState, reward) , by recording the steps that it experiences along the way, and then at the planning stage, the model being learnt is applied n times to reinforce the learning process. In a nutshell, the process can be summarised as", "But things do not always go in this way, as the environment can be complex and dynamic.", "Models may be incorrect because the environment is stochastic and only a limited number of samples have been observed, or because the model was learned using function approximation that has generalised imperfectly, or simply because the environment has changed and its new behaviour has not yet been observed. When the model is incorrect, the planning process is likely to compute a suboptimal policy.", "Consider a case called shortcut maze, in which the environment is dynamically changing.", "An agent starts at S and aims to reach G as fast as possible, and the black grey blocks are areas that the agent can not pass through. The picture on the left stands for the original setting, and our agent is able to find the shortest path, through the left side of the board all the way to G , using Dyna-Q method. However, the environment will change at a certain time stamp, and a shortcut on the rightmost of the board will be open. Under this setting, will the Dyna-Q agent still be able to find the optimal solution?", "The answer is no. Even with \u03f5-greedy method, which the agent always explore with a certain probability, it is very unlikely to locate the optimal path from the leftmost to the rightmost, as the already-learnt Q function would always guide the agent to choose the path on the left, and there is not strong enough motivations, or rewards pushing the agent to explore other path.", "How to resolve the problem? The essence is to keep the agent to be able to explore new states to fit the changing environment, and the trick that drives agent to explore is to give reward. Thus, here we introduce the theory of Dyna-Q+:", "The agent keeps track for each state\u2013action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect. To encourage behaviour that tests long-untried actions, a special \u201cbonus reward\u201d is given on simulated experiences involving these actions. In particular, if the modelled reward for a transition is r, and the transition has not been tried in \u03c4 time steps, then planning updates are done as if that transition produced a reward of r + \u03ba*sqrt(\u03c4), for some small \u03ba. This encourages the agent to keep testing all accessible state transitions and even to find long sequences of actions in order to carry out such tests.", "To summarise, the algorithm is totally the same as Dyna-Q except that it keeps track of number of time a state has been visited, and give reward to state that has long not been visited(as these state could have possibly changed as time goes on).", "Now let\u2019s make some modifications on Dyna-Q and implement Dyna-Q+(As the basic settings are mostly the same, the following codings I will majorly focus on the difference). You can also check the full implementation here.", "In the init function, two components are added in order to give reward to non-visited states. self.time keeps track of the total time steps within each episode(it will be reset after the game ends), and self.timeWeight is essentially the \u03ba in the reward function, which signifies how much we would like the agent to explore. Besides those general settings, a model of environment is again initialised, but this time it is a mapping of (currentState, action) -> (reward, nxtState, timestep) .", "To make the code more organised, an update model function is defined:", "The function updates the model at each time step. One needs to notice that we would like consider actions that has never been tried before in the planning stage, and we define those actions lead the agent back to the original state with a reward of 0, thus you can see that for action never occured has a model of:", "With reward set to 0, state set as the same and time step set to 1(so that the number of times not visited this state could be high). In addition, for action that take place in that state would be marked with the current time step.", "The play function follows the same structure as in Dyna-Q method:", "After each step, we update the model by calling self.updateModel() function, and the time step stored in the model is used in the following loop to add on to the reward:", "Besides the original reward by taking the specific action in a state, an extra reward is assigned. The self.time - _time is in essential the number of times the not been visited.", "By giving extra reward to non-explored state, Dyna-Q+ is more likely to detect the changing environment, while Dyna-Q can hardly do it.", "Referring to the result from Sutton\u2019s book, when the environment changes at time step 3000, the Dyna-Q+ method is able to gradually sense the changes and find the optimal solution in the end, while Dyna-Q always follows the same path it discovers previously. In fact, as the planning step reinforces the experience in Dyna-Q, the more planning steps, the less likely for Dyna-Q agent to find the optimal path, in contrast, the planning steps in Dyna-Q+ increases values for not-explore-enough states and actions, resulting the agent more likely to explore and find the optimal path.", "We have now gone through the basics of formulating a reinforcement learning with dynamic environment. You might have noticed that in the planning stage, there are actually many invalid updates, especially at the beginning phase when the Q functions of all states and actions are 0 and rewards along the way are also 0. In these scenarios, the temporary difference,", "equals 0, thus the Q function updates of many states, actions are still 0. So here comes the question: are we able to update more efficiently?", "Here I will introduce a method called priority sweeping, which focuses on updating non-zero values in the planning stage. The intuition is since many updates are 0s, are we able to only update values that are higher than a certain threshold, thus make the learning faster?", "It is natural to prioritise the updates according to a measure of their urgency, and perform them in order of priority. This is the idea behind prioritised sweeping. A queue is maintained of every state\u2013action pair whose estimated value would change nontrivially if updated , prioritised by the size of the change.", "There are few points you need to notice:", "The reason that all precedent states, following a non-trivial updated state, need to be updated is as the current state update value is non-trivial, backward update will absolutely result in non-zero updates. (Consider the example we talked in value iteration where we carried a backward update when the goal is reached, all states value updated along the way are non-zero and useful)", "Time to get our hands on the implementation, and you can check the full implementation here.", "In the init function, we initialised a threshold \u03b8 , a priority queue to store state and action pairs by their priority, and a dictionary of predecessors in order to update all states lead to the current updated state.", "The major difference lies in the play function:", "At each step, instead of direct update the Q value of current state, action pair, a tmp_diff is recorded and insert into the priority queue if the value is big enough. And then, both the model and predecessors are updated, notice that the predecessor dict is a mapping of nxtState -> List((currentState, action), ...) , as many states and actions could lead to a same state.", "In the planning stage(in side the for loop), the top priority state, action pair is retrieved ( self.queue.get(1) ), and all states(within the pre_state_action_list ) lead to that state are updated.", "As we have talked above, priority sweeping updates non-trivial values along the process and thus is more efficient and faster.", "Refer to the plot from Sutton\u2019s book, priority is way faster to find the optimal solution than Dyna-Q.", "The algorithm introduced above is for deterministic environment, for non-deterministic environment, as Sutton stated:", "Extensions of prioritized sweeping to stochastic environments are straightforward. The model is maintained by keeping counts of the number of times each state\u2013action pair has been experienced and of what the next states were. It is natural then to update each pair not with a sample update, as we have been using so far, but with an expected update, taking into account all possible next states and their probabilities of occurring.", "In this article, we learnt two algorithms, and the key points are:", "And lastly, please check out my Github. You are welcomed to contribute, and if you have any questions or suggestions, please raise comment below!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F572dfee4cceb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----572dfee4cceb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8", "anchor_text": "article"}, {"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8", "anchor_text": "article"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaQ%2B.py", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/PrioritySweeping.py", "anchor_text": "here"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----572dfee4cceb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----572dfee4cceb---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----572dfee4cceb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----572dfee4cceb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----572dfee4cceb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F572dfee4cceb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----572dfee4cceb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----572dfee4cceb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----572dfee4cceb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----572dfee4cceb--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-extension-572dfee4cceb&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}