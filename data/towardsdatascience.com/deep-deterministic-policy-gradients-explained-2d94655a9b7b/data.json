{"url": "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b", "time": 1682995454.0198379, "path": "towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b/", "webpage": {"metadata": {"title": "Deep Deterministic Policy Gradients Explained | by Chris Yoon | Towards Data Science", "h1": "Deep Deterministic Policy Gradients Explained", "description": "This post is a thorough review of Deepmind\u2019s publication \u201cContinuous Control With Deep Reinforcement Learning\u201d (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63", "anchor_text": "policy gradients", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "actor critic methods", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "Wikipedia provides a thorough explanation of the Ornstein-Uhlenbeck Process.", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra, Continuous control with deep reinforcement learning, CoRR abs/1509.02971 (2015).", "paragraph_index": 30}, {"url": "https://qr.ae/TW8NAa", "anchor_text": "[1] Edouard Leurent\u2019s answer to Quora post \u201cWhy do we use the Ornstein Uhlenbeck Process in the exploration of DDPG?\u201d", "paragraph_index": 31}], "all_paragraphs": ["This post is a thorough review of Deepmind\u2019s publication \u201cContinuous Control With Deep Reinforcement Learning\u201d (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is presented, and is written for people who wish to understand the DDPG algorithm. If you are interested only in the implementation, you can skip to the final section of this post.", "This post is written with the assumption that the reader is familiar with basic reinforcement learning concepts, value & policy learning, and actor critic methods. If you are not completely familiar with those concepts, I have also written about policy gradients and actor critic methods.", "Familiarity with python and PyTorch will also be really helpful for reading through this post. If you are not familiar with PyTorch, try to follow the code snippets as if they are pseudo-code.", "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.", "The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space", "The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here\u2019s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence. For example:", "So, we have the standard Actor & Critic architecture for the deterministic policy network and the Q network:", "And we initialize the networks and target networks as:", "So, here\u2019s the pseudo-code of the algorithm that we want to implement:", "We are going to break this down into:", "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache \u2014 a \u201creplay buffer.\u201d Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.", "Here\u2019s how the replay buffer looks like:", "Why do we use experience replay? In optimization asks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.", "The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:", "However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:", "* Note that the original Q value is calculated with the value network, not the target value network.", "For the policy function, our objective is to maximize the expected return:", "To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.", "But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:", "Where the optimizers use Adaptive Moment Estimation (ADAM):", "We make a copy of the target network parameters and have them slowly track those of the learned networks via \u201csoft updates,\u201d as illustrated below:", "This can be implemented very simply:", "In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output (Uhlenbeck & Ornstein, 1930):", "The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or \u201cfreezing\u201d the overall dynamics [1]. Wikipedia provides a thorough explanation of the Ornstein-Uhlenbeck Process.", "Here\u2019s a python implementation written by Pong et al:", "So we input the action produced by the actor network into get_action() function, and get a new action to which the temporally correlated noise is added.", "We have here the Replay Buffer, the Ornstein-Uhlenbeck Process, and the normalized Action Wrapper for OpenAI Gym continuous control environments in utils.py:", "And the Actor & Critic networks in models.py:", "And the DDPG agent in ddpg.py:", "And we can see if the DDPG agent learns optimal policy for the classic Inverted Pendulum task:", "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra, Continuous control with deep reinforcement learning, CoRR abs/1509.02971 (2015).", "[1] Edouard Leurent\u2019s answer to Quora post \u201cWhy do we use the Ornstein Uhlenbeck Process in the exploration of DDPG?\u201d", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2d94655a9b7b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----2d94655a9b7b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63", "anchor_text": "policy gradients"}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "actor critic methods"}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "Wikipedia provides a thorough explanation of the Ornstein-Uhlenbeck Process."}, {"url": "https://github.com/thechrisyoon08/Reinforcement-Learning", "anchor_text": "thechrisyoon08/Reinforcement-LearningModular implementations of reinforcement learning algorithms with Python and PyTorch \u2026github.com"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra, Continuous control with deep reinforcement learning, CoRR abs/1509.02971 (2015)."}, {"url": "https://qr.ae/TW8NAa", "anchor_text": "[1] Edouard Leurent\u2019s answer to Quora post \u201cWhy do we use the Ornstein Uhlenbeck Process in the exploration of DDPG?\u201d"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----2d94655a9b7b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2d94655a9b7b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2d94655a9b7b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/control?source=post_page-----2d94655a9b7b---------------control-----------------", "anchor_text": "Control"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&user=Chris+Yoon&userId=b24112d01863&source=-----2d94655a9b7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&user=Chris+Yoon&userId=b24112d01863&source=-----2d94655a9b7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2d94655a9b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2d94655a9b7b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2d94655a9b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}