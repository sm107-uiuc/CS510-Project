{"url": "https://towardsdatascience.com/clean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734", "time": 1682995962.94957, "path": "towardsdatascience.com/clean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734/", "webpage": {"metadata": {"title": "Clean Up your own Model Data without leaving Jupyter | by Dan Lester | Towards Data Science", "h1": "Clean Up your own Model Data without leaving Jupyter", "description": "Many machine learning projects start with a Jupyter notebook, and the first few lines of the notebook load the training data. Beyond a quick sanity check that the data looks about right, it can be\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater", "paragraph_index": 1}, {"url": "https://github.com/ideonate/butterflies", "anchor_text": "Jupyter Notebooks on GitHub here", "paragraph_index": 5}, {"url": "https://www.fast.ai/", "anchor_text": "fast.ai", "paragraph_index": 5}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/classify-butterfly-images-with-deep-learning-in-keras-b3101fe0f98", "anchor_text": "Towards Data Science article by Bert Caramans", "paragraph_index": 6}, {"url": "https://github.com/ideonate/butterflies/blob/master/1%20-%20Butterfly%20Downloads.ipynb", "anchor_text": "first notebook", "paragraph_index": 7}, {"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater tool", "paragraph_index": 8}, {"url": "https://github.com/ideonate/butterflies/blob/master/2%20-%20Butterfly%20Innotater.ipynb", "anchor_text": "second notebook", "paragraph_index": 11}, {"url": "https://github.com/ideonate/butterflies/blob/master/2%20-%20Butterfly%20Innotater.ipynb", "anchor_text": "our notebook", "paragraph_index": 26}, {"url": "https://github.com/ideonate/butterflies", "anchor_text": "butterflies GitHub repo", "paragraph_index": 28}, {"url": "https://github.com/ideonate/butterflies/blob/master/3%20-%20Basic%20Train.ipynb", "anchor_text": "3 - Basic Train.ipynb", "paragraph_index": 29}, {"url": "https://github.com/ideonate/butterflies/blob/master/4%20-%20BBox%20Train%20and%20Generate.ipynb", "anchor_text": "4 - BBox Train and Generate.ipynb", "paragraph_index": 36}, {"url": "https://github.com/ideonate/butterflies/blob/master/5%20-%20Zoomed%20Cropped%20Train.ipynb", "anchor_text": "5 - Zoomed Cropped Train.ipynb", "paragraph_index": 40}, {"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater GitHub page", "paragraph_index": 44}, {"url": "http://containds.com", "anchor_text": "containds.com", "paragraph_index": 46}], "all_paragraphs": ["Many machine learning projects start with a Jupyter notebook, and the first few lines of the notebook load the training data. Beyond a quick sanity check that the data looks about right, it can be disruptive to the flow of your programming if you need to step back out of the notebook to clean up or annotate your data.", "This article introduces a new open source tool, Innotater, that provides an interactive Jupyter widget allowing a developer to annotate their data directly inline within the notebook. Having an easy way to clean and augment data can quickly lead to better predictive models.", "We will use this approach in a computer vision task to, first of all, manually filter out images that should never have made it into our dataset in the first place, improving a simple butterfly classifier.", "We then use Innotater to draw butterfly bounding boxes on a subset of our images. This allows us to train a bounding box prediction model that we run on every image in our dataset so we can generate cropped, zoomed-in versions of the originals. Then we go on to train a new classifier with an improved accuracy since the model doesn\u2019t need to consider as much irrelevant background imagery.", "This article is aimed at readers with some existing understanding of computer vision models in Jupyter notebooks who may be interested in finding out about the tools for annotating images, and also the \u2018trick\u2019 used to improve the model by drawing our own bounding boxes manually and writing a new model to zoom in on that.", "All the code is available in Jupyter Notebooks on GitHub here so you can follow along. It is written in Python and uses fast.ai which is a framework that sits on top of PyTorch. Fast.ai keeps things at a pretty high level and provides best practice defaults, so the deep learning code is light.", "The task of building a classifier to identify two different butterfly species is borrowed from a previous Towards Data Science article by Bert Caramans. He wrote a script to download photos from Flickr that are tagged either \u201cGatekeeper\u201d or \u201cMeadow Brown\u201d \u2014 two butterfly species that are notoriously confused when volunteers attempt to count butterfly populations in the wild for conservation purposes.", "Our first notebook downloads the images from Flickr. Instead of the usual \u2018one folder per class\u2019 file storage model, we actually place all images in one folder and build a CSV file listing the supposed class (based on Flickr tag) of each image. The reasons for doing things this way are so that we can extend the CSV to record our manually-drawn bounding boxes, and also easily change the class of the image if we find that the incorrect species has been identified. We can do these things within the Innotater then easily save the modified classes and bounding boxes back to the CSV.", "The Innotater tool is designed to be a quick and easy way to step through your images and mark important facts or augment data on each image.", "There are a few things we want to note about each image:", "Bounding boxes will allow us to build a more accurate model at a later stage, although to build our first simple classifier we only actually need data from the first two points above.", "The second notebook is used to perform these steps. Please note that you can\u2019t see the Innotater widget in GitHub previews, so hopefully the screenshot below will bring it to life. We use Pandas to read in the CSV and extract three NumPy matrices that will be fed into the Innotater.", "classes is a NumPy list of 0's or 1's specifying whether each image in the dataset is (currently) the Gatekeeper or Meadow Brown species.", "excludes is also an array of 0\u2019s or 1\u2019s. It starts off as all 0\u2019s but we may turn an entry to 1 in order to exclude the corresponding image from our dataset going forward.", "bboxes is a four-column matrix containing x,y,w,h of our bounding box for each image, where (x,y) is the top left corner of the box, w is the width, and h is the height. These all start as 0\u2019s until we draw any boxes manually.", "Each row of the matrices above corresponds to the \u2018filename\u2019 column in the Pandas DataFrame which we loaded from the CSV.", "We\u2019re nearly ready to invoke the Innotater (at home you\u2019ll need to pip install jupyter_innotater too!), but first we need to think about the order in which we step through the images.", "Due to the way the CSV was created, we have nearly 500 images of Meadow Brown butterflies at the start of the file, and then nearly 500 Gatekeeper butterflies in the second half. Since we\u2019re not expecting to draw bounding boxes on every single image \u2014 maybe just 200 or so \u2014 if we step through in the default order then this causes a problem. If we draw bounding boxes on each image we see until we\u2019ve annotated enough then we\u2019ll only have bounding boxes on a subset of the first butterfly species. Gatekeeper butterflies won\u2019t have any!", "So in cell number 7 we use a bit of Python/NumPy manipulation to create a new mapping called indexes which specifies a new ordering based on index numbers. The new ordering shows the first Meadow Brown, then the first Gatekeeper, then the second Meadow Brown, and so on\u2026", "In order to view and edit all important aspects of the dataset, this is how we cause the user interface of the Innotater to appear:", "The syntax for launching the Innotater widget is designed to be simple and flexible. The format is Innotater(inputs, targets, indexes=indexes) where inputs and targets are arrays (or just single items) of special Innotation objects which are essentially wrappers around your dataset\u2019s matrix representations. Generally, inputs wrap data on the \u2018x\u2019 side of your data science problem and don\u2019t expect to be altered; targets are the \u2018y\u2019 side and may need changing \u2014 for example, changing the classification or entering bounding box data.", "Innotation classes are flexible in terms of the data format you provide; you just need to make sure you pick the right subclass of Innotation for the type of data. For example, the images themselves (the \u2018x\u2019 side of our machine learning task) just need to be wrapped in an ImageInnotation object like this: ImageInnotation(filenames, path='./images'). The path argument is optional if your filenames are already absolute or relative to the working folder, and in fact you don\u2019t need to supply filenames: you can provide already-loaded matrices, perhaps imported using open_image in Open CV2.", "On the targets side, we use BinaryClassInnotation(excludes) to represent the 0\u2019s and 1\u2019s of the excludes array as a checkbox next to each image. The excludes variable isn\u2019t really on the \u2018y\u2019 side of our problem, but we want to be able to edit it, and we\u2019ll use it to filter out images where excludes==1 going forward.", "Genuine \u2018y\u2019 side targets include the classification of butterflies, turned into a listbox component throughMultiClassInnotation(classes, classes=cats). Note we could have used BinaryClassInnotation here again since we only have two classes (0 or 1), but a checkbox doesn\u2019t feel right to switch between two different species (\u2018check the box for Gatekeeper, uncheck for the other species\u2019), and a listbox approach scales if we want to add more species in future. The classes variable itself can be in many forms: a simple Python list of 0\u2019s and 1\u2019s, a NumPy column vector, or a two-dimensional one-hot encoding of the data. The Innotater inspects your data and works with it accordingly.", "The most interesting Innotation class we\u2019re going to use is perhaps BoundingBoxInnotation(bboxes) which initially displays as a single text box where we can enter each box\u2019s (x, y, w, h) shape as a comma-separated list of numbers. Even better, it automatically connects to the ImageInnotation we provided in inputs so that we can draw the box over the image itself and have our bounding box co-ordinates set automatically to represent the shape we\u2019ve drawn!", "The full code to instantiate the widget is:", "Using the Next/Prev buttons you can step through each image and draw boxes, change classes, or check the \u2018excludes\u2019 checkbox. As you do so, the underlying Python variables bboxes, classes, and excludes will update instantly. So at any point, in our notebook we can visit cell 12 below the widget, set the updated variables back into the Pandas DataFrame (the variable called df) and write the CSV file to disk:", "The way the notebook is set up means we can come back in a different notebook session, load the latest CSV values in, and continue annotating. As long as you explicitly save the CSV in each session you don\u2019t have to annotate all the data in one sitting.", "Now we\u2019ve inspected and annotated our data, let\u2019s do something with it! There are three notebooks involved in this section, numbered 3 to 5 in the butterflies GitHub repo.", "First of all, in 3 - Basic Train.ipynb, after eliminating any images marked as \u2018exclude\u2019 by ourselves in the Innotater, we just train a basic classifier model. This is \u2018cats or dogs\u2019 in the canonical machine learning tutorial. The fast.ai framework does so much of this for us that there really is very little machine learning code here.", "Most code is boilerplate borrowed from fast.ai examples. The code is commented to explain what\u2019s happening: loading the CSV into a Pandas DataFrame, using that to make a \u2018DataBunch\u2019 object containing train and test datasets, then using that object to provide training data to a pre-trained ResNet50 model. It uses an Adam optimiser to train with most existing layers frozen for 10 epochs; then the model is \u2018unfrozen\u2019 so that all layers can be fine-tuned for a further 5 epochs.", "Choosing our validation set is something that needed some thought in this project. Reserving 20% of our dataset for validation purposes seems sensible, and there is a fast.ai function to do so by random selection. But this leads to \u2018data leakage\u2019 \u2014 images in the validation set might be very similar to images in the training set, allowing the model to \u2018cheat\u2019 by clinging to irrelevant artefacts of those images. This happens because images from the same Flickr album will typically sit sequentially in the DataFrame. So a safer approach is to take the first 80% of each class\u2019 images for training and leave the remaining 20% for validation. This way, in the worst case we only split one album across train and validation sets.", "With the basic model we end up with an accuracy of 82%. Not too bad!", "The whole intention of this project was to see if drawing our own bounding boxes could help us build a better model. The theory was that building a model to predict tight bounding boxes, showing exactly where the butterfly features in the image, would mean we can crop and zoom into the butterfly itself and hopefully train a classifier explicitly on the zoomed images.", "For a new unseen butterfly photo, we would run our classification process in two stages: first, predict the bounding box so we can zoom in on the butterfly; secondly, run our classifier on the zoomed image.", "These two stages are developed in the final two notebooks, numbered 4 and 5 in GitHub.", "The first part of notebook 4 - BBox Train and Generate.ipynb works very similarly to notebook 3 in that it uses similar infrastructure to train a model. In this case, we\u2019re predicting bounding boxes instead of just \u20180 or 1\u2019 classification, so it\u2019s a bit more involved. To build the model, we first remove any images where we didn\u2019t get round to drawing a bounding box \u2014 remember we never intended to annotate all images. We also have to write our own fast.ai classes to handle bounding boxes (fast.ai\u2019s own infrastructure for this wasn\u2019t quite ready at the time of writing).", "Optimisation is similar, but we use the L1 loss measurement (sum of absolute horizontal and vertical distances between target and predicted co-ordinates) in order to see how well the model is performing against our manual drawings. The notebook shows a few different attempts to try to get better bounding box predictions \u2014 it\u2019s a bit messier than the previous notebook \u2014 but in any case, by the end we have some reasonable-looking bounding boxes. We could do a lot better, and the boxes often cut off important butterfly markings that will probably be meaningful to the classifier in the next stage! Anyway, you can certainly try to improve on this, but let\u2019s keep going\u2026", "The last cell in notebook 4 applies the model to all images in the CSV (except those with excludes marked as 1) in order to output our cropped and zoomed images. At training time we could only make use of those images where a bounding box was present, but now we\u2019ve trained the model we can apply it to every single image to get a full set of bounding box predictions. The code runs through each image and generates a \u2018zoomed\u2019 version of the image based on those bounding box co-ordinates \u2014 each new image hopefully containing a nice big centred butterfly.", "Our original set of images would have some butterflies featuring in a relatively small section of the overall image photo. This introduces a lot of noise in the borders, and since our images are resized to 256 pixels square in the preprocessing of our \u2018basic train model\u2019, a lot more of the butterfly itself should find its way into the neural network\u2019s layers if we train again on our zoomed images.", "Since we already performed all the zooming and cropping for all images at the end of notebook 4, you\u2019ll find that notebook 5 - Zoomed Cropped Train.ipynb is pretty much an exact copy of notebook 3, except it runs on the new zoomed images (which were saved into a \u2018zoomed\u2019 subfolder). It seems only fair to use the same training steps that we used when we trained the basic model before: we want to be able to compare the models to see if the model trained on zoomed images performs better.", "By the end of training, we see an accuracy of 84% (up from 82% in the previous version). That\u2019s definitely going in the right direction!", "In truth, you could do a much better job training all of these models \u2014 my aim was never to teach you to successfully train computer vision neural networks. It\u2019s entirely possible that a single better-structured neural network could emulate some of the \u2018zooming\u2019 that takes place in our combined model. The input data themselves are shaky since, for a relatively small dataset, we can only hope that Flickr users all take a consistent approach to taking photos then tagging and uploading them.", "But I hope this project shows that the Innotater is a fun way to get your hands dirty with the data, and not only clean up your dataset but also encapsulate \u2018human insight\u2019 manually that might not otherwise find its way into your modelling process. Ultimately, in this example we are already relying on humans to label the butterfly species manually, so why not take it a bit further yourself and teach your model what a butterfly looks like in the first place?", "At the time of writing, images, single bounding boxes, and the listbox/checkbox controls are the only available wrappers for Innotater data. The ways in which they can be combined is already very flexible, but of course you might need other annotation types (maybe multiple bounding boxes and different shapes) depending on your project. Please do get in touch describing the problems you\u2019re facing with your data, or limitations in trying to use the Innotater, and further development can incorporate your ideas and solutions! Further details can be found on the Innotater GitHub page.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Entrepreneur building tools for data scientists: containds.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbdbcc9001734&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dslester?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslester?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Dan Lester"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6bb58cd796c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&user=Dan+Lester&userId=b6bb58cd796c&source=post_page-b6bb58cd796c----bdbcc9001734---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater"}, {"url": "https://github.com/ideonate/butterflies", "anchor_text": "Jupyter Notebooks on GitHub here"}, {"url": "https://www.fast.ai/", "anchor_text": "fast.ai"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://towardsdatascience.com/classify-butterfly-images-with-deep-learning-in-keras-b3101fe0f98", "anchor_text": "Towards Data Science article by Bert Caramans"}, {"url": "https://github.com/ideonate/butterflies/blob/master/1%20-%20Butterfly%20Downloads.ipynb", "anchor_text": "first notebook"}, {"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater tool"}, {"url": "https://github.com/ideonate/butterflies/blob/master/2%20-%20Butterfly%20Innotater.ipynb", "anchor_text": "second notebook"}, {"url": "https://github.com/ideonate/butterflies/blob/master/2%20-%20Butterfly%20Innotater.ipynb", "anchor_text": "our notebook"}, {"url": "https://github.com/ideonate/butterflies", "anchor_text": "butterflies GitHub repo"}, {"url": "https://github.com/ideonate/butterflies/blob/master/3%20-%20Basic%20Train.ipynb", "anchor_text": "3 - Basic Train.ipynb"}, {"url": "https://github.com/ideonate/butterflies/blob/master/4%20-%20BBox%20Train%20and%20Generate.ipynb", "anchor_text": "4 - BBox Train and Generate.ipynb"}, {"url": "https://github.com/ideonate/butterflies/blob/master/5%20-%20Zoomed%20Cropped%20Train.ipynb", "anchor_text": "5 - Zoomed Cropped Train.ipynb"}, {"url": "https://github.com/ideonate/jupyter-innotater", "anchor_text": "Innotater GitHub page"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bdbcc9001734---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/pandas?source=post_page-----bdbcc9001734---------------pandas-----------------", "anchor_text": "Pandas"}, {"url": "https://medium.com/tag/fastai?source=post_page-----bdbcc9001734---------------fastai-----------------", "anchor_text": "Fastai"}, {"url": "https://medium.com/tag/jupyter?source=post_page-----bdbcc9001734---------------jupyter-----------------", "anchor_text": "Jupyter"}, {"url": "https://medium.com/tag/jupyter-notebook?source=post_page-----bdbcc9001734---------------jupyter_notebook-----------------", "anchor_text": "Jupyter Notebook"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&user=Dan+Lester&userId=b6bb58cd796c&source=-----bdbcc9001734---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&user=Dan+Lester&userId=b6bb58cd796c&source=-----bdbcc9001734---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbdbcc9001734&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bdbcc9001734---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bdbcc9001734--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bdbcc9001734--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bdbcc9001734--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslester?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslester?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dan Lester"}, {"url": "https://medium.com/@dslester/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "432 Followers"}, {"url": "http://containds.com", "anchor_text": "containds.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6bb58cd796c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&user=Dan+Lester&userId=b6bb58cd796c&source=post_page-b6bb58cd796c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa12e45f04087&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734&newsletterV3=b6bb58cd796c&newsletterV3Id=a12e45f04087&user=Dan+Lester&userId=b6bb58cd796c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}