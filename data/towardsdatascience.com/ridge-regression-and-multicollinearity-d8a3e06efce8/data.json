{"url": "https://towardsdatascience.com/ridge-regression-and-multicollinearity-d8a3e06efce8", "time": 1683002817.157532, "path": "towardsdatascience.com/ridge-regression-and-multicollinearity-d8a3e06efce8/", "webpage": {"metadata": {"title": "SVD in Machine Learning: Ridge Regression and Multicollinearity | by Kunyu He | Towards Data Science", "h1": "SVD in Machine Learning: Ridge Regression and Multicollinearity", "description": "This article discusses what is multicollinearity, how can it compromise least squares, and how ridge regression helps avoid that from a perspective of singular value decomposition (SVD). It is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.wikiwand.com/en/Singular_value_decomposition", "anchor_text": "singular value decomposition", "paragraph_index": 0}, {"url": "https://voices.uchicago.edu/willett/", "anchor_text": "Rebecca Willet", "paragraph_index": 0}, {"url": "https://voices.uchicago.edu/willett/teaching/fall-2019-mathematical-foundations-of-machine-learning/", "anchor_text": "Mathematical Foundations of Machine Learning", "paragraph_index": 0}, {"url": "http://mlwiki.org/index.php/Normal_Equation#Normal_Equation", "anchor_text": "normal equation", "paragraph_index": 8}, {"url": "https://www.rdocumentation.org/packages/faraway/versions/1.0.7/topics/seatpos", "anchor_text": "seatpos", "paragraph_index": 20}, {"url": "https://www.wikiwand.com/en/Root-mean-square_deviation", "anchor_text": "RMSE", "paragraph_index": 28}], "all_paragraphs": ["This article discusses what is multicollinearity, how can it compromise least squares, and how ridge regression helps avoid that from a perspective of singular value decomposition (SVD). It is heavily based on Professor Rebecca Willet\u2019s course Mathematical Foundations of Machine Learning and it assumes basic knowledge of linear algebra.", "In this section, we will look at multicollinearity and how can it compromise least squares.", "Consider a matrix X of shape n \u00d7 p. For its columns X\u2081, X\u2082, \u2026, X\u209a \u2208 \u211d\u207f, we say they are linearly independent when \u2211\u03b1\u1d62X\u1d62 = 0 if and only if \u03b1\u1d62 = 0 for i = 1, 2, \u2026, p. Intuitively, none of the columns in X can be written as a weighted sum of the others. The other way around, if it\u2019s not the case for some columns, we call them linearly dependent. Assume rank(X) = r, then (p \u2014 r) columns of X are linearly dependent.", "Multicollinearity, in regression terms, refers to a phenomenon where a predictor in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In other words, the predictor is approximately a linear combination of the others. Perfect multicollinearity indicates linear dependency in the feature matrix. Intuitively, it implies redundancy in our features that some of them fail to provide unique and/or independent information to the regression.", "Multicollinearity matters not only theoretically, but also for the practice. The coefficient estimates may change erratically in response to small changes in the model or the data, and themselves do not make sense at all. Why is that? We\u2019ll look at it from an SVD perspective. Before that, below is a quick recap on SVD.", "For the matrix X, there always exists matrices U, \u03a3, V such that:", "Where both U and V are orthogonal:", "The diagonal elements of \u03a3 are called singular values (\u03c3\u2081 \u2265 \u03c3\u2082 \u2265 \u2026 \u2265 \u03c3\u209a \u2265 0). The number of non-zero singular values is the rank of the matrix X, and the columns of \u03a3 are the basis for the rows of X.", "Recall that for the feature matrix X and the target variable y, least squares attempts to approximate the solution of the linear system y = Xw by minimizing the sum of squares of the residuals \u2225y \u2014 Xw\u2225\u00b2. The weights vector can be written with the normal equation:", "Note that A is invertible if and only if n \u2265 p and rank(X) = p. Now it\u2019s not hard to see why perfect multicollinearity is a major problem for least squares: it implies that the feature matrix is not full-rank so we cannot find a proper set of coefficients that minimize the sum of squared residuals.", "However, why multicollinearity, or strong multicollinearity in specific, is problematic, either? Let\u2019s find out from an SVD perspective.", "Consider the true weights w, we know that y = Xw + \u03f5, where \u03f5 is some neglectable noise or error:", "We can see that the least squares coefficients deviate from the true weights by \u03f5 multiplied by some inflation term. Take a closer look at the inflation term as in the previous post linked at the end of this article:", "If all the columns of X are linearly independent, we still have p singular values and \u03c3\u2081 \u2265 \u03c3\u2082 \u2265 \u2026 \u2265 \u03c3\u209a > 0. However, with the presence of multicollinearity, some \u03c3\u1d62, \u03c3\u209a for example, will be close to zero. Then the diagonal element 1/\u03c3\u209a will be huge, leading to a really large inflation term and therefore a great deviation in the least squares weights from the true weights. Intuitively, multicollinearity can compromise least squares as it leads to small singular values. The estimation errors of the coefficients are inflated by the reciprocals of those singular values and therefore become too large to be neglected.", "How can we avoid this? One possibility is ridge regression.", "In this section, we will understand how ridge regression helps avoid the problems caused by multicollinearity in linear regression through a formal derivation.", "Ridge regression builds on least squares by adding a regularization term in the cost function so that it becomes \u2225y \u2014 Xw\u2225\u00b2 + \u03bb\u2225w\u2225\u00b2, where \u03bb indicates the strength of regularization. We can write the cost function f(w) as:", "Then we can compute the gradient and set it to zero:", "Take a closer look at the middle part of the expression:", "Consider \u03c3\u209a \u2248 0, this time \u03c3\u209a/(\u03c3\u209a\u00b2 + \u03bb) \u2248 0 if and only if \u03bb \u2260 0. Therefore, with ridge regression, the coefficients of unimportant features will be close to zero (but will not be exactly 0 unless there is perfect multicollinearity) and the error term would not be inflated to an explosion. Note that when there is no regularization (\u03bb = 0) things go back to least squares. Also, for most occasions (\u03c3\u209a \u226b \u03bb), \u03c3\u209a/(\u03c3\u209a\u00b2 + \u03bb) \u2248 1/\u03c3\u209a just like in least squares.", "In this section, we\u2019ll work on a sample dataset seatposto verify our previous findings.", "Car drivers like to adjust the seat position for their own comfort. Car designers would find it helpful to know where different drivers will position the seat depending on their size and age. Researchers at the HuMoSim laboratory at the University of Michigan collected data on 38 drivers.", "The dataset contains the following features:", "We want to fit a linear model that predicts hipcenter based on all the other features in the dataset. From the descriptions, some features can be closely related to one another. For example, Weight and Ht, Ht and HtShoes. Plot the correlation matrix:", "The plot above confirms our guess. We have strong multicollinearity in our feature matrix. The good news is that our target hipcenter is strongly correlated with most of the features and we can expect a good fit. Codes that produces the correlation plot above:", "Use the following code chunk to:", "For simplicity, first look at a model with only Ht and HtShoes as predictors.", "Surprisingly, although Ht and HtShoes are nearly perfectly correlated, their partial effects on hipcenter have the opposite signs. This can be a result of strong multicollinearity. Fit a ridge regression model with \u03bb = 10 instead.", "The coefficients of ridge regression seem to make more sense. Compare its test RMSE with that of the least squares.", "For the bivariate linear model, ridge regression results in a better ability to generalize. However, since ridge regression introduces a regularization term, its bias can be higher in exchange for a lower variance sometimes, which may lead to worse fit.", "At last, let\u2019s look at the full model and check how the magnitudes of the coefficients differ across least squares and ridge regression, and how they change with the strength of penalty, \u03bb.", "We can see that least squares weights differ greatly from ridge regression weights on Ht and HtShoes as expected. Ridge regression weights get closer to zero as the penalty gets stronger. Codes that produces the plot above:", "This article is written by Kunyu He. Kunyu is a current master\u2019s student at The University of Chicago. He finds it interesting to understand statistical modeling and machine learning techniques, apply them to real-world data, and help create end-to-end solutions in the financial services industry. Connect with Kunyu on LinkedIn! \ud83d\udc37", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist in training at the MSc Computational Analysis program at The University of Chicago."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd8a3e06efce8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kunyuhe?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kunyuhe?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Kunyu He"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7cbdd7d493fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&user=Kunyu+He&userId=7cbdd7d493fd&source=post_page-7cbdd7d493fd----d8a3e06efce8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/suju-165106/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4727668", "anchor_text": "Susanne Jutzeler"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4727668", "anchor_text": "Pixabay"}, {"url": "https://www.wikiwand.com/en/Singular_value_decomposition", "anchor_text": "singular value decomposition"}, {"url": "https://voices.uchicago.edu/willett/", "anchor_text": "Rebecca Willet"}, {"url": "https://voices.uchicago.edu/willett/teaching/fall-2019-mathematical-foundations-of-machine-learning/", "anchor_text": "Mathematical Foundations of Machine Learning"}, {"url": "http://mlwiki.org/index.php/Normal_Equation#Normal_Equation", "anchor_text": "normal equation"}, {"url": "https://www.rdocumentation.org/packages/faraway/versions/1.0.7/topics/seatpos", "anchor_text": "seatpos"}, {"url": "https://www.wikiwand.com/en/Root-mean-square_deviation", "anchor_text": "RMSE"}, {"url": "https://www.linkedin.com/in/kunyuhe/", "anchor_text": "Kunyu He - Incoming Global Quantitative Strategies Off-Cycle Analyst - Bank of America Merrill\u2026Data scientist in training in the MSc Computational Analysis program at The University of Chicago. Passionate about the\u2026www.linkedin.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d8a3e06efce8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/multicollinearity?source=post_page-----d8a3e06efce8---------------multicollinearity-----------------", "anchor_text": "Multicollinearity"}, {"url": "https://medium.com/tag/ridge-regression?source=post_page-----d8a3e06efce8---------------ridge_regression-----------------", "anchor_text": "Ridge Regression"}, {"url": "https://medium.com/tag/svd?source=post_page-----d8a3e06efce8---------------svd-----------------", "anchor_text": "Svd"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d8a3e06efce8---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&user=Kunyu+He&userId=7cbdd7d493fd&source=-----d8a3e06efce8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&user=Kunyu+He&userId=7cbdd7d493fd&source=-----d8a3e06efce8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd8a3e06efce8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d8a3e06efce8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d8a3e06efce8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kunyuhe?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kunyuhe?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kunyu He"}, {"url": "https://medium.com/@kunyuhe/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "57 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7cbdd7d493fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&user=Kunyu+He&userId=7cbdd7d493fd&source=post_page-7cbdd7d493fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa75edff08c2a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fridge-regression-and-multicollinearity-d8a3e06efce8&newsletterV3=7cbdd7d493fd&newsletterV3Id=a75edff08c2a&user=Kunyu+He&userId=7cbdd7d493fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}