{"url": "https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec", "time": 1683014670.940087, "path": "towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec/", "webpage": {"metadata": {"title": "Meta-labeling and Stacking. How to boost your machine learning\u2026 | by Ke Gui | Towards Data Science", "h1": "Meta-labeling and Stacking", "description": "Advances in Financial Machine Learning by Marcos Prado. 7. Fractionally Differentiated Features is Chapter 5 about Fractionally Differentiated Features. 8. Data Labelling is Chapter 3 about The Triple-barrier Method. And 9. Meta-labeling is Chapter 3.6 on page 50."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 0}, {"url": "https://github.com/hudson-and-thames/mlfinlab", "anchor_text": "mlfinlab", "paragraph_index": 2}, {"url": "https://medium.com/@kegui/how-to-install-mlfinlab-without-error-messages-896e2fb43c2f", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089", "anchor_text": "Advances in Financial Machine Learning", "paragraph_index": 3}, {"url": "https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55", "anchor_text": "Fractionally Differentiated Features", "paragraph_index": 3}, {"url": "https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55", "anchor_text": "Fractionally Differentiated Features", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/the-triple-barrier-method-251268419dcd", "anchor_text": "Data Labelling", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec", "anchor_text": "Meta-labeling", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "here", "paragraph_index": 41}, {"url": "https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw", "anchor_text": "StatQuest by Josh Starmer", "paragraph_index": 41}, {"url": "https://sanchom.wordpress.com/tag/average-precision/", "anchor_text": "Here", "paragraph_index": 44}, {"url": "http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf", "anchor_text": "more informative", "paragraph_index": 45}, {"url": "https://dl.acm.org/doi/10.1145/2808194.2809481", "anchor_text": "When both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system.", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers", "anchor_text": "harmonic mean", "paragraph_index": 46}, {"url": "https://medium.com/@kegui/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b", "anchor_text": "Keras-tuner", "paragraph_index": 69}, {"url": "https://medium.com/@kegui/how-to-do-cross-validation-in-keras-tuner-db4b2dbe079a", "anchor_text": "my previous articles", "paragraph_index": 69}, {"url": "https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/", "anchor_text": "A Pirate\u2019s Guide to Accuracy, Precision, Recall, and Other Scores", "paragraph_index": 133}, {"url": "https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.&text=The%20role%20of%20feature%20importance%20in%20a%20predictive%20modeling%20problem.", "anchor_text": "How to Calculate Feature Importance With Python", "paragraph_index": 134}], "all_paragraphs": ["Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "Warning: There is no magical formula or Holy Grail here, though a new world might open the door for you.", "Note 1: How to install mlfinlab package without error messages can be found here.", "Note 2: If you are reading Advances in Financial Machine Learning by Marcos Prado. 7. Fractionally Differentiated Features is Chapter 5 about Fractionally Differentiated Features. 8. Data Labelling is Chapter 3 about The Triple-barrier Method. And 9. Meta-labeling is Chapter 3.6 on page 50.", "Note 3: Your results may vary given the stochastic nature of the algorithm or evaluation procedure or differences in numerical precision. But I did find a lot of people achieved higher scores because they normalized their train and test data in the wrong way. At the end of this article, I will reveal the big secret of high scores.", "Meta-labeling has been sitting on my writing list for a long time. It is a useful and powerful machine learning tool to be collected in any data scientists\u2019 toolbox, no matter what models you are using. Unfortunately, I hardly found any decent tutorial on this topic. Whereas stacking is one of the popular ensemble methods for matching learning. Stacking involves training a learning algorithm to combine the predictions of several other learning algorithms. As we know the basic idea of ensemble learning is to facilitate better predictive performance than could be obtained from any of the constituent learning algorithms alone. It is a \u201cwisdom of crowds\u201d approach distils information from several models into a set of highly accurate results. By that definition, meta-labeling should belong to ensemble methods as well.", "While stacking and meta-labeling share some similarities, they are fundamentally different. Stacking basically involves two steps. First, all the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. The processes of stacking are:", "Easy enough, we can think of it as adding extra features to our training data.", "Whereas meta-labeling utilized two layers of models, but with specifically different purposes. According to Marcos Lopez de Prado in his book Advances in Financial Machine Learning, Chapter 3, pg 50 (there is not much useful information online apart from this book).", "Meta-labeling is particularly helpful when you want to achieve higher F1-scores. First, we build a model that achieves high recall, even if the precision is not particularly high. Second, we correct for the low precision by applying meta-labeling to the positives predicted by the primary model.", "The central idea is to create a secondary ML model that learns how to use the primary model. This leads to improved performance metrics, including: Accuracy, Precision, Recall, and F1-Score etc..", "The reason why meta-labeling works as articulate in Marcos\u2019 book:", "Binary classification problems present a trade-off between type-I errors (false positives) and type-II errors (false negatives). In general, increasing the true positive rate of a binary classifier will tend to increase its false positive rate. The receiver operating characteristic (ROC) curve of a binary classifier measures the cost of increasing the true positive rate, in terms of accepting higher false positive rates.", "Generally speaking, the process of meta-labelling is like that:", "Meta-labelling is the topic I have spent a great deal of time trying to figure out the best way to apply this method. But still, there are too many unknowns, for instance:", "When we evaluate a method, it really depends on which metrics we pick and how well you optimise your model architecture, hyper-parameters etc..", "2. Can it apply to different models?", "As this method will use 2 different models, model variation is another thing we need to consider.", "3. Does meta-labelling only apply to certain data?", "Most of the references on this topic are from time-series data, what about non-sequence data?", "And so on so forth, but those questions are all about the limitation of meta-labeling , which I certainly would like to explore in this article. But due to the length limitation of this article, I may not be able to cover all of them.", "What differentiates meta-labeling from ensemble method, especially stacking, is the meta-labeling adding prediction from the primary model to both the features and label, whereas stacking only use it as a new feature. I can understand that those additional features (predictions) are represented by the models used to do the prediction. But why use it in labels? Even though that is probably where the name meta-labeling came from. Most importantly, is there any information leakage by adding predictions to the label? Well, I guess certainly there will be leakage from the primary model to the secondary model and that is the on purpose to get better scores.", "As many discussions can be found online on ensemble learning, especially stacking, meta-labeling has rarely had enough study. For that reason, this article will be focused on meta-labeling and its comparison with stacking.", "The following are the libraries used in this article.", "On the purpose of studying meta-labeling and stacking, we can generate a dummy binary classification dataset by sklearn.datasets.make_classification().But it is more fun to use a real dataset. The dataset we use here is credit card fraud detection by the ULB machine learning group. Let\u2019s have a quick look at the data first.", "pd.read_csv() can read the url and unzip the zipped file at the same time. We don\u2019t need any other functions or library to do that anymore.", "The dataset contains 28 anonymized features, 1 \u201camount\u201d feature, 1 \u201ctime\u201d feature, and 1 target variable-Class. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The features are anonymized to protect the privacy of the customers, which are the result of a PCA transformation, as the dataset is in the public domain. The only features which have not been transformed with PCA are \u2018Time\u2019 and \u2018Amount\u2019. Feature \u2018Time\u2019 contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature \u2018Amount\u2019 is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature \u2018Class\u2019 is the response variable, \u20180\u2019 as the target variable corresponds to the non-fraudulent cases whereas \u20181\u2019 in target variable corresponds to fraudulent cases.", "There is also a minimal correlation between variables \u2014 This may be as a result of PCA transformed variables.", "The reason why I choose this dataset is because it is harder to achieve a high score for both precision and recall from this dataset. The dataset is highly unbalanced since it has 492 (0.17%) frauds out of 284,807 transactions. 99.83% of transactions in this dataset were not fraudulent while only 0.17% were fraudulent.", "With such a small amount of fraudulent data, we have to be careful with our data process and model selection. The algorithms can easily cheat on us by predicting all the test data as fraudulent. With 99.9% of the dataset is Negative (non-frauds), the network will cleverly predict all to be Negative, leading to more than 99% accuracy. The result will look good but useless.", "That is why we need better metrics apart from accuracy.", "There is no single best measurement for results quality, the problem domain and data in question determine appropriate approaches.", "Most of the machine learning using accuracy as the default metric, if we use accuracy as the metric, as we know accuracy is the sum of True Negative and True Positive divided by total dataset size. Considering the True Negative is overwhelm the True Positive, the accuracy can be very high, but won\u2019t indicate the power of your model to classify the fraudulent data. That said, there are a few measurements that are commonly introduced thanks to their conceptual simplicity, ease of implementation, and wide usefulness. As we don\u2019t really know which one is the best for this dataset, I will list all of them in the article.", "And the following will give a little bit of introduction to each metric. Of course, there are plenty of materials about metrics can be found online. If you already knew most of them, still we can refresh our memories before jumping to the next section.", "I put these three metrics together as they are well-related.", "According to Wikipedia, precision is the number of correct results divided by the number of all returned results, whereas recall is the number of correct results divided by the number of results that should have been returned.", "And this graph below explains these two concepts better than 1000 words.", "Receiver Operator Characteristic (ROC) is the area under the curve where x is the false positive rate (FPR) and y is the true positive rate (TPR), are commonly used to present results for binary decision problems in machine learning. AUC is the area under the ROC curve that represents the tradeoff between Recall (TPR) and Specificity (FPR). Like the other metrics, AUC is between 0 and 1, with .5 as the expected value of random prediction.", "AUC-ROC curve is a performance measurement for the classification problem at various thresholds settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much model is capable of distinguishing between classes.", "Unlike precision-recall curves, ROC (Receiver Operator Characteristic) curves work best for balanced data sets.", "AUC near to the 1 which means it has a good measure of separability. A poor model has AUC near to the 0 which means it has the worst measure of separability. In fact, it means it is reciprocating the result.", "A well-articulated article on this topic can be found here. Also, a nice video of ROC and AUC can be found from StatQuest by Josh Starmer.", "Average Precision is a single number used to summarise a Precision-Recall curve(PR AUC), that makes it possible to compare different models. PR AUC is the area under the curve where x is recall and y is precision. The general definition for the Average Precision (AP) is finding the area under the precision-recall curve above.", "Precision and recall are always between 0 and 1. Therefore, AP falls within 0 and 1 also.", "Here is a nice article about this topic.", "However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm\u2019s performance. When both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system.", "The traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall. We calculate the F1-score as the harmonic mean of precision and recall to accomplish just that. While we could take the simple average of the two scores, harmonic means are more resistant to outliers. Thus, the F1-score is a balanced metric that appropriately quantifies the correctness of models across many domains.", "F1 score is applicable for any particular point of the ROC curve. This point may represent for example a particular threshold value in a binary classifier and thus corresponds to a particular value of precision and recall.", "Remember, F1 score is a smart way to represent both recall and precision. For F1 score to be high, both precision and recall should be high.", "Thus, the ROC curve is for various different levels of thresholds and has many F1 score values for various points on its curve.", "With such highly unbalanced dataset, a confusion matrix doesn\u2019t make much sense. I add it to the metrics collection, merely for reference. It can be self-explained by the following table:", "I summarized all the metrics in one function for the easy call later on.", "These evaluation metrics are very important. It is kind of like an interface between data scientists and business people. For most people who only heard of AI but never trained on any model before will not pay much attention to things like log loss, cross-entropy, and other cost functions. That is why we need metrics to intuitively explain results to business people. The ability to convey complex results as simply as possible to non-data scientists is one of the essential skills for applied data scientists to master.", "The following are those three models that I am going to explore:", "The reason I picked those 3 models is that they are highly uncorrelated. The standalone solutions need to be relatively uncorrelated. If they are very correlated, once put them into an ensemble model, the strengths of one will mirror those of the rest, and the same will be true with the weaknesses. We will see little benefit from diversifying via an ensemble.", "Before we get to the models, there are still a few things that need to be done, namely, feature-wise normalization, input and labels separation and splitting training and testing data.", "As the dataset is highly unbalanced, the parameter stratify =data_yfrom scikit-learn\u2019s train_test_split() function comes so handily. Data is split in a stratified fashion in a blink.", "You may notice that I use data_original, x_test_orignal, y_test_orignal as the variable name. I want to put aside those three sub-datasets as there will be a lot of tweaks on those original data and we don\u2019t want to mess around with test data by any means.", "Training dataset has 213605 transaction records with 29 features while testing dataset has 71202 transaction records with 29 features.", "Among 71202 transaction records in the testing dataset, only 123 records are fraudulent.", "Note that the test data are standardized by the mean and standard deviation of the training data. You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization. In another word, never ever touch your test data!", "With data ready and metrics settled, here come the models:", "In the beginning, as this article is not about achieving the best score, I used the default setting of Logistic Regression from sklearn. The precision score is very low, around 0.07. With the same default setting, some of the tutorial online on the same dataset with Logistic Regression shows a really high score of precision. However, the way they process the test data is problematic. To get a better result GridSearchCV was employed to search for the best parameters.", "The best score I got is", "Let\u2019s check the results and metrics score on test dataset:", "Pretty good, remember the number of fraudulent transactions is 123.", "For lightBGM, I further assign the 1/4 of the train data as validation dataset.", "The result is decent, no further hyper-parameter tunning conducted.", "3. Model 3 (Deep Neuron Network)", "For Deep Neuron Network (DNN), I used two callbacks, the EarlyStopping() and ReduceLROnPlateau() to achieve better results. Again, as the results are decent, no hyper-parameters tunning implemented. (Keras-tuner is really good, you can check my previous articles to get a better understanding.)", "It is a simple 3 layers DNN with small units number (32, 16, 8) to avoid overfitting.", "DNN is very sensitive to input features. If you add time feature into training data, the result is some kind of odd. But once you remove the time feature, it goes to normal.", "Now we can train our data on model 3.", "Without any hyper-parameter tunning, the results are similar to optimised Model 1 Logistic Regression. Generally, if we include similarly strong solutions from different machine learning families (such as one from random forests and one from neural networks), the ensemble of the solutions will lead to a better result than any of the standalone solutions. This is because eachof the standalone solutions has different strengths and weaknesses. Byincluding the standalone solutions together in an ensemble, the strengthsof some of the models compensate for the weaknesses of the others, and vice versa. The results so far from 3 very different models seem to satisfy our request for stacking and meta-labeling.", "As stacking is kind of like adding new features to the input data and new features come from the prediction of primary models. We first do a feature engineering to stack all the data.", "Now add the new features (predictions from 3 models) to the training dataset.", "Then, the same process applies to the test dataset.", "As we have new features added to the previous dataset, it would be interesting to see the correlation of those new features.", "We do see the predictions from primary models are highly correlated, which is not surprising. The information from the 1st model leaks into the 2nd model as they share the same training data. As long as the test data is intact, we will prefer more information flowing into the 2nd model to achieve better results.", "Now we need to go through all those tedious but necessary data processes.", "When it comes to stacking, there are important caveats, though. If the standalone solutions are similarly strong, the ensemble will have better performance than any of the standalone solutions. But if one of the solutions is much better than the others, the ensemble\u2019s performance will equal the performance of the best standalone solution; the subpar solutions will contribute nothing to the ensemble\u2019s performance.", "As model 2 (lightBGM) has the highest precision so far, we will use model 2 as the secondary model.", "Here 120 positive results identified in the comparison of 102 cases on the same model without stacking.", "The recall improves a lot from 0.77 to 0.85 and there is a modest improvement on f1 score and average precision as well.", "2. Model 3 DNN as the secondary model", "What happens if we use DNN as the second model?", "Well, here much less positive results from the same model, 88 vs 116, that indicates higher precision and low recall.", "3. Model 1logistic regression as the secondary model", "I didn\u2019t expect better results, purely curious.", "Almost all the metrics have more or less improvement, and actually, the results are not too bad comparing with lightBGM or DNN as the second model.", "As meta-labeling will require to add new features to both input and labels. I wrote another function to deal with that.", "With data ready, for the first experiment, I will use the logistic regression as the primary model and lightBGM as the secondary model.", "Again, let\u2019s check the correlation of meta-data.", "Very strong correlation between the label and added features (prediction from model 1). The high Pearson correlation coefficient indicates more information leaked from the first model into the 2nd model.", "Now, again, we need to go through all those tedious but necessary data processes.", "Then normalize the test and train dataset.", "and split the training dataset to get the validation data again.", "After all this, we can finally go to the model.", "After we have the prediction from the meta-model, we combine the result with the prediction from the primary model.", "Doesn\u2019t look like there is a difference. Let\u2019s see all the metrics.", "The results are much more balanced between precision and recall now. The precision, recall, and f1 score all are improved from 0.93, 0.77, 0.84 to 0.96, 0.81, 0.88 respectively along with other metrics.", "2. 1st model: logreg and 2nd model: DNN", "As the first model is still logistic regression, there is no need to update the data. Let\u2019s go to the model straight away.", "Again, the final results will be the intersection of primary prediction and secondary model prediction.", "Well, I seriously start to doubt the necessity of the final step.", "Well, the precision does getting better at the cost of the lower recall. Just like what happened on the stacking method with DNN as the 2nd model, but slightly better.", "3. 1st model: logreg + lightBGM and 2nd model: DNN", "The way how the stacking method works make me wondering what happens if I take both model 1 and model 2 as primary models and model 3 as the final model? Will that improve the final results? Let\u2019s try it this way.", "Because this time we will have extra feature needs to be added to the input, I rewrote the data processing function again.", "Then, we will apply the function to both train data and test data.", "And split the train data to give validation dataset.", "Ok, let\u2019s train our data on model 3.", "Very likely, the final step won\u2019t change anything for this highly unbalanced dataset. Let\u2019s check the final score.", "It is better for all the metrics in comparison of stacking method with DNN as the 2nd model, or standalone DNN. But not as good as the lightBGM as the 2nd models.", "4. 1st model: logreg + DNN and 2nd model: lightBGM", "As lightBGM seems better than others as the 2nd model, this is the last combination I would like to try. We need to reprocess our dataset again.", "Now, we can start to train our model.", "The result is better than the previous one.", "As we know now, stacking and meta-labeling are kind of like feature engineering methods to add extra features to the training data. But how important of these added new features comparing with the original features. Thanks to feature_importance() function in scikit-learn, we can now implement this function to get some sense of the importance of those features.", "By definition, there are two importance type, \u201csplit\u201d and \u201cgain\u201d. If \u201csplit\u201d, the result contains numbers of times the feature is used in a model. If \u201cgain\u201d, the result contains total gains of splits which use the feature. Let\u2019s have a look at both of them.", "Both importance type show meta-data are far more important than original features. We can see that the same results go to our best model.", "From the feature importance value, we know that predictions from primary models have the biggest impact on the second model's results, from the perspective of both how many times the feature is used in a model ( \u201csplit\u201d) and total gains of splits which use the feature(\u201cgain\u201d). We also know that from the correlation plot, predictions from primary models have really high correlations (~0.9) with the labels(\u201cClass\u201d).", "Also, we know that there will be information leakage from the primary models to the second model, which is preferred though.", "So, if there is slight leakage from test data to the training data, the leaked information will be amplified by the ways mentioned above. And models like DNN are really good at taking the shortcut and pick that information and give really high scores.", "Let me show you in the code.", "If you normalize all your data in one go before splitting your data to training and testing datasets, the mean and standard deviation you used are from both training data and testing data. Some of the information from the test dataset will be shared with the training dataset, then amplified by meta-labeling afterwards.", "With logistic regression and DNN as the primary models and lightBGM as the second model, the following are the scores on test data.", "Mind you, these scores are achieved by the basic models without optimization and the default setting. It looks good in numbers but useless for unknown data.", "All in all, the best result is using meta-labeling with lightBGM as the 2nd model and logistic regress as the primary model.", "With a bit of effort, we can get better scores. The extra improvement may not seems dramatic, but it is the deal of win or loss in some competitions when the scores of 1st place and 2nd place are so close.", "I would like to end this article with 2 of my favourite laws on metrics:", "\u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d", "\u201cThe more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures\u201d", "3. A Pirate\u2019s Guide to Accuracy, Precision, Recall, and Other Scores", "5. How to Calculate Feature Importance With Python", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An ordinary guy who wants to be the reason someone believes in the goodness of people. He is living at Brisbane, Australia, with a lovely backyard."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff17a7f9804ec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kegui.medium.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": ""}, {"url": "https://kegui.medium.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Ke Gui"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2f79e4a45fc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&user=Ke+Gui&userId=2f79e4a45fc7&source=post_page-2f79e4a45fc7----f17a7f9804ec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/feature-engineering-feature-selection-8c1d57af18d2", "anchor_text": "\ud83d\udcc8Python for finance series"}, {"url": "http://skuawk.com/", "anchor_text": "Dave Gandy"}, {"url": "https://creativecommons.org/licenses/publicdomain/", "anchor_text": "Public Domain Dedication License"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "https://github.com/hudson-and-thames/mlfinlab", "anchor_text": "mlfinlab"}, {"url": "https://medium.com/@kegui/how-to-install-mlfinlab-without-error-messages-896e2fb43c2f", "anchor_text": "here"}, {"url": "https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089", "anchor_text": "Advances in Financial Machine Learning"}, {"url": "https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55", "anchor_text": "Fractionally Differentiated Features"}, {"url": "https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55", "anchor_text": "Fractionally Differentiated Features"}, {"url": "https://towardsdatascience.com/the-triple-barrier-method-251268419dcd", "anchor_text": "Data Labelling"}, {"url": "https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec", "anchor_text": "Meta-labeling"}, {"url": "https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa", "anchor_text": "Identifying Outliers"}, {"url": "https://medium.com/better-programming/identifying-outliers-part-two-4c00b2523362", "anchor_text": "Identifying Outliers \u2014 Part Two"}, {"url": "https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b", "anchor_text": "Identifying Outliers \u2014 Part Three"}, {"url": "https://towardsdatascience.com/data-whispering-eebb77a422da", "anchor_text": "Stylized Facts"}, {"url": "https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2", "anchor_text": "Feature Engineering & Feature Selection"}, {"url": "https://towardsdatascience.com/data-transformation-e7b3b4268151", "anchor_text": "Data Transformation"}, {"url": "https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55", "anchor_text": "Fractionally Differentiated Features"}, {"url": "https://towardsdatascience.com/the-triple-barrier-method-251268419dcd", "anchor_text": "Data Labelling"}, {"url": "https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec", "anchor_text": "Meta-labeling and Stacking"}, {"url": "https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip", "anchor_text": "https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip"}, {"url": "https://en.wikipedia.org/wiki/Precision_and_recall", "anchor_text": "Precision and recall from Wikipedia"}, {"url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "here"}, {"url": "https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw", "anchor_text": "StatQuest by Josh Starmer"}, {"url": "https://sanchom.wordpress.com/tag/average-precision/", "anchor_text": "Here"}, {"url": "http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf", "anchor_text": "more informative"}, {"url": "https://dl.acm.org/doi/10.1145/2808194.2809481", "anchor_text": "When both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system."}, {"url": "https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers", "anchor_text": "harmonic mean"}, {"url": "https://medium.com/@kegui/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b", "anchor_text": "Keras-tuner"}, {"url": "https://medium.com/@kegui/how-to-do-cross-validation-in-keras-tuner-db4b2dbe079a", "anchor_text": "my previous articles"}, {"url": "https://en.wikipedia.org/wiki/Goodhart%27s_law", "anchor_text": "Goodhart\u2019s law"}, {"url": "https://en.wikipedia.org/wiki/Campbell%27s_law", "anchor_text": "Campbell\u2019s law"}, {"url": "https://www.quantopian.com/posts/introduction-to-advances-in-financial-machine-learning-by-lopez-de-prado", "anchor_text": "Introduction to \u201cAdvances in Financial Machine Learning\u201d by Lopez de Prado"}, {"url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "anchor_text": "Measuring Performance: AUC (AUROC)"}, {"url": "https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/", "anchor_text": "A Pirate\u2019s Guide to Accuracy, Precision, Recall, and Other Scores"}, {"url": "https://sanchom.wordpress.com/tag/average-precision/", "anchor_text": "Average precision"}, {"url": "https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.&text=The%20role%20of%20feature%20importance%20in%20a%20predictive%20modeling%20problem.", "anchor_text": "How to Calculate Feature Importance With Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f17a7f9804ec---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f17a7f9804ec---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/metrics?source=post_page-----f17a7f9804ec---------------metrics-----------------", "anchor_text": "Metrics"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----f17a7f9804ec---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/finance?source=post_page-----f17a7f9804ec---------------finance-----------------", "anchor_text": "Finance"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&user=Ke+Gui&userId=2f79e4a45fc7&source=-----f17a7f9804ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&user=Ke+Gui&userId=2f79e4a45fc7&source=-----f17a7f9804ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff17a7f9804ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f17a7f9804ec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f17a7f9804ec--------------------------------", "anchor_text": ""}, {"url": "https://kegui.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kegui.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ke Gui"}, {"url": "https://kegui.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "290 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2f79e4a45fc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&user=Ke+Gui&userId=2f79e4a45fc7&source=post_page-2f79e4a45fc7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8085a44e86a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-labeling-and-stacking-f17a7f9804ec&newsletterV3=2f79e4a45fc7&newsletterV3Id=8085a44e86a2&user=Ke+Gui&userId=2f79e4a45fc7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}