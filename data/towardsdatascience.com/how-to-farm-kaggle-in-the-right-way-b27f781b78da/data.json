{"url": "https://towardsdatascience.com/how-to-farm-kaggle-in-the-right-way-b27f781b78da", "time": 1682994985.23665, "path": "towardsdatascience.com/how-to-farm-kaggle-in-the-right-way-b27f781b78da/", "webpage": {"metadata": {"title": "How to \u201cfarm\u201d Kaggle in the right way | by Alex Kruegger | Towards Data Science", "h1": "How to \u201cfarm\u201d Kaggle in the right way", "description": "In this article author suggests a bit of advice and approaches how to effectively use Kaggle as competition platform to improve practical skills in ML/DS with maximum efficiency and profitability"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/progression", "anchor_text": "Kaggle Competition Master", "paragraph_index": 1}, {"url": "https://www.kaggle.com/kruegger/competitions", "anchor_text": "top-200 world ranking on Kaggle", "paragraph_index": 1}, {"url": "https://mlcourse.ai/", "anchor_text": "an amazing course on Machine Learning", "paragraph_index": 2}, {"url": "https://mlcourse.ai/", "anchor_text": "the course", "paragraph_index": 3}, {"url": "https://habr.com/ru/users/yorko/", "anchor_text": "Yury Kashnitskiy (yorko)", "paragraph_index": 4}, {"url": "https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking", "anchor_text": "user identification through the sequence of visited sites", "paragraph_index": 5}, {"url": "https://www.kaggle.com/c/how-good-is-your-medium-article", "anchor_text": "predict the popularity of Medium articles", "paragraph_index": 5}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "the first version of ML course from Andrew Ng", "paragraph_index": 7}, {"url": "https://www.coursera.org/specializations/machine-learning", "anchor_text": "MIPT specialization", "paragraph_index": 7}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "House Prices: Advanced Regression Techniques", "paragraph_index": 8}, {"url": "https://www.kaggle.com/progression", "anchor_text": "Kaggle Competition Master", "paragraph_index": 9}, {"url": "https://www.kaggle.com/kruegger/competitions", "anchor_text": "top-200 world ranking on Kaggle", "paragraph_index": 9}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 10}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge", "anchor_text": "Mercari", "paragraph_index": 18}, {"url": "https://www.kaggle.com/kingarthur7", "anchor_text": "Arthur Stsepanenka (arthur)", "paragraph_index": 19}, {"url": "https://www.kaggle.com/lopuhin", "anchor_text": "Konstantin Lopuhin (kostia)", "paragraph_index": 19}, {"url": "https://www.kaggle.com/sergeifironov", "anchor_text": "Sergei Fironov (sergeif)", "paragraph_index": 19}, {"url": "https://www.kaggle.com/lopuhin", "anchor_text": "Konstantin Lopuhin (kostia)", "paragraph_index": 19}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/50256", "anchor_text": "first place", "paragraph_index": 19}, {"url": "https://www.kaggle.com/paweljankiewicz", "anchor_text": "Pawe\u0142 Jankiewicz", "paragraph_index": 19}, {"url": "https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s", "anchor_text": "did the post", "paragraph_index": 19}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge", "anchor_text": "Mercari", "paragraph_index": 21}, {"url": "https://wikipedia.org/wiki/CRISP-DM", "anchor_text": "CRISP-DM", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "(Vladimir Iglovikov (ternaus)", "paragraph_index": 32}, {"url": "https://dev.by/news/angarsk-minsk", "anchor_text": "ones", "paragraph_index": 34}, {"url": "https://dev.by/news/zachem-pobeditel-kaggle-i-topcoder-pereehal-v-minsk", "anchor_text": "two", "paragraph_index": 34}, {"url": "https://habr.com/users/cepera_ang/", "anchor_text": "Sergey Mushinskiy (cepera_ang)", "paragraph_index": 34}, {"url": "https://habr.com/users/albu/", "anchor_text": "Alexander Buslaev (albu)", "paragraph_index": 34}, {"url": "https://www.linkedin.com/in/venheads/", "anchor_text": "Valeriy Babushkin (venheads)", "paragraph_index": 35}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection", "anchor_text": "Talking Data", "paragraph_index": 45}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/leaderboard", "anchor_text": "took 8th place", "paragraph_index": 45}, {"url": "https://www.kaggle.com/ppleskov", "anchor_text": "Pavel Pleskov (ppleskov)", "paragraph_index": 45}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "(Vladimir Iglovikov (ternaus)", "paragraph_index": 52}, {"url": "https://towardsdatascience.com/ask-me-anything-session-with-a-kaggle-grandmaster-vladimir-i-iglovikov-942ad6a06acd", "anchor_text": "\u201cAsk Me Anything session with a Kaggle Grandmaster Vladimir I. Iglovikov\u201d", "paragraph_index": 52}, {"url": "https://image.slidesharecdn.com/kaggle-presentation-160918221740/95/kaggle-presentation-37-638.jpg?cb=1474237144", "anchor_text": "OOF \u2014 out of folds", "paragraph_index": 57}, {"url": "https://www.kaggle.com/c/sp-society-camera-model-identification", "anchor_text": "Camera Identification", "paragraph_index": 62}, {"url": "https://www.kaggle.com/c/sp-society-camera-model-identification/leaderboard", "anchor_text": "leaderboard", "paragraph_index": 62}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "Vladimir Iglovikov (ternaus)", "paragraph_index": 62}, {"url": "https://towardsdatascience.com/forensic-deep-learning-kaggle-camera-model-identification-challenge-f6a3892561bd", "anchor_text": "\u201cThe importance of data augmentation\u201d", "paragraph_index": 62}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56105", "anchor_text": "I had to go through memmap", "paragraph_index": 68}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "House Prices: Advanced Regression Techniques", "paragraph_index": 70}, {"url": "https://www.coursera.org/learn/competitive-data-science/lecture/b5Gxv/concept-of-mean-encoding", "anchor_text": "mean target encoding", "paragraph_index": 83}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Toxic", "paragraph_index": 85}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "building seq-2-seq models", "paragraph_index": 98}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit", "paragraph_index": 102}, {"url": "http://ods.ai/", "anchor_text": "http://ods.ai/", "paragraph_index": 109}, {"url": "https://mlcourse.ai/", "anchor_text": "https://mlcourse.ai/", "paragraph_index": 110}, {"url": "https://www.youtube.com/channel/UCeq6ZIlvC9SVsfhfKnSvM9w/videos", "anchor_text": "the set of video mltrainings (Rus)", "paragraph_index": 111}, {"url": "https://www.coursera.org/specializations/aml", "anchor_text": "specialization", "paragraph_index": 112}, {"url": "https://www.coursera.org/learn/competitive-data-science", "anchor_text": "How to Win a Data Science Competition: Learn from Top Kagglers", "paragraph_index": 112}, {"url": "http://ods.ai", "anchor_text": "ods.ai", "paragraph_index": 115}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "Vladimir Iglovikov (ternaus)", "paragraph_index": 115}, {"url": "https://habr.com/users/yorko/", "anchor_text": "Yury Kashnitskiy (yorko)", "paragraph_index": 115}, {"url": "https://www.linkedin.com/in/venheads/", "anchor_text": "Valeriy Babushkin (venheads)", "paragraph_index": 115}, {"url": "https://www.linkedin.com/in/avpronkin/", "anchor_text": "Alexey Pronkin (pronkin_alexey)", "paragraph_index": 115}, {"url": "https://habr.com/users/dpetrov_ml/", "anchor_text": "Dmitry Petrov (dmitry_petrov)", "paragraph_index": 115}, {"url": "https://www.kaggle.com/drn01z3", "anchor_text": "Artur Kuzin (n01z3)", "paragraph_index": 115}, {"url": "https://www.linkedin.com/in/nikita-zavgorodnii/", "anchor_text": "Nikita Zavgorodnii (njz)", "paragraph_index": 116}, {"url": "https://www.linkedin.com/in/iskrov", "anchor_text": "Alexey Iskrov (optimystic)", "paragraph_index": 117}, {"url": "http://www.kaggle.com/fatefulstudent", "anchor_text": "Vlad Ivanov(fateful)", "paragraph_index": 117}, {"url": "https://habr.com/ru/company/ods/blog/426227/", "anchor_text": "published at habr.com", "paragraph_index": 120}], "all_paragraphs": ["farm (farming) \u2014 gaming tactic where a player performs repetitive actions to gain experience, points or some form of in-game currency.", "These methods helped me with getting a Kaggle Competition Master title in six months just taking three competitions in solo mode. They also helped me while I climbed to the top-200 world ranking on Kaggle (at the time of writing the original article 2018\u201310\u201318). I hope this answers the question of why I took the liberty to write an article of this kind.", "Recently (February 11th, 2019) a new session of an amazing course on Machine Learning has been launched. This course is in English, it was created with input from a bunch of Kaggle Grandmasters and I highly recommend it as a starting course to anyone who wants to start his/her career in Data Science. As per usual, after the completion of this course (or any course for that matter) the question among students arose: where and how can we obtain the practical experience to consolidate newly acquired theoretical knowledge. If you ask this question on any relevant forum, the answer is likely to be \u201cKaggle.\u201d OK, Kaggle is nice, but where to start and how to use this platform for the improvement of practical skills most efficiently? In this article, I will try to give answers to these questions based on my own experience, and describe how to avoid general mistakes in the field of competitive Data Science and speed up the process of boosting your skills while having fun in the process.", "A few words about the course from its creators:", "mlcourse.ai is one of the major activities of the OpenDataScience community. Yury Kashnitskiy (yorko) & Co. (~60 more people) show that in principle you don\u2019t need a University to acquire modern technical skills; moreover, you can do so entirely for free. The main idea of the course is an optimal combination of theory and practice. On the one hand, the presentation of fundamental concepts involves some math, on the other hand \u2014 a lot of assignments, Inclass Kaggle competitions and projects will, with some effort from your side, improve your machine learning skills. Not to mention the competitive nature of the course \u2014 interactive student rating makes it fun to participate and motivates to endure till the end. Also, the course takes place in a truly vibrant community.", "There are a couple of Kaggle In Class competitions held during the course. Both are very interesting and required feature engineering. The first one is on user identification through the sequence of visited sites. The second one tries to predict the popularity of Medium articles. In a few assignments, you need to do your best to beat baselines in these competitions.", "Paying tribute to the course and its creators, we continue our story\u2026", "I remember myself a year and a half ago, when I passed the first version of ML course from Andrew Ng, completed MIPT specialization, read a mountain of books\u2026 Needless to say, I had a lot of theoretical knowledge, but when I tried to solve a basic task from real life, I hit the wall. I didn\u2019t know where to start. I understood the way how to solve the problem, understood which algorithms to use, but it was very hard to write the code, having to google tips, looking at sklearn help, etc constantly. Why? Because I didn\u2019t have the working pipelines for such a task as well as practical experience.", "It\u2019s not going to work that way, I thought and went on to Kaggle. Starting out from the ranking competition was scary, and the first goal was Getting Started competition \u201cHouse Prices: Advanced Regression Techniques\u201d. While working on this competition the approach of efficient kaggle farming was formed, which is described in this article.", "The approaches described in this article are not unique, there is no \u201cknow-how\u201d, all the methods and techniques are well-known and straightforward, but this does not lessen their effectiveness. These methods helped me with getting a Kaggle Competition Master title in six months just taking three competitions in solo mode. They also helped me while I climbed to the top-200 world ranking on Kaggle (at the time of writing the original article 2018\u201310\u201318). I hope this answers the question of why I took the liberty to write an article of this kind.", "Kaggle is one of the most famous platforms to host competitions for Data Science. In every competition, the sponsor hosts the real task, provides a description of the task, the data for this task, the metric used to evaluate the solution and also sets deadlines and prizes. Participants are usually given from 3 to 5 attempts a day to \u201csubmit\u201d a solution for this task.", "The data is divided into a training set and a test set. You are given the value of a target variable for the training part, but not for the testing part. The participants should create a model which, when trained on the training part of the data, achieve the maximum result (according to the chosen metric) on the test set.", "Each participant makes a prediction for a test set and sends the result on Kaggle, then the robot (which knows the target variable for the test set) evaluates the received result, scores it and displays it on the leaderboard.", "But it\u2019s not so simple, test data, in turn, is divided in a certain proportion for the public and private part. During the competition, the submitted solution is evaluated according to the established metric on the public part of the test set, and the result is put to the \u201cpublic\u201d leaderboard \u2014 in which participants can evaluate the quality of their models while competition is still going. The final solutions (usually two of the participant\u2019s choice) are evaluated on the private part of the test data, and the result goes to the private leaderboard, which only becomes visible after the competition ends and according to which, in fact, the final ratings are evaluated and prizes and medals are distributed.", "Thus, during the competition, the participants only have information on how their models behaved (a result or score it produced) on the public part of the test data. If in the case of a spherical chicken in a vacuum, private part of the data has the same distribution and statistics as the public then everything is likely to be fine. If it\u2019s not then the model, which proved to be good for the public part of the test set may fail on the private part. It\u2019s called \u201coverfitting on a public board\u201d. This leads to the \u201cleaderboard flight\u201d when people from 10th place on the public leaderboard crash down to 1000\u20132000 position on the private part since their chosen model was overfitted and was unable to give the required accuracy on unseen data.", "How to avoid it? First of all, you need to build a proper validation scheme, creating which is the first lesson in almost all courses of Data Science. Why is it so important? Well, if your model cannot give a correct prediction on a data set which it had never seen before then, it wouldn\u2019t matter whatever fancy technique you use or how complex your neural network was. You cannot release such a model in production because the results for the unseen data would be worthless.", "For each competition, Kaggle admins create its own separate page with sections for data, timelines, description of the metric and \u2014 the most valuable parts for us \u2014 the forum and the kernels.", "The forum is a standard forum where people can discuss and share ideas. But the kernel is much more interesting. In fact, it is the opportunity to run your code that has direct access to the competition data in the Kaggle cloud (similar to AWS, GCE, etc.) For each kernel limited computational resources are provided, so if the data set is not too big you can work with it directly from the browser on the Kaggle website. You can write the code, execute it, debug, and of course submit results. Two years ago, Kaggle was acquired by Google, so it is not surprising that \u201cunder the hood\u201d this functionality uses Google Cloud Engine.", "Moreover, there were several competitions (Mercari for example) where you could work with data through the kernel only. It is an exciting format, it compensates the differences in hardware among the participants and makes you use your brain on the subject of code optimization and approaches, as kernels have a hard limit on the resources at the time (4 cores / 16 GB RAM / 60 minutes run-time / 1 GB scratch disk space and output). Working on this competition, the author learned more about the optimization of neural networks, than from any theoretical course. Got a score a bit lower than the gold tier, finished 23rd in solo mode, but got lots of experience and joy\u2026", "I am glad to take this opportunity once again to say Thanks to my colleagues from the ods.ai \u2014 Arthur Stsepanenka (arthur), Konstantin Lopuhin (kostia), Sergei Fironov (sergeif) for the advice and support in this competition. In general, there were many interesting moments, Konstantin Lopuhin (kostia), who took the first place in this competition together with Pawe\u0142 Jankiewicz, afterward did the post that was called \u201creference humiliation in 75 rows\u201d, they published the 75 lines kernel which gave the result for the golden leaderboard zone. You really have to see it!", "Okay, let\u2019s return to the competitions. People write code and publish kernels with solutions, interesting ideas, etc. In every competition one or two beautiful EDA (exploratory data analysis) kernels emerge with a detailed description of the dataset, attributes statistics, characteristics, etc, usually in a few weeks from the start. And a couple of baseline kernels (basic solutions), which, of course, do not show the best results on the leaderboard, but you can use them as a starting point for creating your own solutions.", "In fact, there is no difference what platform you play. Kaggle is just one of the first and the most popular, with a great community and comfortable environment (I hope they will modify the kernels for stability and performance. Many people remember the hell that was going on at Mercari) But, in general, the platform is very convenient and self-sufficient, and kaggle ranks (Master/Grandmaster) are still valuable.", "A bit off the top on the subject of competitive Data Science. Very often, in articles, conversations, and other communication the opinion, that experience gained during the competition can\u2019t be used in the real-life problems, that all these people are only tuning 5th decimal digit of the score, that is insanity and too far from reality. Let us look at this question a bit closer:", "As a practicing Data Science-professionals, in contrast to the people from academy and science, we should and will solve real business problems in our work. In other words, (here is the reference to the CRISP-DM) to solve this problem it is necessary:", "The first four items on this list are not taught anywhere (correct me if there are such courses \u2014 I will enroll to it immediately), the only way is to learn from the experience of colleagues working in a specific industry. But the last clause \u2014 from selecting the model and further, you can, and you should improve in the competitions.", "In any competition, most of the work was already completed for us by the sponsors. We already have a business objective, the selected approximating metric, collected data, and our task is to build a working pipeline out of all of these. And here\u2019s where the skills will improve \u2014 how to deal with missing values, how to prepare data for neural networks and trees (and why neural networks require a special approach), how to build a validation scheme, how to not overfit, how to select appropriate hyperparameters, and many more other \u201chow to\u201d, competent execution of which distinguishes the experts from a passerbys in our profession.", "Basically, all the newcomers come to Kaggle to improve their practical experience, but do not forget that, in addition, there are at least two additional purposes:", "The key thing to remember is that these three goals are completely different, different approaches are required to achieve them, and they should not be mixed especially during the initial phase!", "Pay your attention to the words \u201cinitial phase\u201d, so when you improve your skills \u2014 these three goals will merge into one and will be achieved in parallel, but while you are just starting \u2014 don\u2019t mix them up! This way you will avoid pain, frustration, and resentment for this unjust world.", "Let\u2019s walk briefly on the objectives from the bottom up:", "(*) blending public kernels \u2014 technics to farm medals, when the kernels with high scores on the public leaderboard are selected, their predictions are averaged (blended), and the result submitted. Usually, this method leads to hard overfitting on the public leaderboard but sometimes allows you to get almost silver. The author, at the initial stage, does not recommend this approach (read below about the belt and the pants).", "I recommend first to choose the \u201cexperience\u201d goal and stick with it until you feel ready to work on two/three goals at the same time.", "There are two more things worth mentioning (Vladimir Iglovikov (ternaus) \u2014 thanks for the reminder).", "The first is to convert the effort invested in Kaggle into a new, more interesting and/or well-paid job. For people who understand the topic the line in the CV \u201cKaggle Competition Master\u201d, and other achievements still worth something.", "As an illustration of this point, you can read two interviews (ones, two, please note that these interviews are in the Russian language) with our colleagues Sergey Mushinskiy (cepera_ang) and Alexander Buslaev (albu)", "And also the opinion from Valeriy Babushkin (venheads):", "Valeriy Babushkin \u2014 Head of Data Science at X5 Retail Group (lead a team of 50+ people divided into 5 departments: CV, Machine Learning, Data Analysis, NLP and Ad Hoc), Analytics Team Lead at Yandex Advisor", "Kaggle Competition Master is an excellent proxy metric for assessing the future team member. Of course, in connection with the recent events in the form of teams of 30 participants and with almost nobody of them making anything it requires a more careful study of the profile than before, but it\u2019s still a matter of minutes. People who have achieved the title of master, with high probability, are able to write at least an average quality code, well versed in machine learning, are able to clean the data and to build sustainable solutions. If you don\u2019t have a master rank, the mere fact of participation is also a plus, at least the candidate knows about that Kaggle exists and did spend some time on familiarizing himself with it. And if solution he contributed with was something more than a public kernel (which is quite easy to check), it is a good chance for a specific conversation about technical details that is much better and more interesting than the classical job interview for a fresh grad which provides much less information of how people in the future will succeed in the job. The only thing we have to fear, and what I came across is that some people think that Data Science work is the same as at Kaggle, which is totally not true. Many think that Data Science = Machine Learning, which is also a mistake.", "The second point is that many tasks can be arranged in the form of pre-prints or articles, on the one hand, it allows the knowledge that the collective intelligence got during the competition not to die in the wilds of the forum, and on the other hand it adds another line to authors\u2019 portfolio and +1 to visibility, that in any case positively affects the career and the citation index.", "For example, this is the list of our colleagues\u2019 works at the end of several competitions:", "Let me explain. Practically in every competition closer to its termination somebody writes the public kernel with a solution that shifts the entire leaderboard up, but you, with your solution, down. And every time at the forum The Pain Starts! \u201cOh, here I had a solution for the silver, and now I\u2019m not even in the bronze. What the hell is going on, let\u2019s bring everything back.\u201d", "Remember that Kaggle is the competitive Data Science and your place on the leaderboard depends only on you. Not from the guy that posted the kernel, not from the stars that came together or not, just on how much effort you put into the decision and whether you used all the possible ways to improve it.", "If the public kernel knocks you off your place on the leaderboard \u2014 it is not your place.", "Instead of having to pour out the pain for the injustice of the world \u2014 thank this guy. Seriously, the public kernel with a better solution than yours means that you have missed something in your own pipeline. Locate it, improve it and go around all the hamsters with the same score. Remember that to return to the place you just have to be a little better than this public kernel.", "Just how I was upset by this moment in the first competition! My hands were falling, and I was ready to give up. A moment ago you were in silver, and here you are now at\u2026 the bottom of the leaderboard. It\u2019s ok, I just had to get myself together to understand where and what I missed to alter my decision and to return to the game.", "Moreover, this moment will be present only in the early phase of your competitive process. The more experienced you become, the less you will feel the influence of the published kernel and the stars. In a recent competition (Talking Data, where our team took 8th place) there was such kernel, but it had been mentioned just in one line in our team chat from Pavel Pleskov (ppleskov): \u201cGuys, I blended it with our solution, it had become worse, so I threw it away\u201d. That means that all useful signal, which was pulled from the data by that kernel was already pulled by our models.", "And, btw, about the medals, remember:", "\u201cThe belt without skills serve only to hold up one\u2019s pants\u201d\u00a9", "My recommendation is python 3.6 with jupyter notebook under ubuntu. Python has already become the de facto standard in Data Science, given a large number of libraries and community. Jupyter, especially with the presence of jupyter_contrib_nbextensions, is very convenient for rapid prototyping, analysis and data processing. Ubuntu is easy itself, plus the part of data processing is sometimes more natural to do in bash instead of python.", "After installation of jupyter_contrib_nbextensions I recommend to enable:", "And your life will become much more comfortable and more pleasant.", "Once your pipeline becomes more or less stable, I recommend moving your code to the individual modules. Believe me \u2014 you will rewrite it more than once or twice or even five times, and that\u2019s ok.", "There is the opposite approach where the participants are trying to use jupyter notebook as infrequently as possible and only when necessary, preferring just to write pipeline by scripts. (The adept of this option is, for example, (Vladimir Iglovikov (ternaus)). You can also read \u201cAsk Me Anything session with a Kaggle Grandmaster Vladimir I. Iglovikov\u201d where he is describing such approach.", "And there are those who are trying to combine jupyter with any IDE, such as Pycharm.", "Each approach has the right to exist, and each has its pros and cons, as they say, tastes differ. Choose what you are comfortable to work with.", "But under any scenario remember the rule", "save the code for each submission/OOF made. (see below)", "(*) OOF \u2014 out of folds, a technique of obtaining predictions of the model for the training part of a dataset using cross-validation. Is crucial for the further ensemble of several solutions.", "How? Well, there are at least three options:", "In General, in the community, there is a tendency of gradual transition to the third option, because the first and the second ones have their drawbacks, but they are simple, reliable and, honestly, for Kaggle, they are more than enough.", "A few more words about python for those who is not a programmer. Don\u2019t fear it. Your task is to understand the basic code structure and the basic essence of language. It will give you the ability to understand other people\u2019s kernels code and write your own libraries. On the Internet there are many excellent courses for beginners, maybe in the comments, somebody will give you the links. Unfortunately (or fortunately) I can not evaluate the quality of such courses so I won\u2019t put such links in the article.", "All techniques described in this article are based on working with tabular and text data. Working with pictures is a separate task with separate frameworks. At a basic level, it\u2019s good to be able to work with them, at least to run something like ResNet/VGG and get features, but deeper and more subtle work with them is a separate and very broad topic not covered in this article.", "I am not so good at working with imagery data. The only attempt to do it was in the competition Camera Identification, in which, by the way, the teams with the tag [ods.ai] blew up the whole leaderboard, so the Kaggle admins had to come to our slack to check that everything was according to rules. In this contest, I\u2019ve got a silver medal with the 46th place, but when I read the description of the top solutions from our colleagues, I understood that this was my best place as they were using a real black magic with pictures augmentation, additioning more than 300GB of data, sacrificing, and other things. For example, you can read the post from Vladimir Iglovikov (ternaus) about \u201cThe importance of data augmentation\u201d.", "In General, if you want to start working with images, then you need other frameworks and other guides.", "Your main task is to write pipelines (issued in the form of jupyter notebooks + modules) for the following tasks:", "In public kernels, all these tasks are usually gathered in one code, but I highly recommend to create a separate notebook and a separate module (or set of modules) for each of these subtasks. This approach will help you later.", "To prevent a possible holy war remember \u2014 the structure of this framework is not the truth in the last instance, there are many other ways to structure pipelines, and this is just one of them.", "Data is transferred between the modules as CSV, or feather/pickle/hdf \u2014 the way you prefer and what you are used to or enjoy doing.", "In fact, a lot still depends on the amount of data. In the TalkingData competition, for example, I had to go through memmap to avoid out of memory error when creating the dataset for lgb.", "In other cases, the master data is stored in hdf/feather, something small (like the set of selected attributes) in the CSV. I repeat \u2014 there is no template, this is just one of the possible ways. You should try all of them and select the one which is best for you.", "Start with any Getting started competition (as already mentioned, I started with House Prices: Advanced Regression Techniques), and begin to create your pipelines and notebooks. Read the public kernels, copy pieces of code, procedures, approaches, etc., then run your pipeline over your data, submit \u2014 look at the results, improve, rinse and repeat.", "The main task at this stage is to create an efficiently operating full cycle pipeline \u2014 from reading and cleaning the data to creating the final submission.", "A sample list of what should be ready and work at 100% before proceeding to the next step:", "Choose any competition you like and let\u2019s start\u2026", "At this time (2018\u201302\u201310) the arrows were removed from the leaderboard of all competitions. I don\u2019t know the reason but hope that it will come back.", "Do not continue until you build a robust validation schema \u2014 !!!", "OK, you got robust validation, what to do next?", "Remember that your goal at this stage is to gain experience! We have to fill our pipeline with working approaches and methods, and our modules by working code. Don\u2019t bother about the medals \u2014 I mean, it would be great if you could immediately take some place in the leaderboard, but if not, don\u2019t get upset. We didn\u2019t come here for five minutes, so medals and ranks are not going anywhere.", "OK, the competition is over, you are somewhere at the leaderboard, you learn something and want to go to the next competition?", "And here your personal hell begins", "And only after that move on to the next competition.", "No, I\u2019m not joking. Yes, you can do it easier. It\u2019s for you to decide.", "Why wait 5 days and not read the forums immediately? Well, you are losing the opportunity to ask some questions, but at this stage (in my opinion) it\u2019s better to read the already formed threads with discussions of solutions then ask questions that either someone already asked, or it\u2019s better not to ask them yet, and look for the answer yourself.", "Why should you do this exactly? Well, again \u2014 the purpose of this stage is to develop your own database of solutions, methods, and approaches. To create a working and efficient pipelines. So in the next competition you do not waste time and just say \u2014 yeah, mean target encoding could be used here, and by the way, I have the correct code for it using folds in folds. Or, well, I remember in that task the best way was to use ensemble using scipy.optimize and by the way I have the code already ready for it.", "With the same approach, you should participate in a few other competitions. Each time we notice that the entries on our sheets are becoming fewer and fewer, and the code in modules larger and larger. Gradually, the task of analysis converges to ensuring that you read the description of the solution, say yeah, hell yeah! And add to your library one or two new methods or approaches.", "After this, the learning mode changes to error-correction mode. The base library is ready, now it just must be used correctly. After each competition, you read the description of the solutions, look what you did, what could have been done better, what you missed, or where you specifically messed up, as I had in Toxic. I was going quite well in the underbelly of the gold zone, and in the private fell down 1500 positions. It was insulting to tears\u2026 but I calmed down, found a bug, wrote a post to our Slack community \u2014 and learned a lesson.", "A sign of establishing working mode is the fact that one of the descriptions of top solutions will be written under your nickname.", "What roughly speaking should be in your pipeline by the end of this stage:", "The author created several metaclasses separately for linear and tree-based models with the same external interface in order to neutralize the differences in API between the different realization of models libraries. But now he can run different models of the same type (LGB or XGB for example) by one line of code over one processed data set.", "As in any sport, in competitive Data Science, there is a lot of sweat and a lot of work. This is neither good nor bad, it is a fact. Participation in competitions (if you correctly approach the process) boosts technical skills significantly, plus more or less develops a sporting spirit \u2014 when you really do not want to do something, but you get up to the laptop, rebuild the model and run it to improve this damned 5th decimal digit of your score.", "So go play Kaggle \u2014 to farm experience, medals, and fun!", "In this section, I\u2019ll try to describe the basic idea behind my pipelines collected for a year and a half. Again \u2014 this approach does not claim universality or uniqueness, but perhaps you can find it useful.", "The inputs are a dataset, the attributes, the prefix for the new attributes and additional parameters. The outputs are a new dataset with new attributes and a list of these attributes. Further, this new dataset is stored in a separate pickle/feather.", "What it gives \u2014 we are able to assemble working dataset from pre-built parts quickly. For example, we made three different handling of categories \u2014 Label Encoding / OHE / Frequency and stored them in three separate feathers, and then at the modeling stage, all we need is just to play with these blocks, creating different datasets for training by one elegant move.", "If it is necessary to assemble another working dataset \u2014 we just need to replace pickle_list, reload and work with a new dataset.", "A basic set of functions on tabular data (real and categorical) includes various categories encoding, the projection of the numerical attributes to categorical, as well as various transformations.", "Universal Swiss army knife for combining the attributes, it gets the list of source attributes and the list of transform functions as input, and its output is, as usual, dataset and a list of the new attributes.", "For processing text data a separate module is used, including different methods of preprocessing, tokenization, lemmatization/stemming, translation in the frequency table, etc., using sklearn, nltk and keras.", "Time series are handled in a separate module with functions to convert source dataset for common tasks (regression/classification), as well as for sequence-to-sequence tasks. Thank you Fran\u00e7ois Chollet for keras update, so now building seq-2-seq models do not look like a voodoo ritual of demons invocation.", "There are, by the way, the functions of conventional statistical analysis of series in the same module \u2014 test for stationarity, STL-decomposition, etc\u2026 it is very helpful at the initial stage of analysis to \u201cfeel\u201d the time series and to see what it actually is.", "In other words, to work with the LGB we create a model", "And now in our code, we can operate with the model universarly.", "This pipeline was once again tested in the recent competition from Home Credit, careful and accurate application of all the blocks and modules brought the author the 94th place and a silver medal.", "The author is actually willing to express a seditious thought that for tabular data and a decent pipeline, the final result for any competition must be in the top 100 of the leaderboard. Of course there are exceptions, but in general, this statement seems correct.", "How to solve Kaggle \u2014 in a team or solo \u2014 mainly depends on a person (and a team), but my advice for those who are just starting \u2014 try to start solo. Why? I\u2019ll try to explain my point of view:", "These tips reflect the experience of the author and can (and should) be verified by your own experiments", "The author really pleased that in this competition he has built a robust scheme of cross-validation (3x10 fold), which has kept a score and brought 42nd place.", "Remember \u2014 everything around you is the digits, and the possibilities to use them depend only on your imagination. Use classification instead of regression, consider the sequence as a picture, etc.", "Note that some materials below are in the Russian language,", "http://ods.ai/ \u2014 for those who want to join the best Data Science community", "https://mlcourse.ai/ \u2014 website of the nice Machine Learning course", "Overall, I would highly recommend viewing the set of video mltrainings (Rus) using the method that was described in this article, there are a lot of interesting approaches and techniques.", "You can learn more methods and approaches to solve problems on Kaggle from the second course of this specialization \u2014 How to Win a Data Science Competition: Learn from Top Kagglers.", "The topic of Data Science in general and competitive Data Science, in particular, is \u201cas inexhaustible as the atom\u201d \u00a9. In this article, the author only scratched the surface of the topic of leveling practical skills using a competitive platform. If you are interested \u2014 sign up on Kaggle, inspect it, collect experience \u2014 and write your own articles. The higher amount of good content the better for all of us!", "Anticipating questions \u2014 no, author\u2019s pipelines and libraries are not yet made public.", "Many thanks to the colleagues from the ods.ai: Vladimir Iglovikov (ternaus), Yury Kashnitskiy (yorko), Valeriy Babushkin (venheads), Alexey Pronkin (pronkin_alexey), Dmitry Petrov (dmitry_petrov), Artur Kuzin (n01z3), as well as everyone who read the article prior to publication for corrections and reviews.", "Special thanks to Nikita Zavgorodnii (njz) \u2014 for the final proofreading.", "Also thanks to Alexey Iskrov (optimystic) and Vlad Ivanov(fateful) for the translation corrections.", "Thank you for your attention, I hope this article will be useful to someone.", "My nickname at Kaggle / ods.ai: kruegger", "This article was originally written in the Russian language and published at habr.com on 2018\u201310\u201318.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb27f781b78da&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b27f781b78da--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kruegger?source=post_page-----b27f781b78da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kruegger?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Alex Kruegger"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba231dfdfa23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&user=Alex+Kruegger&userId=ba231dfdfa23&source=post_page-ba231dfdfa23----b27f781b78da---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/progression", "anchor_text": "Kaggle Competition Master"}, {"url": "https://www.kaggle.com/kruegger/competitions", "anchor_text": "top-200 world ranking on Kaggle"}, {"url": "https://mlcourse.ai/", "anchor_text": "an amazing course on Machine Learning"}, {"url": "https://mlcourse.ai/", "anchor_text": "the course"}, {"url": "https://habr.com/ru/users/yorko/", "anchor_text": "Yury Kashnitskiy (yorko)"}, {"url": "https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking", "anchor_text": "user identification through the sequence of visited sites"}, {"url": "https://www.kaggle.com/c/how-good-is-your-medium-article", "anchor_text": "predict the popularity of Medium articles"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "the first version of ML course from Andrew Ng"}, {"url": "https://www.coursera.org/specializations/machine-learning", "anchor_text": "MIPT specialization"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "House Prices: Advanced Regression Techniques"}, {"url": "https://www.kaggle.com/progression", "anchor_text": "Kaggle Competition Master"}, {"url": "https://www.kaggle.com/kruegger/competitions", "anchor_text": "top-200 world ranking on Kaggle"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge", "anchor_text": "Mercari"}, {"url": "https://www.kaggle.com/kingarthur7", "anchor_text": "Arthur Stsepanenka (arthur)"}, {"url": "https://www.kaggle.com/lopuhin", "anchor_text": "Konstantin Lopuhin (kostia)"}, {"url": "https://www.kaggle.com/sergeifironov", "anchor_text": "Sergei Fironov (sergeif)"}, {"url": "https://www.kaggle.com/lopuhin", "anchor_text": "Konstantin Lopuhin (kostia)"}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/50256", "anchor_text": "first place"}, {"url": "https://www.kaggle.com/paweljankiewicz", "anchor_text": "Pawe\u0142 Jankiewicz"}, {"url": "https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s", "anchor_text": "did the post"}, {"url": "https://www.kaggle.com/c/mercari-price-suggestion-challenge", "anchor_text": "Mercari"}, {"url": "https://wikipedia.org/wiki/CRISP-DM", "anchor_text": "CRISP-DM"}, {"url": "https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4", "anchor_text": "the metric"}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "(Vladimir Iglovikov (ternaus)"}, {"url": "https://dev.by/news/angarsk-minsk", "anchor_text": "ones"}, {"url": "https://dev.by/news/zachem-pobeditel-kaggle-i-topcoder-pereehal-v-minsk", "anchor_text": "two"}, {"url": "https://habr.com/users/cepera_ang/", "anchor_text": "Sergey Mushinskiy (cepera_ang)"}, {"url": "https://habr.com/users/albu/", "anchor_text": "Alexander Buslaev (albu)"}, {"url": "https://www.linkedin.com/in/venheads/", "anchor_text": "Valeriy Babushkin (venheads)"}, {"url": "https://arxiv.org/abs/1706.06169", "anchor_text": "Satellite imagery feature detection using deep convolutional neural network: A Kaggle competition"}, {"url": "https://arxiv.org/abs/1801.05746", "anchor_text": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation"}, {"url": "https://arxiv.org/abs/1804.08024", "anchor_text": "Angiodysplasia Detection and Localization Using Deep Convolutional Neural Networks"}, {"url": "https://arxiv.org/abs/1803.01207", "anchor_text": "Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Buslaev_Fully_Convolutional_Network_CVPR_2018_paper.pdf", "anchor_text": "Fully convolutional network for automatic road extraction from satellite imagery"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Iglovikov_TernausNetV2_Fully_Convolutional_CVPR_2018_paper.pdf", "anchor_text": "Ternausnetv2: Fully convolutional network for instance segmentation"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Seferbekov_Feature_Pyramid_Network_CVPR_2018_paper.pdf", "anchor_text": "Feature pyramid network for multi-class land segmentation"}, {"url": "https://doi.org/10.1007/978-3-030-00889-5_34", "anchor_text": "Paediatric Bone Age Assessment Using Deep Convolutional Neural Networks"}, {"url": "https://arxiv.org/abs/1810.02981", "anchor_text": "Camera Model Identification Using Convolutional Neural Networks"}, {"url": "https://arxiv.org/abs/1810.02364", "anchor_text": "Deep Learning Approaches for Understanding Simple Speech Commands"}, {"url": "https://doi.org/10.1007/978-3-319-93000-8_83", "anchor_text": "Deep Convolutional Neural Networks for Breast Cancer Histology Image Analysis"}, {"url": "https://doi.org/10.1101/225508", "anchor_text": "Diabetic Retinopathy detection through integration of Deep Learning classification framework"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Rakhlin_Land_Cover_Classification_CVPR_2018_paper.pdf", "anchor_text": "Land Cover Classification from Satellite Imagery With U-Net and Lovasz-Softmax Loss"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Davydow_Land_Cover_Classification_CVPR_2018_paper.pdf", "anchor_text": "Land Cover Classification With Superpixels and Jaccard Index Post-Optimization"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Golovanov_Building_Detection_From_CVPR_2018_paper.pdf", "anchor_text": "Building Detection from Satellite Imagery Using a Composite Loss Function"}, {"url": "http://lsis.univ-tln.fr/~glotin/icml4b_material_bck/ICML4B_short_Smirnov_CNN_RightWhale.pdf", "anchor_text": "North Atlantic Right Whale Call Detection with Convolutional Neural Networks"}, {"url": "https://arxiv.org/abs/1711.06922", "anchor_text": "Run, skeleton, run: skeletal model in a physics-based simulation"}, {"url": "https://arxiv.org/abs/1804.00361", "anchor_text": "Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments"}, {"url": "https://www.sciencedirect.com/science/article/pii/S2212671614000146", "anchor_text": "Comparison of Regularization Methods for ImageNet Classification with Deep Convolutional Neural Networks"}, {"url": "http://openaccess.thecvf.com/content_ICCV_2017_workshops/w27/html/Smirnov_Doppelganger_Mining_for_ICCV_2017_paper.html", "anchor_text": "Doppelganger Mining for Face Representation Learning"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018_workshops/w1/html/Smirnov_Hard_Example_Mining_CVPR_2018_paper.html", "anchor_text": "Hard Example Mining with Auxiliary Embeddings"}, {"url": "https://arxiv.org/abs/1803.04037", "anchor_text": "Sales forecasting using WaveNet within the framework of the Kaggle competition"}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection", "anchor_text": "Talking Data"}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/leaderboard", "anchor_text": "took 8th place"}, {"url": "https://www.kaggle.com/ppleskov", "anchor_text": "Pavel Pleskov (ppleskov)"}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "(Vladimir Iglovikov (ternaus)"}, {"url": "https://towardsdatascience.com/ask-me-anything-session-with-a-kaggle-grandmaster-vladimir-i-iglovikov-942ad6a06acd", "anchor_text": "\u201cAsk Me Anything session with a Kaggle Grandmaster Vladimir I. Iglovikov\u201d"}, {"url": "https://image.slidesharecdn.com/kaggle-presentation-160918221740/95/kaggle-presentation-37-638.jpg?cb=1474237144", "anchor_text": "OOF \u2014 out of folds"}, {"url": "https://github.com/", "anchor_text": "GitHub"}, {"url": "https://bitbucket.org/", "anchor_text": "BitBucket"}, {"url": "https://dvc.org/", "anchor_text": "DVC"}, {"url": "https://www.kaggle.com/c/sp-society-camera-model-identification", "anchor_text": "Camera Identification"}, {"url": "https://www.kaggle.com/c/sp-society-camera-model-identification/leaderboard", "anchor_text": "leaderboard"}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "Vladimir Iglovikov (ternaus)"}, {"url": "https://towardsdatascience.com/forensic-deep-learning-kaggle-camera-model-identification-challenge-f6a3892561bd", "anchor_text": "\u201cThe importance of data augmentation\u201d"}, {"url": "https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/", "anchor_text": "FM/FFM"}, {"url": "https://medium.com/@iamHarin/feature-extraction-from-text-e5f5c1b36fe9", "anchor_text": "Vectorizers, TF-IDF"}, {"url": "https://machinelearningmastery.com/what-are-word-embeddings/", "anchor_text": "Embeddings"}, {"url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56105", "anchor_text": "I had to go through memmap"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "House Prices: Advanced Regression Techniques"}, {"url": "https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard", "anchor_text": "the leaderboard of the Mercedes"}, {"url": "https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36103", "anchor_text": "discussion"}, {"url": "https://habr.com/company/ods/blog/336168/", "anchor_text": "the article(Rus)"}, {"url": "https://www.youtube.com/watch?v=HT3QpRp2ewA", "anchor_text": "the speech(Rus) of"}, {"url": "https://www.kaggle.com/daniel89", "anchor_text": "Danila Savenkov (danila_savenkov)"}, {"url": "https://www.coursera.org/learn/competitive-data-science", "anchor_text": "How to Win a Data Science Competition: Learn from Top Kagglers\u201d"}, {"url": "https://www.coursera.org/learn/competitive-data-science/lecture/b5Gxv/concept-of-mean-encoding", "anchor_text": "mean target encoding"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Toxic"}, {"url": "https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f", "anchor_text": "autoencoders"}, {"url": "https://towardsdatascience.com/genetic-algorithm-implementation-in-python-5ab67bb124a6", "anchor_text": "Genetic Algorithms"}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "building seq-2-seq models"}, {"url": "https://towardsdatascience.com/genetic-algorithm-implementation-in-python-5ab67bb124a6", "anchor_text": "genetic algorithms"}, {"url": "https://github.com/DEAP/deap", "anchor_text": "this library"}, {"url": "https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf", "anchor_text": "a method from Caruana"}, {"url": "https://keras.io/", "anchor_text": "keras"}, {"url": "https://keras.io/getting-started/functional-api-guide/", "anchor_text": "a functional style"}, {"url": "https://pytorch.org/", "anchor_text": "pytorch"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit"}, {"url": "https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard", "anchor_text": "the leaderboard of the Mercedes"}, {"url": "https://github.com/fmfn/BayesianOptimization/blob/master/README.md", "anchor_text": "Bayesian optimization"}, {"url": "https://github.com/hyperopt/hyperopt", "anchor_text": "hyperopt"}, {"url": "http://ods.ai/", "anchor_text": "ods.ai"}, {"url": "http://ods.ai/", "anchor_text": "http://ods.ai/"}, {"url": "https://mlcourse.ai/", "anchor_text": "https://mlcourse.ai/"}, {"url": "https://www.kaggle.com/general/68205", "anchor_text": "https://www.Kaggle.com/general/68205"}, {"url": "https://www.youtube.com/channel/UCeq6ZIlvC9SVsfhfKnSvM9w/videos", "anchor_text": "the set of video mltrainings (Rus)"}, {"url": "https://www.youtube.com/watch?v=fXnzjJMbujc", "anchor_text": "very good video about how to become a grandmaster :)"}, {"url": "https://www.kaggle.com/ppleskov", "anchor_text": "Pavel Pleskov (ppleskov)"}, {"url": "https://www.youtube.com/watch?v=g335THJxkto", "anchor_text": "video about the hacking, unconventional approach and target mean encoding on the example of BNP Paribas competition"}, {"url": "https://www.kaggle.com/stasg7", "anchor_text": "Stanislav Semenov (stasg7)"}, {"url": "https://www.youtube.com/watch?v=GT4G7Vawt0Q", "anchor_text": "Another video with Stanislav \u201cWhat does Kaggle teach\u201d"}, {"url": "https://www.coursera.org/specializations/aml", "anchor_text": "specialization"}, {"url": "https://www.coursera.org/learn/competitive-data-science", "anchor_text": "How to Win a Data Science Competition: Learn from Top Kagglers"}, {"url": "https://sites.google.com/view/lauraepp/parameters", "anchor_text": "Laurae++, XGBoost/LightGBP parameters"}, {"url": "https://github.com/facebookresearch/fastText", "anchor_text": "FastText \u2014 embeddings for text from Facebook"}, {"url": "https://github.com/anttttti/Wordbatch", "anchor_text": "WordBatch/FTRL/FM-FTRL \u2014 a set of libraries"}, {"url": "https://www.kaggle.com/anttip", "anchor_text": "@anttip"}, {"url": "https://github.com/alexeygrigorev/libftrl-python", "anchor_text": "Another implementation of the FTRL"}, {"url": "https://github.com/fmfn/BayesianOptimization/blob/master/README.md", "anchor_text": "Bayesian Optimization library for selection of hyperparameters"}, {"url": "https://github.com/RGF-team/rgf", "anchor_text": "Regularized Greedy Forest (RGF) library \u2014 another tree method"}, {"url": "http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/", "anchor_text": "A Kaggler\u2019s Guide to Model Stacking in Practice"}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5 is a wonderful library for visualization of weights of the models"}, {"url": "https://www.kaggle.com/lopuhin", "anchor_text": "Konstantin Lopuhin (kostia)"}, {"url": "https://www.kaggle.com/ogrellier/feature-selection-target-permutations", "anchor_text": "Feature selection: target permutations, and follow on the links inside"}, {"url": "https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3", "anchor_text": "Feature Importance Measures for Tree Models"}, {"url": "https://www.kaggle.com/ogrellier/feature-selection-with-null-importances", "anchor_text": "Feature selection with null importance"}, {"url": "https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases", "anchor_text": "Brief introduction about autoencoder"}, {"url": "https://www.slideshare.net/markpeng/general-tips-for-participating-kaggle-competitions", "anchor_text": "Presentation about Kaggle on SlideShare"}, {"url": "https://www.slideshare.net/HJvanVeen/kaggle-presentation?qid=9945759e-a06f-447d-bcfb-2a15592f30b6&v=&b=&from_search=11", "anchor_text": "Another one"}, {"url": "https://www.slideshare.net/search/slideshow?searchfrom=header&q=kaggle", "anchor_text": "Lots of interesting things here"}, {"url": "https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions", "anchor_text": "Winning solutions of kaggle competitions"}, {"url": "https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle-updated/", "anchor_text": "Data Science Glossary on Kaggle"}, {"url": "http://ods.ai", "anchor_text": "ods.ai"}, {"url": "https://www.linkedin.com/in/iglovikov/", "anchor_text": "Vladimir Iglovikov (ternaus)"}, {"url": "https://habr.com/users/yorko/", "anchor_text": "Yury Kashnitskiy (yorko)"}, {"url": "https://www.linkedin.com/in/venheads/", "anchor_text": "Valeriy Babushkin (venheads)"}, {"url": "https://www.linkedin.com/in/avpronkin/", "anchor_text": "Alexey Pronkin (pronkin_alexey)"}, {"url": "https://habr.com/users/dpetrov_ml/", "anchor_text": "Dmitry Petrov (dmitry_petrov)"}, {"url": "https://www.kaggle.com/drn01z3", "anchor_text": "Artur Kuzin (n01z3)"}, {"url": "https://www.linkedin.com/in/nikita-zavgorodnii/", "anchor_text": "Nikita Zavgorodnii (njz)"}, {"url": "https://www.linkedin.com/in/iskrov", "anchor_text": "Alexey Iskrov (optimystic)"}, {"url": "http://www.kaggle.com/fatefulstudent", "anchor_text": "Vlad Ivanov(fateful)"}, {"url": "https://habr.com/ru/company/ods/blog/426227/", "anchor_text": "published at habr.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b27f781b78da---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b27f781b78da---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----b27f781b78da---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/programming?source=post_page-----b27f781b78da---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b27f781b78da---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&user=Alex+Kruegger&userId=ba231dfdfa23&source=-----b27f781b78da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&user=Alex+Kruegger&userId=ba231dfdfa23&source=-----b27f781b78da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb27f781b78da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b27f781b78da---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b27f781b78da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b27f781b78da--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b27f781b78da--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b27f781b78da--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b27f781b78da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kruegger?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kruegger?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alex Kruegger"}, {"url": "https://medium.com/@kruegger/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "67 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba231dfdfa23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&user=Alex+Kruegger&userId=ba231dfdfa23&source=post_page-ba231dfdfa23--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc2f1ac1e2d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-farm-kaggle-in-the-right-way-b27f781b78da&newsletterV3=ba231dfdfa23&newsletterV3Id=c2f1ac1e2d8e&user=Alex+Kruegger&userId=ba231dfdfa23&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}