{"url": "https://towardsdatascience.com/using-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1", "time": 1683006651.052317, "path": "towardsdatascience.com/using-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1/", "webpage": {"metadata": {"title": "Fit multiple propensity models and choose the best performing one for implementing a profit optimisation | by Diego Usai | Towards Data Science", "h1": "Fit multiple propensity models and choose the best performing one for implementing a profit optimisation", "description": "In this day and age, a business that leverages data to understand the drivers of customers\u2019 behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Predictive_modelling", "anchor_text": "Propensity Modelling", "paragraph_index": 1}, {"url": "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing", "anchor_text": "Portuguese Bank Marketing", "paragraph_index": 7}, {"url": "https://archive.ics.uci.edu/ml/datasets", "anchor_text": "UCI Machine Learning Repository", "paragraph_index": 7}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling/tree/master/01_data/bank-direct-marketing.csv", "anchor_text": "(bank-direct-marketing.csv", "paragraph_index": 8}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", "anchor_text": "bank-additional-full.csv", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Binary_classification", "anchor_text": "binary classification problem", "paragraph_index": 8}, {"url": "https://diegousai.io/2020/01/propensity-modelling-data-preparation", "anchor_text": "Propensity Modelling \u2014 Data Preparation and Exploratory Data Analysis.", "paragraph_index": 9}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "on my GitHub repo for this project", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis", "anchor_text": "Exploratory Data Analysis (EDA)", "paragraph_index": 10}, {"url": "https://business-science.github.io/correlationfunnel/", "anchor_text": "correlationfunnel", "paragraph_index": 10}, {"url": "https://diegousai.io/2020/01/propensity-modelling-data-preparation", "anchor_text": "Propensity Modelling - Data Preparation and Exploratory Data Analysis.", "paragraph_index": 34}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "on my GitHub repo", "paragraph_index": 35}, {"url": "https://uc-r.github.io/dalex", "anchor_text": "Model Interpretability with DALEX", "paragraph_index": 48}, {"url": "https://pbiecek.github.io/ema/introduction.html", "anchor_text": "Explanatory Model Analysis", "paragraph_index": 53}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic (ROC)", "paragraph_index": 58}, {"url": "https://stats.stackexchange.com/", "anchor_text": "Cross Validated", "paragraph_index": 85}, {"url": "https://stats.stackexchange.com/questions/93202/odds-ratio-from-decision-tree-and-random-forest", "anchor_text": "odds ratio from decision tree and random forest", "paragraph_index": 85}, {"url": "https://diegousai.io/2020/02/propensity-modelling-estimate-compare-models/", "anchor_text": "Propensity Modelling \u2014 Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology.", "paragraph_index": 88}, {"url": "https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/", "anchor_text": "Local Interpretable Model-agnostic Explanations (LIME)", "paragraph_index": 90}, {"url": "https://en.wikipedia.org/wiki/F1_score", "anchor_text": "F1 score", "paragraph_index": 98}, {"url": "https://www.goodreads.com/book/show/17912916-data-science-for-business", "anchor_text": "Data Science for Business", "paragraph_index": 109}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "Propensity Modelling", "paragraph_index": 131}, {"url": "http://www.linkedin.com/in/diegousaiuk/", "anchor_text": "www.linkedin.com/in/diegousaiuk/", "paragraph_index": 133}], "all_paragraphs": ["In this day and age, a business that leverages data to understand the drivers of customers\u2019 behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.", "One trialled and tested approach to tease this type of insight out of data is Propensity Modelling, which combines information such as a customers\u2019 demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on a mobile app, webpage dwell time, etc.), user experience (customer service phone and email wait times, number of refunds, average shipping times), and user behaviour (purchase value on different time-scales, number of days since most recent purchase, the time between offer and conversion, etc.) to estimate the likelihood of a certain customer profile to perform a certain type of behaviour (e.g. the purchase of a product).", "Once you understand the probability of a certain customer to interact with a brand, buy a product or sign up for a service, you can use this information to create scenarios, be it minimising marketing expenditure, maximising acquisition targets, and optimise email send frequency or depth of discount.", "In this project, I\u2019m analysing the results of a bank direct marketing campaign to sell term a deposit its existing clients in order to identify what type of characteristics make a customer more likely to respond. The marketing campaigns were based on phone calls and more than one contact to the same person was required at times.", "First, I am going to carry out extensive data exploration and use the results and insights to prepare the data for analysis.", "Then, I\u2019m estimating the number of models and assess their performance and fit to the data using a model-agnostic methodology that enables to compare traditional \u201cglass-box\u201d models and \u201cblack-box\u201d models.", "Last, I\u2019ll fit one final model that combines findings from the exploratory analysis and insight from models\u2019 selection and use it to run a revenue optimisation.", "The Data is the Portuguese Bank Marketing set from the UCI Machine Learning Repository and describes the direct marketing campaigns carried out by a Portuguese banking institution aimed at selling term deposits/certificate of deposits to their customers. The marketing campaigns were based on phone calls to potential buyers from May 2008 to November 2010.", "The data I\u2019m using (bank-direct-marketing.csv) is a modified version of the bank-additional-full.csv and contains 41,188 examples with 21 different variables (10 continuous, 10 categorical plus the target variable). In particular, the target subscribed is a binary response variable indicating whether the client subscribed (\u2018Yes\u2019 or numeric value 1) to a term deposit or not (\u2018No\u2019 or numeric value 0), which make this a binary classification problem.", "The data required some manipulation to get into a usable format, details of which can be found on my webpage: Propensity Modelling \u2014 Data Preparation and Exploratory Data Analysis. Here I simply load up the pre-cleansed data I am hosting on my GitHub repo for this project", "Although an integral part of any Data Science project and crucial to the full success of the analysis, Exploratory Data Analysis (EDA) can be an incredibly labour intensive and time consuming process. Recent years have seen a proliferation of approaches and libraries aimed at speeding up the process and in this project I\u2019m going to sample one of the \u201cnew kids on the block\u201d ( the correlationfunnel ) and combine its results with a more traditional EDA.", "With 3 simple steps correlationfunnel can produce a graph that arranges predictors top to bottom in descending order of absolute correlation with the target variable. Features at the top of the funnel are expected to have stronger predictive power in a model.", "This approach offers a quick way to identify a hierarchy of expected predictive power for all variables and gives an early indication of which predictors should feature strongly/weakly in any model.", "Zooming in on the top 5 features we can see that certain characteristics have a greater correlation with the target variable (subscribing to the term deposit product) when:", "Conversely, variables at the bottom of the funnel, such as day_of_week, housing, and loan. show very little variation compared to the target variable (i.e.: they are very close to the zero correlation point to the response). For that reason, I\u2019m not expecting these features to impact the response.", "Guided by the results of this visual correlation analysis, I will continue to explore the relationship between the target and each of the predictors in the next section. For this, I am going to enlist the help of the brilliant GGally library to visualise a modified version of the correlation matrix with Ggpairs, and plot mosaic charts with the ggmosaic package, a great way to examine the relationship among two or more categorical variables.", "First things first, the target variable: subscribed shows a strong class imbalance, with nearly 89% in the No category to 11% in the Yes category.", "I am going to address class imbalance during the modelling phase by enabling re-sampling, in h2o. This will rebalance the dataset by \u201cshrinking\u201d the prevalent class (\u201cNo\u201d or 0) and ensure that the model adequately detects what variables are driving the \u2018yes\u2019 and \u2018no\u2019 responses.", "Let\u2019s continue with some of the numerical features:", "Although the correlation funnel analysis revealed that duration has the strongest expected predictive power, it is unknown before a call (it\u2019s obviously known afterwards) and offers very little actionable insight or predictive value. Therefore, it should be discarded from any realistic predictive model and will not be used in this analysis.", "age \u2019s density plots have very similar variance compared to the target variable and are centred around the same area. For these reasons, it should not have a great impact on subscribed.", "Despite continuous in nature, pdays and previous are in fact categorical features and are also all strongly right skewed. For these reasons, they will need to be discretised into groups. Both variables are also moderately correlated, suggesting that they may capture the same behaviour.", "Next, I visualise the bank client data with the mosaic charts:", "In line with the correlationfunnel findings, job, education, marital and default all show a good level of variation compared to the target variable, indicating that they would impact the response. In contrast, housing and loan sat at the very bottom of the funnel and are expected to have little influence on the target, given the small variation when split by \u201csubscribed\u201d response.", "default has only 3 observations in the \u2018yes\u2019 level, which will be rolled into the least frequent level as they\u2019re not enough to make a proper inference. Level \u2018unknown\u2019 of the housing and loan variables have a small number of observations and will be rolled into the second smallest category. Lastly, job and education would also benefit from grouping up of least common levels.", "Moving on to the other campaign attributes:", "Although continuous in principal, campaign is more categorical in nature and strongly right skewed, and will need to be discretised into groups. However, we have learned from the earlier correlation analysis that is not expected be a strong drivers of variation in any model.", "On the other hand, poutcome is one of the attributes expected to be have a strong predictive power. The uneven distribution of levels would suggest to roll the least common occurring level (success or scs) into another category. However, contacting a client who previously purchased a term deposit is one of the catacteristics with highest predictive power and needs to be left ungrouped.", "Then, I\u2019m looking at last contact information:", "contact and month should impact the response variable as they both have a good level of variation compared to the target. month would also benefit from grouping up of least common levels.", "In contrast, day_of_week does not appear to impact the response as there is not enough variation between the levels.", "Last but not least, the social and economic attributes:", "All social and economic attributes show a good level of variation compared to the target variable, which suggests that they should all impact the response. They all display a high degree of multi-modality and do not have an even spread through the density plot, and will need to be binned.", "It is also worth noting that, with the exception of consumer confidence index, all other social and economic attributes are strongly correlated to each other, indicating that only one could be included in the model as they are all \u201cpicking up\u201d similar economic trend.", "Following up on the findings from the Exploratory Data Analysis, I\u2019ve discretised categorical and continuous predictors by combining least common levels into \u201cother\u2019 category, set all variables but age as unordered factors ( h2o does not support ordered categorical variables) and shorted level names of some categorical variables to ease visualisations. You can find all the details and the full code on my webpage: Propensity Modelling - Data Preparation and Exploratory Data Analysis.", "Here I simply load up the final dataset hosted on my GitHub repo:", "In order to stick to a reasonable project running time, I\u2019ve opted for h2o as my modelling platform as it offers a number of advantages:", "I\u2019m starting by creating a randomised training and validation set with rsample and save them as train_tbl and test_tbl.", "Then, I start an h2o cluster. I specify the size of the memory cluster to \u201c16G\u201d to help speed things up a bit and turn off the progress bar.", "Next, I sort out response and predictor variables sets. For a classification to be performed, I need to ensure that the response variable is a factor (otherwise h2o will carry out a regression). This was sorted out during the data cleansing and formatting phase.", "For this project, I\u2019m estimating a Generalised Linear Model (a.k.a. Elastic Net), a Random Forest (which h2o refers to at Distributed Random Forest) and a Gradient Boosting Machine (or GBM).", "To implement a grid search for the tree-based models (DRF and GBM), I need to set up a random grid to search for optimal hyper-parameters for the h2o.grid() function. To do so, I start with defining the search parameters to be passed to the hyper_paramsargument:", "I also set up a second list for the search_criteria argument, which helps to manage the models\u2019 estimation running time:", "At last, I can set up the models\u2019 formulations. Note that all models have 2 parameters in common:", "I sort the tree-based model by AUC score and retrieve the lead models from the grid", "There are many libraries (like IML, PDP, VIP, and DALEX to name but the more popular) that help with Machine Learning Interpretability, feature explanation and general performance assessment and they all have gained in popularity in recent years.", "There are a number of methodologies to interpret machine learning results (i.e. local interpretable model-agnostic explanations, partial dependence plots, permutation-based variable importance) but in this project, I examine the DALEX package, which focuses on Model-Agnostic Interpretability and provides a convenient way of comparing performance across multiple models with different structures.", "One of the key advantages of the model-agnostic approach used by DALEX is that you can compare contributions of traditional \u201cglass-box\u201d models to black-box models on the same scale. However, being permutation-based, one of its main drawbacks is that it does not scale well with a large number of predictor variables and larger datasets.", "Currently DALEX does not support some of the more recent ML packages like h2o or xgboost. To make it compatible with such objects, I\u2019ve followed the procedure illustrated by Bradley Boehmke in his brilliant study Model Interpretability with DALEX, from which I\u2019ve drawn lots of inspiration and borrowed some code.", "First, the dataset needs to be in a specific format:", "Then, I create a predict function returning a vector of numeric values, which extracts the probability of the response for binary classification problems.", "Now I can convert my machine learning models into DALEK \u201cexplainers\u201d with the explain() function, which works as a \u201ccontainer\u201d for the parameters.", "At last, I\u2019m ready to pass the explainer objects to several DALEX functions that will help assess and compare the performance of the different models. Given that performance measures may reflect a different aspect of the predictive performance of a model, it is important to evaluate and compare several metrics when appraising a model and with DALEX you can do just that!", "To evaluate and compare my models\u2019 performance, I\u2019ve drawn inspiration from the framework used by Przemyslaw Biecek and Tomasz Burzykowski in their book, Explanatory Model Analysis, which is structured around key questions:", "To get an initial feel for how well my models fit the data, I can use the self-explanatory model_performance() function, which calculates selected model performance measures.", "Based on the metrics available for all models ( accuracy and AUC), I can see that elastic net and gradient boosting are performing roughly on par with one another, with random forest not far behind. AUC ranges between .78-.80 whereas accuracy has a slightly narrower range of .89-.90", "As shown in the previous paragraph, model_performance() also produces residual quantiles that can be plotted to compare absolute residual values across models.", "The DRF and GBM models appear to perform on a par with one another, given the median absolute residuals. Looking at the residuals distribution on the right-hand side, you can see that the median residuals are the lowest for these two models, with the GLM seeing a higher number of tail residuals. This is also mirrored by the boxplots on the left-hand side, where the tree-based models both achieve the lowest median absolute residual value.", "The Receiver Operating Characteristic (ROC) curve is a graphical method that allows to visualise a classification model performance against a random guess, which is represented by the striped line on the graph. The curve plots the true positive rate (TPR) on the y-axis against the false positive rate (FPR) on the x-axis.", "The insight from a ROC curve is two-fold:", "All models performs much better that random guessing and achieves a AUC of .75-.80, with the DRF achieving the highest score of 0.799.", "Each ML algorithm has its own way to assess the importance of each variable: linear models, for instance, refer to their coefficients, whereas tree-based models look at impurity, which makes it difficult to compare variable importance across models.", "DALEX calculates variable importance measures via permutation, which is model agnostics and allows for direct comparison between models of different structure. However, when variable importance scores are based on permutations, we should remember that calculations slow down when the number of features increases.", "Once again, I\u2019m passing the \u201cexplainer\u201d for each single model to the feature_importance() function and setting n_sample to 8000 to use practically all available observations. Although not exorbitant, the total execution time was nearly 30 minute but this is based on a relatively small dataset and the number of variables. Don\u2019t forget that computation speed can be increased by reducing n_sample, which is especially important for larger datasets.", "Now I only have to pass the vip objects to a plotting function: as suggested by the auto-generated x-axis label ( Drop-out loss), the main intuition behind how variable importance is calculated lies in how much the model fit would decrease if the contribution of a selected explanatory variable was removed. The larger the segment, the larger the loss when that variable is dropped from the model.", "I like this plot as it brings together a wealth of information.", "First of all, you can notice that, although with slightly different relative weights, the top 5 features are common to each model, with nr_employed ( employed in the economy) being the single most important predictor in all of them. This consistency is reassuring as it tells us that all models are picking up the same structure and interactions in the data, and gives us the assurance that these features have strong predictive power.", "You can also notice the distinct starting point for the x-axis left edge, which reflects the difference in the RMSE loss between the three models: in this case the elastic net model has the highest RMSE, suggesting the higher number of tail residuals seen earlier in the residual diagnostics is probably penalising the RMSE score.", "After we have identified the relative predictive power of each variable, we may want to investigate how their relationship with the predicted response differ across all three models. Partial Dependence (PD) plots, sometimes also referred to as PD profiles, offer a great way to inspect how each model is responding to a particular predictor.", "We can start with having a look at the single most important feature, nr_employed:", "Although with different average prediction weights, all three models found that bank customers are more likely to sign up to a term deposit when the level of employed in the economy is up to 5.099m (nInf_5099.1). Both elastic net and random forest have found the exact same hierarchy of predictive power among the 3 different levels of nr_employed (less pronounced for the random forest) that we observed in the correlationfunnel analysis, with GBM being the one slightly out of kilter.", "Let\u2019s now take a look at age, a predictor that, if you recall from the EDA, was NOT expected to have an impact on the target variable:", "One thing we notice is that the range of variation in the average prediction (x-axis) is relatively shallow across the age spectrum (y-axis), confirming the finding from the exploratory analysis that this variable would have a low predictive power. Also, both GBM and random forest are using age in a non-linear way, whereas the elastic net model is unable to capture this non-linear dynamic.", "Partial Dependence plots could also work as a diagnostic tool: looking at poutcome (outcome of the previous marketing campaign) reveals that GBM and random forest correctly picked up on a higher probability of signing up when the outcome of a previous campaign was success (scs).", "However, the elastic net model fails to do the same, which could represent a serious flaw as success in a previous campaign had a very strong positive correlation with the target variable.", "I\u2019m going to finish with the month feature as it offers a great example of one of those cases where you may want to override the model\u2019s outcome with industry knowledge and some common sense. Specifically, the GBM model seems to suggest that March, October and December are periods associated with much better odds of success.", "Based on my previous analysis experience of similar financial products, I would not advise a banking organisation to ramp up their direct marketing activity around the weeks in the run to Christmas as this is a period of the year where the consumers\u2019 focus shifts away from this type of purchases.", "All in all random forest is my final model of choice: it appears the more balanced of the three and does not display some of the \u201coddities\u201d seen with variables like month and poutcome.", "I can now further refine my model and reduce its complexity by combining findings from the Exploratory analysis, insight from models\u2019 assessment and a number of industry-specific/common sense considerations.", "For the final model, I\u2019m using the same specification as to the original random forest", "Once again, we sort the model by AUC score and retrieve the lead model", "For brevity, I am visualising the variable importance plot with the vip() function from the namesake package, which returns the ranked contribution of each variable.", "Removing emp_var_rate has allowed education to come into the top 10 features. Understandably, the variables hierarchy and relative predictive power has adjusted and changed slightly but it\u2019s reassuring to see that the other 9 variables were in the previous model\u2019s top 10.", "Lastly, I\u2019m comparing the model\u2019s performance with the original random forest model.", "The AUC has only changed by a fraction of a percent, telling me that the model has maintained its predictive power.", "Being already familiar with odds ratios in the context of a logistic regression, I set out to understand whether the same intuition could be extended to black-box classification models. During my research one very interesting post on Cross Validated stood out for drawing a parallel between odds ratio from decision tree and random forest.", "Basically, this tells us that Partial Dependence plots can be used in a similar way to odds ratios to define what characteristics of a customer profile influence his/her propensity to performing a certain type of behaviour.", "For example, features like job, month and contact would provide context around who, when and how to target:", "NOTE THAT Partial Dependence Plots for all final model\u2019s predictors can be found on my webpage: on my webpage: Propensity Modelling \u2014 Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology.", "Armed with such insight, one can help to shape overall marketing and communication plans to focus on customers more likely to subscribe to a term deposit.", "However, these are based on model-level explainers, which reflect an overall, aggregated view. If you\u2019re interested to understand how a model yields a prediction for a single observation (i.e. what factors influence the likelihood to engage at single customer level), you can resort to the Local Interpretable Model-agnostic Explanations (LIME) method that exploits the concept of a \u201clocal model\u201d. I will be exploring the LIME methodology in a future post.", "For the analysis part of this project, I opted for h2o as my modelling platform. h2o is not only very easy to use but also has a number of built-in functionalities that help speeding up data preparation: it takes care of class imbalance with no need for pre-modelling resampling, automatically __\u201cbinarises\u201c character/factor__ variables, and implements cross-validation without the need for a separate validation frame to be \u201ccarved out\" of the training set.", "After setting up a random grid to search for best hyper-parameters, I\u2019ve estimated the number of models ( a logistic regression, a random forest and a gradient boosting machines) and used the DALEX library to assess and compare their performance through an array of metrics. This library employs a model-agnostic approach that enables to compare traditional \u201cglass-box\u201d models and \u201cblack-box\u201d models on the same scale.", "My final model of choice is the random forest, which I further refined by combining findings from the exploratory analysis, insight gathered from the models\u2019 evaluation and a number of industry-specific/common sense considerations. This ensured a reduced model complexity without compromising on predictive power.", "Now that I have my final model, the last piece of the puzzle is the final \u201cSo what?\u201d question that puts all into perspective. The estimate for the probability of a customer to sign up for a term deposit can be used to create a number of optimised scenarios, ranging from minimising your marketing expenditure, maximising your overall acquisition targets, to driving a certain number of cross-sell opportunities.", "Before I can do that, there are a couple of housekeeping tasks needed to \u201cset up the work scene\u201d and a couple of important concepts to introduce:", "The question the model is trying to answer is \u201c Has this customer signed up for a term deposit following a direct marketing campaign? \u201c and the cut-off (a.k.a. the threshold) is the value that divides the predictions into Yes and No.", "To illustrate the point, I first calculate some predictions by passing the test_tbl data set to the h2o.performance function.", "Like many other machine learning modelling platforms, h2o uses the threshold value associated with the maximum F1 score, which is nothing but a weighted average between precision and recall. In this case, the threshold @ Max F1 is 0.190.", "Now, I use the h2o.predict function to make predictions using the test set. The prediction output comes with three columns: the actual model predictions (predict), and the probabilities associated with that prediction (p0, and p1, corresponding to No and Yes respectively). As you can see, the p1 probability associated with the current cut-off is around 0.0646.", "However, the F1 score is only one way to identify the cut-off. Depending on our goal, we could also decide to use a threshold that, for instance, maximises precision or recall.", "In a commercial setting, the pre-selected threshold @ Max F1 may not necessarily be the optimal choice: enter Precision and Recall!", "Precision shows how sensitive models are to False Positives (i.e. predicting a customer is subscribing when he-she is actually NOT) whereas Recall looks at how sensitive models are to False Negatives (i.e. forecasting that a customer is NOT subscribing whilst he-she is in fact going to do so).", "These metrics are very relevant in a business context because organisations are particularly interested in accurately predicting which customers are truly likely to subscribe (high precision) so that they can target them with advertising strategies and other incentives. At the same time they want to minimise efforts towards customers incorrectly classified as subscribing (high recall) who are instead unlikely to sign up.", "However, as you can see from the chart below, when precision gets higher, recall gets lower and vice versa. This is often referred to as the Precision-Recall tradeoff.", "To fully comprehend this dynamic and its implications, let\u2019s start with taking a look at the cut-off zero and cut-off one points and then see what happens when you start moving the threshold between the two positions:", "When moving to a higher threshold the model becomes more \u201cchoosy\u201d on who it classifies as subscribed = Yes. As a consequence, you become more conservative on who to contact ( higher precision) and reduce your acquisition cost, but at the same time you increase your chance of not reaching prospective subscribes ( lower recall), missing out on potential revenue.", "The key question here is where do you stop? Is there a \u201csweet spot\u201d and if so, how do you find it? Well, that will depend entirely on the goal you want to achieve. In the next section, I\u2019ll be running a mini-optimisation with the goal to maximise profit.", "For this mini-optimisation I\u2019m implementing a simple profit maximisation based on generic costs connected to acquiring a new customer and benefits derived from said acquisition. This can be evolved to include more complex scenarios but it would be outside the scope of this exercise.", "To understand which cut-off value is optimal to use we need to simulate the cost-benefit associated with each threshold point. This is a concept derived from the Expected Value Framework as seen on Data Science for Business", "To do so I need 2 things:", "Expected rates can be conveniently retrieved for all cut-off points using h2o.metric.", "The cost-benefit matrix is a business assessment of the cost and benefit for each of four potential outcomes. To create such a matrix I will have to make a few assumptions about the expenses and advantages that an organisation should consider when carrying out an advertising-led procurement drive.", "Let\u2019s assume that the cost of selling term deposits is of \u00a330 per customer. This would include the likes of performing the direct marketing activity (training the call centre reps, setting time aside for active calls, etc.) and incentives such as offering discounts on another financial product or onboarding onto membership schemes offering benefits and perks. A banking organisation will incur in this type of cost in two cases: when they correctly predict that a customer will subscribe ( true positive, TP), and when they incorrectly predict that a customer will subscribe ( false positive, FP).", "Let\u2019s also assume that the revenue of selling a term deposits to an existing customer is of \u00a380 per customer. The organisation will guarantee this revenue stream when the model predicts that a customer will subscribe and they actually do ( true positive, TP).", "Finally, there\u2019s the true negative (TN) scenario where we correctly predict that a customer won\u2019t subscribe. In this case, we won\u2019t spend any money but won\u2019t earn any revenue.", "Here\u2019s a quick recap of the cost scenarios:", "I create a function to calculate the expected cost using the probability of a positive case (p1) and the cost/benefit associated with a true positive (cb_tp) and a false positive (cb_fp). No need to include the true negative or false negative here as they\u2019re both zero.", "I\u2019m also including the expected_rates data frame created previously with the expected rates for each threshold (400 thresholds, ranging from 0 to 1).", "Now to understand how a multi customer dynamic would work, I\u2019m creating a hypothetical 10 customer group to test my function on. This is a simplified view in that I\u2019m applying the same cost and revenue structure to all customers but the cost/benefit framework can be tailored to the individual customer to reflect their separate product and service level set up and the process can be easily adapted to optimise towards different KPIs (like net profit, CLV, number of subscriptions, etc.)", "I use purrr to map the expected_profit_func() to each customer, returning a data frame of expected cost per customer by threshold value. This operation creates a nester tibble, which I have to unnest() to expand the data frame to one level.", "Then, I can visualize the expected cost curves for each customer.", "Finally, I can aggregate the expected cost, visualise the final curve and highlight the optimal threshold.", "This has some important business implications. Based on our hypothetical 10-customer group, choosing the optimised threshold of 0.092 would yield a total profit of nearly \u00a3164 compared to the nearly \u00a3147 associated with the automatically selected cut-off of 0.190.", "This would result in an additional expected profit of nearly \u00a31.7 per customer. Assuming that we have a customer base of approximately 500,000, switching to the optimised model could generate an additional expected profit of \u00a3850k!", "It is easy to see that, depending on the size of your business, the magnitude of potential profit increase could be a significant.", "In this project, I\u2019ve used a publicly available dataset to estimate the likelihood of a bank\u2019s existing customers to purchase a financial product following a direct marketing campaign.", "Following a thorough exploration and cleansing of the data, I estimate several models and compare their performance and fit to the data using the DALEX library, which focuses on Model-Agnostic Interpretability. One of its key advantages is the ability to compare contributions of traditional \u201cglass-box\u201d models as well as black-box models on the same scale. However, being permutation-based, one of its main drawbacks is that it does not scale well to large number of predictors and larger datasets.", "Lastly, I take my final model and implemented a multi-customer profit optimization that reveals a potential additional expected profit of nearly \u00a31.7 per customer (or \u00a3850k if you had a 500,000 customer base). Furthermore, I discuss key concepts like the threshold and F1 score and the precision-recall tradeoff and explain why it\u2019s highly important to decide which cutoff to adopt.", "After exploring and cleansing the data, fitting and comparing multiple models and choosing the best one, sticking with the default threshold @ Max F1 would be stopping short of the ultimate \u201cso what?\u201d that puts all that hard work into prospective.", "One final thing: don\u2019t forget to shut-down the h2o instance when you\u2019re done!", "The full R code and all relevant files can be found on my GitHub profile @ Propensity Modelling", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Customer Insight | Business Intelligence | Marketing Analytics | www.linkedin.com/in/diegousaiuk/ | CURRENTLY SEEKING NEW JOB OPPORTUNITIES"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa53f891f0d1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@diegousaiuk?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diegousaiuk?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Diego Usai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21986549abda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&user=Diego+Usai&userId=21986549abda&source=post_page-21986549abda----a53f891f0d1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dmey503?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Dan Meyers"}, {"url": "https://unsplash.com/@dmey503?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Predictive_modelling", "anchor_text": "Propensity Modelling"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing", "anchor_text": "Portuguese Bank Marketing"}, {"url": "https://archive.ics.uci.edu/ml/datasets", "anchor_text": "UCI Machine Learning Repository"}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling/tree/master/01_data/bank-direct-marketing.csv", "anchor_text": "(bank-direct-marketing.csv"}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", "anchor_text": "bank-additional-full.csv"}, {"url": "https://en.wikipedia.org/wiki/Binary_classification", "anchor_text": "binary classification problem"}, {"url": "https://diegousai.io/2020/01/propensity-modelling-data-preparation", "anchor_text": "Propensity Modelling \u2014 Data Preparation and Exploratory Data Analysis."}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "on my GitHub repo for this project"}, {"url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis", "anchor_text": "Exploratory Data Analysis (EDA)"}, {"url": "https://business-science.github.io/correlationfunnel/", "anchor_text": "correlationfunnel"}, {"url": "https://diegousai.io/2020/01/propensity-modelling-data-preparation", "anchor_text": "Propensity Modelling - Data Preparation and Exploratory Data Analysis."}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "on my GitHub repo"}, {"url": "https://uc-r.github.io/dalex", "anchor_text": "Model Interpretability with DALEX"}, {"url": "https://pbiecek.github.io/ema/introduction.html", "anchor_text": "Explanatory Model Analysis"}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic (ROC)"}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve", "anchor_text": "AUC (Area Under the Curve)"}, {"url": "https://stats.stackexchange.com/", "anchor_text": "Cross Validated"}, {"url": "https://stats.stackexchange.com/questions/93202/odds-ratio-from-decision-tree-and-random-forest", "anchor_text": "odds ratio from decision tree and random forest"}, {"url": "https://diegousai.io/2020/02/propensity-modelling-estimate-compare-models/", "anchor_text": "Propensity Modelling \u2014 Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology."}, {"url": "https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/", "anchor_text": "Local Interpretable Model-agnostic Explanations (LIME)"}, {"url": "https://en.wikipedia.org/wiki/F1_score", "anchor_text": "F1 score"}, {"url": "https://www.goodreads.com/book/show/17912916-data-science-for-business", "anchor_text": "Data Science for Business"}, {"url": "https://github.com/DiegoUsaiUK/Propensity_Modelling", "anchor_text": "Propensity Modelling"}, {"url": "http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf", "anchor_text": "A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems"}, {"url": "https://business-science.github.io/correlationfunnel/", "anchor_text": "correlationfunnel Package Vignette"}, {"url": "https://uc-r.github.io/dalex", "anchor_text": "Model Interpretability with DALEX"}, {"url": "https://pbiecek.github.io/ema/introduction.html", "anchor_text": "Explanatory Model Analysis"}, {"url": "https://www.business-science.io/business/2017/10/16/sales_backorder_prediction.html", "anchor_text": "Predictive Sales Analytics: Use Machine Learning to Predict and Optimize Product Backorders"}, {"url": "https://www.goodreads.com/book/show/17912916-data-science-for-business", "anchor_text": "Data Science for Business"}, {"url": "https://diegousai.io/2020/05/propensity-modelling-abridged", "anchor_text": "https://diegousai.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a53f891f0d1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----a53f891f0d1---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/model-interpretability?source=post_page-----a53f891f0d1---------------model_interpretability-----------------", "anchor_text": "Model Interpretability"}, {"url": "https://medium.com/tag/propensity-model?source=post_page-----a53f891f0d1---------------propensity_model-----------------", "anchor_text": "Propensity Model"}, {"url": "https://medium.com/tag/model-selection?source=post_page-----a53f891f0d1---------------model_selection-----------------", "anchor_text": "Model Selection"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&user=Diego+Usai&userId=21986549abda&source=-----a53f891f0d1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&user=Diego+Usai&userId=21986549abda&source=-----a53f891f0d1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa53f891f0d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a53f891f0d1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a53f891f0d1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a53f891f0d1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a53f891f0d1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diegousaiuk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diegousaiuk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Diego Usai"}, {"url": "https://medium.com/@diegousaiuk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "148 Followers"}, {"url": "http://www.linkedin.com/in/diegousaiuk/", "anchor_text": "www.linkedin.com/in/diegousaiuk/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21986549abda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&user=Diego+Usai&userId=21986549abda&source=post_page-21986549abda--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F21986549abda%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-h2o-and-dalex-to-estimate-the-likelihood-to-purchase-a-financial-product-a53f891f0d1&user=Diego+Usai&userId=21986549abda&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}