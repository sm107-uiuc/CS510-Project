{"url": "https://towardsdatascience.com/outperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae", "time": 1683004367.895243, "path": "towardsdatascience.com/outperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae/", "webpage": {"metadata": {"title": "Outperforming Google Cloud AutoML Vision with Tensorflow | by Shane Keller | Towards Data Science", "h1": "Outperforming Google Cloud AutoML Vision with Tensorflow", "description": "There are hundreds of blog posts on machine learning and deep learning projects, and I\u2019ve learned a lot from the ones that I\u2019ve read. I wanted to add to this body of knowledge by discussing a deep\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/skeller88/deep_learning_project", "anchor_text": "Github repo", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9", "anchor_text": "this article on file formats for machine learning", "paragraph_index": 9}, {"url": "https://cloud.google.com/vision/automl/docs/prepare", "anchor_text": "The product does not currently accept .npy or .tiff files", "paragraph_index": 10}, {"url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data", "anchor_text": "tensorflow tutorial", "paragraph_index": 12}, {"url": "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets", "anchor_text": "kaggle kernel", "paragraph_index": 12}, {"url": "https://github.com/albumentations-team/albumentations", "anchor_text": "albumentations", "paragraph_index": 16}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc", "anchor_text": "blog post discussions where people ran into installation problems", "paragraph_index": 21}, {"url": "https://cloud.google.com/ai-platform/deep-learning-vm/docs/images", "anchor_text": "GCP offers Deep Learning VM images", "paragraph_index": 22}, {"url": "https://github.com/NVIDIA/nvidia-docker", "anchor_text": "nvidia-docker toolkit", "paragraph_index": 22}, {"url": "https://github.com/mviereck/x11docker/wiki/NVIDIA-driver-support-for-docker-container", "anchor_text": "X11docker", "paragraph_index": 23}, {"url": "https://cloud.google.com/ai-platform/deep-learning-containers/docs/", "anchor_text": "GCP Deep Learning Container", "paragraph_index": 24}, {"url": "https://cloud.google.com/ai-platform-notebooks/", "anchor_text": "Google Cloud AI Platform Notebooks", "paragraph_index": 24}, {"url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers", "anchor_text": "Kubernetes Engine documentation", "paragraph_index": 25}, {"url": "https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/pytorch/containers/quickstart/mnist/Dockerfile-gpu", "anchor_text": "cloudml-samples repo", "paragraph_index": 25}, {"url": "https://github.com/skeller88/deep_learning_project#create-gcp-instance-from-google-image-family", "anchor_text": "gcloud command to create the GCP VM instance", "paragraph_index": 26}, {"url": "https://github.com/skeller88/deep_learning_project/blob/master/data_science/jupyter_tensorflow_notebook/startup_script.sh", "anchor_text": "GCP VM instance startup script", "paragraph_index": 26}, {"url": "https://github.com/skeller88/deep_learning_project/blob/master/data_science/jupyter_tensorflow_notebook/Dockerfile", "anchor_text": "Dockerfile", "paragraph_index": 26}, {"url": "https://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines", "anchor_text": "Andrej Karpathy", "paragraph_index": 28}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Andrew Ng", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "James Dellinger gives a great overview", "paragraph_index": 29}, {"url": "https://www.tensorflow.org/guide/data_performance#optimize_performance", "anchor_text": "optimized tf.data API pipeline", "paragraph_index": 37}, {"url": "https://stackoverflow.com/questions/47143521/where-to-apply-batch-normalization-on-standard-cnns/59939495#59939495", "anchor_text": "There\u2019s a lot of debate on specific BN layer placement", "paragraph_index": 43}, {"url": "https://github.com/albumentations-team/albumentations", "anchor_text": "albumentations", "paragraph_index": 45}, {"url": "https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview", "anchor_text": "Google\u2019s hyperparameter tuning product", "paragraph_index": 49}, {"url": "http://hyperopt.github.io/hyperopt/", "anchor_text": "this one", "paragraph_index": 49}, {"url": "https://hyperopt.github.io/hyperopt/", "anchor_text": "hyperopt", "paragraph_index": 49}, {"url": "https://github.com/hyperopt/hyperopt/issues/253#issuecomment-295871038", "anchor_text": "this highly rated comment on hyperopt", "paragraph_index": 49}, {"url": "https://www.tensorflow.org/tfx/serving/docker", "anchor_text": "TensorFlow Serving", "paragraph_index": 60}, {"url": "https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching", "anchor_text": "request batching", "paragraph_index": 60}, {"url": "https://www.statworx.com/de/blog/a-performance-benchmark-of-google-automl-vision-using-fashion-mnist/", "anchor_text": "a performance benchmark", "paragraph_index": 62}, {"url": "https://cloud.google.com/vision/automl/pricing", "anchor_text": "$3.15 per node hour", "paragraph_index": 63}, {"url": "https://www.linkedin.com/in/shane-keller-ba891315/", "anchor_text": "linkedin", "paragraph_index": 66}, {"url": "https://github.com/skeller88", "anchor_text": "my GitHub", "paragraph_index": 67}, {"url": "https://github.com/skeller88/deep_learning_project", "anchor_text": "repository", "paragraph_index": 67}, {"url": "http://bigearth.net/static/documents/BigEarthNet_IGARSS_2019.pdf", "anchor_text": "BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding", "paragraph_index": 68}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=He%2C+K", "anchor_text": "He", "paragraph_index": 69}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X", "anchor_text": "Zhang", "paragraph_index": 69}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+S", "anchor_text": "Ren", "paragraph_index": 69}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J", "anchor_text": "J. Sun", "paragraph_index": 69}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "paragraph_index": 69}, {"url": "https://scholar.google.com/citations?user=cBDOkNIAAAAJ&hl=en&oi=sra", "anchor_text": "X Li", "paragraph_index": 70}, {"url": "https://scholar.google.com/citations?user=vlu_3ksAAAAJ&hl=en&oi=sra", "anchor_text": "S Chen", "paragraph_index": 70}, {"url": "https://scholar.google.com/citations?user=PksdgoUAAAAJ&hl=en&oi=sra", "anchor_text": "X Hu", "paragraph_index": 70}, {"url": "https://scholar.google.com/citations?user=6CIDtZQAAAAJ&hl=en&oi=sra", "anchor_text": "J Yang", "paragraph_index": 70}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.html", "anchor_text": "Understanding the disharmony between dropout and batch normalization by variance shift", "paragraph_index": 70}, {"url": "https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf", "anchor_text": "Hidden Technical Debt in Machine Learning Systems", "paragraph_index": 71}], "all_paragraphs": ["There are hundreds of blog posts on machine learning and deep learning projects, and I\u2019ve learned a lot from the ones that I\u2019ve read. I wanted to add to this body of knowledge by discussing a deep learning side project that I worked on recently. This project/blog post has some aspects to it that I think make it worth a read:", "The project had the following steps, and this blog post structure mirrors those steps:", "I\u2019ve shared the project code in a Github repo.", "Cloud detection in satellite images is an important classification problem. It\u2019s used heavily in the field of Remote Sensing, because clouds obscure the land underneath, and too many cloudy images in a dataset make it harder for a model to learn meaningful patterns. A robust cloud classifier would make it easier for researchers to automatically screen for cloud images, and construct larger and more accurately labeled datasets.", "The BigEarth\u00b9 dataset is the largest public satellite image dataset in the world, and was created in 2019 by a German research group at Technische Universit\u00e4t Berlin. The dataset consists of nearly 600,000 120x120 pixel satellite image tiles, each tile with 13 bands, that have been labeled according to land use (river, urban, and so on), whether or not the image is obscured by clouds, and other metadata such as image coordinates. I built a CNN binary classifier that uses the cloud label and the red, green, and blue bands from the each image tile to classify the image tile as cloudy (cloud images) or not cloudy (no_cloud images).", "Here are some cloud images that I converted from tiff to jpg for the purposes of data exploration, with the class labels and image grid locations printed out underneath.", "To check that the labels made sense and that the project was feasible, I looked at 100 random jpg images and classified them myself as cloud or no_cloud. I saw a number of images that were difficult to classify, like the images at the graph positions (2, 0) and (2, 1). If I were doing this deep learning project for a production app, I would investigate these difficult labels a bit more to make sure that they weren\u2019t misclassified. Nevertheless, I was able to classify 77% of the 100 images correctly, which gave me confidence to move forward with a deep learning model.", "There are about 600,000 satellite images, and the dataset is 60GB in a gzipped tarfile. The ETL process consisted of fetching the tarfile, extracting it, generating some summary pixel statistics, and transforming the images into usable formats for deep learning on Google AutoML Vision, Google Cloud VM, and a local laptop.", "The ETL pipeline consisted of the following steps:", "I stored the bands for an image tile in the same .npy file in order to improve read performance at training time. I chose the .npy file format because it is compatible with both keras and pytorch (in case I wanted to rewrite my code in pytorch at some point). I stored the files on disk rather than in Google Cloud Storage because that made it easier to achieve high read throughput, and because the RGB dataset was reasonably small at 50GB. I\u2019d highly recommend this article on file formats for machine learning if you\u2019re interested in digging more into the topic.", "I stored another copy of the bands for an image tile in a .png file because that file format is compatible with Google AutoML Vision. The product does not currently accept .npy or .tiff files.", "I created a dataset that included all 9000 cloud images and 9000 random no_cloud images, and I created a small training dataset by taking a random sample of 1200 cloud images and 1200 no_cloud images from the large dataset. I used the larger dataset to train the winning model from the small dataset training rounds.", "I created a balanced dataset because I wanted to scope the problem to set myself up for success. If I had more time I would create an imbalanced dataset that more accurately reflected the proportion of cloud to no_cloud images in the dataset. There\u2019s an informative tensorflow tutorial and a kaggle kernel that outline different strategies for dealing with imbalanced data.", "I wanted to get a sense of the distribution of pixel values in the entire dataset and in the sample for both cloud images and no_cloud images. I also wanted to confirm that the pixel value distributions were different between the cloud and no_cloud images, and consistent within each class between the sample and the overall dataset. There were too many pixel values to fit in memory without using a high memory (>60GB) instance, so I used dask to compute the statistics on all 600K images (labeled all in the chart below), and the per class statistics on the large and small datasets.", "The mean and std cloud pixel values were higher than the no_cloud pixel values across all bands, which provided evidence that the classification task was learnable.", "The mean and std pixel values were similar within each class between the sample and the overall dataset, which gave me a reasonably high confidence that the sample was representative of the dataset.", "Now that I had a basic understanding of and more confidence in the data, I could get a baseline with a Google AutoML Vision model. I created a dataset by uploading the sample .png files to Google Cloud Storage. Note that the destination GCS bucket must be in the us-central-1 region. The AutoML documentation recommends that you provide as many images as possible at training time, so to help with validation and test accuracy I used the albumentations library to create a flipped, rotated and a flipped + rotated image for each image in the sample. I added these augmented images to the dataset.", "I uploaded a .csv file containing the dataset splits and the GCS image blob paths. AutoML can make splits for you, but I needed to do the splits myself in order to train my own models on the same data.", "Then I trained the model on both the small and the large datasets. Performance was a good start at 82% precision and recall. Later in this blog post you\u2019ll see how my model stacked up against Google\u2019s.", "I got another simple baseline by implementing incremental learning with a logistic regression-based SGDClassifier that took flattened image arrays as the input. Training accuracy was 63% and validation accuracy was 65%.", "I needed a model development workflow that would allow me to easily transition between prototyping work on a local laptop, and heavier duty training work on a cloud instances. I had to be able to run tensorflow on jupyterhub in both CPU and GPU mode, use my own code modules, configure dependencies, persist notebooks in between runs, and connect to a GPU on the cloud instance. Docker with conda was the obvious solution, but I had to figure out the details.", "There are a lot of moving parts to a deep learning environment: the NVIDIA driver, CUDA toolkit, and the conda environment that installs jupyter, python, tensorflow, and other libraries. I did some research on best practices and found a ton of different options, along with a graveyard of unanswered StackOverflow questions, Github issues, and blog post discussions where people ran into installation problems. It was very sad. I wanted to avoid this happening to me.", "GCP offers Deep Learning VM images, which install NVIDIA driver libraries and tensorflow libraries onto the VM. But the VM didn\u2019t provide everything I needed because since I was running a container on the VM, I needed to be able to run the CUDA toolkit and Tensorflow from within the container. I also wanted to understand how the installation process works, and GCP doesn\u2019t make its Deep Learning VM image source code public. Finally, I wanted a solution that would set me up for a Kubernetes deployment down the road if I wanted one. So I decided to use the GCP Deep Learning VM to install the CUDA driver and the nvidia-docker toolkit to run a GPU-accelerated Docker container, and then install everything else into a Docker container that I ran on the VM.", "I installed the CUDA Driver on the GCP VM host itself because since NVIDIA\u2019s driver code is proprietary, solutions to install the driver in a Docker container are not widely supported. X11docker provides support, but I didn\u2019t find many examples of people using it and was weary of adding yet another tool. I used a GCP common-cu100 base VM image, which installs the NVIDIA driver and CUDA 10.0 toolkit on the instance. Note that even though the CUDA toolkit was installed on the host, I used the toolkit in the container.", "As far as configuring the Docker image to run on the VM, I considered using a GCP Deep Learning Container as a base Docker image, but the product is in beta as of June 2019, and the source Dockerfiles are not public. That lack of transparency would make it difficult to debug any issues. I had similar concerns about Google Cloud AI Platform Notebooks (managed Jupyterhub on Google Cloud) because they are running beta Deep Learning Containers.", "Google\u2019s Kubernetes Engine documentation and cloudml-samples repo both have examples of using nvidia/cuda as a base image to take care of CUDA toolkit installation and PATH configuration. So I used the nvidia/cuda:10.0-devel-ubuntu18.04 image and installed miniconda on top to set up a python3.6, jupyterhub, and tensorflow-2.0.0-gpu environment.", "Here\u2019s the gcloud command to create the GCP VM instance, the GCP VM instance startup script, and the Dockerfile to build the image that runs on the VM instance.", "Now that I had a workflow, I ran the docker container locally and prototyped a Keras CNN binary classifier that would classify cloud images with as 1and no_cloud images as 0.", "I followed a combination of the model development processes recommended by Andrej Karpathy and Andrew Ng. First, I set up training and evaluation with the simplest model possible that followed current best practices.", "The neural network used ReLU activation, so I used Kaiming/he_uniform initialization\u00b2 rather than Xavier initialization for weight initialization. James Dellinger gives a great overview of the differences. Here are the key takeaways:", "When using a ReLU activation, a single layer will, on average have standard deviation that\u2019s very close to the square root of the number of input connections, divided by the square root of two, or \u221a512/\u221a2 in our example.", "Keeping the standard deviation of layers\u2019 activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing.", "So Kaiming initialization multiplies each initial activation by \u221a2/\u221an , where n is the number of incoming connections coming into a given layer from the previous layer\u2019s output (also known as the \u201cfan-in\u201d).", "This smarter initialization means that the network is more likely to converge, especially if it\u2019s deep:", "I learned that Keras by default uses Xavier/glorot_uniform, but there is an open Tensorflow ticket to change that default to he_uniform.", "The weight initialization parameter is a good example of how it can be dangerous to use default parameters in a machine learning method without understanding them, because they may be the incorrect parameters for the task.", "I used an Adam optimizer, because Adam optimizers converge fast and are generally accepted to work well for a variety of tasks. I chose a learning rate of 3e-4 because I\u2019ve seen many people use that learning rate (including Andrej Karpathy). It\u2019s also an order of magnitude less than the default learning rate for Adam in Tensorflow, which I was ok with because the dataset was small.", "I fed data to the model via an optimized tf.data API pipeline that leveraged prefetching and parallel image augmentation to prepare the batches to feed to the neural network model. That significantly sped up training speed.", "I sanity checked the data in the batches being fed into the neural network, the predictions of the untrained neural network, and the performance of the untrained neural network. Since this was a binary classification problem, binary cross entropy was used for the loss function.", "I checked that the binary cross entropy loss of the neural network predictions was not too far away from the approximate loss of a model that randomly predicts a class probability between 0 and 1. I also checked that the distribution of the predictions was centered around .5.", "It\u2019s useful to separate the bias reduction (fitting) steps from the regularization steps, because if a model can\u2019t overfit the data, that indicates bugs or a limitation in the dataset. To that end, I overfit the network on the training set only. I repeatedly trained the model on 10 observations with both positive and negative examples of the cloud class until accuracy was 100%.", "I confirmed that the predicted and actual values matched up exactly.", "Now that I had confidence in my training process, I deployed my docker training image to a Google Deep Learning VM and started it up. I added a custom Keras callback that subclassed ModelCheckpoint and wrote the best model and the experiment metadata to GCS every time a new best accuracy was achieved. That allowed me to persist the models for deployment, and analyze experiment results on a less expensive local machine after training was finished.", "I regularized the model from the prototyping stage with validation data and Batch Normalization (BN) to minimize overfitting and increase accuracy on the validation set, while keeping as much accuracy as possible on the training set. I added a validation set and changed the cost function to use validation loss instead of training loss. There\u2019s a lot of debate on specific BN layer placement, but I also added BN layers to the neural network after the activation layers.", "These changes resulted in an initial validation accuracy of .862, and train accuracy of .999.", "There was still a gap to close between train and validation performance, so next I added an image augmentation step to the pipeline that used the albumentations library to randomly flip and/or rotate each image in a batch. That increased validation accuracy from .862 to .929 while decreasing train accuracy from .999 to .928.", "I visualized the neural network output to make sure it was reasonable. The distribution of the predicted class probabilities showed what I expected, as the classifier had clearly separated the classes.", "I explored the addition of Dropout layers to improve validation performance. Li, Xiang et. al.\u00b3 showed that insertion of Dropout layers before Batch Normalization layers in a model architecture may negatively affect the performance of the later BN layers. These BN layers learn to normalize a certain covariate shift in the data during training, but at test time, when Dropout is turned off, that covariate shift is no longer accurate. So I trained a model with a Dropout layer after the last BN layer. It did not result in any improvement over the BN + augmentation model, and took longer to train. So I performed hyperparameter tuning on the BN + augmentation model without Dropout.", "I performed hyperparameter tuning on the optimizer and the learning rate in order to improve the best model from the initial neural network architecture search. I chose these parameters because I already had a neural network architecture that worked well. If I didn\u2019t have a good neural network base performance, I would\u2019ve used a Neural Architecture Search as part of the hyperparameter tuning step.", "Bayesian Optimization Search (BO) is the current recommended method for hyperparameter tuning over Random Search, because it reaches an optimal result in fewer iterations. There\u2019s no published paper I found that does an explicit comparison of the two search methods, but the default optimization method of Google\u2019s hyperparameter tuning product, the supported optimization methods of most automl libraries, and several ML talks like this one all recommend BO over Random Search. I used the hyperopt library to perform the optimization. Worth mentioning is that I saw many tutorials using hp.choice on numerical hyperparameters, which is incorrect because that decreases the efficiency and effectiveness of the search by removing the information conveyed by the numerical ordering of those parameters. See this highly rated comment on hyperopt for further discussion.", "The best model had an Adam optimizer and a learning rate of 2e-4, which is the same as the untuned optimizer and almost the same as the untuned learning rate of 3e-4. It was partly luck that my initial learning rate and optimizer performed well, so it was good to confirm via hyperparameter tuning that I\u2019d found one of the best models in that search space. There was an improvement due to hyperparameter tuning, because the best model increased validation accuracy from .929 to .945 while keeping train accuracy at .928. Training time on the winning model was a little over 8 minutes.", "I also did transfer learning with Resnet50V2 as a feature extractor. It achieved .921 validation accuracy in 25 minutes of training time.", "I trained the best model from the small dataset on the large dataset containing all of the 9000 cloud images and 9000 no_cloud images. Validation accuracy decreased from .945 to .935, but training accuracy remained the same at .928, which gave evidence that the model was overfitting less than it had on the small dataset. While the size of the large dataset was 9 times the size of the small dataset, training time only increased by 3 times to 27 minutes. As in previous model runs, predicted class probabilities were what I would expect, with clear class separation.", "It seemed worth training the Resnet50V2 classifier on the large dataset as well. It had a promising validation accuracy on the small dataset and the more complex a model is, the more it can benefit from additional data. Training time was too slow to try hyperparameter tuning, so I stuck with the Adam optimizer and a learning rate of 3e-4 and found a model with .905 validation accuracy in 82 minutes of training time. I was disappointed that validation accuracy didn\u2019t improve more, but it was still a reasonably good result.", "I evaluated the test performance of the strongest model (let\u2019s call it hp_tuned_keras_cnn) and the resnet50_v2 model on both the small and the large datasets, and compared these results against the performance of the Google AutoML model (google_automl) on the small and large datasets. On both datasets, with a 0.5 class prediction threshold, hp_tuned_keras_cnn and resnet50_v2 outperformed google_automl on precision and recall. Here are the results for large dataset performance:", "A confusion matrix of each model shows that both models performed best on classifying no_cloud images as no_cloud, but struggled the most with Type II error (false negatives) on cloud images. That makes sense given my visual observation of the confusing and distorted shapes in the cloud images.", "I considered the real world applications of this classifier. One application would be using this model to screen out images with clouds, in preparation for some other image classification task. If I had a large amount of images, I would prioritize recall for the cloud classification task over precision, because a false positive would result in an image not being used in the dataset, but a false negative would result in a noisy cloud image being added to the dataset. Of course, precision would still need to be high, because otherwise too many images (and possibly too many images of the same land type) would be screened out.", "Given that business context, I compared the models when targeting a higher recall. At a .26 cloud prediction threshold,hp_tuned_keras_cnn had a recall of 95.8% and a precision of 91.2%. At a .25 cloud prediction threshold,google_automl had the same recall, but a precision of 75.3%. hp_tuned_keras_cnn had the cloud detection capabilities that I was looking for. Next steps would be to test this model on an unbalanced dataset, and then see how the model performed on a novel dataset.", "For deployment, I used a Docker container that ran Flask and tensorflow in a conda environment, and loaded the model from Google Cloud Storage at runtime. A client made an inference request by flattening an image tensor and sending the JSON-serialized tensor to the server. The server reshaped the tensor, made the class prediction, and sent back the class prediction as JSON.", "I deployed the container on a Google VM Instance without a GPU, and got model server response speeds of 100ms (50ms inference + 50ms data processing and other request overhead).", "In a mature production system, I would make some improvements to this deployment. 100ms of latency is above the 70ms threshold that is noticeable by humans, so to decrease latency I would run the container with TensorFlow Serving with request batching instead of Flask and on a GPU instead of a CPU. Deploying the model on Kubernetes would add horizontal scalability as well.", "I would also add more MLOps functionality to the deployment. D. Sculley et. al.\u2074 discuss sources of hidden technical debt in ML systems. One powerful recommendation is to monitor model prediction distribution. If the distribution changes dramatically, that could indicate a change in the nature of the input data, a regression in the newest deployment of the model, or both.", "AutoML Vision is an interesting product with a lot of potential. I could see AutoML Vision having its place in a situation where a non-technical person or an engineer wants deep learning functionality but has no one to implement it. However, I was underwhelmed by AutoML\u2019s performance because I was able to beat it with a simple model and a transfer learned model. I would\u2019ve expected AutoML to at least exceed those benchmarks. It could be that this dataset doesn\u2019t play to AutoML\u2019s strengths, but I also found a performance benchmark where AutoML underperformed a state-of-the-art model on the Fashion-MNIST dataset by 93.9% to 96.7%.", "AutoML isn\u2019t cheap either. Training costs $3.15 per node hour, with a minimum of 16 hours. That adds up fast if you follow the recommended training times for your dataset size, and that doesn\u2019t include pricing for inference requests, or repeated training runs that are needed for many production grade models. Training the small AutoML model for 16 node hours cost $38, and training the large AutoML model for 65 node hours cost $195. For comparison, the small hp_tuned_keras_cnn model was trained for 8 minutes at a cost of $.40, and the large hp_tuned_keras_cnn model was trained for 27 minutes at a cost of $1.35. Given that running a Tesla V100 preemptible instance costs $.82 per node hour, it would be nice if Google could offer that price point with a looser SLA. Again, perhaps this product\u2019s cost is acceptable to an organization that doesn\u2019t have enough ML expertise to go around.", "Whew! Congratulations and thank you if you\u2019ve read this far. I hope you learned something useful from my experience. This project was a lot of fun and gave me a chance to play more with the latest ML tech.", "I\u2019m looking forward to building more machine learning-driven systems in the future, both in my job and in my side projects. I\u2019m excited about the growth of tools in the machine learning space, across all parts of the stack. As these tools mature, they will make machine learning more accessible to everyone. I\u2019m also blown away by how fast neural network best practices are evolving. It will be interesting to see where this innovation leads.", "Interested in connecting or working together? Share your responses below or drop me a line on linkedin.", "The code used in this article is available on my GitHub in this repository.", "[1] G. Sumbul, M. Charfuelan, B. Demir, V. Markl, BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding (2019), IEEE International Conference on Geoscience and Remote Sensing Symposium", "[2] K. He, X. Zhang, S. Ren, J. Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015), The IEEE International Conference on Computer Vision (ICCVV)", "[3] X Li, S Chen, X Hu, J Yang, Understanding the disharmony between dropout and batch normalization by variance shift (2019), The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "[4] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, Hidden Technical Debt in Machine Learning Systems (2015), Advances in Neural Information Processing Systems 28 (NIPS)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F34a45e3860ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@skeller88?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skeller88?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Shane Keller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cca5b02c92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&user=Shane+Keller&userId=2cca5b02c92&source=post_page-2cca5b02c92----34a45e3860ae---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.istockphoto.com/portfolio/sharply_done?mediatype=photography", "anchor_text": "sharply_done"}, {"url": "https://www.istockphoto.com/photo/xl-satellite-dish-twilight-gm156725813-15556869", "anchor_text": "iStock"}, {"url": "https://hyperopt.github.io/hyperopt/", "anchor_text": "hyperopt"}, {"url": "https://github.com/skeller88/deep_learning_project", "anchor_text": "Github repo"}, {"url": "https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9", "anchor_text": "this article on file formats for machine learning"}, {"url": "https://cloud.google.com/vision/automl/docs/prepare", "anchor_text": "The product does not currently accept .npy or .tiff files"}, {"url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data", "anchor_text": "tensorflow tutorial"}, {"url": "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets", "anchor_text": "kaggle kernel"}, {"url": "https://github.com/albumentations-team/albumentations", "anchor_text": "albumentations"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier"}, {"url": "https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc", "anchor_text": "blog post discussions where people ran into installation problems"}, {"url": "https://cloud.google.com/ai-platform/deep-learning-vm/docs/images", "anchor_text": "GCP offers Deep Learning VM images"}, {"url": "https://github.com/NVIDIA/nvidia-docker", "anchor_text": "nvidia-docker toolkit"}, {"url": "https://github.com/mviereck/x11docker/wiki/NVIDIA-driver-support-for-docker-container", "anchor_text": "X11docker"}, {"url": "https://github.com/NVIDIA/nvidia-docker/blob/master/README.md", "anchor_text": "Source"}, {"url": "https://cloud.google.com/ai-platform/deep-learning-containers/docs/", "anchor_text": "GCP Deep Learning Container"}, {"url": "https://cloud.google.com/ai-platform-notebooks/", "anchor_text": "Google Cloud AI Platform Notebooks"}, {"url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers", "anchor_text": "Kubernetes Engine documentation"}, {"url": "https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/pytorch/containers/quickstart/mnist/Dockerfile-gpu", "anchor_text": "cloudml-samples repo"}, {"url": "https://github.com/skeller88/deep_learning_project#create-gcp-instance-from-google-image-family", "anchor_text": "gcloud command to create the GCP VM instance"}, {"url": "https://github.com/skeller88/deep_learning_project/blob/master/data_science/jupyter_tensorflow_notebook/startup_script.sh", "anchor_text": "GCP VM instance startup script"}, {"url": "https://github.com/skeller88/deep_learning_project/blob/master/data_science/jupyter_tensorflow_notebook/Dockerfile", "anchor_text": "Dockerfile"}, {"url": "https://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines", "anchor_text": "Andrej Karpathy"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Andrew Ng"}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "James Dellinger gives a great overview"}, {"url": "https://github.com/tensorflow/tensorflow/issues/31303", "anchor_text": ""}, {"url": "https://www.tensorflow.org/guide/data_performance#optimize_performance", "anchor_text": "optimized tf.data API pipeline"}, {"url": "https://stackoverflow.com/questions/47143521/where-to-apply-batch-normalization-on-standard-cnns/59939495#59939495", "anchor_text": "There\u2019s a lot of debate on specific BN layer placement"}, {"url": "https://github.com/albumentations-team/albumentations", "anchor_text": "albumentations"}, {"url": "https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview", "anchor_text": "Google\u2019s hyperparameter tuning product"}, {"url": "http://hyperopt.github.io/hyperopt/", "anchor_text": "this one"}, {"url": "https://hyperopt.github.io/hyperopt/", "anchor_text": "hyperopt"}, {"url": "https://github.com/hyperopt/hyperopt/issues/253#issuecomment-295871038", "anchor_text": "this highly rated comment on hyperopt"}, {"url": "https://www.tensorflow.org/tfx/serving/docker", "anchor_text": "TensorFlow Serving"}, {"url": "https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching", "anchor_text": "request batching"}, {"url": "https://www.statworx.com/de/blog/a-performance-benchmark-of-google-automl-vision-using-fashion-mnist/", "anchor_text": "a performance benchmark"}, {"url": "https://cloud.google.com/vision/automl/pricing", "anchor_text": "$3.15 per node hour"}, {"url": "https://imgflip.com/memegenerator/Success-Kid", "anchor_text": "Source"}, {"url": "https://www.linkedin.com/in/shane-keller-ba891315/", "anchor_text": "linkedin"}, {"url": "https://github.com/skeller88", "anchor_text": "my GitHub"}, {"url": "https://github.com/skeller88/deep_learning_project", "anchor_text": "repository"}, {"url": "http://bigearth.net/static/documents/BigEarthNet_IGARSS_2019.pdf", "anchor_text": "BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=He%2C+K", "anchor_text": "He"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X", "anchor_text": "Zhang"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+S", "anchor_text": "Ren"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J", "anchor_text": "J. Sun"}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"url": "https://scholar.google.com/citations?user=cBDOkNIAAAAJ&hl=en&oi=sra", "anchor_text": "X Li"}, {"url": "https://scholar.google.com/citations?user=vlu_3ksAAAAJ&hl=en&oi=sra", "anchor_text": "S Chen"}, {"url": "https://scholar.google.com/citations?user=PksdgoUAAAAJ&hl=en&oi=sra", "anchor_text": "X Hu"}, {"url": "https://scholar.google.com/citations?user=6CIDtZQAAAAJ&hl=en&oi=sra", "anchor_text": "J Yang"}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.html", "anchor_text": "Understanding the disharmony between dropout and batch normalization by variance shift"}, {"url": "https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf", "anchor_text": "Hidden Technical Debt in Machine Learning Systems"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----34a45e3860ae---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----34a45e3860ae---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/technology?source=post_page-----34a45e3860ae---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----34a45e3860ae---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&user=Shane+Keller&userId=2cca5b02c92&source=-----34a45e3860ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&user=Shane+Keller&userId=2cca5b02c92&source=-----34a45e3860ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F34a45e3860ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----34a45e3860ae---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----34a45e3860ae--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----34a45e3860ae--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----34a45e3860ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skeller88?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@skeller88?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shane Keller"}, {"url": "https://medium.com/@skeller88/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "234 Followers"}, {"url": "https://www.linkedin.com/in/shane-keller-ba891315/", "anchor_text": "https://www.linkedin.com/in/shane-keller-ba891315/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cca5b02c92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&user=Shane+Keller&userId=2cca5b02c92&source=post_page-2cca5b02c92--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5371123b3122&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foutperforming-google-cloud-automl-vision-with-tensorflow-and-google-deep-learning-vm-34a45e3860ae&newsletterV3=2cca5b02c92&newsletterV3Id=5371123b3122&user=Shane+Keller&userId=2cca5b02c92&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}