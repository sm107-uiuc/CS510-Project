{"url": "https://towardsdatascience.com/non-parametric-meta-learning-bd391cd31700", "time": 1683007520.32427, "path": "towardsdatascience.com/non-parametric-meta-learning-bd391cd31700/", "webpage": {"metadata": {"title": "Non-parametric meta-learning. This story covers non-parametric\u2026 | by Qiurui Chen | Towards Data Science", "h1": "Non-parametric meta-learning", "description": "This story covers non-parametric few-shot learning algorithms, which include siamese networks, matching networks, and prototypical networks. It also covers the properties of meta-learning algorithms\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 4 \u2014 Non-Parametric Meta-Learners\u2019", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning", "anchor_text": "matching network paper", "paragraph_index": 5}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf", "anchor_text": "relation networks", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1902.04552.pdf", "anchor_text": "Another approach", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1711.04043", "anchor_text": "another paper", "paragraph_index": 13}, {"url": "https://openreview.net/forum?id=BJfOXnActQ", "anchor_text": "one approach", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1807.05960", "anchor_text": "Another way", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1903.03096", "anchor_text": "The last approach", "paragraph_index": 16}], "all_paragraphs": ["This story covers non-parametric few-shot learning algorithms, which include siamese networks, matching networks, and prototypical networks. It also covers the properties of meta-learning algorithms. This is a summary of the course \u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 4 \u2014 Non-Parametric Meta-Learners\u2019.", "Non-parametric methods are simple and work well in low data regimes in ML, such as nearest neighbours. During meta-test time, few-shot learning is exactly precisely in low data regime, so these non-parametric methods are likely to perform pretty well. But during meta-training, we still want to be parametric because we want to be able to scale to large datasets. The key idea of non-parametric approaches is can we use parametric meta learners to produce effective non-parametric learners?", "The key idea is using non-parametric learners, such as nearest neighbours. The way to do this is to take a test data point and compare it to all the training dataset data points and find the one training data that looks the most similar and then return the label corresponding that training data. The problem is in what space do you compare? with what distance metric? We could do this in pixel space with L2 distance.", "But L2 distance works poorly. For example, L2 distance would choose the right image to be the most similar image compared to the middle one. So instead of using L2 distance in pixel space, we can learn to compare using meta-training data.", "One approach is to train the Siamese network to predict whether or not two images are the same class. So you\u2019re essentially just learning to compare pairs of images and classify whether or not they are the same class. You can learn a more semantic distance between two images. So you could take two images from the met-training dataset, and label these two images into 1 or 0 if they are the same classes or not. You repeat this for different pairs of images in your meta-training dataset. At meta test time, we compare each image X-test to each of your images in your training dataset for that task, then you output the label corresponding to the closest images. In short, at the meta-training time, we are training this binary classifier and then at meta test-time, we are performing an N-way classification by doing each of these pairwise comparisons. Since meta-training(train a binary classifier) and meta-testing(testing an n-way classifier) do not match each other, can we do better to match these two procedures?", "The key idea in the matching network paper is: If we are gonna be doing nearest neighbours at test time in order to match our test query image to each of our training data points, how about we train an embedding space such that nearest neighbours produce accurate predictions?", "So we take each of the images in our training dataset and them into a learned embedding space. We also take our test query image and embed that into an embedding space. And we compare each of these embeddings to make a prediction. So each of these black dots (where f\u03b8 points)will correspond to a comparison between the test embedding and the training embeddings. We take the labels corresponding to each of our training images, and our prediction will correspond to the training labels weighted by their similarity score. The model is trained end-to-end. Most critically here meta-training is about showing what\u2019s happening at meta-testing. During meta-training, you are making comparisons to all the images in your training dataset and at meta test-time you are doing the same thing, making predictions for each of the n-way classification problems.", "The general algorithm for non-parametric methods is basically the same as the algorithm for amortized (the black-box )approach. We first sample a bunch of tasks(step1), we sample a train and test dataset for each of those tasks(step2), then compute predictions using the learned similarity metric (step 3). Note here unlike the parametric methods, we don\u2019t have parameters phi, which are essentially integrated out into this comparison. Then once we have these predictions, we update the parameters of this learned embedding function with respect to the loss function of how accurate our predictions are on the test set (step4). This loss function would be something like cross-entropy, for example, and we\u2019 use the predicted distribution over test labels rather than only the output that the max labeled.", "For one-shot classification where we have one example per class, the matching network is pretty straightforward since it makes comparisons to each of those classes. But what if we have more than one shot? If we have more than one data point per class, then the matching network performs these comparisons independently. Maybe we could aggregate information per class in a way that is smarter than just performing these independent comparisons. That\u2019s what prototypical networks do.", "Prototypical networks aggregate class information to create a prototypical embedding of that class and then perform comparisons to each of those prototypical class embeddings in order to predict the label corresponding to our test image.", "So what this more concretely looks like is, we will have a number of images for different classes. On the left picture, different colors correspond to different image classes in our training dataset for a particular task. We embed each of our training images into this embedding space. And then take the average in this embedding space in order to compute the prototypical embedding for class 1,2, and 3. Then we embed our test image into the same space, same exact space, and compute the distance to each of those prototypical class embeddings. Finally, we can output the class for which it is closest to in this embedding space.", "The equation shows it will embed each of the images in for a particular class into this embedding space and then take the average for each of those images. To compute the class of our test data point, we will take the distance between the embedded test data point in each of those classes and perform a softmax over each of those negative distance in order to compute the probability for the test data point. In this case, d corresponds to Euclidean or cosine distance.", "Basically these non-parametric approaches embed your data points and then do nearest neighbours in that learned embedding space. One challenge is what if you want to reason about the more complex relationship between data points rather than just doing nearest neighbours in your embedding space? In principle, if you have an expressive enough encoder in your embedding space, then nearest neighbours should be able to represent a wide range of complex relationships, particularly for high-dimensional embedding spaces. But in practice, people have found it to be useful to think about more expressive ways to perform these types of comparisons.", "For example, relation networks basically take prototypical networks and learn the non-linear relation module on top of those embeddings. This basically just corresponds to learning that function D in prototypical networks, instead of using a Euclidean distance metric or a cosine distance metric. So it\u2019s learning both the embedding and the distance metric. Another approach is to instead of having a single prototype per class, have a mixture of prototypes per class. This allows you to, for example, represent more multimodal class distributions to maybe one class. Lastly, another paper performs an embedding on all of our data points and then do some sort of message passing scheme in order to think about how these different data points relate to each other and in order to make the predicted output. What this does is it uses graph neural networks in order to perform this message passing and differentiate through it.", "parametric approaches. First, we compare this in the computation graph perspective.", "The black-box approaches are repressing this computation graph at a completely black-box approach. The optimization-based approaches can be viewed as embedding an optimization into the computation graph. This view can also be taken for non-parametric approaches. For your test datapoint, prototypical networks embed it and compare it to each of your per class prototypes, where those per task prototypes are computed using the embedding of each of those classes\u2019 data points.", "With this view, we can also mix components of the computation graph to generate hybrid models. So one approach which is a bit of a hybrid of black-box and optimization-based approaches, or maybe an optimization-based non-parametric approach (it is dependent on the way you view things). This hybrid approach tries to condition a model on an embedding of the training dataset and also run gradient descent on that model. Although these sources of information by conditioning on the data in a direct way as well as through gradient descent could potentially be redundant, in practice it works well. Another way is embedding of your function and then do gradient descent on that embedding space. So this paper use relation network to embed your training dataset and think about how different data points relate to one another. And then they decode this embedding into the parameters of a neural network that makes predictions about new data points. Instead of running gradient descent on the parameters of that function, they run gradient descent in the learned embedding space Z, which produces different functions. You can essentially view it as running gradient descent on a lower-dimensional space of your weights rather than running gradient descent in the original space of your weights. The last approach is doing something exactly look like MAML, but initialize the last layer of the network to correspond to prototypical networks. It is basically a specific form of a particular choice of the network architecture for MAML that initializes it to do something like a comparison-based prediction.", "we can also compare those three approaches in the algorithm properties perspective. Consistency and expressive power are two properties that important for most applications. Expressive power is the ability to represent a range of learning procedures, it measures scalability and applicability to a range of domains. Consistency means learned learning procedure will solve tasks with enough data regardless of the properties of that task. For example, gradient descent approach corresponds to a consistent learning procedure because if we just run gradient descent at test time, you can expect given enough data for that test task, you will be able to solve a task regardless of what your meta-training data was. Consistency would reduce reliance on meta-training tasks, which leads to good out of distribution (OOD) performance.", "For supervised learning, very deep models usually are expressive (this does not hold for some RL algorithms). So the black-box approach has complete expressive power while the optimization-based approach is expressive for very deep models and the non-parametric approach is expressive for most architectures. In terms of consistency, black-box is not consistent at all, while the optimization-based approach is consistent since it reduces to gradient descent problems. Non-parametric methods are consistent in the sense that if your embedding is not losing information about the input, which is important for making decisions, then as you accumulate more and more data, asymptotically it will eventually get to a data point that\u2019s arbitrarily close to your test datapoint and then be able to make the correct prediction for that test data point.", "There are some other (dis)advantages to these methods. As the black-box approach is easy to combine with a variety of learning problems, such as supervised learning and RL; but it is often data-inefficient since you need to train a neural network from scratch, and it is challenging to optimize since there\u2019s no inductive bias at the initialization. The optimization-based approach has positive inductive bias at the start of meat-learning and it handles varying k and large k well. For example, if you have more data than what you trained on, these approaches still tend to work well because they are consistent. It is model-agnostic. But it includes second-order optimization, which leads to this approach compute and memory intensive. Non-parametric approaches are entirely feedforward, they don\u2019t evolve any backpropagation within that computation graph, so as a result, they tend to be very computationally fast and easy to optimize. But if you test them on more k than what they\u2019re trained on, they tend to underperform what other algorithms are able to achieve. So they are harder to generalize to varying K and hard to scale to very large K. Besides, so far, they are limited to classification.", "Generally, well-tuned versions of each perform comparably on existing few-shot benchmarks. Which method to use depends on your use-case.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MSc in Computer Science at UT."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbd391cd31700&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bd391cd31700--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rachel_95942?source=post_page-----bd391cd31700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d----bd391cd31700---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 4 \u2014 Non-Parametric Meta-Learners\u2019"}, {"url": "https://wordart.com/", "anchor_text": "ordArt"}, {"url": "https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 4 \u2014 Non-Parametric Meta-Learners\u2019"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning", "anchor_text": "matching network paper"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf", "anchor_text": "relation networks"}, {"url": "https://arxiv.org/pdf/1902.04552.pdf", "anchor_text": "Another approach"}, {"url": "https://arxiv.org/abs/1711.04043", "anchor_text": "another paper"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "https://openreview.net/forum?id=BJfOXnActQ", "anchor_text": "one approach"}, {"url": "https://arxiv.org/abs/1807.05960", "anchor_text": "Another way"}, {"url": "https://arxiv.org/abs/1903.03096", "anchor_text": "The last approach"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "the course slide"}, {"url": "https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 4 \u2014 Non-Parametric Meta-Learners"}, {"url": "http://cs330.stanford.edu/slides/cs330_lecture4.pdf", "anchor_text": "he course slide"}, {"url": "https://arxiv.org/abs/1801.03924", "anchor_text": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"}, {"url": "https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf", "anchor_text": "Siamese Neural Networks for One-shot Image Recognition"}, {"url": "https://arxiv.org/abs/1606.04080", "anchor_text": "Matching Networks for One-Shot Learning"}, {"url": "https://arxiv.org/abs/1703.05175", "anchor_text": "Prototypical Networks for Few-shot Learning"}, {"url": "https://arxiv.org/pdf/1902.04552", "anchor_text": "Infinite Mixture Prototypes for Few-Shot Learning \u2014 arXiv"}, {"url": "https://arxiv.org/abs/1711.04043", "anchor_text": "Few-Shot Learning with Graph Neural Networks"}, {"url": "https://arxiv.org/abs/1711.06025", "anchor_text": "Learning to Compare: Relation Network for Few-Shot Learning"}, {"url": "https://arxiv.org/pdf/1806.02817", "anchor_text": "Probabilistic Model-Agnostic Meta-Learning \u2014 arXiv"}, {"url": "https://arxiv.org/abs/1807.05960", "anchor_text": "Meta-Learning with Latent Embedding Optimization"}, {"url": "https://medium.com/tag/meta-learning?source=post_page-----bd391cd31700---------------meta_learning-----------------", "anchor_text": "Meta Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bd391cd31700---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bd391cd31700---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----bd391cd31700---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----bd391cd31700---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbd391cd31700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bd391cd31700---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bd391cd31700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bd391cd31700--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bd391cd31700--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bd391cd31700--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bd391cd31700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/@rachel_95942/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "77 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5d77130b77c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnon-parametric-meta-learning-bd391cd31700&newsletterV3=cbd32a3f303d&newsletterV3Id=5d77130b77c3&user=Qiurui+Chen&userId=cbd32a3f303d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}