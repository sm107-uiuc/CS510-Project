{"url": "https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72", "time": 1682995042.2188659, "path": "towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72/", "webpage": {"metadata": {"title": "Word2vec from Scratch with NumPy. How to implement a Word2vec model with\u2026 | by Ivan Chen | Towards Data Science", "h1": "Word2vec from Scratch with NumPy", "description": "Recently, I have been working with several projects related to NLP at work. Some of them had something to do with training the company\u2019s in-house word embedding. At work, the tasks were mostly done\u2026"}, "outgoing_paragraph_urls": [{"url": "https://radimrehurek.com/gensim/", "anchor_text": "gensim", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2vec", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Efficient Estimation of Word Representations in Vector Space", "paragraph_index": 4}, {"url": "https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb", "anchor_text": "this notebook", "paragraph_index": 38}], "all_paragraphs": ["Recently, I have been working with several projects related to NLP at work. Some of them had something to do with training the company\u2019s in-house word embedding. At work, the tasks were mostly done with the help of a Python library: gensim. However, I decided to implement a Word2vec model from scratch just with the help of Python and NumPy because reinventing the wheel is usually an awesome way to learn something deeply.", "Word embedding is nothing fancy but methods to represent words in a numerical way. More specifically, methods to map vocabularies to vectors.", "The most straightforward method could be using one-hot encoding to map each word to a one-hot vector.", "Although one-hot encoding is quite simple, there are several downsides. The most notable one is that it is not easy to measure relationships between words in a mathematical way.", "Word2vec is a neural network structure to generate word embedding by training the model on a supervised classification problem. Such a method was first introduced in the paper Efficient Estimation of Word Representations in Vector Space by Mikolov et al.,2013 and was proven to be quite successful in achieving word embedding that could used to measure syntactic and semantic similarities between words.", "In Mikolov et al.,2013, two model architectures were presented, Continuous Bag-of-Words model and Skip-gram model. I will be diving into the latter in this article.", "In order to explain the skip-gram model, I randomly quote a piece of text from a book that I am currently reading, The Little Book of Common Sense Investing by John Bogle:", "After the deduction of the costs of investing, beating the stock market is a loser\u2019s game.", "As I have mentioned above, it is a supervised classification problem that the word2vec model tries to optimize. More specifically, given a \u201ccontext word\u201d, we want to train a model such that the model can predict a \u201ctarget word\u201d, one of the words appeared within a predefined window size from the context word.", "Take the above sentence for example, given a context word \u201cinvesting\u201d and a window size of 5, we would like the model to generate one of the underlying words. (one of the words in [deduction, of, the costs, beating, stock, market, is] in the case.)", "The following shows the original diagram presented in the paper by Mikolov et al.,2013.", "I made another graph with a little bit more details", "The word embedding layer is essentially a matrix with a shape of (# of unique words in the corpus, word embedding size). Each row of the matrix represent a word in the corpus. Word embedding size is a hyper-parameter to be decided and can be thought as how many features that we would like to use to represent each word. The latter part of the model is simply a logistic regression in a neural network form.", "In the training process, the word embedding layer and the dense layer are being trained such that the model is able to predict target words given a context word at the end of the training process. After training such a model with a huge amount of data, the word embedding layer will end up becoming a representation of words which could demonstrate many kinds of cool relationships between words in a mathematical way. (Those who are interested in more details can refer to the original paper.)", "To generate training data, we tokenize text first. There are many techniques out there when it comes to tokenize text data, such as getting rid of words appearing in very high or very low frequency. I just split the text with a simple regex since the focus of the article is not tokenization.", "Next, we assign an integer to each word as its id. In addition, using word_to_id and id_to_word to record the mapping relationships.", "The follow is the code for generating training data:", "After generating training data, let\u2019s move on to the model. Similar to the majority of neural network models, the steps to train the word2vec model are initializing weights (parameters that we want to train), propagating forward, calculating the cost, propagating backward and updating the weights. The whole process will be repeated for several iterations based on how many epochs we want to train.", "There are two layers in the model needed to be initialized and trained, the word embedding layer and the dense layer.", "The shape of the word embedding will be (vocab_size, emb_size) . Why is that? If we\u2019d like to use a vector with emb_size elements to represent a vocabulary and the total number of vocabularies our corpus is vocab_size, then we can represent all the vocabularies with a vocab_size x emb_size matrix with each row representing a word.", "The shape of the dense layer will be (vocab_size, emb_size) . How come? The operation that would be performed in this layer is a matrix multiplication. The input of this layer will be (emb_size, # of training instances)and we\u2019d like the output to be (vocab_size, # of training instances)(For each word, we would like to know what the probability that the word appears with the given input word). Note: I do not include biases in the dense layer.", "The following is the code for initialization:", "The are three steps in the forward propagation, obtaining input word\u2019s vector representation from word embedding, passing the vector to the dense layer and then applying softmax function to the output of the dense layer.", "In some literatures, the input is presented as a one-hot vector (Let\u2019s say an one-hot vector with i-th element being 1). By multiplying the word embedding matrix and the one-hot vector, we can get the vector representing the input word. However, the result of performing matrix multiplication is essentially the same as selecting the ith row of the word embedding matrix. We can save lots of computational time by simply selecting the row associating with the input word.", "The rest of the process is just a multi-class linear regression model.", "The following graph could be used to recall the main operation of the dense layer.", "Afterwards, we apply softmax function to the output of the dense layer which gives us the probability of each word appearing near the given input word. The following equation could be used to remind what softmax function is.", "The following is code for forward propagation:", "Here, we would use cross entropy to calculate cost:", "The following is code for cost computation:", "During the back propagation process, we would like to calculate gradients of the trainable weights with respect to the loss function and update the weight with its associated gradient. Back propagation is the methods used to calculate those gradients. It is nothing fancy but chain rule in Calculus:", "It is the weights in the dense layer and the word embedding layer that we would like to train. Therefore we need to calculate gradients for those weights:", "The next step is to update the weights with the following formula:", "The following is code for backward propagation:", "Note: You might have been wondering why there is a factor of 1/m in dL_dW while not in dL_dword_vec . In each pass, we process m training examples together. For weights in the dense layer, we would like to update them with the average of the m gradient descents. For weights in the word vector, each vector has its own weights which lead to its own gradient descent so we do not need to aggregate the m gradient descents while we updating.", "To train the model, repeat the process of forward propagation, backward propagation and weight updating. During the training, the cost after each epoch should have decreasing trend.", "The following is the code for training the model:", "After train the model with data generated from the example sentence above with a window size of 3 for 5000 epochs (with a simple learning rate decay), we can see the model can output most neighboring words given each word as an input word.", "You can find the end-to-end process in this notebook. Feel free to download and play around.", "It is my first Medium post. Thank you guys for reading. Feel free to provide me with feedback or to ask me questions.", "It\u2019s fine to stop reading here but if you are interested in some optimization that I found needed when I tried to train with a huger dataset, please continue to read.", "When I tried to train the model above with a larger dataset, I found the memory consumption kept increasing during the training process and the python kernel finally shut down. Later on, I figured out the issue had to do with the way I fed the labels Y into the model.", "In the original code, each label is a one hot vector which used a bunch of zeros and a single one to represent the labeled output word. When the vocabulary size grows bigger, we waste so much memory to the zeros that do not provide us useful information.", "The memory consumption problem goes away after I start to feed the label with its associated word ind only. We have decreased the space from O(vocabulary size * m) to O(m).", "The following is my code implementation (There are only 2 places needed to be changed):", "In order optimize training time, the regular softmax above can be replaces with hierarchical softmax. However, this article has been a little too long here so we will save the topic for next time.", "Thank you for reading more. Again, please feel free to provide me with feedback or to ask me questions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer at a startup"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8786ddd49e72&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ujhuyz0110?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ujhuyz0110?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Ivan Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b48002a27a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&user=Ivan+Chen&userId=f2b48002a27a&source=post_page-f2b48002a27a----8786ddd49e72---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "gensim"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2vec"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Efficient Estimation of Word Representations in Vector Space"}, {"url": "https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb", "anchor_text": "this notebook"}, {"url": "https://github.com/ujhuyz0110/written_notes/blob/master/softmax_gradient.pdf", "anchor_text": "https://github.com/ujhuyz0110/written_notes/blob/master/softmax_gradient.pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8786ddd49e72---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8786ddd49e72---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----8786ddd49e72---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/numpy?source=post_page-----8786ddd49e72---------------numpy-----------------", "anchor_text": "Numpy"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----8786ddd49e72---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&user=Ivan+Chen&userId=f2b48002a27a&source=-----8786ddd49e72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&user=Ivan+Chen&userId=f2b48002a27a&source=-----8786ddd49e72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8786ddd49e72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8786ddd49e72---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8786ddd49e72--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8786ddd49e72--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8786ddd49e72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ujhuyz0110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ujhuyz0110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ivan Chen"}, {"url": "https://medium.com/@ujhuyz0110/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "175 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b48002a27a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&user=Ivan+Chen&userId=f2b48002a27a&source=post_page-f2b48002a27a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff2b48002a27a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-from-scratch-with-numpy-8786ddd49e72&user=Ivan+Chen&userId=f2b48002a27a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}