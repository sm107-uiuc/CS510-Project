{"url": "https://towardsdatascience.com/evolution-of-word-representations-in-nlp-d4483fe23e93", "time": 1683001939.311476, "path": "towardsdatascience.com/evolution-of-word-representations-in-nlp-d4483fe23e93/", "webpage": {"metadata": {"title": "Evolution of text representation in NLP | by Joao Schapke | Towards Data Science", "h1": "Evolution of text representation in NLP", "description": "Information can be represented in multiple ways while keeping the same meaning. We may pass information through multiple languages, we may represent somethings with mathematical expressions or by\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Information can be represented in multiple ways while keeping the same meaning. We may pass information through multiple languages, we may represent somethings with mathematical expressions or by drawings. We choose the representations which are more fit to convey the information we want to pass. In Natural Language Processing we have to convert text data into something the machine can manipulate. Numbers! There is a number (no pun intended) of ways to make this conversion. A simple one would be to give each word in a text a particular id. But, not all representations are equal, some are more sophisticated and carry more information than others, which will impact the performance of a NLP model.", "With today\u2019s computing power we have the capability of building ML models capable of performing very complex tasks and handling a lot of data. We want to shove our models the most information we can get.", "One of the simplest forms of representation is one-hot encodings. They convert a single word into a vector of N dimensions, filled with zeros and with a single (hot) position with a one.", "The vector is the size of the dictionary being used, each position represent a word, the position with a one represents the word mapped by the word vector. The concept is straightforward and was used in the earlier stages of NLP. It is cheap, requires very little computing power to convert text data into one hot encoding data, and it\u2019s easy to implement.", "But, it does not carry any extra information about the data and it\u2019s a extremely bloated representation. The representation is just a large scramble of ones and zeros identifying words, a NLP model needs to learn from zero what each of the vectors mean.", "Bag of words is a simplifying representation which represents text as a multiset (bag). Text gets converted into a vector, dictionary sized, with each index representing a word and the value at the index is the number of times the word appeared in the text. The intuition behind the representation is that words themselves carry the information needed for understanding.", "But, this is not the case for every sentence. The order of the words in a sentence may alter its meaning. Bag of words disregards ordering of sentences, showcasing one of the reasons it\u2019s not as powerful as word embeddings. It also does not give any information of what each word means, the models are given little knowledge a priori.", "Word embedding is a mapping of words into vectors.", "A word gets converted into a vector of N dimensions, where each of these dimensions will have an underlying meaning. How the word relates to these underlying meanings will define the values for the dimension.", "A word embedding of 2 dimensions may map words as follows:", "In the example we notice that the first dimension has a high value for dog and man, while the second dimension for apple and banana. Which may mean that the first dimension is related to animals, and the second dimension to fruits. Note also that kiwi gets a midway value in both dimensions.", "Two dimensions are too little to represent the meaning of words, unless we want an embedding that serves at differing fruits from animals. Embeddings usually have 50, 300, 600 or 900 dimensions.", "Embeddings by themselves are pretty amazing and can do some cute tricks.", "We can imagine that in an embedding the difference between in the representation of the word \u2018man\u2019 and \u2018woman\u2019 is a value which accounts for the difference in sex. The same would be be true for \u2018king\u2019 and \u2018queen\u2019. Therefore we have that:", "Therefore if we have the vector for \u2018man\u2019, \u2018woman\u2019, and \u2018boy\u2019, we can expect that <woman> + <boy> \u2014 <man> will result in <girl>. Which means that the word embedding\u2019s dimensions learn the meaning of words, and is able to tell that \u2018woman\u2019 and \u2018man\u2019 represent similar entities.", "How to train a word embedding.", "Word embeddings are typically produced by training a model in a unsupervised task with a large corpus. Such as trying to predict missing words in sentences.", "Training an embedding is expensive computationally. For most problems downloading available trained embeddings is the best choice. If your corpus is too specific (ex. has a lot of jargon) you might want to train your own embedding.", "As we saw in our example with the word kiwi, embeddings don\u2019t know how to handle words that have multiple meanings. Embeddings will average the meanings into a single vector, with the meaning which appears more frequent on the dataset getting a higher weight.", "Word embedding also fell short to words it had never seen before, such as neologisms, misspellings and out of vocabulary words.", "To remediate some of the problems with word embedding most of the popular models of today tokenize words by breaking into \u2018features\u2019, and train an embedding using these features instead of the word itself.", "The tokenizer might split the word \u2018google\u2019 into the pieces \u2018goog\u2019 and \u2018le\u2019 and the embedding will learn those features. And it might do something similar with verbs in the past participle, ex: \u2018watched\u2019 \u2192 \u2018watch\u2019, \u2018ed\u2019. Now, suppose that the word \u2018googled\u2019 wasn\u2019t in the embedding\u2019s training set. The tokenizer might split the word into the features: \u2018goog\u2019, \u2018le\u2019, \u2018ed\u2019, all of which the embedding has seen before and has a proper feature vector for each. So the embedding even not having previously seen such word it might still give a good representation to it.", "This intuition can be applied to misspellings, neologisms and out of word vocabulary and it greatly improve the performance of word embedding.", "Sophisticated word representation are a cheap way to give NLP models more knowledge a priori, and is incredibly useful for tasks which data is scarce and hard to collect. For this reason we see a progression of word representations with increasing complexity.", "Word embedding are extremely powerful and have become ubiquitous in NLP. We explored some of the intuitions behind word embedding and more primitive word representations and we saw why some might be more advantageous than others.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4483fe23e93&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@joaoschapke?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joaoschapke?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Joao Schapke"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc8300ba82a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&user=Joao+Schapke&userId=dc8300ba82a4&source=post_page-dc8300ba82a4----d4483fe23e93---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@opticonor?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Conor Luddy"}, {"url": "https://unsplash.com/s/photos/language?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e", "anchor_text": "Image source"}, {"url": "https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e", "anchor_text": "Image Source"}, {"url": "https://tensorflow.org", "anchor_text": "https://www.tensorflow.org"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d4483fe23e93---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d4483fe23e93---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d4483fe23e93---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----d4483fe23e93---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&user=Joao+Schapke&userId=dc8300ba82a4&source=-----d4483fe23e93---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&user=Joao+Schapke&userId=dc8300ba82a4&source=-----d4483fe23e93---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd4483fe23e93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d4483fe23e93---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4483fe23e93--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d4483fe23e93--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d4483fe23e93--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joaoschapke?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joaoschapke?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Joao Schapke"}, {"url": "https://medium.com/@joaoschapke/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "78 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc8300ba82a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&user=Joao+Schapke&userId=dc8300ba82a4&source=post_page-dc8300ba82a4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F712b93dccd6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-word-representations-in-nlp-d4483fe23e93&newsletterV3=dc8300ba82a4&newsletterV3Id=712b93dccd6a&user=Joao+Schapke&userId=dc8300ba82a4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}