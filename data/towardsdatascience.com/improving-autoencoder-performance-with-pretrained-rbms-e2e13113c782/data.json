{"url": "https://towardsdatascience.com/improving-autoencoder-performance-with-pretrained-rbms-e2e13113c782", "time": 1683017891.544457, "path": "towardsdatascience.com/improving-autoencoder-performance-with-pretrained-rbms-e2e13113c782/", "webpage": {"metadata": {"title": "Improving Autoencoder Performance with Pretrained RBMs | by Eugene Tang | Towards Data Science", "h1": "Improving Autoencoder Performance with Pretrained RBMs", "description": "Autoencoders are unsupervised neural networks used for representation learning. They create a low-dimensional representation of the original input data. The learned low-dimensional representation is\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "1", "paragraph_index": 1}, {"url": "https://github.com/eugenet12/pytorch-rbm-autoencoder", "anchor_text": "Github repo", "paragraph_index": 5}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "2", "paragraph_index": 6}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "this great paper", "paragraph_index": 10}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "3", "paragraph_index": 10}, {"url": "https://github.com/eugenet12/pytorch-rbm-autoencoder", "anchor_text": "Github repo", "paragraph_index": 16}, {"url": "http://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "Reducing the Dimensionality of Data with Neural Networks", "paragraph_index": 17}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "Training Restricted Boltzmann Machines: An Introduction", "paragraph_index": 18}], "all_paragraphs": ["Autoencoders are unsupervised neural networks used for representation learning. They create a low-dimensional representation of the original input data. The learned low-dimensional representation is then used as input to downstream models. While autoencoders are effective, training autoencoders is hard. They often get stuck in local minima and produce representations that are not very useful.", "This post will go over a method introduced by Hinton and Salakhutdinov [1] that can dramatically improve autoencoder performance by initializing autoencoders with pretrained Restricted Boltzmann Machines (RBMs). While this technique has been around, it\u2019s an often overlooked method for improving model performance.", "Autoencoders are a combination of two networks: an encoder and a decoder. Raw input is given to the encoder network, which transforms the data to a low-dimensional representation. The low-dimensional representation is then given to the decoder network, which tries to reconstruct the original input. They are trained by trying to make the reconstructed input from the decoder as close to the original input as possible. Deep autoencoders are autoencoders with many layers, like the one in the image above.", "RBMs are generative neural networks that learn a probability distribution over its input. Structurally, they can be seen as a two-layer network with one input (\u201cvisible\u201d) layer and one hidden layer. The first layer, the \u201cvisible\u201d layer, contains the original input while the second layer, the \u201chidden\u201d layer, contains a representation of the original input. Similar to autoencoders, RBMs try to make the reconstructed input from the \u201chidden\u201d layer as close to the original input as possible. Unlike autoencoders, RBMs use the same matrix for \u201cencoding\u201d and \u201cdecoding.\u201d Trained RBMs can be used as layers in neural networks. This property allows us to stack RBMs to create an autoencoder.", "The difficulty of training deep autoencoders is that they will often get stuck if they start off in a bad initial state. To address this, Hinton and Salakhutdinov found that they could use pretrained RBMs to create a good initialization state for the deep autoencoders. Let\u2019s say that you wanted to create a 625\u20132000\u20131000\u2013500\u201330 autoencoder. You would first train a 625\u20131000 RBM, then use the output of the 625\u20132000 RBM to train a 2000\u20131000 RBM, and so on. After you\u2019ve trained the 4 RBMs, you would then duplicate and stack them to create the encoder and decoder layers of the autoencoder as seen in the diagram below. The researchers found that they could fine-tune the resulting autoencoder to perform much better than if they had directly trained an autoencoder with no pretrained RBMs.", "Now that we understand how the technique works, let\u2019s make our own autoencoder! I didn\u2019t find any great pytorch tutorials implementing this technique, so I created an open-source version of the code in this Github repo. The Github repo also has GPU compatible code which is excluded in the snippets here. The code portion of this tutorial assumes some familiarity with pytorch.", "We\u2019ll run the autoencoder on the MNIST dataset, a dataset of handwritten digits [2]. First, we load the data from pytorch and flatten the data into a single 784-dimensional vector.", "We\u2019ll start with the hardest part, training our RBM models. Hinton and Salakhutdinov employ some tricks that most RBM implementations don\u2019t contain. I\u2019ll point out these tricks as they come.", "In the constructor, we set up the initial parameters as well as some extra matrices for momentum during training. Note that this class does not extend pytorch\u2019s nn.Module because we will be implementing our own weight update function.", "Next, we add methods to convert the visible input to the hidden representation and the hidden representation back to reconstructed visible input. Both methods return the activation probabilities, while the sample_h method also returns the observed hidden state as well. Of note, we have the option to allow the hidden representation to be modeled by a Gaussian distribution rather than a Bernoulli distribution because the researchers found that allowing the hidden state of the last layer to be continuous allows it to take advantage of more nuanced differences in the data.", "Finally, we add a method for updating the weights. This method uses contrastive divergence to update the weights rather than typical traditional backward propagation. RBMs are usually implemented this way, and we will keep with tradition here. For more details on the theory behind training RBMs, see this great paper [3].", "Now that we have the RBM class setup, let\u2019s train. For training, we take the input and send it through the RBM to get the reconstructed input. We then use contrastive divergence to update the weights based on how different the original input and reconstructed input are from each other, as mentioned above. After training, we use the RBM model to create new inputs for the next RBM model in the chain.", "Next, let\u2019s take our pretrained RBMs and create an autoencoder. The following class takes a list of pretrained RBMs and uses them to initialize a deep autoencoder. The autoencoder is a feed-forward network with linear transformations and sigmoid activations. Of note, we don\u2019t use the sigmoid activation in the last encoding layer (250\u20132) because the RBM initializing this layer has a Gaussian hidden state. We separate the encode and decode portions of the network into their own functions for conceptual clarity.", "We then pass the RBM models we trained to the deep autoencoder for initialization and use a typical pytorch training loop to fine-tune the autoencoder. We use the mean-squared error (MSE) loss to measure reconstruction loss and the Adam optimizer to update the parameters.", "After the fine-tuning, our autoencoder model is able to create a very close reproduction with an MSE loss of just 0.0303 after reducing the data to just two dimensions. The autoencoder seems to learned a smoothed-out version of each digit, which is much better than the blurred reconstructed images we saw at the beginning of this article.", "As a final test, let\u2019s run the MNIST test dataset through our autoencoder\u2019s encoder and plot the 2d representation. For comparison, we also show the 2d representation from running the commonly-used Principal Component Analysis (PCA). Our deep autoencoder is able to separate the digits much more cleanly than PCA. Awesome!", "Hope you enjoyed learning about this neat technique and seeing examples of code that show how to implement it. You can checkout this Github repo for the full code and a demo notebook. If you use what you read here to improve your own autoencoders, let me know how it goes!", "[1] G. Hinton and R. Salakhutidnov, Reducing the Dimensionality of Data with Neural Networks (2006), Science", "[3] A. Fischer and C. Igel, Training Restricted Boltzmann Machines: An Introduction (2014), Pattern Recognition", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist and Software Engineer. Interested in seeing how technology and data science can help improve the world."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe2e13113c782&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://eugene-c-tang.medium.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": ""}, {"url": "https://eugene-c-tang.medium.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Eugene Tang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8a6d720851&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&user=Eugene+Tang&userId=ec8a6d720851&source=post_page-ec8a6d720851----e2e13113c782---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://unsplash.com/@joshuaearle?utm_source=medium&utm_medium=referral", "anchor_text": "Joshua Earle"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "1"}, {"url": "https://commons.wikimedia.org/w/index.php?title=User:Qwertyus&action=edit&redlink=1", "anchor_text": "Qwertyus"}, {"url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine#/media/File:Restricted_Boltzmann_machine.svg", "anchor_text": "CC BY-SA 3.0"}, {"url": "http://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "1"}, {"url": "https://github.com/eugenet12/pytorch-rbm-autoencoder", "anchor_text": "Github repo"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "2"}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "this great paper"}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "3"}, {"url": "https://github.com/eugenet12/pytorch-rbm-autoencoder", "anchor_text": "Github repo"}, {"url": "http://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "Reducing the Dimensionality of Data with Neural Networks"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "The MNIST Database"}, {"url": "https://christian-igel.github.io/paper/TRBMAI.pdf", "anchor_text": "Training Restricted Boltzmann Machines: An Introduction"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----e2e13113c782---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/rbm?source=post_page-----e2e13113c782---------------rbm-----------------", "anchor_text": "Rbm"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e2e13113c782---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----e2e13113c782---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----e2e13113c782---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&user=Eugene+Tang&userId=ec8a6d720851&source=-----e2e13113c782---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&user=Eugene+Tang&userId=ec8a6d720851&source=-----e2e13113c782---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe2e13113c782&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e2e13113c782---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e2e13113c782--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2e13113c782--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2e13113c782--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2e13113c782--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2e13113c782--------------------------------", "anchor_text": ""}, {"url": "https://eugene-c-tang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://eugene-c-tang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eugene Tang"}, {"url": "https://eugene-c-tang.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8a6d720851&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&user=Eugene+Tang&userId=ec8a6d720851&source=post_page-ec8a6d720851--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fec8a6d720851%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-autoencoder-performance-with-pretrained-rbms-e2e13113c782&user=Eugene+Tang&userId=ec8a6d720851&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}