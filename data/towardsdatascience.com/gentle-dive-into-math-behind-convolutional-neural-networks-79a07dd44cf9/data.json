{"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9", "time": 1682995755.266358, "path": "towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9/", "webpage": {"metadata": {"title": "Gentle Dive into Math Behind Convolutional Neural Networks | by Piotr Skalski | Towards Data Science", "h1": "Gentle Dive into Math Behind Convolutional Neural Networks", "description": "Autonomous driving, healthcare or retail are just some of the areas where Computer Vision has allowed us to achieve things that, until recently, were considered impossible. Today the dream of a self\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "this series", "paragraph_index": 1}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "previous article", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "other articles", "paragraph_index": 21}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter", "paragraph_index": 21}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium", "paragraph_index": 21}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub", "paragraph_index": 21}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle", "paragraph_index": 21}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai", "paragraph_index": 23}], "all_paragraphs": ["Autonomous driving, healthcare or retail are just some of the areas where Computer Vision has allowed us to achieve things that, until recently, were considered impossible. Today the dream of a self driving car or automated grocery store does not sound so futuristic anymore. In fact, we are using Computer Vision every day \u2014 when we unlock the phone with our face or automatically retouch photos before posting them on social media. Convolutional Neural Networks are possibly the most crucial building blocks behind this huge successes. This time we are going to broaden our understanding of how neural networks work with ideas specific to CNNs. Be advise, the article will include quite complex math equations, but don\u2019t be discouraged if you are not comfortable with linear algebra and differential calculus. My goal is not to make you remember those formulas, but to provide you with the intuition of what is happening underneath.", "Side notes: For the first time I decided to enrich my artwork with an audio version and I kindly invite you to listen to it. You will find a link to Soundcloud above. In this article I focus mainly on things typical to CNNs. If you are looking for more general information about deep neural networks I encourage you to read my other posts from this series. As usual, full source code with visualizations and comments can be found on my GitHub. Let\u2019s start!", "In the past we got to know the so-called densely connected neural networks. These are networks whose neurons are divided into groups forming successive layers. Each such unit is connected to every single neuron from the neighboring layers. An example of such an architecture is shown in the figure below.", "This approach works well when we solve classification problem based on a limited set of defined features \u2014 for example, we predict a football player\u2019s position based on the statistics he logs during games. However, the situation becomes more complicated when working with photos. Of course, we could treat the brightness of each pixel as a separate feature and pass it on as an input to our dense network. Unfortunately, in order to make it work for a typical smartphone photo, our network would have to contain tens or even hundreds of millions of neurons. On the other hand, we could scale our photo down, but we would lose valuable information in the process. Right away we see that a traditional strategy does nothing for us\u2014 we need a new clever way to use as much data as possible, but at the same time reduce the number of necessary calculations and parameters. That\u2019s when CNNs comes into play.", "Let\u2019s start by taking a minute to explain how digital images are stored. Most of you probably realize that they are actually huge matrices of numbers. Each such number corresponds to the brightness of a single pixel. In the RGB model, the colour image is actually composed of three such matrices corresponding to three colour channels \u2014 red, green and blue. In black-and-white images we only need one matrix. Each of these matrices stores values from 0 to 255. This range is a compromise between the efficacy of storing information about the image (256 values fit perfectly in 1 byte) and the sensitivity of the human eye (we distinguish a limited number of shades of the same colour).", "Kernel convolution is not only used in CNNs, but is also a key element of many other Computer Vision algorithms. It is a process where we take a small matrix of numbers (called kernel or filter), we pass it over our image and transform it based on the values from filter. Subsequent feature map values are calculated according to the following formula, where the input image is denoted by f and our kernel by h. The indexes of rows and columns of the result matrix are marked with m and n respectively.", "After placing our filter over a selected pixel, we take each value from kernel and multiply them in pairs with corresponding values from the image. Finally we sum up everything and put the result in the right place in the output feature map. Above we can see how such an operation looks like in micro scale, but what is even more interesting, is what we can achieve by performing it on a full image. Figure 4 shows the results of the convolution with several different filters.", "As we have seen in Figure 3, when we perform convolution over the 6x6 image with a 3x3 kernel, we get a 4x4 feature map. This is because there are only 16 unique positions where we can place our filter inside this picture. Since our image shrinks every time we perform convolution, we can do it only a limited number of times, before our image disappears completely. What\u2019s more, if we look at how our kernel moves through the image we see that the impact of the pixels located on the outskirts is much smaller than those in the center of image. This way we lose some of the information contained in the picture. Below you can see how the position of the pixel changes its influence on the feature map.", "To solve both of these problems we can pad our image with an additional border. For example, if we use 1px padding, we increase the size of our photo to 8x8, so that output of the convolution with the 3x3 filter will be 6x6. Usually in practice we fill in additional padding with zeroes. Depending on whether we use padding or not, we are dealing with two types of convolution \u2014 Valid and Same. Naming is quite unfortunate, so for the sake of clarity: Valid \u2014 means that we use the original image, Same \u2014 we use the border around it, so that the images at the input and output are the same size. In the second case, the padding width, should meet the following equation, where p is padding and f is the filter dimension (usually odd).", "In previous examples, we always shifted our kernel by one pixel. However, step length can also be treated as one of convolution layer hyperparameters. In Figure 6, we can see how the convolution looks like if we use larger stride. When designing our CNN architecture, we can decide to increase the step if we want the receptive fields to overlap less or if we want smaller spatial dimensions of our feature map. The dimensions of the output matrix - taking into account padding and stride - can be calculated using the following formula.", "Convolution over volume is a very important concept, which will allow us not only to work with color images, but even more importantly to apply multiple filters within a single layer. The first important rule is that the filter and the image you want to apply it to, must have the same number of channels. Basically, we proceed very much like in the example from Figure 3, nevertheless this time we multiply the pairs of values from the three-dimensional space. If we want use multiple filters on the same image, we carry out the convolution for each of them separately, stack the results one on top of the other and combine them into a whole. The dimensions of the received tensor (as our 3D matrix can be called) meet the following equation, in which: n \u2014 image size, f \u2014 filter size, nc \u2014 number of channels in the image, p \u2014used padding, s \u2014 used stride, nf \u2014 number of filters.", "The time has finally come to use everything we have learned today and to build a single layer of our CNN. Our methodology is almost identical to the one we used for densely connected neural networks, the only difference is that instead of using a simple matrix multiplication, this time we will use the convolution. Forward propagation consists of two steps. The first one is to calculate the intermediate value Z, which is obtained as a result of the convolution of the input data from the previous layer with W tensor (containing filters), and then adding bias b. The second is the application of a non-linear activation function to our intermediate value (our activation is denoted by g). Fans of matrix equations will find appropriate mathematical formulas below. If any of the operations in question is not clear to you, I highly recommend my previous article, in which I discuss in detail what is happening inside densely connected neural networks. By the way, on illustration below you can see a small visualization, describing the dimensions of tensors used in equation.", "At the beginning of the article I mentioned that densely connected neural networks are poor at working with images, due to the huge number of parameters that would need to be learned. Now that we understand what convolution is all about, let\u2019s consider how it allows us to optimize the calculations. On the Figure below, the 2D convolution has been visualized in a slightly different way \u2014 neurons marked with numbers 1\u20139 form the input layer that receives brightness of subsequent pixels, while units A-D denotes calculated feature map elements. Last but not least, I-IV are the subsequent values from kernel \u2014 these must be learned.", "Now, let\u2019s focus on the two very important attributes of convolution layers. First of all, you can see that not all neurons in the two consecutive layers are connected to each other. For example, unit 1 only affects the value of A. Secondly, we see that some neurons share the same weights. Both of these properties mean that we have much less parameters to learn. By the way, it is worth noting that a single value from the filter affects every element of the feature map \u2014 it will be crucial in the context of backpropagation.", "Anyone who has ever tried to code their own neural network from scratch knows, that forward propagation is less than half the success. The real fun starts when you want to go back. Nowadays, we don\u2019t need to bother with backpropagation \u2014 deep learning frameworks do it for us, but I feel it\u2019s worth knowing what\u2019s going on under the hood. Just like in densely connected neural networks, our goal is to calculate derivatives and later use them to update the values of our parameters in a process called gradient descent.", "In our calculations we will use a chain rule \u2014 which I mentioned in previous articles. We want to assess the influence of the change in the parameters on the resulting features map, and subsequently on the final result. Before we start to go into the details, let us agree on the mathematical notation that we will use \u2014 in order to make my life easier, I will abandon the full notation of the partial derivative in favour of the shortened one visible below. But remember, that when I use this notation, I will always mean the partial derivative of the cost function.", "Our task is to calculate dW[l] and db[l] - which are derivatives associated with parameters of current layer, as well as the value of dA[ l -1] - which will be passed to the previous layer. As shown in Figure 10, we receive the dA[l] as the input. Of course, the dimensions of tensors dW and W, db and b as well as dA and A respectively are the same. The first step is to obtain the intermediate value dZ[l] by applying a derivative of our activation function to our input tensor. According to the chain rule, the result of this operation will be used later.", "Now, we need to deal with backward propagation of the convolution itself, and in order to achieve this goal we will utilise a matrix operation called full convolution \u2014 which is visualised below. Note that during this process we use the kernel, which we previously rotated by 180 degrees. This operation can be described by the following formula, where the filter is denoted by W, and dZ[m,n] is a scalar that belongs to a partial derivative obtained from the previous layer.", "Besides convolution layers, CNNs very often use so-called pooling layers. They are used primarily to reduce the size of the tensor and speed up calculations. This layers are simple - we need to divide our image into different regions, and then perform some operation for each of those parts. For example, for the Max Pool Layer, we select a maximum value from each region and put it in the corresponding place in the output. As in the case of the convolution layer, we have two hyperparameters available \u2014 filter size and stride. Last but not least, if you are performing pooling for a multi-channel image, the pooling for each channel should be done separately.", "In this article we will discuss only max pooling backpropagation, but the rules that we will learn \u2014 with minor adjustments \u2014 are applicable to all types of pooling layers. Since in layers of this type, we don\u2019t have any parameters that we would have to update, our task is only to distribute gradiwents appropriately. As we remember, in the forward propagation for max pooling, we select the maximum value from each region and transfer them to the next layer. It is therefore clear that during back propagation, the gradient should not affect elements of the matrix that were not included in the forward pass. In practice, this is achieved by creating a mask that remembers the position of the values used in the first phase, which we can later utilize to transfer the gradients.", "Congratulations if you managed to get here. Big thanks for the time spent reading this article. If you liked the post, consider sharing it with your friend, or two friends or five friends. If you have noticed any mistakes in the way of thinking, formulas, animations or code, please let me know.", "This article is another part of the \u201cMysteries of Neural Networks\u201d series, if you haven\u2019t had the opportunity yet, read the other articles. Also, if you like my job so far, follow me on Twitter and Medium and see other projects I\u2019m working on, on GitHub and Kaggle. Stay curious!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Growth Engineer @ Roboflow / Founder @ makesense.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F79a07dd44cf9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://skalskip.medium.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Piotr Skalski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----79a07dd44cf9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "this series"}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub"}, {"url": "https://www.maxpixel.net/Idstein-Historic-Center-Truss-Facade-Germany-3748512", "anchor_text": "Original Image"}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "anchor_text": "other articles"}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter"}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium"}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle"}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=---------5------------------", "anchor_text": ""}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----79a07dd44cf9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----79a07dd44cf9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----79a07dd44cf9---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----79a07dd44cf9---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----79a07dd44cf9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=-----79a07dd44cf9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=-----79a07dd44cf9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----79a07dd44cf9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----79a07dd44cf9--------------------------------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Piotr Skalski"}, {"url": "https://skalskip.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.7K Followers"}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd74b5c17627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&newsletterV3=11b65705ec0&newsletterV3Id=fd74b5c17627&user=Piotr+Skalski&userId=11b65705ec0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}