{"url": "https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b", "time": 1682995600.1650949, "path": "towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b/", "webpage": {"metadata": {"title": "Interpretable Machine Learning. Extracting human understandable\u2026 | by Parul Pandey | Towards Data Science", "h1": "Interpretable Machine Learning", "description": "In his book \u2018Interpretable Machine Learning\u2019, Christoph Molnar beautifully encapsulates the essence of ML interpretability through this example: Imagine you are a Data Scientist and in your free time\u2026"}, "outgoing_paragraph_urls": [{"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "Interpretable Machine Learning", "paragraph_index": 1}, {"url": "http://arxiv.org/abs/1702.08608", "anchor_text": "Finale Doshi-Velez", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1702.08608", "anchor_text": "Doshi-Velez and Kim 2017", "paragraph_index": 5}, {"url": "https://www.kaggle.com/dansbecker/use-cases-for-model-insights", "anchor_text": "benefits that interpretability", "paragraph_index": 7}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5", "paragraph_index": 16}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "J. H. Friedman 2001", "paragraph_index": 20}, {"url": "https://pdpbox.readthedocs.io/en/latest/", "anchor_text": "PDPbox.", "paragraph_index": 23}, {"url": "https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80", "anchor_text": "\u00b9", "paragraph_index": 25}, {"url": "https://github.com/slundberg/shap", "anchor_text": "Shap", "paragraph_index": 27}, {"url": "http://Christoph Molnar", "anchor_text": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.Christoph Molnar", "paragraph_index": 38}], "all_paragraphs": ["It\u2019s time to get rid of the black boxes and cultivate trust in Machine Learning", "In his book \u2018Interpretable Machine Learning\u2019, Christoph Molnar beautifully encapsulates the essence of ML interpretability through this example: Imagine you are a Data Scientist and in your free time you try to predict where your friends will go on vacation in the summer based on their Facebook and Twitter data you have. Now, if the predictions turn out to be accurate, your friends might be impressed and could consider you to be a magician who could see the future. If the predictions are wrong, it would still bring no harm to anyone except to your reputation of being a \u201cData Scientist\u201d. Now let\u2019s say it wasn\u2019t a fun project and there were investments involved. Say, you wanted to invest in properties where your friends were likely to holiday. What would happen if the model\u2019s predictions went awry? You would lose money. As long as the model is having no significant impact, its interpretability doesn\u2019t matter so much but when there are implications involved based on a model\u2019s prediction, be it financial or social, interpretability becomes relevant.", "Interpret means to explain or to present in understandable terms. In the context of ML systems, interpretability is the ability to explain or to present in understandable terms to a human[Finale Doshi-Velez]", "Machine Learning models have been branded as \u2018Black Boxes\u2019 by many. This means that though we can get accurate predictions from them, we cannot clearly explain or identify the logic behind these predictions. But how do we go about extracting important insights from the models? What things are to be kept in mind and what features or tools will we need to achieve that? These are the important questions that come to mind when the issue of Model interpretability is raised.", "The question that some people often ask is why aren\u2019t we just content with the results of the model and why are we so hell-bent on knowing why a particular decision was made? A lot of this has to do with the impact that a model might have in the real world. For models which are merely meant to recommend movies will have a far less impact than the ones created to predict the outcome of a drug.", "\u201cThe problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.\u201d (Doshi-Velez and Kim 2017)", "Here is a big picture of explainable machine learning. In a way, we capture the world by collecting raw data and use that data to make further predictions. Essentially, Interpretability is just another layer on the model that helps humans to understand the process.", "Some of the benefits that interpretability brings along are:", "Theory only makes sense as long as we can put it into practice. In case you want a real hang of this topic, you can try the Machine Learning Explainability crash course from Kaggle. It has the right amount of theory and code to put the concepts into perspective and helps to apply model explainability concepts to practical, real-world problems.", "Click on the screenshot below to go directly to the course page. In case you want a brief overview of the contents first, you can continue to read further.", "To interpret a model, we require the following insights :", "Let\u2019s discuss a few techniques that help in extracting the above insights from a model:", "What features does a model think are important? Which features might have a greater impact on the model predictions than the others? This concept is called feature importance and Permutation Importance is a technique used widely for calculating feature importance. It helps us to see when our model produces counterintuitive results, and it helps to show the others when our model is working as we\u2019d hope.", "Permutation Importance works for many scikit-learn estimators. The idea is simple: Randomly permutate or shuffle a single column in the validation dataset leaving all the other columns intact. A feature is considered \u201cimportant\u201d if the model\u2019s accuracy drops a lot and causes an increase in error. On the other hand, a feature is considered \u2018unimportant\u2019 if shuffling its values doesn\u2019t affect the model\u2019s accuracy.", "Consider a model that predicts whether a soccer team will have a \u201cMan of the Game\u201d winner or not based on certain parameters. The player who demonstrates the best play is awarded this title.", "Permutation importance is calculated after a model has been fitted. So, let\u2019s train and fit a RandomForestClassifier model denoted as my_model on the training data.", "Permutation Importance is calculated using the ELI5 library. ELI5 is a Python library which allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models.", "Calculating and Displaying importance using the eli5 library:", "(Here val_X,val_y denote the validation sets respectively)", "And now, for the complete example and to test your understanding, go to the Kaggle page by clicking the link below:", "The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model( J. H. Friedman 2001). PDPs show how a feature affects predictions. PDP can show the relationship between the target and the selected features via 1D or 2D plots.", "PDPs are also calculated after a model has been fit. In the soccer problem that we discussed above, there were a lot of features like passes made, shots taken, goals scored, etc. We start by considering a single row. Say the row represents a team that had the ball 50% of the time, made 100 passes, took 10 shots, and scored 1 goal.", "We proceed by fitting our model and calculating the probability of a team having a player that won the \u201cMan of the Game\u201d which is our target variable. Next, we would choose a variable and continuously alter its value. For instance, we will calculate the outcome if the team scored 1 goal, 2 goals, 3 goals, and so on. All these values are then plotted and we get a graph of predicted Outcomes vs Goals Scored.", "The library to be used for plotting PDPs is called python partial dependence plot toolbox or simply PDPbox.", "We can also visualize the partial dependence of two features at once using 2D Partial plots.", "SHAP which stands for Shapley Additive exPlanation helps to break down a prediction to show the impact of each feature. It is based on Shapley values, a technique used in game theory to determine how much each player in a collaborative game has contributed to its success\u00b9. Normally, getting the trade-off between accuracy and interpretability just right can be a difficult balancing act but SHAP values can deliver both.", "Again, going with the soccer example where we wanted to predict the probability of a team having a player that won the \u201cMan of the Game\u201d. SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we\u2019d make if that feature took some baseline value.", "SHAP values are calculated using the Shap library which can be installed easily from PyPI or conda.", "Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature). Let\u2019s say we wanted to know what was the prediction when the team scored 3 goals instead of some fixed baseline no. If we are able to answer this, we could perform the same steps for other features as follows:", "Hence the prediction can be decomposed into a graph like this:", "The above explanation shows features each contributing to pushing the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue", "SHAP values have a deeper theory than what I have explained here. make sure to through the link below to get a complete understanding.", "Aggregating many SHAP values can provide even more detailed insights into the model.", "To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The summary plot tells which features are most important, and also their range of effects over the dataset.", "The point in the upper left was for a team that scored few goals, reducing the prediction by 0.25.", "While a SHAP summary plot gives a general overview of each feature, a SHAP dependence plot shows how the model output varies by a feature value. SHAP dependence contribution plots provide a similar insight to PDPs, but they add a lot more detail.", "The above Dependence Contribution plots suggest that having the ball increases a team\u2019s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.", "Machine Learning doesn\u2019t have to be a black box anymore. What use is a good model if we cannot explain the results to others? Interpretability is as important as creating a model. To achieve wider acceptance among the population, it is crucial that Machine learning systems are able to provide satisfactory explanations for their decisions. As Albert Einstein said,\u201d If you can\u2019t explain it simply, you don\u2019t understand it well enough\u201d.", "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.Christoph Molnar", "Machine Learning Explainability Micro Course: Kaggle", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Data Scientist @H2O.ai | Author of Machine Learning for High-Risk Applications"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1dec0f2f3e6b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pandeyparul.medium.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28----1dec0f2f3e6b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@pixabay?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pixabay"}, {"url": "https://www.pexels.com/photo/arrows-box-business-chalk-533189/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "Interpretable Machine Learning"}, {"url": "http://arxiv.org/abs/1702.08608", "anchor_text": "Finale Doshi-Velez"}, {"url": "https://christophm.github.io/interpretable-ml-book/terminology.html", "anchor_text": "Source: interpretable-ml-book"}, {"url": "https://arxiv.org/abs/1702.08608", "anchor_text": "Doshi-Velez and Kim 2017"}, {"url": "https://christophm.github.io/interpretable-ml-book/agnostic.html", "anchor_text": "The big picture of explainable machine learning."}, {"url": "https://www.kaggle.com/dansbecker/use-cases-for-model-insights", "anchor_text": "benefits that interpretability"}, {"url": "https://www.kaggle.com/learn/machine-learning-explainability", "anchor_text": ""}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5"}, {"url": "https://www.kaggle.com/dansbecker/permutation-importance", "anchor_text": ""}, {"url": "https://www.kaggle.com/dansbecker/partial-plots", "anchor_text": "Partial Dependence Plots"}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "J. H. Friedman 2001"}, {"url": "https://pdpbox.readthedocs.io/en/latest/", "anchor_text": "PDPbox."}, {"url": "https://www.kaggle.com/dansbecker/partial-plots", "anchor_text": ""}, {"url": "https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80", "anchor_text": "\u00b9"}, {"url": "https://github.com/slundberg/shap", "anchor_text": "Shap"}, {"url": "https://i.imgur.com/JVD2U7k.png", "anchor_text": "link"}, {"url": "https://www.kaggle.com/dansbecker/shap-values", "anchor_text": ""}, {"url": "https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values", "anchor_text": ""}, {"url": "http://Christoph Molnar", "anchor_text": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.Christoph Molnar"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1dec0f2f3e6b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1dec0f2f3e6b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1dec0f2f3e6b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1dec0f2f3e6b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----1dec0f2f3e6b---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&user=Parul+Pandey&userId=7053de462a28&source=-----1dec0f2f3e6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&user=Parul+Pandey&userId=7053de462a28&source=-----1dec0f2f3e6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1dec0f2f3e6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1dec0f2f3e6b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1dec0f2f3e6b--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://pandeyparul.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5be6ccf82bc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretable-machine-learning-1dec0f2f3e6b&newsletterV3=7053de462a28&newsletterV3Id=5be6ccf82bc8&user=Parul+Pandey&userId=7053de462a28&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}