{"url": "https://towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f", "time": 1683009920.878412, "path": "towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f/", "webpage": {"metadata": {"title": "Support Vector Machines | Towards Data Science", "h1": "Decoding Support Vector Machines", "description": "SVM is a very simple yet powerful supervised machine learning algorithm that can be used for classification as well as regression though its popularly used for classification. They perform really\u2026"}, "outgoing_paragraph_urls": [{"url": "http://cs229.stanford.edu/notes/cs229-notes3.pdf", "anchor_text": "CS229 Lecture Notes by Andrew Ng", "paragraph_index": 51}], "all_paragraphs": ["SVM is a very simple yet powerful supervised machine learning algorithm that can be used for classification as well as regression though its popularly used for classification. They perform really well in small to medium sized datasets and are extremely easy to tune.", "In this blog post we will build our intuition of support vector machines and see some math behind it. We will first understand what large margin classifiers are and understand the loss function and the cost function of this algorithm. We will then see how regularization works for SVM and what governs the bias/variance trade off. Finally we will learn about the coolest feature of SVM, that is the Kernel trick.", "You must have some pre-requisite knowledge of how linear regression and logistic regression work in order to easily grasp the concepts. I would suggest you to take notes while reading in order to make the most out of this article, it is going to be a long and interesting journey. So, without further ado lets dive in.", "Lets right away begin with an example, say we have some data which contains 2 classes and for simplicity lets assume it has only 2 features, we can separate these 2 classes in many different ways. We can use linear as well as non-linear decision boundaries to do so.", "What SVM does is that it tries to separate these 2 classes as widely as possible and hence in our example it will choose the yellow line as its decision boundary.", "If the yellow line is our decision boundary then the green and red class points circled (figure 2) are the closest points to our decision boundary. The distance between these points is called margin and SVM tries to maximize this margin. This is the reason why support vector machines are also called large margin classifiers, this enables SVM to have a better generalization accuracy.", "In high dimensional space these points are nothing but n-dimensional vectors where n is the number of features in the data. A sample of points that are closest to the decision boundary (here the circled red and green points) are called support vectors. I will be calling the green support vectors as positive support vectors and the red as negative support vectors. The decision boundary is entirely dependent on these points as they are the ones which decide the length of our margin. If we change the support vectors, our decision boundary will change and that also means that points other than the support vectors don`t really matter in forming our decision boundary.", "To find the decision boundary we must :", "The hypothesis for SVM is fairly straight forward, for weights w", "Here a key point that you need to understand is that this hypothesis is nothing but the distance between a data point and the decision boundary, so whenever I say the word hypothesis just think of it as nothing but this distance .", "Before we see what exactly the loss function is for SVM let us look at the cost for a single training example", "The first term is the loss for when y = 1 and the second term is the loss when y = 0 and \u201cy hat\u201d is nothing but our hypothesis defined in Figure 3. I know I have given out a lot of equations don`t worry, let`s start making sense out of them.", "This is how the cost looks like when the actual classes are 1 and 0 respectively. If you don`t understand where the 1 and -1 came from don`t worry about it you will understand it when we see the formula for the loss function.", "Lets first look at the first case when y = 1. We know that out prediction is going to be 1 when the hypothesis is non-negative. So for those values of our hypothesis our cost must be at its minimum but that is not the case here. Why is it that our cost reaches its minimum i.e the value of 0 only when hypothesis is greater than or equal to 1 instead of 0? The answer can be found in the idea of the large margin classifier.", "Our hypothesis is going to be 1 for all the class 1 points that lie exactly at the margin or in other words hypothesis is 1 for our positive support vectors. So for all the points which are further away from the decision boundary (even more further than the support vectors) the hypothesis is going to have a value greater than 1 and consequently the the cost is going to get minimized. If we have points that are between the margin i.e. they are even more close to the decision boundary than the support vectors are, for them the hypothesis is going to be lesser than 1. The closer these points get to the decision boundary the higher our cost gets. However these points are still classified as 1 as long as the hypothesis is non-negative.", "Now lets look at the second case when y = 0", "Similar to out first case here the value of the hypothesis is going to be -1 for all the class 0 points lying exactly at the margin distance or in other words for all the negative support vectors the hypothesis is going to be -1. As the hypothesis starts increasing in the negative direction or the data points start getting even more further away the the negative support vectors from the decision boundary (in our case as the red points start getting further away from decision boundary) the cost reaches 0 i.e. the minimum value. Here as well when the points lie between the margin or as they start getting closer to the decision boundary the cost start increasing but the points are classified as 0 as long as the hypothesis is negative.", "So to summarize, the SVM cost function penalizes not just for mis-classification but also when the points start getting closer to the decision boundary, hence our optimization algorithm must find weights such that a distance (or margin) is maintained between the classes.", "Now that we have understood the intuition behind SVM by seeing the cost of a single training example, let`s just see what the formula for hinge loss is.", "Now if you look at the graphs again you will understand where the 1 and -1 came from. The loss is basically just saying that", "We have seen how the hinge loss looks like and also seen the cost of a single training example, all we need to do now is combine these 2 equations and form the cost function for \u201cm\u201d training examples. This is what we get:", "Wait a second\u2026.where did all these extra letters come from??", "In the above equation, \u201cC\u201d is called the regularization parameter. It is a hyper parameter which controls the amount of regularization. However, there is a slight difference in the regularization in SVM to the regularization one sees in logistic regression or linear regression. Let us take the example of logistic regression, the cost function is of the following form:", "Here, B is the regularization term (L2 or L1) and A is the cost term or as some people call it the fit term. In logistic regression by varying the value of \u201clambda\u201d which is our regularization parameter we are basically telling the optimization algorithm the amount of attention it needs to give the regularization term and that is how we control the bias/variance trade off. But the SVM cost looks something like this:", "Now here as well A is the fit term and B is the regularization term however, the regularization parameter is associated with the fit term. So, now by varying the value of our regularization parameter we are telling the optimization algorithm how much attention to give to the fit term or how much attention to NOT give the regularization term. Intuitively, C and lambda are inversely related.", "Say for example we have a very large value of C. Even in the presence of a single outlier a large value of C will try to minimize mis-classified points and we will get the purple decision boundary instead of the green one which we would have gotten if the value of C was reasonable.", "If C is very small then the margin will be larger but the chance of mis-classification will increase.", "So basically, as C increases the tendency of our model to overfit the data increases and as C decreases we are more prone to underfit the data.", "The kernel trick is what makes support vector machines so powerful. It allows the algorithm to learn more complex decision boundaries and not just linear ones. So what are kernels? Lets first look at the hypothesis and the cost function for using SVM with kernels.", "As you can see there is not much change in the equations. Instead of X the hypothesis has \u2018f\u2019 in it which is a matrix of new features. We get the values of these features from a function and this function is what we call a kernel . Now in our cost \u201cy hat\u201d represents this newly formulated hypothesis.", "Now lets understand how these functions are calculated. Say we have a data set with 2 features, and we visualize a single training example below, here l1,l2 and l3 are some landmarks that we are selecting, don`t worry about what their values are for now.", "Our Kernels are a function of X with these landmarks. Using these functions we calculate new features for our model. So, for each landmark we will have one feature as follows:", "These functions or kernels can be of different types like Gaussian Kernel, Polynomial Kernel or if we just use our generic features without using kernels it is sometimes called as Linear kernel. Gaussian Kernels are most widely used so we will be discussing that in this post.", "This is what our features would look like if we use Gaussian kernels:", "Coming back to our example before let\u2019s understand how using Gaussian kernels affects our decision boundary.", "If we assume that X is almost equal to l1 (the distance between them is almost 0) then Gaussian kernel outputs a value close to 1.", "Another case could be that if X is very far away from a point say l1 then the kernel will output a value close to 0.", "So larger the distance between the point and a landmark, lower is the value of the feature and closer the distance between the points and a landmark, higher is the feature value. Taking an example now lets see how the decision boundary will be formed.", "Here, X1 is very close to l1, it is also close to l2 and relatively far away from l3, so we have higher values for f1 and f2 ,but f3 has a very low value. (I have just assumed these values for explanation purposes). Plugging it into our hypothesis we will get a positive value and predict that X1 is 1.", "X2 on the other hand is closer to l3 and far from l1 and l2. So the f1 and f2 values are going to be very small and f3 is going to be larger. Plugging in the values gives us a negative hypothesis and hence we predict X2 as 0.", "From this we can see that the points which are lying closer to l1 and l2, are being predicted as 1 and points closer to l3 are being predicted as 0, so this is the approximate decision boundary that we are getting.", "This gave us an intuitive understanding of how using kernels can allow SVM to learn more complex functions than just a linear one, but how are these landmarks selected?", "The answer to this question is very simple, each landmark corresponds to each of our data points. That means for a data set with \u201cm\u201d training examples, we will have \u201cm\u201d landmarks and hence we will be calculating \u201cm\u201d new features.", "So for a data set with \u201cm\u201d examples we will calculate \u201cm\u201d features. Now this way we calculate the features for all of our data points. One thing to note here is that for ith data point the ith feature value is always going to be 1 (using the Gaussian kernel)", "We have already discussed how the decision boundary is affected when we vary C. If C is large then there is high variance and low bias, and if C is small then there is high bias and low variance.", "Lets explore the effect of \u03c3 on our bias and variance. Gaussian kernels are most widely used so it is good to know how \u03c3 affects our decision function. For simplicity let us assume that we have a data set with only 1 feature", "Here l(i) is the ith landmark, we can see that if the \u03c3 is large (green line) then the Gaussian kernel varies very smoothly from one training example to another, no matter what the value of our training example or our landmark is if the \u03c3 is very large then the change is going to be minimal and hence our model will underfit the data. We will have high bias and low variance.", "Now for a small value of \u03c3 (red line) the change in the Gaussian kernel is going to be drastic from one training example to another, and hence it will give us an overfitted model. We will have high variance and low bias. So, as \u03c3 increases the bias increases and as \u03c3 decreases variance increases.", "For optimizing the SVM cost formulation we use SMO (sequential minimal optimization). It is not necessary to understand how the cost function of SVM is optimized to be able to implement SVM, most of the software packages available can do it for us. However, if you are interested in the math behind it then you can check out references", "With this we can conclude our discussion of support vector machines, congratulations if you stuck till the end you must now have an intuitive understanding of the working of SVM algorithm.", "Hope you found this post useful and easy to understand, please provide valuable feedback in the comment section :)", "CS229 Lecture Notes by Andrew Ng", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b81d2f7b76f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Arpan Mishra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea811b84fa53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=post_page-ea811b84fa53----5b81d2f7b76f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@honeyyanibel?utm_source=medium&utm_medium=referral", "anchor_text": "Honey Yanibel Minaya Cruz"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://futuristicon.com/author/adamrad/", "anchor_text": "Adam"}, {"url": "https://futuristicon.com/how-to-start-with-machine-learning/", "anchor_text": "futuristicon"}, {"url": "http://cs229.stanford.edu/notes/cs229-notes3.pdf", "anchor_text": "CS229 Lecture Notes by Andrew Ng"}, {"url": "https://www.youtube.com/playlist?list=PLNeKWBMsAzboNdqcm4YY9x7Z2s9n9q_Tb", "anchor_text": "Andrew Ng Lecture Videos"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5b81d2f7b76f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b81d2f7b76f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----5b81d2f7b76f---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/tag/classification?source=post_page-----5b81d2f7b76f---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----5b81d2f7b76f---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=-----5b81d2f7b76f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=-----5b81d2f7b76f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b81d2f7b76f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b81d2f7b76f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arpan Mishra"}, {"url": "https://medium.com/@mishraarpan6/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "26 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea811b84fa53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=post_page-ea811b84fa53--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fea811b84fa53%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}