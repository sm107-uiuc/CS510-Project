{"url": "https://towardsdatascience.com/interpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57", "time": 1683018020.886078, "path": "towardsdatascience.com/interpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57/", "webpage": {"metadata": {"title": "Interpreting Deep Neural Networks using Integrated Gradients | by Pramod Pai | Towards Data Science", "h1": "Interpreting Deep Neural Networks using Integrated Gradients", "description": "After having seen how various interpretability techniques are being applied to traditional ML models like XGBoost and Random Forest, we now shift our focus to Neural Networks. Neural networks are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks", "paragraph_index": 12}, {"url": "http://wicketsoft.com", "anchor_text": "wicketsoft.com", "paragraph_index": 24}, {"url": "https://www.linkedin.com/in/paipramod/", "anchor_text": "https://www.linkedin.com/in/paipramod/", "paragraph_index": 24}, {"url": "https://twitter.com/pai_lfc", "anchor_text": "https://twitter.com/pai_lfc", "paragraph_index": 24}], "all_paragraphs": ["After having seen how various interpretability techniques are being applied to traditional ML models like XGBoost and Random Forest, we now shift our focus to Neural Networks.", "Neural networks are great at approximating complex functions that represent real-world events. This has enabled some wonderful feats not just in tech but in areas of Finance, Biology, Physics, etc. The improvement in accuracy and breaking records for state-of-the-art seems to be happening at a breakneck speed. This comes at a cost. As the models get bigger and more complex, they lose the characteristics that make them interpretable. It is very important to learn how and why a machine learning model behaves a certain way while making predictions. We would want to ask questions along the following lines.", "We look to the paper Axiomatic Attribution for Deep Networks to understand how we can take a step closer to understanding the decisions made by a deep neural network.", "A Neural Network is a mathematical function, just as f(x) = x\u00b2 is. The function output is heavily dependent on x, or the input. If someone told us that f(x) evaluated to a trillion, we would say that the input was a relatively large number. In other words, input to the mathematical function shown above absolutely decides the output. The large output can be attributed to a relatively large input. This attribution to the input is something that can help us understand a neural network\u2019s prediction. For example, when a neural network predicts the image that it was shown as a \u2018cat\u2019, the pixels in the image belonging to the cat attributed to the prediction. If there was a score for this, then the attribution score for those pixels would be very high given that it was a well-trained model.", "The research paper Axiomatic Attribution for Deep Networks defines axioms for the correctness of attribution methods \u2014 methods that generate attribution scores for inputs to deep networks. Axioms are nothing but desirable characteristics that we want these methods to have, so we can trust that they will do a good job of attributing the right scores to the right input features. The paper mentioned above highlights two such axioms:", "We will get back to what these mean later in the post. For now, we will focus on an attribution method that satisfies both these axioms. This method is called Integrated Gradients. Unfortunately, most of the similar methods do not satisfy one of these two axioms, which makes learning Integrated Gradients a fruitful task. Another desirable feature of the Integrated Gradients is that it does not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, which can be done quite easily in any modern neural network programming framework, allowing even novice practitioners to easily apply the technique.", "Here is a simple way to understand Integrated Gradients.", "An Attribution method scores the input data based on the predictions the model makes, i.e. it attributes the predictions to its input signals or features, using scores for each feature. For example, In sentiment classification for movie reviews, the input text could be \u2018It was a fantastic performance\u2019. Using an attribution method, we could generate a score for each word in the input the model predicted for. And these scores associated with each of these words could tell us what part they played in an instance of a prediction.", "Integrated Gradients is one such method. In rough terms, it is equal to (feature x gradient). The gradient is the signal that tells the neural network how much to increase or decrease a certain weight/coefficient in the network during backpropagation. It relies heavily on the input features to do so. Therefore, the gradient associated with each input feature with respect to the output (partial derivative dout/din) can help us get a clue about how important a feature is. But there is one small problem.", "It becomes easier to formalize this problem using the example of image classification using deep learning.", "Consider the above image. We correctly predict the image being a \u2018Fireboat\u2019. We calculate the gradients per feature, in this case, a pixel in the image, to get the dark image on the right. But it looks like noise and is nowhere close to identifying the fireboat. Then how did the model get the prediction right? Did the model randomly guess that the correct prediction was a fireboat?", "It turns out that the model function is flat in the vicinity of the input of a well-trained model. What we mean by that is, assume that the model had only one input feature and one weight value \u2014 y = f(x) or y = w * x. If the error in the network is zero, in other words, if the goal weight is reached as shown below, the slope at that point (which is the derivative dy/dx) is 0.", "Therefore in the (feature x gradient) paradigm, we get (feature x 0). This explains the black pixels and random noise in the image. There is a way to counter this as shown in the paper Axiomatic Attribution for Deep Networks.", "We arrive at the next and final important concept in understanding Integrated Gradients, The Baseline. Now, the next step is to desaturate the network in order to see the effect of the input features on the output predictions. To perform this, we take the image in question and dial down the brightness/intensity of the image all the way down to black as shown below.", "Next, we scale the intensity slowly on a range of 0\u20131 to make the black image look more and more like the fireboat image. We see in the graph above that around the 0.3 mark, the score reaches 1.0 and the gradients level off. This is basically the saturated network that we previously discussed. The gradients that we need to compute the attributions lie below this mark. And the black image is called the baseline.", "In the case of NLP models, the baseline is a zero-embedding word vector.", "Now that we have looked at Integrated Gradients, Attribution Scores, and the Baseline, It is a good time to visit the two axioms that were discussed earlier in the post.", "An attribution method satisfies Sensitivity if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.", "Gradients violate sensitivity: consider the following function. It is a one variable, one ReLU network (ReLU is nothing but max(0,x)).", "Suppose the baseline is x=0 and the input is x=5. The range of this function is 0 to 1. For any value greater than or equal to 1, the function returns 1. Early interpretability methods for neural networks assigned feature importance scores using gradients. A reason to choose Integrated Gradients over gradients is that gradients break sensitivity according to the definition above as the differing feature x=5 is being given a zero attribution.", "Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations. For example a CNN and an RNN classifying a piece of text to have a positive sentiment. Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks. To motivate this, notice that attribution can be roughly defined as assigning the blame (or credit) for the output to the input features. Such a definition does not refer to implementation details. Other popular attribution methods such as DeepLift and LRP break Implementation Invariance. The integrated Gradient method does not.", "These are some great properties that Integrated Gradients have which make them a very good choice to explain any differentiable model.", "Integrated gradients attribute the predictions of a deep network to its inputs. It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification. I feel that the Integrated Gradients method is a very good place to start the journey into model interpretability for deep neural networks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Grad Student, Northeastern University | Computer Vision Co-op @ wicketsoft.com | https://www.linkedin.com/in/paipramod/ | https://twitter.com/pai_lfc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff9b8ecdd3c57&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://devpramod.medium.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": ""}, {"url": "https://devpramod.medium.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Pramod Pai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F397bc4535f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&user=Pramod+Pai&userId=397bc4535f34&source=post_page-397bc4535f34----f9b8ecdd3c57---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@victor_g?utm_source=medium&utm_medium=referral", "anchor_text": "Victor Garcia"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks"}, {"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks"}, {"url": "https://commons.wikimedia.org/wiki/File:San_Francisco_fireboat_showing_off.jpg", "anchor_text": "Wikimedia Commons"}, {"url": "https://arxiv.org/pdf/1703.01365.pdf", "anchor_text": "Axiomatic Attribution for Deep Networks"}, {"url": "https://www.slideshare.net/KrishnaramKenthapadi/explainable-ai-in-industry-kdd-2019-tutorial", "anchor_text": "Explainable AI in Industry (KDD 2019 Tutorial)"}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "Axiomatic Attribution for Deep NetworksWe study the problem of attributing the prediction of a deep network to its input features, a problem previously\u2026arxiv.org"}, {"url": "https://www.manning.com/books/grokking-deep-learning", "anchor_text": "Grokking Deep LearningGrokking Deep Learning teaches you to build deep learning neural networks from scratch! In his engaging style, seasoned\u2026www.manning.com"}, {"url": "https://github.com/pair-code/lit", "anchor_text": "https://github.com/pair-code/lit"}, {"url": "https://blog.fiddler.ai/", "anchor_text": "blog.fiddler.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&user=Pramod+Pai&userId=397bc4535f34&source=-----f9b8ecdd3c57---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&user=Pramod+Pai&userId=397bc4535f34&source=-----f9b8ecdd3c57---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff9b8ecdd3c57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f9b8ecdd3c57---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f9b8ecdd3c57--------------------------------", "anchor_text": ""}, {"url": "https://devpramod.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://devpramod.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pramod Pai"}, {"url": "https://devpramod.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8 Followers"}, {"url": "http://wicketsoft.com", "anchor_text": "wicketsoft.com"}, {"url": "https://www.linkedin.com/in/paipramod/", "anchor_text": "https://www.linkedin.com/in/paipramod/"}, {"url": "https://twitter.com/pai_lfc", "anchor_text": "https://twitter.com/pai_lfc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F397bc4535f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&user=Pramod+Pai&userId=397bc4535f34&source=post_page-397bc4535f34--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F397bc4535f34%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-deep-neural-networks-using-integrated-gradients-f9b8ecdd3c57&user=Pramod+Pai&userId=397bc4535f34&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}