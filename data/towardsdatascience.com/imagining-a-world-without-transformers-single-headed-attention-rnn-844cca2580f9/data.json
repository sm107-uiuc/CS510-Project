{"url": "https://towardsdatascience.com/imagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9", "time": 1683002663.4923909, "path": "towardsdatascience.com/imagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9/", "webpage": {"metadata": {"title": "Imagining a world without Transformers \u2014 Single Headed Attention RNN | by Akashdeep Singh Jaswal | Towards Data Science", "h1": "Imagining a world without Transformers \u2014 Single Headed Attention RNN", "description": "Distilling key ideas from one of the most entertaining NLP papers picturing a world without the BERT family of models - \"Single Headed Attention RNN - Stop thinking with your head\""}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "Single Headed Attention RNN: Stop Thinking With Your Head", "paragraph_index": 0}, {"url": "https://smerity.com/abme.html", "anchor_text": "Stephen Merity", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "universal approximation theorem", "paragraph_index": 1}, {"url": "http://prize.hutter1.net/", "anchor_text": "Hutter Prize", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866", "anchor_text": "this blog post", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10", "anchor_text": "byte pair encoding", "paragraph_index": 8}, {"url": "https://www.linkedin.com/in/akashjaswal/", "anchor_text": "https://www.linkedin.com/in/akashjaswal/", "paragraph_index": 15}], "all_paragraphs": ["2019 was the year of the Transformer \u2014 tons of work done (over 5000 citations) in different domains and NLP problems on extending and applying BERT. There was one paper published in November 2019 that brought back old memories from not so long ago when RNNs ruled almost all NLP tasks. This blogpost summarizes key takeaways from the paper Single Headed Attention RNN: Stop Thinking With Your Head by Stephen Merity. The author shares some very interesting ideas \u2014 and the paper is written in a satirical manner with many subtle references throughout making it a fun read. On the other hand, it becomes slightly hard to consistently follow and assimilate for some folks (I read it a few times to fully comprehend). I will try my best to summarize the main points being communicated by the paper. I\u2019ll also try to set some context for readers who may not be very well versed with the necessary background to make this post useful for all. Here are the main points we\u2019ll dive deeper into:", "Language modeling is the task of predicting the next word in a sequence given the words that have appeared before. Many NLP tasks including machine translation, speech recognition, etc are powered by language models at their core. The author makes an interesting reference to the universal approximation theorem which states that a neural network, when equipped with enough parameters, is capable of picking up on all the finer intricacies of any task at hand. Theoretically, a language model is just a mere reflection of the way human language is written, without any significant original intelligence of its own.", "The Hutter Prize encourages the task of compressing natural language text as a proxy of being able to learn and reproduce text sequences in the most efficient way possible, specifically, how much can the 100 MB text file (enwik8) from Wikipedia be compressed. In this paper, the author demonstrates that a simple LSTM based model (with some modifications) with a single attention head can come very close in performance in comparison to many advanced transformer-based models despite the low memory and resource consumption.", "The author describes the architecture of the SHA-RNN model as an upgraded version of the AWD-LSTM model introduced in Merity et al. (2018a). For what it\u2019s worth, it may be a good idea to do a quick refresher of the AWD-LSTM model to establish a better understanding of the SHA-RNN model. The ASGD Weight-Dropped LSTM model applied several regularization techniques to LSTMs that made it a powerful language modeling standard, but here are the two that stood out the most:", "2. NT-ASGD: As its name suggests, Non-monotonically triggered averaged stochastic gradient descent is a variant of traditional SGD in two senses. Firstly, it takes into consideration weights from previous iterations by averaging them with the current training step weights returned during the backward pass. Secondly, this averaging is only performed when the model\u2019s primary evaluation metric fails to improve over a certain period of training iterations.", "With this background, we can look at some of the main components of the SHA-RNN model architecture:", "As shown in the figure, Each SHA-RNN layer contains only a single head of attention that helps with keeping the memory consumption of the model to the minimum by eliminating the need to update and maintain multiple matrices. Multi-headed attention, on the other hand, requires the computation of each head to be fused to produce the final output of a transformer layer. The SHA-RNN also majorly exploits the sequentiality in the text rather than the transformers that only subtly track the notion of a sequence through the time and positional encoding signals which might introduce more parameters for the model to keep track of.", "The SHA-RNN layer is loaded with Layer Normalization steps at several points to normalize features across each feature dimension which has been previously proven to also reduce training time and increased stability. There is a feedforward layer that is named the \u201cboom\u201d layer which essentially creates the final output vector using GeLU (Gaussian error linear unit) activation. Although the AWD-LSTM used the NT-ASGD optimizer, the author reported findings consistent with previous experiments using the LAMB optimizer (Layer-wise Adaptive Moments optimizer for Batch training) resulted in improved convergence for larger batch sizes. Going into the finer details of the LAMB optimizer might be tricky to do in this setting but I like this blog post that does a great job of explaining it if you were interested to learn more.", "The author points out the importance of varying tokenization schemes playing a significant role in how we train, evaluate and compare different language models. Especially, comparing perplexity scores between word and sub-word level models might not be very straightforward. Wordpiece tokenizers often split words into smaller composite subwords that lead to a tighter vocabulary and better utilization of model parameters \u2014 which is great. Additionally, during the training process, the target token is passed as the next input (teacher forcing) to the layer causing the model to gain a slight advantage in scenarios where the initial part of the word heavily influences the subsequent parts of a given word. A great example that illustrates this in practice is when the model has to predict \u201cDumbledore\u201d, there is a lot of information obtained from the first token \u201cD\u201d in the word pieces [D][umble][dore] as opposed to word tokenized model that would just have one shot at getting the prediction right. If you were wondering how some of these wordpiece tokenization algorithms work, my last blog post was about repurposing the byte pair encoding for subword tokenization.", "We\u2019ve experienced massive breakthrough\u2019s in the realm of deep learning in the last few years, mainly due to the availability of massive compute resources (GPU/TPU clusters) and our ability to use these resources efficiently (distributed machine learning). While most large organizations have unlimited compute at their fingertips, it can be hard for independent researchers to reproduce results. Pretraining BERT, for instance, takes up to 4 days on 16 Google cloud TPUs easily costing over a few tens of thousands of dollars \u2014 not sure about yours, but that would certainly burn a big hole in my pocket.", "\u201cAll my best work seems to come from being relatively low resourced and creative anyway.\u201d", "It is commendable that the author was able to train the SHA-RNN model on a single GPU machine in less than 24 hours and without extensive hyperparameter tuning.", "\u201cWhy crowd a single direction of progress like moths swarming a light bulb?\u201d", "It\u2019s amazing how much progress the research community has made in the last year since the introduction of the transformer layer. A lot of the work done by the author in the SHA-RNN paper is also inspired by this work and all the many intelligent works of the past. But, the author brings up the importance of having variety in and competing model architectures in the researchers\u2019 toolkit to be able to pick the appropriate models for the given task at hand rather than falling back on just one type of model. This also calls for better tooling and improvements in efficiency for not just a select few architectures but advancing all facets of the research streams collectively. But in the end, it is hard to say what could have been, but never forget where you came from because you might have to go back someday!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer | I enjoy being uncomfortable and solving complex problems through smart (AI) solutions. https://www.linkedin.com/in/akashjaswal/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F844cca2580f9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----844cca2580f9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JaswalAkash?source=post_page-----844cca2580f9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JaswalAkash?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Akashdeep Singh Jaswal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd094b607db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&user=Akashdeep+Singh+Jaswal&userId=cd094b607db9&source=post_page-cd094b607db9----844cca2580f9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "Single Headed Attention RNN: Stop Thinking With Your Head"}, {"url": "https://smerity.com/abme.html", "anchor_text": "Stephen Merity"}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "universal approximation theorem"}, {"url": "http://prize.hutter1.net/", "anchor_text": "Hutter Prize"}, {"url": "https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866", "anchor_text": "this blog post"}, {"url": "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10", "anchor_text": "byte pair encoding"}, {"url": "https://github.com/Smerity/sha-rnn", "anchor_text": "https://github.com/Smerity/sha-rnn"}, {"url": "https://soundcloud.com/twiml/single-headed-attention-rnn", "anchor_text": "https://soundcloud.com/twiml/single-headed-attention-rnn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----844cca2580f9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----844cca2580f9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----844cca2580f9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----844cca2580f9---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----844cca2580f9---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&user=Akashdeep+Singh+Jaswal&userId=cd094b607db9&source=-----844cca2580f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&user=Akashdeep+Singh+Jaswal&userId=cd094b607db9&source=-----844cca2580f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F844cca2580f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----844cca2580f9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----844cca2580f9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----844cca2580f9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----844cca2580f9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----844cca2580f9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----844cca2580f9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JaswalAkash?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JaswalAkash?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akashdeep Singh Jaswal"}, {"url": "https://medium.com/@JaswalAkash/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "85 Followers"}, {"url": "https://www.linkedin.com/in/akashjaswal/", "anchor_text": "https://www.linkedin.com/in/akashjaswal/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd094b607db9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&user=Akashdeep+Singh+Jaswal&userId=cd094b607db9&source=post_page-cd094b607db9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F22472106014f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9&newsletterV3=cd094b607db9&newsletterV3Id=22472106014f&user=Akashdeep+Singh+Jaswal&userId=cd094b607db9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}