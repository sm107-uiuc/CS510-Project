{"url": "https://towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d", "time": 1682997383.7452009, "path": "towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d/", "webpage": {"metadata": {"title": "An Introduction to Dimensionality Reduction | by Peter Grant | Towards Data Science", "h1": "An Introduction to Dimensionality Reduction", "description": "High dimensional data sets provide one of the largest challenges in all of data science. The challenge is quite straightforward. Machine learning algorithms require the data set to be dense in order\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "-Nearest Neighbors and the Curse of Dimensionality", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "-Nearest Neighbors and the Curse of Dimensionality", "paragraph_index": 3}, {"url": "https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1491957662&linkCode=as2&tag=petergrantpub-20&linkId=5618a11d4981bc89d1a94f420f60e7e7", "anchor_text": "Python for Data Analysis by Wes McKinney", "paragraph_index": 6}, {"url": "https://www.amazon.com/gp/product/B07DWG4T95/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=B07DWG4T95&linkCode=as2&tag=petergrantpub-20&linkId=1730c8c31237e6d84168ea0bff730b8c", "anchor_text": "Hands on Data Visualization with Bokeh", "paragraph_index": 6}, {"url": "https://www.amazon.com/gp/product/1492041130/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1492041130&linkCode=as2&tag=petergrantpub-20&linkId=14af0a99fb1fa5304b60064bb3c1274f", "anchor_text": "Data Science from Scratch", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/introducing-k-nearest-neighbors-7bcd10f938c5", "anchor_text": "k-nearest neighbors algorithm", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "every dimension which gets very rare when there are many dimensions", "paragraph_index": 16}], "all_paragraphs": ["High dimensional data sets provide one of the largest challenges in all of data science. The challenge is quite straightforward. Machine learning algorithms require the data set to be dense in order to make accurate predictions. Data spaces get extremely vast as more and more dimensions are added. Vast data spaces require extremely large data sets to maintain density. This challenge can either result in inaccurate predictions from models (For instance, I described how the k-nearest neighbors algorithm can be rendered useless with high dimension data spaces in k-Nearest Neighbors and the Curse of Dimensionality) or data sets that are too large for computers to reasonably handle.", "Fortunately data scientists have identified a solution to this problem. It\u2019s called dimensionality reduction. The essence of dimensionality reduction is simple: You search the data set for trends that imply the data set is acting along a different dimension than your original assumption, then you transform your data accordingly. In this way you can reduce the number of dimensions in your data set.", "Dimensionality reduction works by identifying the dimensions that truly matter in your data set. Once the really important dimensions have been identified, you can transform your data set so that the points are represented along these dimensions instead of the dimensions that they were originally presented with.", "Let\u2019s discuss this in terms of an example. In k-Nearest Neighbors and the Curse of Dimensionality I presented the somewhat tongue in cheek example of the Garden Gnome Demarcation Line. In this example pretend that you\u2019re studying the locations of garden gnomes in a city, and plotting them. You start off by identifying the latitude and longitude of each garden gnome in the city, then plot them in a 2 dimensional graph. The two dimensions are North-South and East-West, represented in miles from the center of the city. This is a perfectly reasonable way to present the data and, with only two dimensions, would suffice perfectly.", "However, pretend that you\u2019re a perfectionist and want to present the data using as few dimensions as possible. You also notice that the garden gnomes, for some odd reason, form a nearly perfect line running from the North-East corner of the city to the South-West corner of the city. The data set isn\u2019t a cloud of garden gnome locations, it\u2019s a line. Noticing this, you realize that you could plot the location of each garden gnome in terms of distance from the city center in the NorthEast-SouthWest direction. And this representation would only require one dimension.", "The next step is then to perform the transformation. You need to calculate the distance of each point from the center of the city along the new axis, and declare that your new data set. The new distance can be done algebraically. And once that\u2019s done, the resulting profile can be plotted showing your data set in a single dimension.", "We\u2019ll walk through this process using the prior example of the Garden Gnome Demarcation Line. I\u2019ll do all of this work in python, using the capabilities of pandas and numpy. For detailed instructions on how to use those tools, I highly recommend reading Python for Data Analysis by Wes McKinney (Wes is the creator of pandas, so you can bet he knows what he\u2019s talking about. I\u2019ll be generating plots using the python package bokeh. A useful introduction to that package can be found in Hands on Data Visualization with Bokeh.", "I\u2019ve constructed my data set in a pandas dataframe called GardenGnomeLocations. It has an index for 31 entries, and columns representing the location of each garden gnome along the North-South and East-West axis. The columns are named \u2018NorthSouth (mi)\u2019 and \u2018EastWest (mi)\u2019.", "Figure 1 presents the original data set, showing the location of each garden gnome in the city along the North-South and East-West axes. Each circle represents the location of a garden gnome relative to the city center. Notice the previously described trend presented in the data; the location of the garden gnomes is, for some mysterious reason, a straight line running from the North-East corner of the city to the South-West corner of the city.", "Upon noticing that trend it becomes clear that the location of garden gnomes can really be presented using a single dimension, a single axis. We can use this knowledge to create a new axis, the NorthEast-SouthWest axis (Or, as I enjoy calling it, the Garden Gnome Demarcation Line). Then the data can be presented in a single dimension, as the distance from the city center along that axis.", "To perform this translation, we use the pythagorean theorem on each data point. To calculate the distance of each garden gnome from the city center along our new axis, we need to use a for loop, the pandas.loc function, the math.sqrt function, and some algebraic expressions. The code calculating the new distance and adding it to the \u2018Distance (mi)\u2019 column in the data frame is as follows:", "The for loop at the top of the code block tells the script to perform this calculation for each row in the GardenGnomeLocations dataframe, and to use the variable i to keep track of it\u2019s location in the dataframe. The second line performs the actual calculation and stores the data in the appropriate location in the dataframe. You\u2019ll note that the equation boils down to c = sqrt(a\u00b2 + b\u00b2), which is the common form of the pythagorean theorem. The first term, GardenGnomeLocations.loc[i, \u2018Distance (mi)\u2019] tells python that the value we\u2019re about to calculate should be placed in row i of column \u2018Distance (mi)\u2019 in the GardenGnomeLocations dataframe. The other side of the equation uses the corresponding data from the \u2018NorthSouth (mi)\u2019 and \u2018EastWest (mi)\u2019 columns to calculate the result. Keep in mind that the pythagorean theorem will only return absolute values of the distance. To overcome that, we add the final two lines of code. The first determines whether the values of the original data point were positive or negative. The second then multiplies the distance by -1, to make it negative, if the original values were negative.", "Plotting this data results in Figure 2. The data looks very similar, because this plot presents it in 2 dimensions, but notice the axes in this case. Instead of presenting each plot using the location in the North-South and East-West axes, Figure 2 shows the location of each gnome only in terms of the distance from the city center. In one dimension.", "This reduction to a single dimension makes the data set much easier to use. Fewer points are needed to ensure that the data set is dense enough to return accurate predictions. Fewer data points means reduced computational time for the same quality results. And some algorithms, like the k-nearest neighbors algorithm, are much more likely to return useful results than with higher dimensions.", "This transformation does make it harder to interpret the data, however. Originally the data was easily understood; each data point referred to the location of the garden gnome in terms we commonly use. We could fine the distance North/South, and the distance East/West and simple use that. Now instead we have a distance from the city center, but the data doesn\u2019t clearly state the direction. In order to make sense of this data, we must retain all prior data sets and algorithms. In that way we can later translate the data from the new axis back to the original axis, so it can be easily understood and used.", "Unfortunately, no it isn\u2019t really that simple. This was an overly simplified, somewhat comical, example meant to demonstrate the fundamental concepts. In reality no data set will operate along a perfectly shaped line like this data set did, and it won\u2019t be possible to transform the data by simply using the pythagorean theorem. Real data sets will look more like clouds of data with vague trends hidden in them, and you\u2019ll need to use a technique called principal component analysis to transform the data. This technique is beyond the scope of this introductory article, but Joel Gros does an excellent job of demonstrating how to implement it in Data Science from Scratch.", "Highly dimensional data sets provide a serious challenge for data scientists. As the number of dimensions increases, so does the data space. As more and more dimensions are added, the space becomes extremely large. This large space makes it hard for most machine learning algorithms to function because gaps in the data set present areas that the models cannot match. Some algorithms, such as the k-nearest neighbors algorithm, are especially sensitive because they require data points to be close in every dimension which gets very rare when there are many dimensions.", "The obvious solution to the dimensionality problem is larger data sets. This can be used to maintain data density; however, very large data spaces require very large data sets. These data sets can get too large, overwhelming the ability of computers to perform the necessary calculations. In that case, the solution is to apply dimensionality reduction.", "Dimensionality reduction is the practice of noticing when data points align along different axes from the ones that are originally used, and transforming the data sets to present them along those axes instead. We demonstrated this using the example of garden gnomes spread throughout the city. Originally they were plotted on the intuitive North-South and East-West axes. However, after inspecting the data, it became clear that they were oriented along a separate axis running from the North-East corner of the city to the South-West corner. Translating the data set accordingly reduced the data set from one dimension to two. While this is a small example, the principal can be applied to much larger data sets.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Scientist at Lawrence Berkeley National Laboratory who also teaches skills you need to build a fulfilling career."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c7d1c80737d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://petergrant-81989.medium.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Peter Grant"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F985f2ff02845&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&user=Peter+Grant&userId=985f2ff02845&source=post_page-985f2ff02845----8c7d1c80737d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "-Nearest Neighbors and the Curse of Dimensionality"}, {"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "-Nearest Neighbors and the Curse of Dimensionality"}, {"url": "https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1491957662&linkCode=as2&tag=petergrantpub-20&linkId=5618a11d4981bc89d1a94f420f60e7e7", "anchor_text": "Python for Data Analysis by Wes McKinney"}, {"url": "https://www.amazon.com/gp/product/B07DWG4T95/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=B07DWG4T95&linkCode=as2&tag=petergrantpub-20&linkId=1730c8c31237e6d84168ea0bff730b8c", "anchor_text": "Hands on Data Visualization with Bokeh"}, {"url": "https://www.amazon.com/gp/product/1492041130/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1492041130&linkCode=as2&tag=petergrantpub-20&linkId=14af0a99fb1fa5304b60064bb3c1274f", "anchor_text": "Data Science from Scratch"}, {"url": "https://towardsdatascience.com/introducing-k-nearest-neighbors-7bcd10f938c5", "anchor_text": "k-nearest neighbors algorithm"}, {"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "anchor_text": "every dimension which gets very rare when there are many dimensions"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8c7d1c80737d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c7d1c80737d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8c7d1c80737d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8c7d1c80737d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----8c7d1c80737d---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&user=Peter+Grant&userId=985f2ff02845&source=-----8c7d1c80737d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&user=Peter+Grant&userId=985f2ff02845&source=-----8c7d1c80737d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c7d1c80737d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c7d1c80737d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c7d1c80737d--------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Peter Grant"}, {"url": "https://petergrant-81989.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "969 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F985f2ff02845&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&user=Peter+Grant&userId=985f2ff02845&source=post_page-985f2ff02845--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F90e3ec001185&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-dimenstionality-reduction-8c7d1c80737d&newsletterV3=985f2ff02845&newsletterV3Id=90e3ec001185&user=Peter+Grant&userId=985f2ff02845&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}