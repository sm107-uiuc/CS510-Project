{"url": "https://towardsdatascience.com/bayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f", "time": 1683004906.791403, "path": "towardsdatascience.com/bayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f/", "webpage": {"metadata": {"title": "Bayesian Optimization Concept Explained in Layman Terms | by Wei Wang | Towards Data Science", "h1": "Bayesian Optimization Concept Explained in Layman Terms", "description": "Bayesian Optimization has been widely used for the hyperparameter tuning purpose in the Machine Learning world. Despite the fact that there are many terms and math formulas involved, the concept\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this paper", "paragraph_index": 17}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this research paper", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this post", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this article", "paragraph_index": 39}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this article", "paragraph_index": 39}, {"url": "https://www.linkedin.com/in/wiw5087/", "anchor_text": "Linkedin", "paragraph_index": 41}, {"url": "https://twitter.com/WeiW5087", "anchor_text": "Twitter", "paragraph_index": 41}], "all_paragraphs": ["Bayesian Optimization has been widely used for the hyperparameter tuning purpose in the Machine Learning world. Despite the fact that there are many terms and math formulas involved, the concept behind turns out to be very simple. The goal of this article is to share what I learned about Bayesian Optimization with a straightforward interpretation of textbook terminologies, and hopefully, it will help you understand what Bayesian Optimization is in a short period of time.", "For the completeness of the article, let\u2019s start with the basic overview of hyperparameter optimization methods, which are generally 4 types:", "Manual Search, Random Search, Grid Search, and Bayesian Optimization", "Bayesian Optimization differs from Random Search and Grid Search in that it improves the search speed using past performances, whereas the other two methods are uniform (or independent) of past evaluations. In that sense, Bayesian Optimization is like Manual Search. Let\u2019s say you are manually optimizing the hyperparameter of a Random Forest regression model. Firstly, you would try a set of parameters, then look at the result, change one of the parameters, rerun, and compare the results, so that way you know whether you are going towards the right direction. Bayesian Optimization does a similar thing \u2014 the performance of your past hyperparameter affects the future decision. In comparison, Random Search and Grid Search do not take into account past performance when determining new hyperparameters to evaluate. Thus, Bayesian Optimization is a much more efficient method.", "Let\u2019s continue using our example of optimizing hyperparameters for a Random Forest regression model. Say we want to find a set of hyperparameters that will minimize RMSE. Here, the function to compute RMSE is called the objective function. If we were to know the probability distribution of our objective function, (in simple words, if we were to know the shape of the objective function), then we can simply compute the gradient descent and find the global minimum. However since we don\u2019t know the distributions of RMSE score (this is actually what we are trying to find out), we need Bayesian Optimization to help us decipher this black-box model.", "Bayesian Optimization builds a probability model of the objective function and uses it to select hyperparameter to evaluate in the true objective function.", "This sentence might sound complicated but actually delivers a simple message. Let\u2019s break it down:", "\u201cBayesian Optimization builds a probability model of the objective function\u201d", "The true objective function is a fixed function. Let\u2019s say it is supposed to look like Fig 1, but as I mentioned, we don\u2019t know this at the beginning of the hyperparameter tuning.", "If there are unlimited resources, we would compute every single point of the objective function so that we know its actual shape (In our example, keep calling the Random Forest Regression model until we have the RMSE scores for all possible hyperparameter combinations). However, that\u2019s impossible. So let\u2019s say we only have 10 samples from the true objective function, represented as black circles in Fig 2:", "Using these 10 samples, we need to build a surrogate model (also called the response surface model) to approximate the true objective function. Take a look at Fig 3. The surrogate model is represented as the blue line. The blue shade represents the deviation.", "A surrogate model by definition is \u201cthe probability representation of the objective function\u201d, which is essentially a model trained on the (hyperparameter, true objective function score) pairs. In math, it is p(objective function score | hyperparameter). There are different ways to construct a surrogate model, but I will come back to this later.", "\u201cAnd use it to select hyperparameters\u201d", "now we have 10 samples of the objective function and how should we decide which parameter to try as the 11th sample? We need to build an acquisition function (also called the selection function). The next hyperparameter of choice is where the acquisition function is maximized. In Fig 4, the green shade is the acquisition function and the red straight line is where it is maximized. Therefore the corresponding hyperparameter and its objective function score, represented as a red circle, is used as the 11th sample to update the surrogate model.", "\u201cTo evaluate in the true objective function\u201d", "As described above, after using an acquisition function to determine the next hyperparameter, the true objective function score of this new hyperparameter is obtained. Since the surrogate model has trained on the (hyperparameter, true objective function score) pairs, adding a new data point updates the surrogate model.", "\u2026Repeat the above steps until the max time or max iteration is reached. And boom! You now (hopefully) have an accurate approximation of the true objective function and can easily find the global minimum from the past evaluated samples. Your Bayesian Optimization completes!", "To summarize, let\u2019s look at the below pseudo-code in Fig 5, which comes from this paper:", "Here, SMBO stands for Sequential Model-Based Optimization, which is another name of Bayesian Optimization. It is \u201csequential\u201d because the hyperparameters are added to update the surrogate model one by one; it is \u201cmodel-based\u201d because it approximates the true objective function with a surrogate model that is cheaper to evaluate.", "H : Observation History of (hyperparameter, score) pairT : Max Number of Iterationsf : True Objective Function (in our example, the RMSE function)M : Surrogate Function, which is updated whenever a new sample is addedS : Acquisition Function x* : The Next Chosen Hyperparameter to Evaluate", "Let\u2019s go through this loop one more time.", "Repeat until the max number of iterations is reached. In the end, the history of (hyperparameter, true objective function score) is returned. Note that the last record is not necessarily the best-achieved score. You would have to sort the score to find the best hyperparameter.", "Rather than getting into the math details of the surrogate models and acquisition function, I will only give a general description of the commonly used types. If you are interested in learning more about how the acquisition function works with different surrogate models, check this research paper.", "Let\u2019s start by explaining what the acquisition function is so that we can explain how each type of surrogate model is optimized.", "The most common acquisition function is the Expected Improvement. The formula looks like this:", "p(y|x): the surrogate model. y is the true objective function score and x is the hyperparameter", "y*: the minimum observed true objective function score so far", "Expected Improvement is built on top of the surrogate model, meaning that different surrogate models would result in different ways of optimizing this acquisition function. We will discuss this in the following sections.", "The majority of the research papers use the Gaussian Process model as the surrogate model for its simplicity and ease of optimization. Gaussian Process directly models P(y|x). It uses the history of (hyperparameter, true objective function score) as (x, y) to construct the multivariate Gaussian distributions.", "To maximize the Expected Improvement result for the Gaussian Process model, the new score should be less than the current minimum score (y < y*), so that max(y* \u2014 y, 0) can be a large positive number.", "Let\u2019s look at a concrete example in Fig 6(which I borrowed from this post):", "Let\u2019s say in Fig 6 the lowest score = 12, then y* = 12. The Expected Improvement function will look into the regions where the uncertainty is high and the mean function is close to or lower than y*. The n_estimators that yield the highest Expected Improvement using the multivariate Gaussian distributions would be used as the next input to the real objective function.", "Another surrogate model that is implemented in some python packages (e.g. hyperopt library) is TPE. First recall the Bayes Rule which is shown below:", "While Gaussian Process models p(y|x) directly, TPE models p(x|y), which is the probability distribution of hyperparameters given the objective function score.", "Let\u2019s continue using Fig 7 as an example. Instead of choosing y* = 12 as the Gaussian Process, TPE algorithm chooses y* to be some quantile \u03b3 of the observed y values so that p(y < y*) = \u03b3. In other words, TPE chooses y* to be some number that\u2019s a bit higher than the best-observed score so that it can separate the current observations into two clusters: better than y* and worse than y*. See Fig 7 as an illustration.", "Given the separate scores, TPE then constructs separate distributions for the hyperparameters. Thus p(x|y) is written as:", "where l(x) is the distribution of the hyperparameters when the score is lower than the threshold y* and g(x) is the distribution when the score is higher than y*.", "The Expected Improvement formula for TPE is then changed to:", "and after some math transformation, it becomes:", "The end formula means that to yield a high Expected Improvement, points with high probability under l(x) and low probability under g(x), should be chosen as the next hyperparameter. This meets our intuition that the next hyperparameter should come from the area under the threshold rather than the area above the threshold. To learn more about the TPE surrogate model, refer to this article or this article.", "In this article, I explained the Bayesian Optimization concept (hopefully) in a straightforward way. For those who want to learn more, below is the list of resources that I have found helpful:", "All right, that\u2019s it! Congratulations on following me all the way through here! I hope my article can somewhat rescue you from hours of juggling around the overwhelming terms and math associated with Bayesian Optimization. For questions and comments, let\u2019s connect on Linkedin and Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d2bcdeaf12f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wiw5087ad?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wiw5087ad?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Wei Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F186ea5a3d42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&user=Wei+Wang&userId=186ea5a3d42&source=post_page-186ea5a3d42----1d2bcdeaf12f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kazuend?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "kazuend"}, {"url": "https://unsplash.com/s/photos/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this paper"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this research paper"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this post"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this article"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "Algorithms for Hyper-Parameter Optimization"}, {"url": "https://www.youtube.com/watch?v=u6MG_UTwiIQ", "anchor_text": "Bayesian Methods for Machine Learning"}, {"url": "https://www.youtube.com/watch?v=C5nqEHpdyoE", "anchor_text": "Bayesian Optimization with extensions, applications, and other sundry items"}, {"url": "https://www.linkedin.com/in/wiw5087/", "anchor_text": "Linkedin"}, {"url": "https://twitter.com/WeiW5087", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d2bcdeaf12f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----1d2bcdeaf12f---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1d2bcdeaf12f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1d2bcdeaf12f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----1d2bcdeaf12f---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&user=Wei+Wang&userId=186ea5a3d42&source=-----1d2bcdeaf12f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&user=Wei+Wang&userId=186ea5a3d42&source=-----1d2bcdeaf12f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d2bcdeaf12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d2bcdeaf12f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d2bcdeaf12f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wiw5087ad?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wiw5087ad?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wei Wang"}, {"url": "https://medium.com/@wiw5087ad/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "119 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F186ea5a3d42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&user=Wei+Wang&userId=186ea5a3d42&source=post_page-186ea5a3d42--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F465bc60abf84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f&newsletterV3=186ea5a3d42&newsletterV3Id=465bc60abf84&user=Wei+Wang&userId=186ea5a3d42&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}