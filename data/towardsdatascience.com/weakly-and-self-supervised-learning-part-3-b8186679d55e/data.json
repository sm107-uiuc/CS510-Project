{"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-3-b8186679d55e", "time": 1683012548.52009, "path": "towardsdatascience.com/weakly-and-self-supervised-learning-part-3-b8186679d55e/", "webpage": {"metadata": {"title": "Weakly and Self-supervised Learning \u2014 Part 3 | by Andreas Maier | Towards Data Science", "h1": "Weakly and Self-supervised Learning \u2014 Part 3", "description": "In this tutorial, we introduce different ideas to generate labels for self-supervised learning. Ideas include spatial, textural, and temporal information."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-1-ddfdf8377f1d", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/EqMwbP7Smxg", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-4-2fbfd10280b3", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 21}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 21}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 21}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 21}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 21}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 21}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 21}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog", "paragraph_index": 21}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. Try it yourself! If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning! So today, we want to start talking about ideas that are called self-supervised learning. We want to obtain labels by self-supervision and will look into what this term actually means, what the core ideas are in the next couple of videos.", "So, this is part three of weakly and self-supervised learning. Today, we actually start talking about self-supervised learning. There are a couple of views around self-supervised learning and you can essentially split them into two parts. You can say, one is how to get the self-supervised labels and the other part is that you work on the losses in order to embed those labels. We have particular losses that are suited for the self-supervision. So, let\u2019s start with the definition. The motivation is you could say that classically people in machine learning believed that supervision is, of course, the approach that produces the best results. But, we have these massive amounts of labels that we need. So, you could actually very quickly then come to the conclusion that the AI revolution will not be supervised. This is very clearly visible in the following statement by Yann LeCun. \u201cMost of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.\u201d Of course, this is substantiated by observations in biology and how humans and animals learn.", "The idea of self-supervision is that you try to use information that you already have about your problem to come up with some surrogate labels that allow you to do training processes. The key ideas here on this slide by Yann LeCun can be summarized as follows: You try to predict the future from the past, you can predict the future also from the recent past, you predict the past from the present or the top from the bottom. Also, an option could be to predict the occluded from the visible. You pretend that there is a part of the input that you don\u2019t know and predict that. This essentially allows you to come up with a surrogate task. With the surrogate task, you can already perform training. The nice thing is you don\u2019t need any label at all because you intrinsically use the structure of the data.", "Essentially, self-supervised learning is an unsupervised learning approach. But every now and then, you need to make clear that you\u2019re doing something new in a domain that has been researched on for many decades. So, you may not refer to the term unsupervised anymore and Yann LeCun actually proposed the term self-supervised learning. He realized that unsupervised is a loaded and confusing term. So although the ideas have already been around before, the term self-supervised learning has been established. It makes sense to use this term to concentrate on a particular kind of unsupervised learning. So, you could say it\u2019s a subcategory of unsupervised learning. It uses pretext surrogates of pseudo tasks in a supervised fashion. This essentially means you can use all of the supervised learning methods and you have labels that are automatically generated. They can then be used as a measurement of correctness to create a loss in order to train your weights. The idea is then that this is beneficial for downstream tasks like retrieval, supervised, or semi-supervised classification, and so on. By the way in this kind of broad definition, you could also argue that generative models like generative adversarial networks are also some kind of self-supervised learning method. So essentially, Yann LeCun had this very nice idea to frame this kind of learning in a new way. If you do so, this is, of course, very helpful because you can make clear that you\u2019re doing something new and you\u2019re different from the many unsupervised learning approaches that have been out there for a very long time.", "So, let\u2019s look into some of these ideas. There are, of course, these pretext tasks and you can work with generation-based methods. So, you can use GANs, you can do things like super-resolution approaches. There, you downsample and try to predict the higher resolution image. You can do inpainting approaches or colorization. Of course, this also works with videos. You can work with context-based methods. Here, you try to solve things like the jigsaw puzzle or clustering. In semantic label-based methods, you can do things like trying to estimate moving objects or predict the relative depth. Then there\u2019s also cross-modal methods where you try to use information from more than one modality. You have a linked sensor system, let\u2019s say you have a depth camera and an RGB camera. Then, you can link the two and try to predict the one from the other. If you have an attached sensor, let\u2019s say you have a car, you\u2019re moving, and you have a GPS sensor or any other sensory system that will tell you how your car is moving, then you can try to predict the ego-motion from the actual video sequence.", "So let\u2019s look into this in a bit of more detail and look at image-based self-supervised learning techniques to refine representation learning, the first idea, the generative ones. You can, for example, do image colorization where it is very easy to generate labels.", "You start with color images, compute essentially the average over the channels that gives you a gray value image. Then, you try to predict the original color again. You can use a kind of scene and encoder/decoder approach in order to predict the correct color maps.", "Furthermore, you can also go into inpainting. You can occlude parts of the image and then try to predict those. This then essentially results in the task that you try to predict a complete image where you then compare to the actual full image to the prediction that was created by your generator. You can train these things for example in a GAN-type of loss setting. We have a discriminator that then tells you whether this was a good inpainting result or not.", "There are also ideas about spatial context. Here, a very common approach is to solve a jigsaw puzzle. You take a patch or actually, you take nine patches of the image and you essentially try to predict whether you have the center patch, let\u2019s say, for the face of the cat here, you have one. Then, you want to predict what is the ID of the patch that is shown. So, you put in two images and try to predict the correct location of the second patch. Notice that this is a bit tricky because there is a trivial solution possible. This happens if you have boundary patterns that are continuing. If you have continuing textures, then it may occur that the actual patch can very easily be detected in the next patch because the texture is continued. So, you should use large enough gaps in order to get around this problem. Color may be tricky too. You can use chromatic aberration and pre-process the images by shifting green and magenta towards gray, or you randomly drop two of the color channels in order to avoid that you\u2019re only learning about color.", "There\u2019s an improved version of the jigsaw puzzle. In jigsaw puzzle++, the idea is that you essentially randomize the order of the patches and you try to predict the correct location of each patch. Now, the cool thing about this is if you have 9 tiles then you have 9! possible permutations. This is more than 300-thousand. So, we can create plenty of labels for this task and you see that it\u2019s actually key that you do it in the right way. So, it\u2019s not just that you have a lot of permutations.", "You also want to make sure that there is an appropriate average Hamming distance. You can see if you actually obey this idea, then this makes a difference. If you have a too low average Hamming distance, the jigsaw task accuracy is not very high. If you increase it, the jigsaw task accuracy also increases. There\u2019s a very high likelihood that this high accuracy then will also go to the actual task of interest. So here, this is a detection task and with a high jigsaw task accuracy, you also build a better detector.", "Well, which other ideas could we be interested in? Of course, you can do similar things with rotation. Then, you try to predict the correct rotation of the image. This is also a very cheap label that you can generate.", "Let\u2019s look a bit into context similarity. Here, the idea is that you want to figure out whether this image is from the same or a different context. So, you can pick an input patch, then you augment it. You use different ways of augmentation like changes in color contrast, slight movement, and general pixel transformations: You can also add noise and this gives you essentially for every patch a large number of other patches that should show the same content. Now, you can repeat that with other patches and this allows you to build a large database. With those patches, you can then train whether it\u2019s the same patch or not, and you can then discriminate these several good classes and train your system similarly.", "A different approach is that you use clustering. This is work by my colleague Vincent Christlein. He was interested in building better features for writer identification. So, he started with detecting key points. The key points then allow you to extract patches. At the same time, the key points if detected with algorithms like SIFT, come with a feature descriptor. On the future descriptor, you can then perform clustering and the clusters that you get are probably already quite good training samples. So, you use the cluster-ID in order to train for example a ResNet for the prediction of the respective patch. This way, you can use a completely unlabeled data set, do the clustering, generate pseudo labels, and train your system. Vincent has shown that this actually gives quite a bit of performance in order to improve representation learning.", "This idea has then been developed further. You can do it in an alternating manner. This idea is called DeepCluster. Now, you take some input, you have a convnet, and you then essentially start from an untrained Network. You do clustering on the generated features and with the clustering, e.g. simply k-means, you can then generate pseudo labels that allow backpropagation and the training of the representation learning. Now, of course, if you start with random initialization, the clustering is probably not very good. So, you want to alternate between the classification and the clustering. Then, this allows you to build also a very powerful convolutional neural network. There are also some problems with trivial solutions that you want to avoid. So, you want to reassign empty clusters and, of course, you can use tricks like weighting the contribution of an input by the inverse of the size of its assigned cluster.", "You can even build on this idea further. This leads to self-labeling with optimal transport in [24]. Here, they essentially further developed DeepCluster. Instead of using clustering, they\u2019re using the Sinkhorn-Knopp algorithm in order to determine pseudo labels. The idea here is that you try to predict the optimal transport. Here, we can see this example of an optimal transport problem. Let\u2019s say, you have supplies in Warehouse A and Warehouse B. Each of them have 25 laptops and you have a need in each of Shop 1 and Shop 2 of 25 laptops. Then, you can see that you want to ship those laptops to the respective shops. Of course, you take the closest one and all of the laptops from Warehouse A, in this case, go to Shop 1 and all of the laptops of Warehouse B go to Shop 2.", "The nice thing about this algorithm is that you can find a linear version of this. So, you can essentially express all of this with linear algebra and then this means that you can also embed it into a neural network. If you compare DeepCluser to the optimal transport, then you may want to keep in mind if you don\u2019t have a separate clustering loss, this can lead to degenerate solutions. Also, keep in mind that the clustering approach minimizes the same cross-entropy loss that the network also seeks to optimize.", "Well, there\u2019s a couple of more ideas. We know these recipes like multi-task learning. You can also do multi-task learning for self-supervised learning. An example here is that you use synthetic imagery. So, you have some synthetic images where you can generate the depth, the surface normal, or also the contours. Then, you can use those as labels in order to train your network and produce a good representation. Additionally, you can also minimize the feature space domain differences between real and synthetic data in a kind of GAN setup. This leads also to very good representation learning.", "Next time, we want to talk about ideas on how to work with the losses and make them more suited towards the self-supervised learning task. We will see that in particular, the contrastive losses are very useful for this. So, thank you very much for listening and see you in the next video. Bye-bye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced. If you are interested in generating transcripts from video lectures try AutoBlog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8186679d55e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akmaier.medium.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----b8186679d55e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!"}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-1-ddfdf8377f1d", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/EqMwbP7Smxg", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-4-2fbfd10280b3", "anchor_text": "Next Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/h8jWocE_3tI", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/rbZ8ck_1nZk", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/h8jWocE_3tI", "anchor_text": "YouTube"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b8186679d55e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b8186679d55e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b8186679d55e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b8186679d55e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----b8186679d55e---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&user=Andreas+Maier&userId=b1444918afee&source=-----b8186679d55e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&user=Andreas+Maier&userId=b1444918afee&source=-----b8186679d55e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb8186679d55e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b8186679d55e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b8186679d55e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8186679d55e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8186679d55e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b8186679d55e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b8186679d55e--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-3-b8186679d55e&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}