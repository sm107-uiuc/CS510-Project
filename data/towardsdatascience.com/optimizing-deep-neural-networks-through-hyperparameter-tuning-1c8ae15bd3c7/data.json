{"url": "https://towardsdatascience.com/optimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7", "time": 1683016224.7704918, "path": "towardsdatascience.com/optimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7/", "webpage": {"metadata": {"title": "Optimizing Deep Learning Algorithms through Hyperparameter Tuning | by Hasara Samson | Towards Data Science", "h1": "Optimizing Deep Learning Algorithms through Hyperparameter Tuning", "description": "Before getting into tunning and optimizing Machine Learning algorithms, let\u2019s see what Hyperparameters really are. Hyperparameters are a set of parameters that determine how the neural network is\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Before getting into tunning and optimizing Machine Learning algorithms, let\u2019s see what Hyperparameters really are. Hyperparameters are a set of parameters that determine how the neural network is trained and the structure of the neural network. Basically, hyperparameters play a vital role in deciding whether the neural network you trained is applicable to the problem you are trying to solve or not.", "In practice, Applied Machine Learning is a highly iterative process, in simpler words a trial and error process. Even very experienced Deep Learning practitioners fail to get these \u201cHyperparameters\u201d right the first time. The process is to turn your idea into code, experiment, and evaluate the outcome and make changes and repeat the process until you get the expected or otherwise a satisfactory outcome.", "Let\u2019s take a few of the hyperparameters one by one and go into details.", "Learning rate is a hyperparameter with a small positive value (often range between 0.0 to 1.0) which controls how quickly the model adapts to the problem, in other words learning rates decides how far the weights should be in the direction of gradient to meet the global minimum.", "If the learning rate is too low (figure 1) training will progress very slowly. If the learning rate is set too high as it is shown in figure 3, training may not converge at the global minimum instead overshoots it and continue to make the loss of the model even worse. Getting the right learning rate at once is not possible (unless you are extremely experienced and lucky at the same time). As it is with all the hyperparameters it is a trial and error scenario. However, a decaying learning rate( will decrease as the training progresses) is preferred over a fixed learning rate. Decay of learning rate can be a time-based decay, step decay, or exponential decay.", "Batch size can be defined as the number of data (samples) that will be trained by the Neural Network at one go. Let\u2019s assume you have 1000 samples with you that need to be trained. You can either propagate all the 1000 samples through the Neural Network at once or you can propagate them in mini-batches, let\u2019s say 100 at one go. Neural Network will take the first 100 samples initially and train the network and then the next 100 samples and we can let the neural network continue this until it is propagated by all the samples. The advantages of using mini-batches are that they require less memory and training is faster than with the full batch.", "The number of epochs is the number of times the learning algorithm will go through the entire training dataset. One might think as to a higher number of epochs will result in a more accurate model which is not always true.", "As you can see from the above diagram, as the number of epochs increases both the training error and validation error decreases, but only up to a certain point. After that validation error keeps increasing due to the overfitting. Therefore, in order for the model to perform well in the face of new data, training should come to an end at the point where both training and validation error is at the lowest and the number of epochs at that point is the ideal number of epochs. In libraries such as TensorFlow, we have the option to write code in such a way that when the validation error reaches a certain expected value, training will be terminated.", "These hyperparameters heavily depend on the problem dimensions they are applied to. Hidden layers are the layers in between the input layer and the output layer of the neural network. For a simpler problem, a few hidden layers with few hidden units will be sufficient but the more complex the problem gets, the more hidden layers and hidden units will be needed. The rule of thumb in practice is to keep the number of hidden units between the units in the input and output layer. However, it should be kept in mind that an unwanted higher number of hidden layers and hidden units will not only be a waste of computational power but also will result in overfitting which will decrease the accuracy of the neural network.", "Dropout is a regularization technique used to decrease the validation error, in other words, to avoid overfitting. Simply put, dropout means randomly ignoring neurons (units) and by doing so those will be ignored in the forward and backpropagation of the neural network. During the training, neurons start to co-dependent on each other which restricts harnessing the power of an independent neuron and dropping out several neurons randomly we are able to prevent this.", "Dropouts are given as percentages Let\u2019s you have given 20% as the dropout value, 1 in every 5 neurons will be randomly dropped and ignored in the training. Dropouts increase the iterations that need to be performed in order to converge at the global minimum but the training time for each iteration will be reduced.", "Activation functions are basically mathematical equations that used to introduce non-linearity to the neural network. Now let\u2019s get into the math,", "As you can see activation functions literally decide how the outputs from each neuron should behave and therefore the right activation function should be selected with care based on the problem you are going to apply the trained model into. Sigmoid, ReLu, and tanh are some of the widely used activation functions. Each and every activation function has its strengths and flaws. To get an even deeper understanding of the activation functions refer to my article linked below.", "As I have mentioned at the beginning of this article hyperparameter tuning is traditionally done manually which is an iterative process. However, there is ongoing research on optimizing algorithms. Grid Search, Random Search, and Bayesian Optimization are a few of the widely known optimization algorithms, however, these algorithms can be computationally expensive and have pros and cons of its own which is out of the scope of this article. (Maybe a topic for another day!).", "Thank you for reading and hope I could be of help.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A tech-savvy IT undergraduate with a passion for Deep Learning, Programming and Volunteering."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1c8ae15bd3c7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hasarasamson?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hasarasamson?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Hasara Samson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4650abf8e2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&user=Hasara+Samson&userId=4650abf8e2e2&source=post_page-4650abf8e2e2----1c8ae15bd3c7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dnevozhai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Denys Nevozhai"}, {"url": "https://unsplash.com/s/photos/network?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/getting-to-know-activation-functions-in-neural-networks-125405b67428", "anchor_text": "Getting to know Activation Functions in Neural Networks.What are activation functions and why should you know about them?towardsdatascience.com"}, {"url": "https://missinglink.ai/guides/neural-network-concepts/hyperparameters-optimization-methods-and-real-world-model-management/", "anchor_text": "Hyperparameters: Optimization Methods and Real World Model Management."}, {"url": "https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/", "anchor_text": "Improving Neural Networks: Hyperparameter Tuning, Regularization, and more."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1c8ae15bd3c7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1c8ae15bd3c7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----1c8ae15bd3c7---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1c8ae15bd3c7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----1c8ae15bd3c7---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&user=Hasara+Samson&userId=4650abf8e2e2&source=-----1c8ae15bd3c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&user=Hasara+Samson&userId=4650abf8e2e2&source=-----1c8ae15bd3c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1c8ae15bd3c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1c8ae15bd3c7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1c8ae15bd3c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hasarasamson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hasarasamson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hasara Samson"}, {"url": "https://medium.com/@hasarasamson/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "155 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4650abf8e2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&user=Hasara+Samson&userId=4650abf8e2e2&source=post_page-4650abf8e2e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F448956ad763b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-deep-neural-networks-through-hyperparameter-tuning-1c8ae15bd3c7&newsletterV3=4650abf8e2e2&newsletterV3Id=448956ad763b&user=Hasara+Samson&userId=4650abf8e2e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}