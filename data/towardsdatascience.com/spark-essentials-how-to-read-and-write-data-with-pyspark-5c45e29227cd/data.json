{"url": "https://towardsdatascience.com/spark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd", "time": 1683017399.243606, "path": "towardsdatascience.com/spark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd/", "webpage": {"metadata": {"title": "Apache Spark Tutorial - Beginners Guide to Read and Write data using PySpark | Towards Data Science", "h1": "Apache Spark Tutorial\u2014 How to Read and Write Data With PySpark", "description": "A PySpark cheat sheet for novice Data Engineers"}, "outgoing_paragraph_urls": [{"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet", "paragraph_index": 33}, {"url": "https://delta.io/", "anchor_text": "Delta Lake", "paragraph_index": 41}, {"url": "https://docs.databricks.com/data/tables.html#managed-and-unmanaged-tables", "anchor_text": "unmanaged table", "paragraph_index": 47}, {"url": "https://bit.ly/3frXGhR", "anchor_text": "interesting Spark end-end tutorial", "paragraph_index": 51}], "all_paragraphs": ["Buddy is a novice Data Engineer who has recently come across Spark, a popular big data processing framework.", "Considering the fact that Spark is being seamlessly integrated with cloud data platforms like Azure, AWS, and GCP Buddy has now realized its existential certainty. This has driven Buddy to jump-start his Spark journey, by tackling the most trivial exercise in a big data processing life cycle - \u201cReading and Writing Data\u201d", "Inundated with work Buddy and his impatient mind unanimously decided to take the shortcut with the following cheat sheet using Python.", "In hindsight, Buddy deems that it is imperative to come to terms with his impatient mind. The shortcut has proven to be effective, but a vast amount of time is being spent on solving minor errors and handling obscure behavior.", "It is time to tackle the details.", "Reading and writing data in Spark is a trivial task, more often than not it is the outset for any form of Big data processing. Buddy wants to know the core syntax for reading and writing data before moving onto specifics.", "DataFrameReader is the foundation for reading data in Spark, it can be accessed via the attribute spark.read", "Read Modes \u2014 Often while reading data from external sources we encounter corrupt data, read modes instruct Spark to handle corrupt data in a specific way.", "There are 3 typical read modes and the default read mode is permissive.", "The foundation for writing data in Spark is the DataFrameWriter, which is accessed per-DataFrame using the attribute dataFrame.write", "Save modes \u2014 specifies what will happen if Spark finds data already at the destination.", "There are 4 typical save modes and the default mode is errorIfExists", "That\u2019s a great primer! Buddy seems to now understand the reasoning behind the errors that have been tormenting him. He would like to expand on this knowledge by diving into some of the frequently encountered file types and how to handle them.", "To read a CSV file you must first create a DataFrameReader and set a number of options.", "Here we load a CSV file and tell Spark that the file contains a header row. This step is guaranteed to trigger a Spark job.", "Spark job: block of parallel computation that executes some task.", "A job is triggered every time we are physically required to touch the data. In this case, the DataFrameReader has to peek at the first line of the file to figure out how many columns of data we have in the file.", "When reading data you always need to consider the overhead of datatypes. There are two ways to handle this in Spark, InferSchema or user-defined schema.", "inferSchema option tells the reader to infer data types from the source file. This results in an additional pass over the file resulting in two Spark jobs being triggered. It is an expensive operation because Spark must automatically go through the CSV file and infer the schema for each column.", "The preferred option while reading any file would be to enforce a custom schema, this ensures that the data types are consistent and avoids any unexpected behavior.", "In order to do that you first declare the schema to be enforced, and then read the data by setting schema option.", "As a result of pre-defining the schema for your data, you avoid triggering any jobs. Spark did not see the need to peek into the file since we took care of the schema. This is known as lazy evaluation which is a crucial optimization technique in Spark.", "Writing data in Spark is fairly simple, as we defined in the core syntax to write out data we need a dataFrame with actual data in it, through which we can access the DataFrameWriter.", "Here we write the contents of the data frame into a CSV file. Setting the write mode to overwrite will completely overwrite any data that already exists in the destination.", "What you expect as a result of the previous command is a single CSV file output, however, you would see that the file you intended to write is in fact a folder with numerous files within it. This is further confirmed by peeking into the contents of outputPath.", "This is an important aspect of Spark distributed engine and it reflects the number of partitions in our dataFrame at the time we write it out. The number of files generated would be different if we had repartitioned the dataFrame before writing it out.", "Partitioning simply means dividing a large data set into smaller chunks(partitions). In Spark they are the basic units of parallelism and it allows you to control where data is stored as you write it.", "Reading JSON isn\u2019t that much different from reading CSV files, you can either read using inferSchema or by defining your own schema.", "Here we read the JSON file by asking Spark to infer the schema, we only need one job even while inferring the schema because there is no header in JSON. The column names are extracted from the JSON object\u2019s attributes.", "To maintain consistency we can always define a schema to be applied to the JSON data being read.", "Remember that JSON files can be nested and for a small file manually creating the schema may not be worth the effort, but for a larger file, it is a better option as opposed to the really long and expensive schema-infer process.", "As you would expect writing to a JSON file is identical to a CSV file.", "Again, as with writing to a CSV, the dataset is split into many files reflecting the number of partitions in the dataFrame.", "Apache Parquet is a columnar storage format, free and open-source which provides efficient data compression and plays a pivotal role in Spark Big Data processing.", "Unlike CSV and JSON files, Parquet \u201cfile\u201d is actually a collection of files the bulk of it containing the actual data and a few files that comprise meta-data.", "To read a parquet file we can use a variation of the syntax as shown below both of which perform the same action.", "As you notice we don\u2019t need to specify any kind of schema, the column names and data types are stored in the parquet files themselves.", "The schema inference process is not as expensive as it is for CSV and JSON, since the Parquet reader needs to process only the small-sized meta-data files to implicitly infer the schema rather than the whole file.", "Writing Parquet is as easy as reading it. Simply specify the location for the file to be written.", "The same partitioning rules we defined for CSV and JSON applies here.", "Buddy has never heard of this before, seems like a fairly new concept; deserves a bit of background.", "Delta Lake is a project initiated by Databricks, which is now opensource. Delta lake is an open-source storage layer that helps you build a data lake comprised of one or more tables in Delta Lake format.", "It is an open format based on Parquet that brings ACID transactions into a data lake and other handy features that aim at improving the reliability, quality, and performance of existing data lakes.", "In order to understand how to read from Delta format, it would make sense to first create a delta file.", "In order to create a delta file, you must have a dataFrame with some data to be written. Once you have that, creating a delta is as easy as changing the file type while performing a write. Instead of parquet simply say delta.", "If Delta files already exist you can directly run queries using Spark SQL on the directory of delta using the following syntax:", "In most cases, you would want to create a table using delta files and operate on it using SQL. The notation is : CREATE TABLE USING DELTA LOCATION", "This is called an unmanaged table in Spark SQL. It now serves as an interface between Spark and the data in the storage layer. Any changes made to this table will be reflected in the files and vice-versa. Once the table is created you can query it like any SQL table.", "Apart from writing a dataFrame as delta format, we can perform other batch operations like Append and Merge on delta tables, some of the trivial operations in big data processing pipelines.", "The details coupled with the cheat sheet has helped Buddy circumvent all the problems.", "Spark can do a lot more, and we know that Buddy is not going to stop there!", "If you are looking to serve ML models using Spark here is an interesting Spark end-end tutorial that I found quite insightful. Give it a thumbs up if you like it too!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5c45e29227cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@xavier211192?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@xavier211192?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Prashanth Xavier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5722e3152ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&user=Prashanth+Xavier&userId=5722e3152ce5&source=post_page-5722e3152ce5----5c45e29227cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@krisroller?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kristopher Roller"}, {"url": "https://unsplash.com/s/photos/spark?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/xavier211192/Databricks/blob/main/Spark%20Read_Write%20Cheat%20Sheet.pdf", "anchor_text": "Apache Spark Cheat Sheet"}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet"}, {"url": "https://delta.io/", "anchor_text": "Delta Lake"}, {"url": "https://docs.databricks.com/data/tables.html#managed-and-unmanaged-tables", "anchor_text": "unmanaged table"}, {"url": "https://bit.ly/3frXGhR", "anchor_text": "interesting Spark end-end tutorial"}, {"url": "https://databricks.com/spark/getting-started-with-apache-spark", "anchor_text": "https://databricks.com/spark/getting-started-with-apache-spark"}, {"url": "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html", "anchor_text": "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"}, {"url": "https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/", "anchor_text": "https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/"}, {"url": "https://medium.com/tag/programming?source=post_page-----5c45e29227cd---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----5c45e29227cd---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/python?source=post_page-----5c45e29227cd---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/big-data?source=post_page-----5c45e29227cd---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5c45e29227cd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&user=Prashanth+Xavier&userId=5722e3152ce5&source=-----5c45e29227cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&user=Prashanth+Xavier&userId=5722e3152ce5&source=-----5c45e29227cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5c45e29227cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5c45e29227cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5c45e29227cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5c45e29227cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5c45e29227cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@xavier211192?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@xavier211192?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prashanth Xavier"}, {"url": "https://medium.com/@xavier211192/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "288 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5722e3152ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&user=Prashanth+Xavier&userId=5722e3152ce5&source=post_page-5722e3152ce5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38845c48d898&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd&newsletterV3=5722e3152ce5&newsletterV3Id=38845c48d898&user=Prashanth+Xavier&userId=5722e3152ce5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}