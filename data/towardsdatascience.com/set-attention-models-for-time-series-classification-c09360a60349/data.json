{"url": "https://towardsdatascience.com/set-attention-models-for-time-series-classification-c09360a60349", "time": 1683013813.520271, "path": "towardsdatascience.com/set-attention-models-for-time-series-classification-c09360a60349/", "webpage": {"metadata": {"title": "Set Attention Models for Time Series Classification | by Michael Larionov, PhD | Towards Data Science", "h1": "Set Attention Models for Time Series Classification", "description": "As a data scientist working primarily with business data (sometimes also called \u201ctabular data\u201d), I\u2019m always looking for latest development in the areas of data science that helps work with the more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/graph-neural-network-and-permutation-invariance-979754a08178", "anchor_text": "another blog post", "paragraph_index": 0}, {"url": "https://icml.cc/Conferences/2020", "anchor_text": "ICML 2020", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need", "paragraph_index": 9}, {"url": "https://github.com/BorgwardtLab/Set_Functions_for_Time_Series", "anchor_text": "github repo", "paragraph_index": 24}], "all_paragraphs": ["As a data scientist working primarily with business data (sometimes also called \u201ctabular data\u201d), I\u2019m always looking for latest development in the areas of data science that helps work with the more realistic data. One of this area addresses the fact that business data are rarely \u201ctabular\u201d, but usually are relational in nature. I already discussed working with relational data in another blog post. Deep sets algorithms help you to learn from the data that do not have rectangular shape, but can be represented as a collection of tables or as a graph. So while attending ICML 2020 conference this July I paid special attention to the papers that use deep set learning.", "One of the papers is Set Functions for Time Series by Max Horn and others [1]:", "This is, in my opinion, the best paper of the conference. As the title says, this paper is about learning set functions for time series. Before going any further, let me remind you what is set function.", "Set Function is a function on a set of data that can contain zero to infinity elements. Examples of set function are count(), min(), max(), sum(), mean(), std(), etc. In SQL we call them aggregate function. Elements of the set can be also complex, for example, represent a row of data. Set functions are different from the tabular functions that are common in machine learning. Tabular functions, in contrast, take a row of fixed number of variables as a parameter. The way to deal with a potentially large set is to represent the set function using two auxiliary functions and an aggregate function [2,3]:", "The function h() is applied to an individual element of the set (which is assumed to be a row of data) and maps the elements of the set to a high-dimensional vector space. An aggregate function groups all such vectors of a set into one vector, thus dealing with indefinite number of elements. A new function g() then maps the vector to a final result. Here an aggregate function is mean(), but other aggregate functions can be used, for example, sum(). Functions f() and g() can be trained as neural networks, and the entire model is trained on all available sets.", "Classical time series analysis always assumes that the measurements are taken at fixed intervals, and if several measurements are taken they are aligned. Occasional missing measurements are filled in using imputation techniques.", "In reality, you can have time series are often misaligned and measurements are taken at irregular intervals. A good example of this are Electronic Medical Records (EMR) and other types of medical data. Take a look at this chart from the paper\u2019s poster:", "The four measurements are taken at a different frequency and irregular times. Yes, you can still use imputation, but since here you need to impute a lot of data, the variance of the prediction would be really high.", "The main idea in [1] is to treat the time series as a set. If you do so you can use set function learning algorithms without having to impute any data. The entire time series is a set of tuples (t, z, m), where t is time, z is the measured value, and m is modality. In our case m takes values of blood pressure, heart rate, temperature and glucose. Note that you still have time as a variable, but the model loses explicit dependency of the next measurement on the previous.", "This paradigm shift is similar to the Transformer revolution that was unleashed by the paper Attention is All You Need [4]. There as well the explicit sequence modeling as a recurrent neural network was replaced with representing the sequence as a set and learning using the attention mechanism.", "If you look again at the equation at the beginning of the article, you will see that all elements of the set contribute equally to the result. In reality this may not be the case. For example, a sudden change in blood pressure is a better indicator of the onset of sepsis than an average blood pressure over 6 hours. As I already mentioned, the aggregate function does not have to be sum() or mean(), but in fact can be any function that takes a set of vectors and return a single vector. You can try to handcraft the aggregate function, but would not it be cool if a neural network can learn this function?", "Again, taking an inspiration from [4], the authors introduced a set attention as the weights that determine how much every element of the set contributes to the overall result.", "Thus, instead of the normal set function you have a weighted set function:", "The weights \u03b1(S, s_j) depend on the entire set as well as on a specific set element. The full model is schematically represented using this slide:", "There are several attention heads that are represented with index i. Index j represent an element in a set. The Keys matrix is represented as:", "Here f \u2019(S) is the second set function that is using mean as an aggregate function. This means that there are two more neural networks that are added to learn f \u2019(). The resulting vector is concatenated with s_j and multiplied by a weight matrix W. Note that dimensionality of W does not depend on the number of elements in the set and is defined by the neural network architecture.", "Here d is the dimension of the latent layer. The Query matrix also has defined dimension:", "And, like in [4], the weights are defined using softmax:", "Full neural network architecture diagram is below:", "Another thing inspired by [4] is positional encoding of time.", "Those of us, working on the tabular data, are accustomed to extracting features from the date/time variables. Usually we extract date/time components, such as hour of the day, day of the week, etc. and then apply periodic functions such as sin() and cos() to ensure smoothness. When the authors of [4] were working on the Transformer arhitecture, they decided to use this technique to encode the word position. This idea was brought without much change to [1]. The time is encoded as follows:", "Here bold t is the maximum time scale (in reality, a hyperparameter) and k are integer numbers", "You can easily visualize this using a simple python script.", "As you see, there are several periodic functions of time with several values of frequency. I believe the positional encoding is one of the major factor of success of both [4] and [1]. However, when dealing with real time rather than position of the word, it the encoding is very sensitive to time scaling, because time is a continuous variable as opposed to the position in NLP. It also adds additional hyperparameters to tune for the model. While it works great for NLP, it is not discussed whether other types of encoding will work better for time series. It reminds me somewhat of manual convolution filters pre-LeNet computer vision systems. Is it possible to enhance this neural network architecture to get it to learn time encoding automatically rather than setting it manually? Perhaps 1-dimensional convolution will do the trick.", "Set functions provides a fresh view on the time series data, allowing to treat them as independent observations, which simplifies the data preprocessing for the case of irregular and unaligned measurements. The attention mechanism allows the model to learn the observations that are critical to outcome (thus improving the model interpretability). The experimental studies shows that algorithm is competitive to the state of the art, while is much faster and more interpretable. All the code is available in the authors github repo.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc09360a60349&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@michaellarionov", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaellarionov.medium.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43----c09360a60349---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/icml-2020", "anchor_text": "ICML2020"}, {"url": "https://unsplash.com/@fabrizioverrecchia?utm_source=medium&utm_medium=referral", "anchor_text": "Fabrizio Verrecchia"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/graph-neural-network-and-permutation-invariance-979754a08178", "anchor_text": "another blog post"}, {"url": "https://icml.cc/Conferences/2020", "anchor_text": "ICML 2020"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "Set Functions for Time SeriesDespite the eminent successes of deep neural networks, many architectures are often hard to transfer to\u2026arxiv.org"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://slideslive.com/38928275/set-functions-for-time-series", "anchor_text": "https://slideslive.com/38928275/set-functions-for-time-series"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://slideslive.com/38928275/set-functions-for-time-series", "anchor_text": "https://slideslive.com/38928275/set-functions-for-time-series"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "https://arxiv.org/abs/1909.12064"}, {"url": "https://github.com/BorgwardtLab/Set_Functions_for_Time_Series", "anchor_text": "github repo"}, {"url": "https://arxiv.org/abs/1909.12064", "anchor_text": "arXiv:1909.12064"}, {"url": "https://arxiv.org/abs/1703.06114", "anchor_text": "arXiv:1703.06114"}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "arXiv:1806.01261"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "arXiv:1706.03762"}, {"url": "https://medium.com/tag/icml-2020?source=post_page-----c09360a60349---------------icml_2020-----------------", "anchor_text": "Icml 2020"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c09360a60349---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c09360a60349---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/time-series-model?source=post_page-----c09360a60349---------------time_series_model-----------------", "anchor_text": "Time Series Model"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c09360a60349---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----c09360a60349---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----c09360a60349---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc09360a60349&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c09360a60349---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c09360a60349--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c09360a60349--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c09360a60349--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c09360a60349--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c09360a60349--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://michaellarionov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcf000f0c5fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fset-attention-models-for-time-series-classification-c09360a60349&newsletterV3=f9158ca11a43&newsletterV3Id=cf000f0c5fdd&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}