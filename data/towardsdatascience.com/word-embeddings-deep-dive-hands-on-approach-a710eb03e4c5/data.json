{"url": "https://towardsdatascience.com/word-embeddings-deep-dive-hands-on-approach-a710eb03e4c5", "time": 1683013698.286227, "path": "towardsdatascience.com/word-embeddings-deep-dive-hands-on-approach-a710eb03e4c5/", "webpage": {"metadata": {"title": "Word Embeddings Deep Dive \u2014 A hands-on approach | by Sharvil | Towards Data Science", "h1": "Word Embeddings Deep Dive \u2014 A hands-on approach", "description": "I\u2019m sure most of you would stumble sooner or later on the term \u201cWord Embeddings\u201d as you progress with your Natural Language Processing journey. Word Embeddings has become one of the most significant\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing", "paragraph_index": 0}, {"url": "https://github.com/SharvilN/Word-Embeddings", "anchor_text": "Github", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "language modeling", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Feature_learning", "anchor_text": "feature learning", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Vector_(mathematics)", "anchor_text": "vectors", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Real_numbers", "anchor_text": "real number", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis#:~:text=Latent%20semantic%20analysis%20(LSA)%20is,to%20the%20documents%20and%20terms.", "anchor_text": "Latent semantic analysis", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Efficient Estimation of Word Representations in Vector Space", "paragraph_index": 11}, {"url": "https://groups.google.com/g/word2vec-toolkit/c/LNPeC5gyhmQ/m/p8683JkD6LoJ?pli=1", "anchor_text": "here", "paragraph_index": 17}, {"url": "http://mattmahoney.net/dc/textdata.html", "anchor_text": "text8", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/1607.04606.pdf", "anchor_text": "paper", "paragraph_index": 26}, {"url": "http://projector.tensorflow.org/", "anchor_text": "http://projector.tensorflow.org/", "paragraph_index": 32}, {"url": "https://github.com/SharvilN/Word-Embeddings", "anchor_text": "Github", "paragraph_index": 34}], "all_paragraphs": ["I\u2019m sure most of you would stumble sooner or later on the term \u201cWord Embeddings\u201d as you progress with your Natural Language Processing journey. Word Embeddings has become one of the most significant building blocks for today\u2019s state-of-the-art language models. It\u2019s crucial that we understand what they represent, how they are computed under the hood, and what sets them apart. So let\u2019s begin by understanding what it really means and layout the characteristics and features behind its extensive usage and popularity in the NLP community. Starting with the basic foundations of word embeddings, we\u2019ll gradually explore the depths as we advance through the article. The full code shared in this article is available on Github.", "Let\u2019s take a look at what Wikipedia has to say about word embeddings \u2014", "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.", "In other words \u2014 word embeddings are vectorized, fixed-length, distributed, dense representations of words that interpret a word\u2019s textual meaning by mapping it to a vector of real values. I know that\u2019s a lot to take in all at once. We\u2019ll break the definition into parts and focus on one part at a time.", "Word embeddings are fixed-length vectors \u2014 meaning all the words in our vocabulary would be represented by a vector(of real numbers) of a fixed predefined size that we decide on. Word embeddings of size 3 would look like: \u2018cold\u2019 \u2014 [0.2, 0.5, 0.08], \u2018house\u2019 \u2014 [0.05, 0.1, 0.8]", "Distributed representations \u2014 Word embeddings are based on the distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings. Distributed representations try to comprehend a word\u2019s meaning by considering the company it keeps (context words).", "Dense representations \u2014 this is one of the most prominent features of word embeddings that made them so popular. Traditionally, One Hot Encoding was being used for mapping a word to numerical values. One hot encoding is the process of converting vocabulary words into binary vectors. If vocabulary size is 5 with the words \u2014 {cat, food, house, milk, water}, the cat will be encoded as a binary vector of [1, 0, 0, 0, 0] and milk would be [0, 0, 0, 1, 0] and so on.", "As you might have noticed already, we\u2019re only setting one element using the word index of the entire vector. As the vocabulary size increases, we\u2019d end up using an extensive length sparse vector for encoding a single word which would result in performance and storage penalties because of the curse of dimensionality. In addition to that, such representation is incapable of learning semantic relationships between words which is of essential importance when dealing with textual data.", "To overcome the limitations of one-hot encoding, traditional information retrieval methods have also been tried and implemented with the hope of combating the curse of dimensionality \u2014 TF-IDF, Latent semantic analysis, etc. TF-IDF, LSA both use a document-centric approach making them limited to a subclass of NLP problems. These techniques still can\u2019t effectively capture a word\u2019s meaning in a dense representation.", "Word embeddings eliminate all the above shortcomings and equip us with enriched powerful representations that are capable of capturing contextual and semantic similarities between words.", "Now that we\u2019ve taken a look behind the idea and motivation for word embeddings, we\u2019ll attempt to contemplate one of the most significant and widely used algorithms for learning word embeddings \u2014 Word2Vec. We\u2019ll go through it in detail and define its salient features and characteristics followed by a thorough implementation using tensorflow.", "Word2Vec is a predication based algorithm, for generating word embeddings, which was originally proposed at Google by Mikolov et al. For a deeper understanding of concepts involved I\u2019d suggest you dig into their research paper \u2014 Efficient Estimation of Word Representations in Vector Space", "It proposes two new novel architectures for learning distributed and dense representations of words:", "In the CBOW model, we try to predict the distributed representation of the target word (middle word) from the context words (surrounding words) which lie on either side of the target word within the context window (whose size is configurable but usually is set to 5). For example, \u201cPack my box with five dozen liquor jugs\u201d, a context window of size 2 would have the following pairs \u2014 (context_window, target_word) \u2014 ([five, liquor], dozen), ([dozen, jugs], liquor) and so on.", "The Skip-gram model is similar to the CBOW model, but instead of predicting the current word given the context, it tries to predict the context words from the current word. For example, \u201cPack my box with five dozen liquor jugs\u201d, a context window of size 2 would have the following pairs \u2014 (current_word, context_window) \u2014 (dozen, [five, liquor]), (liquor, [dozen, jugs]) and so on.", "Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words", "CBOW learns to find the word with maximum probability in its context. The context words are averaged and fed to the network to predict the most probable word. For example, \u201cthe cat ate the mouse\u201d \u2014 the network would try to predict \u201cate\u201d from averaged input of \u201cthe cat the mouse\u201d. In this context and other relevant contexts, overtime the model would smooth itself to predict frequent words like ate and would give much less attention to \u201cgobbled\u201d since it would rarely occur. Because of this reason, the quality of their distributed representations suffers.", "Skip gram, on the other hand, learns to predict context words using the target word. Instead of averaging the input context words \u2014 each pair can be separately used to feed the model to predict the other word in that pair. Predicting \u201cthe\u201d from \u201cate\u201d, \u201cate\u201d from \u201cmouse\u201d, etc. Such behavior of training won\u2019t enforce competition between \u201cate\u201d and \u201cgobble\u201d since both would be used in their respective contexts to predict context words. A detailed discussion on the topic can be found here.", "We\u2019re going to implement word2vec algorithm using Skip-gram architecture coupled with negative sampling (will be explained later in the article). So, let\u2019s dive straight into implementation!", "We\u2019re going to use the text8 dataset for the purpose of this article. Text8 is the first 100,000,000 bytes of plain text from Wikipedia. It\u2019s mainly used for testing purposes. Let\u2019s start with loading data:", "Stopwords removal \u2014 We begin with removing stopwords as they bring little to no value for our task of learning word embeddings.", "Subsampling words \u2014 In a large corpus, most frequent words can easily occur hundreds of millions of times and such words usually don\u2019t bring much information to the table. It is of essential importance to cut down on their frequencies to mitigate the negative impact it adds. For example, co-occurrences of \u201cEnglish\u201d and \u201cSpanish\u201d benefit much more than co-occurrences of \u201cEnglish\u201d and \u201cthe\u201d or \u201cSpanish\u201d and \u201cof\u201d. To counter the imbalance between rare and frequent words Mikolov et. al came up with the following heuristic formula for determining probability to drop a particular word:", "where t is threshold value (heuristically set to 1e-5) and f(w) is the frequency of the word.", "Filtering words \u2014 Frequency of words tell us a lot about their importance and significance for our model. Words occurring only once can\u2019t really be represented correctly because of the lack of context words associated with it. To preclude such noise (words) from our data (as we don\u2019t have much information about their whereabouts), we\u2019re keeping words occurring at least five times in our data.", "Generating skipgrams \u2014 First, we tokenize our pre-processed textual data and convert them into corresponding vectorized tokens. After that, we make use of the skipgrams library offered by Keras for generating (word, context) pairs. As it\u2019s description reads:", "Generates skip-gram word pairs. It transforms a sequence of word indexes (list of integers) into tuples of words of the form:", "Negative Sampling \u2014 For every input, we give to the network, we train it using the output from the softmax layer. That means for each input, we\u2019re making very small changes to millions of weights even though we only have one true example. This makes training the network very inefficient and unfeasible. The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. The following snippet generates pairs of (target, context) words also known as skipgrams, and for each input(target, context) pair we also randomly sample a negative (target, ~context) pair. For further reading, refer to this paper by Mikolov et. al", "Now let\u2019s build the model by using the model subclassing method. In the majority of the cases, Sequential and Functional APIs are more appropriate, but you can still use model subclassing if you\u2019re more of an object-oriented developer.", "For each training input, we have a pair of words (target word, context word) that we feed into the model and output as binary value labels (0 or 1) to indicate whether the input tuple is a negative sample or a true sample. Both the input words are fed to the embedding layer to generate encoded representations of size equal to the embedding dimension. The crucial point to note here is we\u2019re sharing the embedding layer between both the inputs.", "These dense encoded vectors are then multiplied element-wise to construct a merged representation. Which in turn is goes through dense and dropout layers before it finally tries to predict the positive or negative nature of input sample.", "With the model created now, we can jump right ahead into training. The model fit() method usually meets the requirements for training but custom training provides you finer control over optimization and other tasks associated with training. You could pick anyone depending on how complex your training\u2019s going to be. Here we have employed a customized approach to train the model.", "For visualizing word embeddings, tensorflow offers a brilliant platform that can be used to load and visualize saved weights vector with just a couple lines of code! Here\u2019s how we do it. First, extract and store the weights of the embedding layer. Then populate the word embeddings as shown below in two files: vecs.tsv which stores the actual vectors and meta.tsv contains associated metadata for visualizing.", "After that hop over to http://projector.tensorflow.org/ and load the files created in the previous step. That\u2019s it! Tensorflow takes care of the rest. Let\u2019s take a look at word embeddings that we learned above after training for 5 epochs.", "The results and the accuracy of test sets are quite significant and promising considering the model was trained within half an hour without any GPU support for the first 5 million bytes! As shown in the above images, \u201cClimate\u201d is encoded as nearest to nautical, warm, cooler, temperatures, salinity, moisture among others. \u201cParliament\u201d is most similar to bicameral, constituencies, ministers, seats, senators, etc. While \u201cMolecules\u201d is very much related to arsenic, compounds, ammonium, synthetic, calcium, etc. Interested readers can further explore and enhance the word embeddings by playing around with a more complex model architecture and larger data!", "The full version of the code snippets shared in this article, along with images and learned word embeddings is made available on Github. If you like this article or have any feedback, please let me know in the comments section below!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Often finds himself lost in the intersection of ML and Crypto"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa710eb03e4c5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sharviln.medium.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": ""}, {"url": "https://sharviln.medium.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Sharvil"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc889837fde3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&user=Sharvil&userId=c889837fde3d&source=post_page-c889837fde3d----a710eb03e4c5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral", "anchor_text": "Romain Vignes"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing"}, {"url": "https://github.com/SharvilN/Word-Embeddings", "anchor_text": "Github"}, {"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "language modeling"}, {"url": "https://en.wikipedia.org/wiki/Feature_learning", "anchor_text": "feature learning"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing"}, {"url": "https://en.wikipedia.org/wiki/Vector_(mathematics)", "anchor_text": "vectors"}, {"url": "https://en.wikipedia.org/wiki/Real_numbers", "anchor_text": "real number"}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF"}, {"url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis#:~:text=Latent%20semantic%20analysis%20(LSA)%20is,to%20the%20documents%20and%20terms.", "anchor_text": "Latent semantic analysis"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Efficient Estimation of Word Representations in Vector Space"}, {"url": "https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures", "anchor_text": "According to Mikolov"}, {"url": "https://groups.google.com/g/word2vec-toolkit/c/LNPeC5gyhmQ/m/p8683JkD6LoJ?pli=1", "anchor_text": "here"}, {"url": "http://mattmahoney.net/dc/textdata.html", "anchor_text": "text8"}, {"url": "https://arxiv.org/pdf/1301.3781v3.pdf", "anchor_text": "Efficient Estimation of Word Representations in Vector Space"}, {"url": "https://arxiv.org/pdf/1607.04606.pdf", "anchor_text": "paper"}, {"url": "http://projector.tensorflow.org/", "anchor_text": "http://projector.tensorflow.org/"}, {"url": "https://github.com/SharvilN/Word-Embeddings", "anchor_text": "Github"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a710eb03e4c5---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----a710eb03e4c5---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a710eb03e4c5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----a710eb03e4c5---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&user=Sharvil&userId=c889837fde3d&source=-----a710eb03e4c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&user=Sharvil&userId=c889837fde3d&source=-----a710eb03e4c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa710eb03e4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a710eb03e4c5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a710eb03e4c5--------------------------------", "anchor_text": ""}, {"url": "https://sharviln.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sharviln.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sharvil"}, {"url": "https://sharviln.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc889837fde3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&user=Sharvil&userId=c889837fde3d&source=post_page-c889837fde3d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb087fee9007e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-deep-dive-hands-on-approach-a710eb03e4c5&newsletterV3=c889837fde3d&newsletterV3Id=b087fee9007e&user=Sharvil&userId=c889837fde3d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}