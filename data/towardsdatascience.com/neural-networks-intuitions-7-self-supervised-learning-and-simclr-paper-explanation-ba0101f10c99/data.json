{"url": "https://towardsdatascience.com/neural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99", "time": 1683005101.44693, "path": "towardsdatascience.com/neural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99/", "webpage": {"metadata": {"title": "Neural Networks Intuitions \u2014 7. Self-Supervised Learning and SimCLR Paper Explanation | by Raghul Asokan | Towards Data Science", "h1": "Neural Networks Intuitions \u2014 7. Self-Supervised Learning and SimCLR Paper Explanation", "description": "Today I will be talking about one of the most important and interesting topics in deep learning domain \u2014 Self-supervised Learning and a recent paper showing SOTA result in self-supervised learning \u2014\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/2002.05709.pdf", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1406.6909.pdf", "anchor_text": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/2002.05709.pdf", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations", "paragraph_index": 30}], "all_paragraphs": ["Today I will be talking about one of the most important and interesting topics in deep learning domain \u2014 Self-supervised Learning and a recent paper showing SOTA result in self-supervised learning \u2014 A Simple Framework for Contrastive Learning of Visual Representations.", "The reason self-supervised learning is crucial is because manual labelling of data at scale is very, very expensive and tedious. Hence the focus is automatically on self-supervised and unsupervised learning domains \u2014 in order to reduce the need for labelling an enormous amount of data.", "Let us first understand what self-supervised learning is and how it helps in solving the above problem of manual labelling(or at least minimize it).", "Throughout this article, I will be explaining things from the perspective of solving a real-world problem.", "Let us consider the problem of image classification with say 1000 classes. And we have unlabelled data (say 1 million images) covering these 1000 classes. How do we go about solving this problem?", "Steps 2 and 3 are very simple. If you have the required frameworks in place, then you can quickly augment and train a classifier.", "Ofcourse step 3 requires us to experiment with different backbones, input image resolution, optimizer, loss function, hyperparameters such as learning rate, batch size and more.", "But the first and foremost step is to get the data labelled. And manually labelling such huge amounts of data requires lots of resources like time, human efforts. And on top of it all, we need to make sure that human error is as less as possible.", "Since labelling all of the data is tedious, let us annotate only a random subset of data.", "I am choosing a random subset for simplicity but we can optimize this selection.", "Now that we are going to annotate a small amount, how can we either reduce the efforts put in for manual labelling or completely eliminate this labelling process? (even though this seems to be an unrealistic goal :)), or at least make use of this large amount of unlabelled data to help improve accuracy.", "One obvious way is to use some open-source datasets which closely resembles our dataset(even though we don\u2019t often find any), train on the open-source dataset and then finetune the network on our dataset.", "This does help in improving the classifier\u2019s accuracy but not by a great extent and is always preferred instead of starting with random weights.", "Btw, why do I say that this is better than random initialization?", "It is because it helps in achieving higher accuracy and faster convergence!", "Since we train our network on an open-source dataset similar to our target data distribution, this means that our network has already learned some relevant features instead of random features. Therefore, once we train the network again on our target dataset, naturally it is expected to perform well and avoids overfitting on the small target set. This can be seen as an equivalent to \u201ctraining the network on more data\u201d.", "But we can do much better by starting with representations learnt from our unlabelled target data distribution rather than with representations learnt from a different data distribution.", "So automatically our next approach is to learn representations from this unlabelled target distribution!", "This technique of using unlabelled data to help the supervised task is called Self-Supervised Learning and the methods which helps in learning representations from this unlabelled target distribution are called pretext tasks.", "Now that we know what self-supervised learning is, let us see what types of pretext tasks are there and how they actually help in learning representations.", "This article will be covering multiple pretext tasks. Hence, I will only be giving an overview of those tasks and how they can be used to solve our problem at hand :-)", "Autoencoder is a neural network which tries to reconstruct the input, thereby learning input data representations.", "The network takes in an image and outputs the same image. The loss generally used is the MSE loss. The main thing to note here is the code(or embedding). This represents the features learned from the input data.", "How does this help in solving our problem?", "We can train an autoencoder(resnet50 encoder with avgpool\u2019s output at the end as the embedding) on our unlabelled data, load the encoder\u2019s weights and train on the labelled subset.", "Paper:- Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks", "The authors propose the following methodology:", "Note that, there can be too many surrogate classes representing the same ground-truth class and since it is impossible to know them beforehand, we could do a clustering of these unlabelled images in the first place(embeddings either from pretrained model on similar dataset or using autoencoder on the same dataset) and then apply transformations to those clusters.", "In an ideal scenario, there will be only one sample per class(which is our surrogate class) and random tranformations are applied to each of those samples and trained.", "Like autoencoders, we can make use of the trained classifier\u2019s weights for our downstream task.", "The paper A Simple Framework for Contrastive Learning of Visual Representations proposes another mechanism using Contrastive Learning to learn useful representations from an unlabelled dataset which can later be used for downstream tasks.", "where sim=cosine similarity, \ud835\udfd9 \u2208 {0, 1} iff k\u2260 i, and \u03c4 denotes a temperature parameter.", "The objective is to learn similar representations for the two positive images and the rest N-1 negative images to have representations completely different from those two. But how is this achieved?", "However, there is one important point here:", "How do we ensure that the (N-1) images used for negative class in a batch do not contain a positive sample, as we don\u2019t know their labels in the first place?", "Unfortunately, this is not clearly stated in the paper nor do I know the answer to the above question. One thing we could do is to pick negative images based on pretrained embeddings, which I don\u2019t think is happening here though!Please let me know if any of you know the answer :-)", "All the above pretext tasks directly learn to classify the underlying classes(except autoencoder), but there are also tasks which tries to optimize for a different objective yet learn useful representations for the downstream tasks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director of AI R&D at Infilect"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fba0101f10c99&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://raghul-719.medium.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Raghul Asokan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3b61249ec9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&user=Raghul+Asokan&userId=e3b61249ec9&source=post_page-e3b61249ec9----ba0101f10c99---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/2002.05709.pdf", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"url": "https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png", "anchor_text": "Architecture of an Autoencoder"}, {"url": "https://arxiv.org/pdf/1406.6909.pdf", "anchor_text": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/1406.6909.pdf", "anchor_text": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/2002.05709.pdf", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"url": "https://arxiv.org/abs/1803.07728", "anchor_text": "Unsupervised Representation Learning by Predicting Image Rotations"}, {"url": "https://arxiv.org/abs/1505.05192", "anchor_text": "Unsupervised Visual Representation Learning by Context Prediction"}, {"url": "https://arxiv.org/pdf/1406.6909.pdf", "anchor_text": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/2002.05709.pdf", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"url": "https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html", "anchor_text": "https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"}, {"url": "https://medium.com/tag/self-supervised-learning?source=post_page-----ba0101f10c99---------------self_supervised_learning-----------------", "anchor_text": "Self Supervised Learning"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----ba0101f10c99---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ba0101f10c99---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ba0101f10c99---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ba0101f10c99---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&user=Raghul+Asokan&userId=e3b61249ec9&source=-----ba0101f10c99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&user=Raghul+Asokan&userId=e3b61249ec9&source=-----ba0101f10c99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fba0101f10c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ba0101f10c99---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ba0101f10c99--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ba0101f10c99--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ba0101f10c99--------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raghul Asokan"}, {"url": "https://raghul-719.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "330 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3b61249ec9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&user=Raghul+Asokan&userId=e3b61249ec9&source=post_page-e3b61249ec9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb225bcb64cc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-7-self-supervised-learning-and-simclr-paper-explanation-ba0101f10c99&newsletterV3=e3b61249ec9&newsletterV3Id=b225bcb64cc2&user=Raghul+Asokan&userId=e3b61249ec9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}