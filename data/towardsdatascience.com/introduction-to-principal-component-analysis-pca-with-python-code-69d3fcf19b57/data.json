{"url": "https://towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57", "time": 1683000638.10988, "path": "towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57/", "webpage": {"metadata": {"title": "Introduction to Principal Component Analysis (PCA) \u2014 with Python code | by Dhruvil Karani | Towards Data Science", "h1": "Introduction to Principal Component Analysis (PCA) \u2014 with Python code", "description": "Imagine a text data having 100000 sentences. The total number of unique words in the text is 10000 (vocabulary size). If each sentence is represented by a vector of length equal to the vocabulary\u2026"}, "outgoing_paragraph_urls": [{"url": "https://stattrek.com/matrix-algebra/covariance-matrix.aspx", "anchor_text": "More on covariance matrix", "paragraph_index": 19}, {"url": "https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset", "anchor_text": "Kaggle", "paragraph_index": 26}, {"url": "https://github.com/DhruvilKarani/PCA-blog-notebook/blob/master/PCA.ipynb", "anchor_text": "GitHub", "paragraph_index": 26}, {"url": "https://thenlp.space/", "anchor_text": "thenlp.space", "paragraph_index": 33}], "all_paragraphs": ["Imagine a text data having 100000 sentences. The total number of unique words in the text is 10000 (vocabulary size). If each sentence is represented by a vector of length equal to the vocabulary size. In this vector of length 10000, each position belongs to a unique word. For the vector of our sentence, if a certain word occurs k times, the value of the element in the vector corresponding to the index of this word is k. For every other word in the vocabulary not present in our sentence, the vector element is 0. This is called one-hot encoding.", "Now, if we create vectors for each of these sentences, we have a set of 100000 vectors. In a matrix form, we have a 100000*10000 size matrix. This means we have 10\u2079 elements in our matrix. In python, to get an estimate of the memory occupied,", "The number, in bytes, translates to 8000 megabytes.", "What if we are able to work on a smaller matrix, one having fewer but more important features. Say, a few 100 features that capture most information about the data. Having more features is usually good. But more features often include noise too. And noise makes any Machine Learning algorithm less robust, especially on small datasets (read about the curse of dimensionality to know more). Moreover, we would save 100\u20131000 times the memory we consume. This is important when you deploy applications to production.", "With this aim, let\u2019s discuss how PCA helps us solve this problem.", "In Machine Learning, we need features for the algorithm to figure out patterns that help differentiate classes of data. More the number of features, more the variance (variation in data) and hence model finds it easy to make \u2018splits\u2019 or \u2018boundaries\u2019. But not all features provide useful information. They can have noise too. If our model starts fitting to this random noise, it would lose its robustness. So here\u2019s the deal \u2014 We want to compose m features from the available feature space that give us maximum variance. Note that we want to compose and not just select m features as it is.", "Say a data point is represented by a vector x and has n features originally.", "Our job is to transform it into a vector z, having fewer dimensions than n, say m", "This happens through a Linear Transformation. Specifically, we multiply a matrix to x that maps it to an m-dimensional space. Consider a matrix W of size n*m. Using W, we wish to transform x to z as \u2014", "A simple interpretation of the above equation \u2014 Suppose you have an original data frame with columns \u2014", "After PCA, if we select, say 3 components, the data frame looks something like \u2014", "Consider an example diagram below. The axis pointing northeast has more spread covered along its direction. Then the second most \u2018covering\u2019 axis points northwest.", "The above description is good enough for a small talk about PCA. But if you are curious about the calculation of W, this section is for you. You may skip it if you aren\u2019t interested in the Math.", "We discussed that PCA is about computing axes along which variance is maximized. Consider a single axis w, along which we wish to maximize the variance. Mathematically,", "To calculate variance, we need mean. In statistics, it is better put as expectation, denoted by E. Expectation of a random variable is its mean. In our case we denote expectation of our original features x as \u03bc. For our transformed features, z our expectation is wt*\u03bc (see picture below)", "If you are confused, refer the equation below, and take mean on LHS and RHS", "Let mean of transformed features be", "The variance of a sample of a random variable z is given by,", "Expressed in terms of expectation and taking w out of the expectation operation (since it is a constant and not dependent on z)", "Turns out, the variance of z is just wt*covariance of x (sigma) *w. Covariance is the variance of a multi-dimensional random variable, in our case, x. It is a square matrix of size k*k, where k is the dimension of x. It is a symmetric matrix. More on covariance matrix", "Before we maximize the variance, we impose an important condition on the norm of our w matrix. The norm should be 1", "Why? Otherwise, the easy way of maximizing variance is by just increasing the norm and not re-aligning the principal axis in a way that maximizes the variance. This condition is included using a Lagrange multiplier", "If we differentiate the above expression, we get \u2014", "Take a look at the last expression. Covariance matrix multiplied by w is equal to a scalar alpha multiplied by w. This means, w is the eigenvector of the covariance matrix and alpha is the corresponding eigenvalue. Using this result,", "If var(z) is to be maximized, then alpha has to be the largest of all eigenvalues. And the corresponding eigenvector is the principal component", "This gives us the first principal component along which the variance explained is maximum compared to any other component. The above was the derivation for the first axis. Other axes are derived similarly. The point was to go a little deep in the math behind PCA.", "The dataset we use is Coursera\u2019s course reviews which is available on Kaggle. The code for this exercise is available on my GitHub repo.", "As the name says, it consists of reviews (sentences) and ratings (1,2,3,4,5). We take 50,000 reviews, perform cleaning and vectorize them using TF-IDF.", "TF-IDF is a scoring method that assigns a score to each word in the sentence. The score is high if the word does not commonly occur in all the reviews but occurs frequently overall. For example, the word course occurs frequently, but it occurs in almost every other review. So TF-IDF score for that word would below. On the other hand, the word poor occurs frequently and is specific to only negative reviews. So its score is higher.", "So for every review, we have fixed length TF-IDF vector. The length of the vector is the size of the vocabulary. In our case, the TF-IDF matrix is of size 50000*5000. Moreover, more than 99% of the elements are 0. Because if a sentence has 10 words, only 10 elements of the 5000 will be non-zero.", "We use PCA to reduce the dimensionality. But how to decide the number of components to take?", "Let\u2019s look at the plot of variance explained vs the number of components or axes", "As you see, the first 250 components explain 50% of the variation. First 500 components explain 70%. As you can see, the incremental benefit diminishes. Of course, if you take 5000 components, the variance explained will be 100%. But then what\u2019s the point of doing PCA?", "I have started my personal blog and I don\u2019t intend to write more amazing articles on Medium. Support my blog by subscribing to thenlp.space", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F69d3fcf19b57&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@karanidhruvil?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanidhruvil?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Dhruvil Karani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f59d024f067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&user=Dhruvil+Karani&userId=8f59d024f067&source=post_page-8f59d024f067----69d3fcf19b57---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://i1.wp.com/enhancedatascience.com/wp-content/uploads/2017/05/PCA1.png?resize=629%2C424", "anchor_text": "https://i1.wp.com/enhancedatascience.com/wp-content/uploads/2017/05/PCA1.png?resize=629%2C424"}, {"url": "https://stattrek.com/matrix-algebra/covariance-matrix.aspx", "anchor_text": "More on covariance matrix"}, {"url": "https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset", "anchor_text": "Kaggle"}, {"url": "https://github.com/DhruvilKarani/PCA-blog-notebook/blob/master/PCA.ipynb", "anchor_text": "GitHub"}, {"url": "https://thenlp.space/", "anchor_text": "thenlp.space"}, {"url": "https://medium.com/tag/data-science?source=post_page-----69d3fcf19b57---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----69d3fcf19b57---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/analytics?source=post_page-----69d3fcf19b57---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/tag/visualization?source=post_page-----69d3fcf19b57---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/tag/statistics?source=post_page-----69d3fcf19b57---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&user=Dhruvil+Karani&userId=8f59d024f067&source=-----69d3fcf19b57---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&user=Dhruvil+Karani&userId=8f59d024f067&source=-----69d3fcf19b57---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F69d3fcf19b57&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----69d3fcf19b57---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----69d3fcf19b57--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanidhruvil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@karanidhruvil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dhruvil Karani"}, {"url": "https://medium.com/@karanidhruvil/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f59d024f067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&user=Dhruvil+Karani&userId=8f59d024f067&source=post_page-8f59d024f067--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e3f9b6d53a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57&newsletterV3=8f59d024f067&newsletterV3Id=7e3f9b6d53a&user=Dhruvil+Karani&userId=8f59d024f067&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}