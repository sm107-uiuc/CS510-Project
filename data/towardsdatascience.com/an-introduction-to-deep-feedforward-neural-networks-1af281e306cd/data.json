{"url": "https://towardsdatascience.com/an-introduction-to-deep-feedforward-neural-networks-1af281e306cd", "time": 1683011237.20618, "path": "towardsdatascience.com/an-introduction-to-deep-feedforward-neural-networks-1af281e306cd/", "webpage": {"metadata": {"title": "An Introduction to Deep Feedforward Neural Networks | by Reza Bagheri | Towards Data Science", "h1": "An Introduction to Deep Feedforward Neural Networks", "description": "The feedforward neural network is the simplest type of artificial neural network which has lots of applications in machine learning. It was the first type of neural network ever created, and a firm\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=77415260637&device=c&keyword=coursera%20deep%20learning%20ai&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493133115&hide_mobile_promo&gclid=CjwKCAjw4pT1BRBUEiwAm5QuRwgTOsOYZ5KBSCJ2uUPnH0uM5tieL87a4aVcmxP_SAtDaaMX2_9prBoCmjEQAvD_BwE", "anchor_text": "Deep Learning Specialization course", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Differentiable_function", "anchor_text": "differentiable functions", "paragraph_index": 68}, {"url": "https://en.wikipedia.org/wiki/Random_variable", "anchor_text": "random variable", "paragraph_index": 164}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood", "paragraph_index": 166}, {"url": "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation", "anchor_text": "maximum likelihood estimation", "paragraph_index": 168}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution", "anchor_text": "multinomial distribution", "paragraph_index": 188}, {"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=77415260637&device=c&keyword=coursera%20deep%20learning%20ai&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493133115&hide_mobile_promo&gclid=CjwKCAjw4pT1BRBUEiwAm5QuRwgTOsOYZ5KBSCJ2uUPnH0uM5tieL87a4aVcmxP_SAtDaaMX2_9prBoCmjEQAvD_BwE", "anchor_text": "Deep Learning Specialization course", "paragraph_index": 287}], "all_paragraphs": ["The feedforward neural network is the simplest type of artificial neural network which has lots of applications in machine learning. It was the first type of neural network ever created, and a firm understanding of this network can help you understand the more complicated architectures like convolutional or recurrent neural nets. This article is inspired by the Deep Learning Specialization course of Andrew Ng in Coursera, and I have used a similar notation to describe the neural net architecture and the related mathematical equations. This course is a very good online resource to start learning about neural nets, but since it was created for a broad range of audiences, some of the mathematical details have been omitted. In this article, I will try to derive all the mathematical equations that describe the feedforward neural net.", "Currently Medium supports superscripts only for numbers, and it has no support for subscripts. So to write the name of the variables, I use this notation: Every character after ^ is a superscript character and every character after _ (and before ^ if its present) is a subscript character. For example", "is written as w_ij^[l] in this notation.", "A neuron is the foundational unit of our brain. The brain is estimated to have around 100 billion neurons, and this massive biological network enables us to think and perceive the world around us. Basically what a neuron does is receiving information from other neurons, processing this information and sending the result to other neurons. This process is shown in Figure 1. A single neuron has some inputs which are received throughout the dendrites. These inputs are summed together in the cell body and transformed into a signal that is sent to other neurons through the axon. The axon is connected to the dendrites of other neurons by synapses. The synapse can act as a weight and make the signal passing through it stronger or weaker based on how often that connection is used.", "This biological understanding of the neuron can be translated into a mathematical model as shown in Figure 1. The artificial neuron takes a vector of input features x_1, x_2, . . . , x_n, and each of them is multiplied by a specific weight, w_1, w_2, . . . , w_n. The weighted inputs are summed together, and a constant value called bias (b) is added to them to produce the net input of the neuron", "The net input is then passed through an activation function g to produce the output a=g(z) which is then transmitted to other neurons", "The activation function is chosen by the designer, but w_i and b are adjusted by some learning rule during the training process of the neural network.", "There are different activation functions that you can use in a neural net, and some of them which are used more commonly are discussed below.", "A binary step function is a threshold-based activation function. If the function\u2019s input (z) is less than or equal to zero, the output of the neuron is zero and if it is above zero, the output is 1", "The step function is not differentiable at point z=0, and its derivative is zero at all the other points. Figure 1 shows a plot of the step function and its derivative.", "The output of a linear activation function is equal to its input multiplied by a constant c", "This function is shown in Figure 2 (left). Its derivate is equal to c", "The use of prime for g signifies differentiation with respect to the argument which is z here. A plot of the derivative of g(z) is shown in Figure 2 (right)", "It is a non-linear activation function that gives a continuous output in the range of 0 to 1. It is defined as", "Sigmoid has the property of being similar to the step function, however, it is continuous and prevents the jump in the output values that exists in the step function. A plot of the sigmoid is shown in Figure 3 (left).", "Sigmoid is a differentiable function, and since we need its derivative later, we can derive it here", "Figure 3 (right) shows a plot of the derivative of the sigmoid.", "It is a non-linear activation function that is similar to the sigmoid but gives a continuous output in the range of -1 to 1. It is defined as", "Figure 4 shows a plot of this function and its derivative.", "ReLU is a very popular activation function in deep neural networks. It is defined as", "Although it looks like a linear function, ReLU is indeed a non-linear function. Figure 5 (left) shows a plot of this function.", "The function has a breakpoint at z=0, and its derivative is not defined at this point. But we can assume that when z is equal to 0 the derivative is either 1 or 0. It has been shown in Figure 5 (right).", "There is another version of the ReLU called the Leaky ReLU which is defined as", "Here when z is negative, the function is not zero. Instead, it has a slight slope equal to c which makes its derivative bigger than zero. Its derivative is", "Figure 6 shows a plot of this function and its derivative.", "Neural networks are often described with vectors and matrices, and this kind of matrix expression will be used throughout this article. To make sure that the reader understands the vector and matrix operations, I will review the matrix algebra in the next section before starting modeling the feedforward neural nets.", "A matrix is a rectangular array of numbers or variables. In this article, we use uppercase boldface letters to represent matrices. We use [A]_ij or a_ij to denote the element of matrix A at row i and column j. For example", "A vector is a matrix with a single row or column. The elements of a vector are usually identified by a single subscript. As a convention, we use lowercase boldface letters for column vectors. For example, x is a column vector", "We use [x]_i or x_i to denote the i-th element of vector x.", "We can add vectors if they have the same number of elements by adding their corresponding elements. So if a and b both have n elements, c = a+b is also a vector with n elements and is defined as", "Similarly, we can add two matrices if they have the same shape. If A and B are both m\u00d7n matrices, then C=A+B is an m\u00d7n matrix defined as", "In linear algebra, the addition of a matrix and a vector is not defined, however, in deep learning, the addition of a matrix and a vector is allowed. If A is an m\u00d7n matrix and b is a column vector with m elements, then C=A+b yields an m\u00d7n matrix and is defined as", "So b is added to each column of A. This form of addition is generally called a broadcasting operation.", "If A is an m\u00d7p matrix and B is a p\u00d7n matrix, the matrix product C=AB (which is an m\u00d7n matrix) is defined as", "The transpose of an m\u00d7n matrix A (which is shown by A superscript T) is an n\u00d7m matrix whose columns are formed from the corresponding rows of A. For example the transpose of A in Eq. 14 is", "The transpose of a row vector becomes a column vector with the same number of elements and vice versa. As a convention, we assume that all vectors are column vectors, so we show a row vector as the transpose of a column vector. For example, x^T is a row vector which is the transpose of the column vector x in Eq. 15", "In fact, the element in the i-th row and j-th column of the transposed matrix is equal to the element in the j-th row and i-th column of the original matrix. So", "Transpose has some important properties. First, the transpose of the transpose of A is A", "In addition, the transpose of a product is the product of the transposes in the reverse order", "To prove it remember the matrix multiplication (Eq. 19). Now based on the definition of matrix transpose (Eq. 20), the left side is", "So both sides of the equation are equal.", "If we have two vectors u and v", "The dot product (or inner product) of these vectors is defined as the transpose of u multiplied by v", "Based on this definition the dot product is commutative so", "If we multiply a column vector u (with m elements) by a row vector v^T (with n elements), the result is an m\u00d7n matrix", "which is a special case of the matrix multiplication rule assuming that u is a matrix with only one column and v is a matrix with only one row (Eq. 19).", "The length (also called the 2-norm) of a vector u with n elements is defined as", "The dot product can be also written in terms of the length of the vectors. If the angle between two vectors u and v is \u03b8, then their dot product can be also written as", "We know that the maximum value of cosine is 1 at \u03b8=0\u2070 and its minimum value is -1 at \u03b8=180\u2070. So if we fix the length of u and v, the maximum of their dot product occurs when they have the same direction (\u03b8=0\u2070), and the minimum of their dot product occurs when they have opposite directions (\u03b8=180\u2070).", "A large matrix can be divided into submatrices or blocks. The blocks can be treated as if they were the elements of the matrix, so the partitioned matrix becomes a matrix of matrices. For example, the matrix", "So we can think of each column of A as a column vector, and A can be thought of as a matrix with just one row. Similarly, we can write A as", "So A can be thought of as a matrix with just one column. Each row of A is now a row vector and each of these row vectors is the transpose of a column vector. When multiplying the partitioned matrices, you can treat them like a regular matrix. For example, if we partition matrix A as a column vector", "and matrix B as a row vector", "Then multiplying A by B gives", "So each element of AB is the dot product of the submatrix (which is a row vector) in the same row of A by the submatrix in the same column of B (which is a column vector). As a special case, multiplying the partitioned matrix A by the column vector c gives", "Ac is a partitioned matrix treated like a column vector and each element of this vector is the dot product of the submatrix in the same row of A (which is a row vector) by the column vector c. Similarly, suppose that we want to multiply matrix A by the partitioned matrix B which only has one row and each of its elements is a column vector. We can write", "Suppose a and b are two vectors of the same dimension", "Then the Hadamard product of a and b is defined as the element-wise product of these vectors", "The Hadamard product of two matrices A and B of the same dimension m\u00d7n is a matrix of the same dimension and is defined as", "So it is the element-wise product of them", "A function can take a vector or matrix and return a scalar. For example, if x is a vector and y is a scalar, then we can define a function like this", "In addition, it is also possible to have a function that takes a matrix and returns another matrix. In this article, we call it a vectorized function. Suppose that A is an m\u00d7n matrix and f is a function that takes a scalar and returns another scalar (for example f: R -> R). Here the notation f(A) denotes the elementwise application of f to A", "A similar equation can be written for a vector. So by simply looking at a function like f(x), we can not say if it returns a scalar, a vector, or a matrix unless know how it has been defined.", "When studying neural networks, we are dealing with multivariable functions, so we should be familiar with multivariable calculus. Suppose that f: R^n->R is a multivariable function", "So f has an n-dimensional input, but it\u2019s a scalar function which means its range is one-dimensional (its output is a scalar quantity).", "The partial derivative of a multivariable function is its derivative with respect to one of those variables, with the others held constant. In fact, the partial derivative of f with respect to x_i", "The total differential of a multivariable function is the sum of its partial differentials arising from the separate variation of the variables", "The total differential measures how f changes as all the variables increase together at point (x_1, x_2, .., x_i, \u2026x_n).", "The chain rule in calculus is used to compute the derivative of a function which is a composition of other functions whose derivatives are known. Suppose that x, y, and z are scalar variables, and f and g are differentiable functions. Assume that y = g(x) and z = f(y) = f(g(x)) . Then based on the chain we have", "We can also generalize the chain rule to vectors. Suppose that x, and y are vectors and z is a scalar. Assume that y = g(x) and z = f(y) = f(g(x)). Now we have", "Suppose that f is a function. The gradient of f with respect to vector x is defined as", "So the gradient of f is a vector, and each element i of the gradient is the partial derivative of f with respect to x_i", "Gradient generalizes the concept of the derivative, and the gradient of f is like the derivative of f with respect to a vector. We can also take the derivative of a function with respect to a matrix. Suppose that X is an m\u00d7n matrix and g is a scalar function. The gradient of g with respect to matrix X is a matrix defined as", "In addition, we can define the derivative of vector y (with n elements) with respect to another vector x (with m elements)", "Now we can express the neuron model equations in vector form. The neuron\u2019s input is the vector", "and weights of the neuron can be shown by the vector", "The dot product of x and w gives z", "The output or the activation of the neuron is", "where w and b are the adjustable parameters of the neuron.", "A training set is defined as", "and each pair (x^(i), y^(i)) is called a training example. Here we use a number inside the parenthesis () as a superscript to refer to the training example number, so m is the number of the training examples in the training set. x^(i) is a called the feature vector or the input vector of the training example i. It is a vector of numbers and each element of this vector is called a feature. Each x^(i) corresponds to a label y^(i). We assume there is an unknown function y= f(x) that maps the feature vectors to the labels, so y^(i)= f(x^(i)). Now the goal of supervised learning is to use the above training set to learn or approximate f. In other words, we want to use the training set to estimate f with another function fhat and then predict the labels using", "where the hat symbol denotes an estimate. We want fhat(x) to be close to f(x) not only for the input vectors in the training set (x^(i)) but also for novel input vectors which are not present in the training set.", "When the label is a numerical variable, we call the learning problem a regression problem, and when it is a categorical variable, the problem is known as classification. In classification, the label is a categorical variable which can be represented by the finite set y^(i) \u2208 {1, 2, . . . , c}, where each number is a class label, and c is the number of classes (the class labels can be anything, but you can always assign a number to them like this). If c = 2 and the class labels are mutually exclusive (it means that each input can only belong to one of the classes), we call it a binary classification. An example is medical testing to determine if a patient has a certain disease or not (Figure 7 top).", "If c > 2 and the class labels are mutually exclusive, it is called multiclass classification. For example, suppose that we want to detect three animals in an image: A dog, a cat, and a panda. But in each image, we can only have one animal. So the labels are mutually exclusive, and this is a multiclass classification problem (Figure 7 middle). In a multiclass problem, each training example is a pair", "We will use a method called one-hot encoding to convert these class numbers into binary values. We convert the scalar label y to a vector y which has c elements. When y is equal to k, the k-th element of y will be one and all other elements will be zero. In fact, the i-th element of y is signaling the presence or absence of class i for the input vector of the example. In each label vector, only one element can be equal to one, and the others should be zero. So for each x^(i) we have a label vector y^(i) with c elements", "Now our training set can be defined as {x^(i), y^(i)}. If the class labels are not mutually exclusive, we call it multilabel classification. Suppose that in the image classification of animals mentioned before, each of them can be present in the image independently. For example, we can have both a dog and a cat in the same image. So the labels are not mutually exclusive anymore, and now we have a multilabel classification problem (Figure 7 bottom).", "In multilabel classification, each class is considered a separate label, and each y^{i} can take a set of classes that are present for that training example. We can use multi-hot encoding to convert these class numbers into binary values. Again, we convert the scalar label y to a vector y which has c elements. The i-th element of y is signaling the presence or absence of class i, so when a class is present y_i=1 and when it is absent y_i=0. However, in each label vector, more than one element can be equal to one since the classes are not mutually exclusive anymore.", "So our training set can be defined as", "A single neuron is a simple computational unit, and to learn complex patterns, we commonly need lots of them to work together. A layer of neurons consists of some neurons operating in parallel. The neurons in each layer are supposed to work at the same time but independently. We can think of the raw data as a separate layer and call it the input layer (Figure 8).", "When we count the layers in neural networks, we don\u2019t include the input layer. So the next layer after the input layer is layer 1. We use a number inside square brackets [] as a superscript to indicate the layer number. So the output or activation of the second neuron in layer one is", "The number of neurons in each layer is denoted by n. So the number of neurons in the first layer is", "and the activation of the last neuron in the first layer will be", "For the input layer, the layer number is assumed to be zero, so the number of input features is", "The wights for the neuron i in the first layer can be represented by the vector", "represents the weight for the input feature j which goes into neuron i in layer 1 (Figure 9).", "We can calculate the activations of the first layer using the weight and input vectors. We can use Eqs. 49 and 50 to calculate the activation of neuron i in layer 1", "where b_i the bias for neuron i in layer 1, and g^[1] is the activation function for each neuron in layer 1. The activation function receives a scalar input (the net input of neuron) and returns another scalar which is the neuron activation.", "Now we can consider a network with several layers. The last layer of the network is called the output layer, and if there are any layers in between, we call them the hidden layers (Figure 10).", "In a feedforward network, the information moves only in the forward direction, from the input layer, through the hidden layers (if they exist), and to the output layer. There are no cycles or loops in this network. Feedforward neural networks are sometimes ambiguously called multilayer perceptrons. The number of neurons in layer l is denoted by", "The net input of neurons in layer l can be represented by the vector", "Similarly, the activation of neurons in layer l can be represented by the activation vector", "and the wights for the neuron i in layer l can be represented by the vector", "represents the weight for the input j (coming from neuron j in layer l-1) going into neuron i in layer l (Figure 11).", "As you see in Figure 11, in layer l, all the inputs are connected to all neurons in that layer. Such a layer is called a dense or fully connected layer.", "Now we can calculate the activation of a neuron in layer l using its weight and input vectors", "where b_i^[l] is the bias for neuron i in layer l, and \u03c3^[l] is the activation function for each neuron in layer l. It is important to note that each layer can have a different activation function, but the neurons in one layer usually have the same activation function. To have a consistent notation we can assume that", "So when l=1, Eq. 60 is converted to Eq. 55 and we don\u2019t need to write it separately. Now Eq. 60 can be used for all the layers of the network.", "We can combine all the weights of a layer into a weight matrix for that layer", "The i-th element of this partitioned matrix is a row vector, and this row vector is the transpose of a column vector which gives the weights for neuron i in layer l. If we expand this matrix we get", "So the (i,j) element of this matrix gives the weight of the connection that goes from the neuron j in layer l-1 to the neuron i in layer l. We can also have a bias vector", "in which the i-th element is the bias for the neuron i in layer l. Now using Eqs. 30 and 59, we can write", "Please note that in the first line of Eq. 65 we added the bias vector to a matrix which is the broadcasting addition defined in Eq. 18.", "If we apply the vectorized activation function (recall Eq. 37) in layer l to the previous equation, using Eqs. 57 and 60 we get", "Now by rearranging the previous equation we finally have", "Eq. 67 is the forward propagation equation for a feedforward neural network. Using this equation we can compute the activations of a layer using the activations of the previous layer. If we apply Eqs. 65 and 67 to the first layer (l=1), then the previous layer is the input layer (Eq. 61), so we have", "Now suppose that we have a training set with m examples. So we have a set of input vectors for the whole training set", "Each example has n^[0] input features as before", "We can now combine all the input vectors in the training set to have an input matrix", "Each element of this partitioned matrix is a column vector and is equal to the input vector for the i-th example of the training set, so each element of this matrix is", "Each of these examples can be used as the input layer for the neural net and we can use Eqs. 65 and 67 to predict the activations of each layer for each training example.", "Note that the superscript [l], refers to the layer number, and the superscript in parenthesis (j) refers to the training example number. Eq. 73 can be also written as", "So these equations give the net input and activation vector of layer l for the training example number j. For the first layer, we can write (using Eqs. 68 and 69)", "In the first line of Eq. 77 we used Eq. 31 to do the multiplication, and in the third line, we used Eq. 75 to simplify it. We can define the net input matrix as", "Here the i-th column of Z^[l] is the net input of layer l for the training example number i. Similarly, we can define the activation matrix as", "If we apply the vectorized activation function to Eq. 78, using Eqs. 76 and 80 we get", "and by combining Eqs. 83 and 85, we finally have", "This equation is the vectorized forward propagation equation for the whole training set. We can also assume that", "Among the activation functions that were introduced before, only the linear activation has a linear relationship with the net input (that is why we call it a linear activation!). But why do we need the nonlinear activation functions? Suppose that all the neurons in layers l-1 and l have linear activation functions. For layer l-1 we can use Eq. 74 and the definition for the linear activation function (Eq. 4) to write", "We can now use this equation to write the activation vector of layer l", "This equation suggests that we can merge layers l-1 and l into one layer with linear activations. It takes a^[l-2] and returns a^[l]. The weight matrix of this new layer is W^[l]W^[l-1], its bias vector is W^[l]b^[l-1]+b^[l], and its activation function is g(z)=c\u00b2z. Now suppose that all the layers of the network have linear activations. Then we can merge all of them into one linear layer. So the network behaves like a single layer neural net, and such a network is not a good choice to learn nonlinear data. As a result, the nonlinear activation functions are essential ingredients of multilayer neural networks.", "Remember that a^[L] was the activation vector of the last layer of the neural network. However, we usually use the vector yhat for the activation of the output layer since it is the final output of the network. So for the example j, we have", "For a regression problem, we have a real value label for each training example, so we usually use a single neuron with a linear activation function. As mentioned before, we have three types of classification problems: binary, multiclass, and multilabel. For each type, we use a different layout for the output layer. Suppose that we have m examples and each example has n^[0] features. In addition, suppose that we have c classes for the labels. So each training example is a pair", "If we have a binary classification problem, then c=2 and the classes are mutually exclusive. In this case, we use a single neuron in the output layer with a sigmoid activation function. We usually use a sigmoid activation function for the output layer since its output is in range of [0,1], and we can interpret it as the probability of its corresponding class. One problem is that most of the activation functions give a continuous output not a binary one. So we need a way to interpret the raw output of the activation function as a binary output. We can define a threshold of 0.5. If the output value for an example is less than or equal to 0.5 it means the example belongs to class 1 and if it is greater than 0.5, it means that it belongs to class 2. This is shown in Figure 12.", "If we have a multilabel problem, then c\u22652, and the classes are not mutually exclusive. We use multi-hot encoding to convert y^(i) to the vector y^(i) which has c elements (Eq. 53). Now our output layer should have c neurons with sigmoid activation each giving the value of one of the elements of y^(i). In fact, the activation of each neuron is indicating whether the input belongs to a certain class or not. So the number of elements of y^(i) is equal to the number of the neurons in the last layer n^[L]. We can still apply the 0.5 thresholds to each neuron to convert the raw activation vector into the binary output which a multi-hot encoded vector (Figure 13).", "If we have a multiclass problem, then c>2, and the classes are mutually exclusive. Here we use one-hot encoding to convert y^(i) to the vector y^(i) which has c elements (Eq. 52). So for each x^(i), we have a label vector y^(i)", "Now our training set can be defined as", "Now our output layer should have c neurons, so n^[L]=c, and the activation of each neuron is indicating whether the input belongs to a certain class or not. However, we cannot use c neurons with sigmoid activations anymore. When we have a one-hot encoded vector, the sum of the elements is always equal to one (since only one of them can be one). It also means that these elements are not independent of each other. When one of these elements becomes one, the others are forced to be zero.", "However, the probabilities produced by c neurons with sigmoid activation functions in the output layer are independent and are not constrained to sum to one. That\u2019s because these activation functions work independently. What we would like to have is a categorical probability distribution for the output vector. So they should be constrained to sum to one. We use the softmax activation function for this purpose.", "The softmax activation function is always added to the last year of neurons (Figure 14), and the last layer with this activation function is called a softmax layer. It has a big difference with the other activation functions mentioned before. It cannot be applied to each neuron independently, instead, it combines the net input of all neurons to calculate their activations.", "The softmax layer is shown in Figure 14 and it looks a little different. Here each circle shows a neuron in the last layer, but the output of each circle is the net input of that neuron, not its activation. These net inputs then go into the softmax activation pictured with a rectangle, and the output of softmax is the activation of the individual neurons.", "The output of the softmax layer (which is indeed the activations of the neurons) is the vector a which has the same number of elements as z (net input vector) and is defined as", "So the softmax layer normalizes its input by dividing each element by the sum of all the elements in the input vector. The exponential function always gives a positive result, and a_i will be positive even if z_i is not. As a result, the activations of the softmax layer is a set of positive numbers that sum up to 1", "and it can be thought of as a probability distribution.", "Now by applying the softmax function to the net input of the last layer of neurons we get a normalized activation vector. The maximum element determines to which class the input vector belongs. So in this way, we can convert the activation vector of softmax into the binary output which is a one-hot encoded vector. For example, if the activation vector of the softmax layer is [0.5 0.15 0.35]^T it will be converted to the binary output vector [1 0 0]^T. For a multiclass problem with c classes, we use a softmax layer with c neurons as the output layer (Figure 15).", "To understand where the softmax function comes from, we should first define the hardmax function. We use hardmax (also called argmax) to determine which element in a vector has the highest value. The hardmax takes a vector z, and returns another vector a. If z_i is the maximum element of z, then a_i=1 otherwise a_i=0. So for example", "Here if z has more than one maximum element, then 1 will be divided between them. So if it has p maximum elements, then a_i=1/p for all of them. For example", "With the above definition, the output elements of hardmax are constrained to sum to one. Softmax is rather a smooth approximation to the hardmax function. We can write the softmax function in a more general form as:", "where \u03b2 is a constant. Now we are going to see what happens if \u03b2 goes to infinity. Suppose that z has p maximum elements and their values is equal to z_max. Now if z_i is one of those maximum elements", "where in the denominator, only the elements with the biggest exponent (z_max) are taken into account when \u03b2 goes to infinity. If z_i is not one of the maximum elements, then we have", "So in fact the general softmax function converges to the hardmax as \u03b2 goes to infinity, and for \u03b2=1 it is a smooth approximation of the hardmax. Hardmax is not a continuous function, so it is not differentiable. As we show later the activation function needs to be differentiable to be used with the learning algorithm, so softmax which is differentiable is used instead.", "We also need to calculate the derivative of softmax. Here a_i is a function of all the elements of z. So we take the derivative of a_i with respect to z_j (which can be any elements of z). Now if i=j, all z_k with k\u2260j are considered to be a constant and their derivative with respect to z_j will be zero", "Softmax is actually a mathematical generalization of the sigmoid function which can be used for multiclass classification under the assumption that the classes are mutually exclusive. Sigmoid is equivalent to a 2-element Softmax function in which we have only two mutually exclusive classes. Let\u2019s call them c_1 and c_2. Since we have two classes, the input vector (z) and the activation vector of the softmax layer (a) should also have two elements. Now we can write", "But the activation vector of softmax is normalized, so", "Here we have one equation with two unknowns which is underdetermined and has an infinite number of solutions. Hence we can fix one of its unknowns. We assume that z_2=0, so we have", "which is the sigmoid activation function for z_1 (remember Eq. 6). So Sigmoid is equivalent to a 2-element softmax where the second element is assumed to be zero.", "We can define the label matrix", "which combines the label vectors for all the examples. We will use the label matrix later. Of course, if we have a binary classification problem or a regression problem, the output label is a scalar. To have a consistent notation, we can assume that it is a matrix with just one row", "Similar to the label matrix we can define the output matrix which combines the network\u2019s output vectors for all the examples", "Again for binary classification or a regression problem we have", "Remember that the output of the network is yhat. For each example x^(i) the output or the network prediction is yhat^(i), and ideally we want y^(i)= yhat^(i). The loss (or error) function is a function that measures the output error. It tells us how far the network output yhat is from the true label y^(i). The quadratic loss function is defined as", "The loss function gives us the error for one specific example. However, we need the average error for all the examples since we want the network to learn all of them together. So we define the quadratic cost function as the average of the loss function of all the examples", "It is it\u2019s also known as the mean squared error (MSE) cost function. J is still a function of y^(i) and yhat^(i). But we know that the network output yhat^(i) is a function of network parameters itself. So the cost function is in fact function of these parameters. Here, w and b (without indices) denote the collection of the weights and biases of all the neurons in the network. Please note that both the loss function and the cost function are scalar functions (they return a scalar quantity). If we only have one neuron at the output layer, then Eqs. 111 and 112 become", "w and b are the adjustable parameters of the network and when we train a neural network, the goal is to find weights and biases that minimize the cost function J(w, b). When the cost function is minimized, we expect to have the minimum classification error for the training set. If we multiply a function with a positive multiplier a, the minimum of aJ(w,b) occurs at the same values of w,b as does the minimum of J(w,b). So the multiplier 1/2 in Eq. 112 has no effect on the minimization of the cost function, and it is usually added to ease the calculations. The MSE cost function is the default cost function for the regression problems.", "The quadratic cost function is not the only function that we can use for a neural network. In fact for a classification problem, we have a better choice called the cross-entropy function. It can be used when the activation of the neurons at the output layer are in the [0,1] range and can be thought of as a probability. So at the output layer, you should either have a single neuron with the sigmoid activation function (binary classification) or more than one neurons with the softmax activation function (multiclass classification).", "Cross-entropy can be defined using the likelihood function. In probability theory, the Bernoulli distribution is the discrete probability distribution of a random variable which can only take two possible values. We can label these values as \u2018success\u2019 and \u2018failure\u2019 or simply 1 and 0. An example is tossing a coin where the outcome is either heads or tails. Now suppose that this random variable which takes the value of 1 with probability p and the value of 0 with probability q=1-p. Here p is the parameter of the Bernoulli distribution. If we call this random variable T and use t for the values that it can take, the probability function of T can be written as follows", "Here f(t|p) is the conditional probability of observing t as a value of T (T=t), given the parameter p. We can also combine the conditions of the previous equation into one equation by writing it as", "When this probability function is regarded as a function of the parameter p, it is called the likelihood function", "Eq. 115 is for one random variable (or one data point). If we have m independent random variables T_1, T_2, \u2026 , T_m with the same Bernoulli distribution (or simply m data points), and t_1, t_2, . . . , t_k denote possible values of these variables, then the likelihood of observing T_1=t_1, T_2=t_2, \u2026, T_m=t_m at the same time is the product of the likelihood of observing T_i=t_i for each data point. Mathematically, the likelihood of our data give parameter p is", "Now suppose that we know the value of t_i, but p is an unknown variable. We want to find the value of p which gives the highest probability for observing a specific value of t_i for each random variable T_i. One way is to find the value of p which maximizes L(p) with that specific values of t_i. In statistics, this method is called the maximum likelihood estimation. So we are looking for", "Argmax is short for Arguments of the Maxima. The argmax of a function is the value of the domain at which the function is maximized. So argmax_p gives the value of p that maximizes L(p). To make the equation simpler, we maximize the natural logarithm of L(p). Since the logarithm is a monotonic function, the maximum of ln L(p) occurs at the same value of p as does the maximum of L(p).", "We call ln L(p) the log-likelihood. Using Eq. 116 we can write", "Now imagine that we have a single neuron with a sigmoid activation function. We have m examples in the training set. The neuron\u2019s activation for example i is yhat^(i) and the true label is y^(i). Since we only have one neuron in the last layer, yhat^(i) is a scalar, not a vector. Here yhat^(i) is a variable that can be changed by changing the network parameters. As mentioned before we define a threshold to convert the activation into a binary output for a single neuron (the binary output is 1 if the activation is greater than the 0.5 and zero otherwise). We can think of yhat^(i) as the probability of getting 1 as the binary output of the neuron.", "We can also think of the binary output of the neuron as a random variable that has a Bernoulli distribution and its parameter is yhat^(i). Now we want to have the log-likelihood of observing the true label y^(i) as the value of this random variable. So in Eq. 117, we can replace p by yhat^(i) and t_i by y^(i). Now we can write the log-likelihood function for this neuron as (for the whole training set)", "There is one more problem. In Eq. 49.5, the parameter p is the same for all the data points since they all follow the same Bernoulli distribution. However, here yhat^(i) may be different for each i since it is a function of the input matrix X^(i), and X^(i) is different for each example. What remains the same for all the data points is the network parameters w and b, and yhat^(i) is also a function of them. So when maximizing ln(L(p), we maximize it with respect to w and b instead of p", "However, instead of maximizing ln L(yhat^(i)), we can minimize its negative", "As mentioned before, if we multiply a function with a positive multiplier a, the minimum of aC(w,b) occurs at the same values of w,b as does the minimum of C(w,b). So we can multiply the term on the right-hand side of Eq. 119 with 1/m and minimize that instead", "Now we can think of -ln L(yhat^(i)) as a new cost function that should be minimized and we call it the binary cross-entropy cost function", "So minimizing this cost function minimizes the network\u2019s error in predicting the true labels of the examples. Binary cross-entropy is the default cost function for a binary classification problem. In this equation, we can assume that the cost function is the average of the loss function over all the examples", "This is similar to what we did in Eq. 112 for the quadratic cost function.", "If we have a multilabel classification with c classes, our output layer should have c neurons with sigmoid activation. Each neuron gives the value of one of the elements of the multi-hot encoded label vector y^(i). These neurons work independently, so each of them can use a binary cross-entropy cost function. Suppose that we have c independent random variables {T_j; j=1..c} with a Bernoulli distribution. We can show them by the random vector", "A random variable is usually shown by an uppercase letter, and since it is also a vector, it is an uppercase boldface letter, so please don\u2019t confuse it with a matrix. The probability function of each random variable is", "where p_i is the distribution parameter of each random variable. These parameters can be represented by the vector", "Now we want to know the probability function of observing a specific value for each of these random variables at the same time. If T_1=t_1 and T_2=t_2,\u2026,T_c=t_c, then the vector", "can represent their values. Now since these random variables are independent, the likelihood function of observing T=t given p is", "We have m data points (each data point is a c-dimensional point here), and each of them can be represented by t^(i). In addition, we assume that for each data point we have a separate distribution parameter. So the likelihood function for m points will be", "Again we can think of the binary output of a neuron as a random variable that has a Bernoulli distribution and its parameter is yhat^(i). Now we want to calculate the log-likelihood of observing the label vector y^(i) for the neurons of the last layer. So in Eq. 127, we can replace p_i^(j) by yhat_i^(j) and t_i^(j) by y_i^(j), and c by n^[L]. Now we can write the log-likelihood function for the last layer and for the whole training set as", "with respect to w and b. So the cost function (with the addition of the multiplier 1/m) will be", "It is the average of this loss function", "But what happens if we have a multiclass problem with more than one neuron at the output layer? Here we can use the multinomial distribution. Suppose that we have a discrete random variable T that can take c different values from the set {1, 2, \u2026, c}, so it\u2019s like a c-sided dice. The probability that it takes the value i is p_i, and", "We can use a one-hot encoded vector to show the current state of this random vector. So we have the vector t with c elements, and when T=j, the j-th element of t will be equal to 1 while the other elements are zero", "So the original random variable T can be represented by a random vector T that can take the different values of t. Since the probability of T=j is p_j, the probability of having t with t_j=1 is the same.", "Since all the elements except the j-th should be zero, we can write", "So the probability function that we observe the equivalent one-hot encoded vector of t for the random vector T given the parameters p is", "Eq. 135 is a special case of a multinomial distribution. Here each data point is a k-dimensional point (t_1, t_2,.., t_c) which is indicated by vector t. So in fact this equation is still for one data point. If we have m data points t^(1), t^(2), \u2026, t^(m), the likelihood will be", "Now We have to minimize the negative of the log-likelihood", "Now assume that we have a softmax layer with n^[L] neurons at the last layer. Our label vector for example x^(j) is y^(j) which has n^[L] elements. The softmax activation of neuron i for example j is yhat_i^(j) and the true label is the i-th element of y^(j) which is equal to y_i^(j).", "Again we can think of yhat_i^(j) as the probability of getting 1 as the binary output of neuron i (for example j). So can also think of the binary output of the softmax layer as a random vector that has a multinomial distribution and its parameter is yhat_i^(j). Each label vector is a possible vector that this random vector can take. Now we want to calculate the log-likelihood that this random vector takes the values of vector y^(i). So in Eq. 138, we can replace p_i^(j) by yhat_i^(j) and t_i^(j) by y_i^(j) and c by n^[L], add 1/m as a multiplier, and minimize it with respect to w and b", "So we can write the cost function as", "This is called the categorical cross-entropy cost function which the default cost function for the multiclass classification problems. Again we can assume that the cost function is the average of the loss function over all the examples", "So far we learned that learning the true label of the examples is equivalent to minimizing the cost function with respect to the network adjustable parameters. Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the negative of the gradient of the function at the current point. Suppose that we have a function f(x_1, x_2, \u2026, x_n) and we want to minimize it with respect to all its variables. We define the vector x as", "So each set of values for {x1, x2, \u2026, xn} can be represented by a point in an n-dimensional space, and the vector x refers to that point. We can assume that function f is a function of this vector", "Now we start with an initial point represented by x_initial, and from this n-dimensional point, we want to move iteratively toward the point x_min that minimizes f(x). So we start at x_initial, and at each step, we find a new point using the previous point", "We keep replacing our current point with a new point using this equation till we get close enough to x_min (within a tolerance). \u0394x is something that still needs to be determined and the gradient descent method tells us how to choose it. From calculus, we know that", "where dx_i means infinitesimal changes in x_i. For a relatively small change in xi, we can write", "Using the definition of the gradient (Eq. 41) and the dot product (Eq. 24), we write the previous equation as", "We want to move in a direction that gives the biggest decrease in \u0394f. Now, remember that the minimum value of the dot product of two vectors is when the vectors are in the opposite direction. So \u0394x should be in the direction of -\u0394f, however, we are free to choose its magnitude. So we can write \u0394x=-\u03b1\u0394f where \u03b1 is a scalar multiplier that can change the magnitude of \u0394x. So we can write Eq. 144 as", "\u03b1 is called the learning rate, and it is allowed to change at every iteration. We can also write Eq. 148 for each element of x to get", "Now we can get back to our cost function C(w,b). Here J is a function of all weights and biases in the network. So we can write", "for all possible values of i, j and l. Now if we assume that we have p adjustable parameters in the network (all the weights and biases in all layers together), these parameters form a p-dimensional space and we can use Eq. 149 for each of them", "These equations can be also written in vector form", "In Eq. 147 the value of \u0394x should be small enough to have a good approximation of \u0394f. As a result, the learning rate (\u03b1) shouldn\u2019t be too big. Otherwise, we may end up with a big step that can even increase f (\u0394f > 0). In addition, if \u03b1 is too small, it makes the steps toward the minimum point too short, and thus the gradient descent algorithm will work very slowly.", "To use the gradient descent method defined by Eqs. 151 and 152, we need to calculate the partial derivative or gradient of the cost function with respect to w and c, and to do that we use an algorithm named backpropagation. So the main goal of backpropagation is to compute", "In the backpropagation method, we first introduce an intermediate quantity", "which is called the error of the i-th neuron in the l-th layer. Using the definition of the gradient (Eq. 41), the previous equation can be written as", "in vector form. We may write the loss function without its arguments and the index of the training example in some of the equations like Eq .156, but you should note that the loss function is always related to a single example.", "The error vector and z^[l] have the same number of elements and using Eq. 41 the error vector for layer l can be also written as", "It is important to note that the error vector is defined for a single training example j. The error for a specific neuron relates the change in the neuron\u2019s net input to the change in the loss function. If we change a neuron\u2019s net input from", "the neuron\u2019s output changes to be", "and this change will propagate through the next layers until it reaches the output layer and changes the loss function which is defined based on the output of the network. The change in the loss function can be approximated as", "In other words, the error is somehow measuring the effect of this neuron in changing the loss function of the whole network. In the backpropagation method, we first compute the error and then compute the partial derivatives of the loss function using the error.", "We start with the definition of error for the last layer of the network, and by applying the chain rule, we can write the error term in terms of partial derivatives with respect to the output of the network (the last layer\u2019s output)", "If we don\u2019t have a softmax layer, then the output of the k-th neuron depends only on its net input z_i^[l] not the net input of other neurons in the last layer (Eqs. 74, 75 and 90), so", "As a result, Eq. 159 simplifies to", "since the other terms in the summation are zero. We know that", "So we can write Eq. 161 as", "where the prime symbol in the last line of the equation means derivation with respect to z_i^[L]. Using the definition of the gradient vector (Eq. 41) and the Hadamard product (Eq. 33), we can write this equation in the vector form as", "Eq. 164 calculates the error vector for the last layer of the network. Please note that it is only valid when you don\u2019t have a softmax layer. If you have the softmax layer then Eq. 160 is not correct anymore (in the softmax layer yhat_i^(j) depends on the net input of all the other neurons in that layer) and you cannot use it to reach Eq. 164. When we have the softmax layer, then we need to derive \u03b4^[L](j) in a different way, and it will be discussed later in this article.", "Now we need to calculate the error vector for the other layers. This time we start with the definition of error for layer l, and by applying the chain rule, we write the error term in terms of partial derivatives with respect to the net input of the next layer of the network (layer l+1)", "Using the definition of error we know that", "So we can write Eq. 165 as", "Using Eq. 75 (for neuron k in layer l+1) we have", "The net inputs of different neurons in one layer do not depend on each other. So", "Now by differentiating Eq. 168 with respect to z_i^[l] and knowing that the weights snd biases are not a function of z_i^[l], we get", "By substituting Eq. 170 back into Eq. 167 we obtain", "Finally, we can use the definition of the matrix transpose (Eq. 20) and matrix product (Eq. 19) and Hadamard product (Eq. 33) to get this equation in vector form", "Eq. 172 gives the error vector for layer l in terms of the error vector for the next layer. Now that we know how to calculate the error vector, we can relate it to the partial derivative of the loss function. Please note that we usually write Eq. 172 as", "without writing the brackets. However, it should always be evaluated from left to right and evaluating it as", "Starting with the error for layer l, we write the error term in terms of partial derivatives with respect to the bias", "From Eq. 74 (for neuron k) we get", "So the bias for each neuron only depends on its own net input, so we have", "Using this equation, we can simplify Eq. 174 to get", "In Eq. 175 the weights and the output of the previous layer do not depend on z_k^[l], so we have", "So by differentiating Eq. 175 with respect to z_k^[l], we get", "Since k is a dummy index, we can replace it with i", "By substituting Eq. 180 back into Eq. 177 we have", "So the gradient of the loss function with respect to the bias vector is equal to the error vector for each layer. Finally, we can write the partial derivative of the loss function with respect to the weight in terms of partial derivatives with respect to the net input", "Based on Eq. 74, we know that the net input for each neuron is only a function of the weights of the inputs of that neuron, so", "So we can simplify Eq. 183 to", "By differentiating Eq. 74 with respect to w_ik^[l] we get", "Now we can substitute Eq. 186 into Eq. 185 to have", "To express this equation in vector form, we should be careful about the dimension of vectors. The left-hand side of Eq. 187 is expressed as", "in matrix form. By looking at Eq. 63, we see that W^[l] has n^[l] rows and n^[l-1] columns. Based on Eq. 43 (\u2202J/\u2202W)^[l] is also a matrix of the same size, so the right side of Eq. 82 in matrix form should have the same size. We know that a^[l-1] is a column vector with n^[l-1] elements (Eq. 57). In addition, the error vector defined in Eq. 157 is a column vector with n^[l] elements. So we need to multiply the error vector by the transpose of a^[l-1] to get a matrix with n^[l] rows and n^[l-1] columns (refer to Eq 26). Hence the vector form of Eq. 187 is", "which gives the gradient of the loss function with respect to the weight matrix. Now that we have all the necessary equations, we can summarize the backpropagation algorithm:", "The reason that this algorithm is called backpropagation is that the error term is calculated backward starting from the output layer of the network. Now we can see how the error is calculated for each loss. The quadratic loss function (Eq. 111) for example j is:", "Now we can use Eq. 163 to calculate the error for the last layer. First, we need to calculate the gradient of the loss function with respect to the output vector. By differentiating Eq. 189 with respect to yhat_i^(j) we have", "The activation of each neuron is independent of the activations of other neurons, so", "By substituting this equation into Eq. 190 we get", "Finally by replacing this equation into Eq. 163 we get", "This equation can be written in vector form as", "When we have a binary classification, the binary cross-entropy loss function (Eq. 122) for example j is:", "Since we only have one neuron in the last layer, the error term and the net input of the last layer will be scalars, not vectors. The gradient of this loss function with respect to the output vector is", "The binary cross-entropy loss is usually used with the sigmoid activation function. So yhat^(j) is a sigmoid activation and from Eq. 7 we have", "Now by replacing this equation into Eq. 163 we get", "If we have a multilabel classification, the binary cross-entropy loss function (Eq. 130) for example j is:", "The gradient of this loss function with respect to the output vector (using Eq. 191) is", "Each neuron in the last layer has a sigmoid activation, so using Eqs. 7 and 162 we can write", "Now we can replace Eqs. 200 and 201 into Eq. 163 to get the error vector", "Finally, we are going to derive the error for the categorical cross-entropy loss. From Eq. 141 we get", "This loss is usually used with the softmax layer and as mentioned before, we cannot use Eq. 164 for that. So we need to derive the error vector directly. The error term will be:", "For the softmax activation, we can use Eq. 101 (by replacing a with yhat) to get", "So we can simplify Eq. 205 to have", "Remember that y^(j) is the one-hot coded label for the example j. So only one of its elements is one and the others are zero. Hence we have", "Now by replacing this equation into Eq. 207 we get", "This equation gives the error term for the categorical cross-entropy loss function when the last layer is a softmax layer.", "Based on Eqs. 164 and 173, we can see that the backpropagation algorithm uses the derivative of the activation function to calculate the error term. So the activation function needs to be differentiable. Of course, we can still use a non-differentiable function and assume that it is differentiable. For example, ReLU is not differentiable at z=0, but we assume that its derivative is either 0 or 1 at this point. The step function cannot be used with backpropagation. It is not differentiable at z=0 like ReLU, but that is not the reason. Its derivative is zero everywhere else, which makes the error term and the gradients of the cost function zero all the time. So the weights and biases won\u2019t be updated with the gradient descent method.", "In the backpropagation algorithm, we defined the error term as the derivative of the loss function with respect to the net input, however, it is also possible to define it with respect to the activation. So we define a new error term for neuron i in layer l and example j", "and we call it the activation error to distinguish it from the error term defined in Eq. 155 (so when we say error we are referring to this equation). The activation error for a specific neuron relates the change in the neuron\u2019s activation to the change in the loss function. First, we need to calculate the activation error of the previous layer", "Using Eq. 74 we can write", "By replacing this equation into the previous one we get", "Please note that the result of the multiplication of the transpose of the weight matrix and the error vector is a vector so it only has one index. So we have", "Now we can also write the error term in terms of the activation error. From Eq. 172 we have", "By replacing Eq. 214 into this equation we get", "which can be written in vector form as", "This equation can be used for all the layers including the last layer. In fact by setting l=L and yhat=a^[L], Eq. 214 turns into Eq. 164. As a result, it cannot be used for the softmax layer. So we need to calculate it directly for the categorical cross-entropy loss function (Eq. 141) when we have a softmax output layer as an example", "Now we can use Eq. 206 to have", "Since y_k^(j) is a one-hot encoded vector we can write", "Finally, by replacing this equation into the previous one we get", "This equation gives the activation error term for the categorical cross-entropy loss function when the last layer is a softmax layer. Unfortunately, it cannot be easily converted to the vector form. Now we can rewrite the backpropagation algorithm using the activation error. Now we can write the backpropagation algorithm using the activation error (In the Deep Learning Specialization course of Andrew Ng, this method of backpropagation has been used).", "The backpropagation algorithm gives us the gradients of the loss function, but what we need for the gradient descent method is the gradients of the cost function in Eqs. 153 and 154. If you refer to Eqs. 111, 123. 129, and 142, you will see that the cost function is the average of the loss function over all the training examples", "Hence we can get the gradient of individual example using the backpropagation algorithm and then take their average over all the training examples to have the gradients of the cost function for the gradient descent method. This method is called the batch or vanilla gradient descent, and is shown below:", "At each iteration, we calculate the gradients of the loss function for all the examples and then take their average to get the gradients of the cost function. Finally we update the weights and biases using them. We repeat updating W^[l] and b^[l] until a stopping criterion is met (for example the change in weights and bias values during an iteration is below a certain threshold). We need to initialize the weights and biases to be able to start the gradient descent. We usually initialize the weights to small random numbers and initialize the biases to be zero or a small constant value.", "Now that we have the backpropagation equations, we can calculate the error term for any layer in the network. Suppose that we want to calculate it for layer l. We first calculate the error term for the output layer and then move backward and calculate the error term for the previous layers until we reach layer l. From Eq. 173 for layer L-1 we have", "Now if replace the error term for layer L (Eq. 164 assuming that the output layer is not a softmax layer) in the previous equation, we get", "And we can continue this procedure until we get \u03b4^[L-2]", "We can use Eq. 171 to write this equation for each element of the error vector", "Based on this equation, each element of the error vector (which is the error for one of the neurons in that layer) is proportional to chained multiplications of the derivative of the activation function of the neurons in the next layers (If we have a softmax layer with categorical cross-entropy loss function, the error term for the last layer won\u2019t include the activation, however, it does not help with the hidden layers).", "Now if the activation function is a sigmoid or a tanh, g\u2019(z) can be a very small number when z is very small or very large (look at Figures 3 and 4). So chained multiplications of these small values can result in an extremely small error term especially if you have a deep network with so many layers. As a result, some or all of the elements of the error vector can be extremely small. Since the gradients of the loss function and cost function are proportional to the error term (refer to Eqs. 182, 188, 224, and 225), they will also become very small, so in Eqs. 153 and 154, the size of the steps will be very small. The final result is the slow-down of the gradient descent method and the network\u2019s learning process. This is called a vanishing gradient problem.", "ReLU and leaky ReLU activation functions can overcome this problem. ReLU activation has two modes of operation. When the net input is greater than or equal to zero, it is active an gives a linear response. However, it should also be a non-linear activation function, so when the net input is less than zero, it becomes inactive and the response is zero (Eq. 10). The derivative of ReLU is equal to 1 when it is active. So the result of chained multiplications of ReLU activation functions in Eq. 230 is simply 1, and the vanishing gradient problem is prevented.", "However, it is possible that we have one or more inactive ReLU activation functions in Eq. 230. The derivative of inactive ReLU is equal to 0, and only one zero is enough to make the whole chain equal to zero. So the error term of some neurons will be zero, and their weights and biases won\u2019t be updated in gradient descent. In leaky ReLU activation functions, when z is below zero, the output is a small number (cz) not zero (Eq. 12). So the whole chain won\u2019t become zero if only a few neurons have a negative z. Of course, if you have lots of leaky ReLU activations with z<0 in the chain of Eq 230, you still get a vanished error term, however, this is acceptable since a chain of inactive neurons is supposed to give a negligible error term.", "The ReLU and leaky ReLU activation can overcome the vanishing gradient problem for deep neural networks, so in fact, they make deep learning possible. So using the Relu activation function the neural network can often learn much faster than using the tanh or the sigmoid activation functions. Leaky ReLU usually works better than the Relu activation function although it\u2019s just not used as much in practice. As mentioned before, in the output layer, we can use sigmoid or softmax activation functions if we have a classification problem or a linear activation for a regression problem. ReLU is usually the default choice for all the other layers.", "Now consider Eq. 230 again. We also have chained multiplications of the weights of the neurons in the next layers. Depending on the initial values of the weights the result can be a very large or very small error term. When the chained multiplication of weight matrices is too small, we end up with the vanishing gradient problem again. When it is very large, the gradients and the step size in the gradient descent can explode. The result is an unstable network, and gradient descent steps cannot converge to the optimal values of weight and biases since the steps are now too big and miss the optimal point. This is called an exploding gradient problem. We can use weight initialization techniques like Xavier and He initialization to overcome this problem.", "The batch gradient descent needs to process all the training examples to find the gradients of the cost function. This is a disadvantage when the number of training examples is very large. In that case, the batch gradient descent needs a long time to converge, so it is not suitable for huge datasets. Another option is stochastic gradient descent. In this algorithm, we first need to randomly shuffle the whole training set. As a result, when we pick an example using its index (i), it will be picked randomly. Here instead of taking the average of the loss function over all the training examples, we use only one training example in every iteration to compute the gradient of the cost function. In fact, we assume that the cost function is equal to the loss function of example i in that iteration, so the gradient of the cost function is equal to the gradient of the loss function", "Since stochastic gradient descent uses one training example in every iteration, it is much faster for larger data sets. However the assumption in Eqs 231 and 232 is not very accurate, and the loss function of one example may not be an accurate estimation of the cost function for the whole training set.", "To make this estimation more accurate we can use the mini-batch gradient descent which is a compromise between the batch and stochastic gradient descent. In mini-batch gradient descent rather than using the complete training set, in every iteration, we use a set of s training examples to compute the gradient of the cost function. We can think of these s examples as a representative sample of the whole training set.", "First, we randomly shuffle the training set and then split it into smaller training sets called mini-batches. Each mini-batch has s example, and s is much smaller than m which is the total number of training examples (If m is not divisible by s, the size of the last mini-batch will be smaller than s, but let\u2019s assume that m is divisible by s). So we will have m/s mini-batches.", "Remember the input matrix in Eq. 71 which combines the input vectors for all the training examples. Suppose that we first shuffle the example and re-index them from 0 to m. Now the input matrix is", "We can now split the input vectors of the examples in this matrix as follows", "So we can split the original n\u00d7m input matrix into m/s mini-batches. Each mini-batch has an n\u00d7s matrix that contains the input vectors of s examples. We use a number inside curly brackets {} as a superscript to indicate the mini-batch number. So X^{t} contains the input vectors of the t-th mini-batch. We can also split the label matrix Y (Eq. 107) similarly. Remember that the number of elements of y^(i) is equal to n^[L], so Y is an n^[L]\u00d7s matrix. Y^{t} has the label vectors of the t-th mini-batch. It is an n^[L]\u00d7s matrix that contains the labels of s examples.", "So generally the t-th mini-batch can be represented by the pair", "Now we assume that the examples in each mini-batch are a representative sample of the whole training set. So the average of the loss functions of the examples in each mini-batch t is an accurate approximation of the cost function of the whole training set. So for the t-th mini-batch, we have", "where the average is taken over all the examples in this mini-batch.", "When the outer for loop completes the iterations from 1 to m/s, we have a complete passe through the training dataset which means that all the examples in the training set have been used to update the gradients of the cost function. This is called one epoch, so the number of epochs is the number of complete passes through the training dataset or the number of iteration of the repeat-until loop in the algorithm. We can stop the repeat-until loop when a stopping criterion is met or we have reached a certain number of epochs. We randomly shuffle the examples in the training set for each epoch.", "Mini-batch gradient descent is faster than batch gradient descent, it is very popular in deep learning. It also results in a smoother convergence compared to stochastic gradient descent since the gradient computed at each step uses more training examples.", "Remember that in Eq. 86 we vectorized the forward propagation equations. We can also vectorize the backpropagation equations to have a fully vectorized mini-batch gradient descent algorithm. Remember the label matrix defined in Eq. 107. Each element of this matrix is", "Similarly each element of the output matrix in Eq. 109 is", "Now we can define the loss vector as", "Using Eq. 41 and the definition of the error vector Eq. 156 we can write the error matrix as", "We also define another matrix to vectorize the activation error and we call it the activation error matrix", "Now for the output layer, we have", "Please note that (g^[L])\u2019 is a vectorized function that is applied to all the elements of Z^[L]. Eq 242 is the equivalent of Eq. 163 for all the examples. Since Eq. 163 cannot be used for softmax layer, Eq. 242 is not valid for that either. If we have the softmax layer, with categorical cross-entropy loss, we can use Eq. 202 to write", "Now we write the vector derivatives of the loss vector", "which means that the gradient of the cost function with respect to the bias vector is the sum of the columns of the error matrix divided by m", "Finally, we need to calculate the gradient of the cost function with respect to weights. Using Eq. 187 we can write", "Now for each element of the gradient of the cost function, we can use Eq. 250 and write", "where in the last line we used the definition of matrix multiplication (Eq. 19). So we have", "Now we have all the vectorized equations to write the mini-batch gradient descent algorithm vectorized over all the examples. Please note that for the matrices in the previous equation are formed based on the examples in each minibatch, so we should add the superscript {t} to them, and to take the average of the gradient of loss functions, we should use the minibatch size s.", "We can also write the vectorized mini-batch gradient descent algorithm using the activation error equations. To do that we only need to use Eq. 214 and write", "Now we can use it to do the backpropagation", "I hope that you enjoyed reading this article. In this rather long article, I tried to give a deep introduction to the mathematics that lies at the heart of feedforward neural networks. However, there are lots of important topics that were not covered here like neural network initialization techniques, regularization, and optimization methods. I hope that I can discuss them in the future.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1af281e306cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://reza-bagheri79.medium.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": ""}, {"url": "https://reza-bagheri79.medium.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Reza Bagheri"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda2d000eaa4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&user=Reza+Bagheri&userId=da2d000eaa4d&source=post_page-da2d000eaa4d----1af281e306cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=77415260637&device=c&keyword=coursera%20deep%20learning%20ai&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493133115&hide_mobile_promo&gclid=CjwKCAjw4pT1BRBUEiwAm5QuRwgTOsOYZ5KBSCJ2uUPnH0uM5tieL87a4aVcmxP_SAtDaaMX2_9prBoCmjEQAvD_BwE", "anchor_text": "Deep Learning Specialization course"}, {"url": "https://en.wikipedia.org/wiki/Differentiable_function", "anchor_text": "differentiable functions"}, {"url": "https://en.wikipedia.org/wiki/Random_variable", "anchor_text": "random variable"}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood"}, {"url": "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation", "anchor_text": "maximum likelihood estimation"}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution", "anchor_text": "multinomial distribution"}, {"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=77415260637&device=c&keyword=coursera%20deep%20learning%20ai&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493133115&hide_mobile_promo&gclid=CjwKCAjw4pT1BRBUEiwAm5QuRwgTOsOYZ5KBSCJ2uUPnH0uM5tieL87a4aVcmxP_SAtDaaMX2_9prBoCmjEQAvD_BwE", "anchor_text": "Deep Learning Specialization course"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----1af281e306cd---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1af281e306cd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/feedforward-net?source=post_page-----1af281e306cd---------------feedforward_net-----------------", "anchor_text": "Feedforward Net"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1af281e306cd---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----1af281e306cd---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&user=Reza+Bagheri&userId=da2d000eaa4d&source=-----1af281e306cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&user=Reza+Bagheri&userId=da2d000eaa4d&source=-----1af281e306cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1af281e306cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1af281e306cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1af281e306cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1af281e306cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1af281e306cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1af281e306cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1af281e306cd--------------------------------", "anchor_text": ""}, {"url": "https://reza-bagheri79.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://reza-bagheri79.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Reza Bagheri"}, {"url": "https://reza-bagheri79.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "915 Followers"}, {"url": "https://www.linkedin.com/in/reza-bagheri-71882a76/", "anchor_text": "https://www.linkedin.com/in/reza-bagheri-71882a76/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda2d000eaa4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&user=Reza+Bagheri&userId=da2d000eaa4d&source=post_page-da2d000eaa4d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f6d4b1775e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-deep-feedforward-neural-networks-1af281e306cd&newsletterV3=da2d000eaa4d&newsletterV3Id=6f6d4b1775e3&user=Reza+Bagheri&userId=da2d000eaa4d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}