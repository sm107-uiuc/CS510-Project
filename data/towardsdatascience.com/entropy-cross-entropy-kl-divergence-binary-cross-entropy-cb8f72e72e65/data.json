{"url": "https://towardsdatascience.com/entropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65", "time": 1683000537.253908, "path": "towardsdatascience.com/entropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65/", "webpage": {"metadata": {"title": "Entropy, Cross Entropy, KL Divergence & Binary Cross Entropy | by Mahendran Venkatachalam | Towards Data Science", "h1": "Entropy, Cross Entropy, KL Divergence & Binary Cross Entropy", "description": "The definition of (Shannon) Entropy wasn\u2019t intuition at first sight for me. Had to get abstract a bit to get a sense of what it meant. Here\u2019s an explanation flow that might work for some of you to\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["The definition of (Shannon) Entropy wasn\u2019t intuition at first sight for me. Had to get abstract a bit to get a sense of what it meant. Here\u2019s an explanation flow that might work for some of you to get there.", "Imagine a coin toss machine that tosses a coin and sends out a signal indicating what the outcome of the toss was. For a fair coin, there are two possible outcomes with equal probability of 0.5. How many bits of information does the machine need to communicate the outcome? Well it\u2019s 1. Say 0 for heads and 1 for tails. Put differently, how many binary answer (yes/no) questions do we need to determine the outcome? It\u2019s 1, the answer to \u201cWas is heads?\u201d will tell us what the outcome was.", "Imagine two coins being tossed. Four possible outcomes (hh, ht, th, tt) with equal probabilities of 0.25. In this case, we need two bits or binary-answer-questions (00, 01, 10, 11) to know what happened. For three coins, it will be 3 and so on.", "So what is this \u201cNo. of bit\u201d number? This is the minimum number of bits needed to convey the outcome of the coin toss. A coin toss machine with one coin, will require to send one bit (0 or 1) to the receiver to communicate what was the output of a given toss. Similarly, two and three coin toss machines will have to send 2 and 3 bits respectively to communicate the output. Using the definition of \u201clog\u201d, this number is also the binary log of the possible outcomes. i.e, ex: 2 power 3 = 8 => 8 possible outcomes requires 3 binary bits to represent outcome. This is also the \u201centropy\u201d for the event.", "So is entropy the minimum number of bits we need to communicate an outcome? Well, not quite, but that\u2019s not a bad place to start. Imagine this scenario: what if the coin toss outcomes are not equally likely? What if we used coins such that it always landed on heads? i.e. the probability of heads is 1 and tails 0. Well, then the machine will always send a \u201c1\u201d. There is nothing we will learn from that signal.", "And what if \u2014 in the example of two coin toss example, the probabilities are not equal? Like depicted below, we could come up with a coding scheme to represent outputs such that it is different for different outcomes. We can\u2019t really apply \u201cbinary log of the possible outcomes\u201d, because there are still four possible outcomes, but it\u2019s not equally probable and so \u201c2 bits\u201d is not universally required.", "Here\u2019s where we need to adjust the way we defined Entropy a bit. What we are really interested in, is not how many bits of data we need to communicate the outcomes, but rather it is \u201chow much information we are really learning from what was communicated\u201d, regardless of the actual physical bits used to communicate.", "In other words, we want Entropy to represent how much \u201cinformation content\u201d is present in the outcome \u2014 however it is communicated to us. We want it to be a quantitative measure of the information content aka uncertainty associated with the event. It will be \u201c0\u201d, meaning \u201cI learnt nothing\u201d or \u201cI was completely certain about the outcome\u201d, for a biased coin that always turns up heads on tossing. And a \u201c1\u201d for a fair coin toss outcome, meaning \u201cI learnt a lot\u201d or \u201cI am most uncertain about\u201d a fair coin toss output as it could either be heads or tails. This uncertainty or information content is what is measured by Entropy.", "While in a way, this might give us a theoretical limit on the number of bits needed to communicate the outcome \u2014 let\u2019s not constrain ourselves with that as the basis of the definition. Bits come in full \u2014 there\u2019s no \u201chalf a bit\u201d. But our refreshed definition of Entropy is a measure of \u201cnews\u201d in the communication. So similar to other measures like area, length, weight, etc., it is continuous and can be any number or fraction. This is Property #1 of Entropy.", "We will go back to Figure 1 and Figure 2 examples to introduce another property for this measure. When we are dealing with events containing equally probable outcomes, the measure should increase with the number of possible outcomes. Figure 1 had a \u201c1\u201d bit entropy and Figure 2 has \u201c2\u201d bits and so on. This is Property #2 of Entropy.", "Side note: Such equally probable instances where the number of possible outcomes is a power of 2 are realistically the places where we can apply the theoretical limit of \u201cwhat I learnt, i.e. information entropy\u201d = \u201cno. of bits needed to communicate the outcome\u201d. In many other cases, this \u201cshortest code\u201d becomes just a theoretical limit and in reality, more bits than information in the message might be needed to communicate the outcomes of the event.", "I said \u201cmany other cases\u201d in the side note, because in addition to such scenarios, it is also possible to achieve the theoretical \u201cshortest code\u201d in scenarios with the number of possible outcomes that are not a power of two and are not equally probable. In scenarios where it is possible to group the outcomes such that the groups are uniformly distributed, we could achieve this. Take a look at the example below with three outcomes.", "Going back to determining how many binary questions we need to derive the outcome, first, we could ask \u201cIs the outcome x1?\u201d \u2014 this is akin to grouping x1 into one and x2,x3 into the other. Both these groups have equal probability of being the outcomes (Probability of x1 = 50% = Sum of Probabilities of x2 and x3 = 33.33% + 16.66%). If the answer is \u201cno\u201d, then we can ask \u201cis it x2? And that\u2019s it, the max questions we need to ask to determine the outcome is 2.", "The equivalent bit representation for this \u201cone question is all I need if outcome is x1 and two is max questions I need to ask otherwise\u201d is provided in the table illustration. Figure 6 below provides a binary tree view of the same. As opposed to previous examples, in this non-equally probable example, you might notice that the no. of bits required to represent outcomes vary by outcome. Using the first bit or question to determine if the most likely outcomes have occurred makes sense if we want to ask fewer questions or fewer bits overall to represent outcomes of such a source.", "This leads us to Property #3 of Entropy. If a choice can be broken down into two successive choices \u2014 i.e. choosing between x1, x2, x3 was broken down into choosing between (x1) and (x2, x3) in the example above \u2014 then the original Entropy should be the weighted sum of the individual Entropies for the groups. In other words, the total expected entropy at a node of the binary tree representation = weighted sum of entropies from the two child nodes, where the weights are the probability that particular child node is true.", "So how do we define this measure? Let\u2019s go back to the \u201cbinary log of the possible outcomes\u201d definition \u2014 which we saw did not hold true for non-equally probable outcomes. If you think about it, the number of possible outcomes in an event with equally probable outcomes is the same as the inverse of the probability. i.e. in a coin toss, no. of possible outcomes = 2 = 1/0.5. Where 0.5 is the probability of the outcomes. For the two coin toss example, no. of possible outcomes = 4 = 1/0.25. So perhaps the right way to define it is not \u201cbinary log of the possible outcomes\u201d, but it is \u201cbinary log of (1/p)\u201d, where \u201cp\u201d is the probability of a given outcome. While this is for an outcome, the Shannon Entropy for the event can then be described as the expected Information content from the event.", "Using this definition, the information content and the expected Entropy for the different examples we have seen so far is presented in the tables in Figures 8 and 9 below. In each, we can notice the three properties have been met. The measure is continuous, it increases monotonically (compare single and double fair coin toss examples) and by definition of expectation \u2014 it is a weighted sum of individual entropies.", "Let\u2019s look at the single unfair coin toss example a bit more closely. You might notice the information content associated with heads will be binary log of (1/0.9) = 0.15 bits. Compare this with a normal coin with 50% probability of heads, the binary log of (1/0.5) = 1 bit. The biased coin has less information associated with heads, as it is heads 90% of the times, i.e. almost always. With such a coin, getting a tail is much more newsworthy than getting a head. That is why the information associated with tails is, binary log of (1/0.1) = 3.32 bits, a lot more than the information associated with a head.", "Put differently, the more rare an outcome is, the more questions we need to ask to determine if it had occurred. \u201cIs the asteroid in our solar system?\u201d is perhaps the first question we need to answer before probing with further, harder, questions to plot it\u2019s course. And before we can confirm that the extremely unlikely scenario of collision with earth is going to occur, we might have asked a whole lot more questions. This is all to say that more uncommon outcomes or scenarios with a lot of uncertainty require learning a lot more information associated with that outcome. Using \u201cminimum bits to represent\u201d is just a very \u201csender of message\u201d specific way of saying the same thing. And Entropy is this measure of information value associated with an event.", "Below is a plot of Entropy in the case of two possibilities with probabilities p and (1-p), that will help notice a couple of interesting observations, reiterating what we saw above. We can notice H is 0 in both the extreme cases of probability 1 and 0. In both those cases, we don\u2019t really learn anything. It is the highest when the possibilities are equally probable. That also makes sense since this is the situation where no bias is involved and where things are completely random and anything can happen.", "Let\u2019s refer back to the examples in Figure 9. Imagine that we were planning to model the communication of the outcomes of the \u201ctwo coin toss\u201d event. Let\u2019s say we started with the assumption that these are fair coins, so we predicted all outcomes are equally probable and therefore started used two bits to representing the outcomes \u2014 as depicted in the left table of Figure 9 (all 0.25). After a while, we observe that these are not really fair coins and the real probability distribution is as shown in the right table of Figure 9 (0.5, 0.25, 0.125, 0.125).", "Now, obviously we are not being efficient here. The Expected entropies for these two distributions are different, 1.75 vs 2, the individual information content for the different outcomes are different and it is obvious our coding scheme could be better. For example, we don\u2019t need two bits to represent the first outcome that has 0.5 probability, just one should be fine.", "How do we get a sense of how much inefficiency is here? This is exactly what Cross Entropy and KL Divergence help us do. Cross Entropy is the expected entropy under the true distribution P when you use a coding scheme optimized for a predicted distribution Q. The table in Figure 10 demonstrates how Cross Entropy is calculated. The information content of outcomes (aka, the coding scheme used for that outcome) is based on Q, but the true distribution P is used as weights for calculating the expected Entropy. This is the Cross Entropy for distributions P, Q. And the Kullback\u2013Leibler divergence is the difference between the Cross Entropy H for PQ and the true Entropy H for P.", "KL for (P||Q) gives the average extra bits required when true distribution P is represented using a coding scheme optimized for Q. Put differently, it would be the information gain we will achieve if we start representing the same event using P, the true distribution, rather than Q the prior distribution. Which would, of course, be different if P were to be the predicted distribution and if a coding scheme optimized for P was used to represent true distribution Q.", "Both a careful look at the formula and intuitively, the KL Divergence, though it is a measure of the difference between the two distributions, it is not a true metric i.e. KL (P||Q) <> KL for (Q||P). However, it can be easily shown that this divergence reduces as Q gets closer to P and will be 0 when P=Q. This is demonstrated in Figure 12.", "And this is what we use as a loss function while training Neural Networks. When we have an image classification problem, the training data and corresponding correct labels represent P, the true distribution. The NN predictions are our estimations Q. The math involved in such single label classifications is relatively more simple, as P will be 1 for a given label and 0 for others.", "Writing things down as an equation (and applying the power rules to get the -1 out):", "In a multi-class classification problem, \u201cn\u201d represents the number of classes. In the example in Figure 13, this was 4. In a binary classification problem, i.e. just like in a coin toss, the number of classes/outputs is 2 and either should be true, and the probability of one is (1 \u2014 probability of other), we can write it down as below:", "We can use this binary cross entropy representation for multi-label classification problems as well. In the example seen in Figure 13, it was a multi-class classification problem where only output can be true i.e. only one label can be tagged to an image. In cases where we can have multiple labels associated with an image, i.e. where the probabilities for each label or output by itself can be anything between 0 and 1 and they are independent of each other, we can still leverage cross entropy for our loss. Each label classification is an independent binary cross entropy problem by itself and the global error can be the sum of the Binary cross entropies across the predicted probabilities of all labels.", "From this point onwards, the appropriate usage of sigmoid vs softmax for multi-classification vs multi-label problems should become a lot more apparent. And hopefully, though a tad long-winded, this post provides a bit of clarity on how these functions work."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcb8f72e72e65&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae----cb8f72e72e65---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb8f72e72e65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----cb8f72e72e65---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb8f72e72e65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=-----cb8f72e72e65---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://A Mathematical Theory of Communication", "anchor_text": "A Mathematical Theory of Communication"}, {"url": "http://A Mathematical Theory of Communication", "anchor_text": "A Mathematical Theory of Communication"}, {"url": "http://A Mathematical Theory of Communication", "anchor_text": "A Mathematical Theory of Communication"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cb8f72e72e65---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/entropy?source=post_page-----cb8f72e72e65---------------entropy-----------------", "anchor_text": "Entropy"}, {"url": "https://medium.com/tag/cross-entropy?source=post_page-----cb8f72e72e65---------------cross_entropy-----------------", "anchor_text": "Cross Entropy"}, {"url": "https://medium.com/tag/kl-divergence?source=post_page-----cb8f72e72e65---------------kl_divergence-----------------", "anchor_text": "Kl Divergence"}, {"url": "https://medium.com/tag/binary-classification?source=post_page-----cb8f72e72e65---------------binary_classification-----------------", "anchor_text": "Binary Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb8f72e72e65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----cb8f72e72e65---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb8f72e72e65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----cb8f72e72e65---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb8f72e72e65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae----cb8f72e72e65---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8b9125d53ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&newsletterV3=4d2735a047ae&newsletterV3Id=b8b9125d53ec&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----cb8f72e72e65---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Written by Mahendran Venkatachalam"}, {"url": "https://medium.com/@mahendran.venkatachalam/followers?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "296 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gotensor.com/", "anchor_text": "https://gotensor.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae----cb8f72e72e65---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8b9125d53ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65&newsletterV3=4d2735a047ae&newsletterV3Id=b8b9125d53ec&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----cb8f72e72e65---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/attention-in-neural-networks-e66920838742?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/attention-in-neural-networks-e66920838742?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Attention in Neural NetworksSome variations of attention architectures"}, {"url": "https://towardsdatascience.com/attention-in-neural-networks-e66920838742?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "\u00b712 min read\u00b7Jul 7, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----e66920838742----0-----------------clap_footer----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/attention-in-neural-networks-e66920838742?source=author_recirc-----cb8f72e72e65----0---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=-----cb8f72e72e65----0-----------------bookmark_preview----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cb8f72e72e65----1---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----cb8f72e72e65----1-----------------bookmark_preview----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cb8f72e72e65----2---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----cb8f72e72e65----2-----------------bookmark_preview----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "An introduction to AttentionThe why and the what"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": "\u00b75 min read\u00b7Jun 29, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda0e838c7cda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----da0e838c7cda----3-----------------clap_footer----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda?source=author_recirc-----cb8f72e72e65----3---------------------8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda0e838c7cda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda&source=-----cb8f72e72e65----3-----------------bookmark_preview----8d566a6d_e4c8_48a6_9c36_0553886a4c8e-------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "See all from Mahendran Venkatachalam"}, {"url": "https://towardsdatascience.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----cb8f72e72e65----0-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----cb8f72e72e65----1-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Prof Bill Buchanan OBE"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Similarity Hashing and Perceptual HashesSean McKeown [here] and myself have just published a paper to arXiv that will be presented at DFRWS (Digital Forensics Research Conference)\u2026"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b79 min read\u00b7Dec 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&user=Prof+Bill+Buchanan+OBE&userId=e680fcaf274b&source=-----963fba36c8b5----0-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----cb8f72e72e65----0---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&source=-----cb8f72e72e65----0-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://marcosanguineti.medium.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://marcosanguineti.medium.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Marco Sanguineti"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Implementing Custom Loss Functions in PyTorchUnderstanding the theory and implementation of custom loss functions in PyTorch using the MNIST dataset"}, {"url": "https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b712 min read\u00b7Jan 16"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=-----50739f9e0ee1----1-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=read_next_recirc-----cb8f72e72e65----1---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&source=-----cb8f72e72e65----1-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://medium.com/dataman-in-ai/the-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://dataman-ai.medium.com/?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://dataman-ai.medium.com/?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Chris Kuo/Dr. Dataman"}, {"url": "https://medium.com/dataman-in-ai?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Dataman in AI"}, {"url": "https://medium.com/dataman-in-ai/the-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "The Intuitions for the Discrete Distributions: Bernoulli, Binomial, Beta, Dirichlet DistributionsMachine learning uses a lot of discrete distributions such as the Bernoulli, Binomial, and Multinomial distributions to solve problems. Two\u2026"}, {"url": "https://medium.com/dataman-in-ai/the-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b716 min read\u00b7Jan 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdataman-in-ai%2Fab538e056f48&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdataman-in-ai%2Fthe-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=-----ab538e056f48----2-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://medium.com/dataman-in-ai/the-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48?source=read_next_recirc-----cb8f72e72e65----2---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab538e056f48&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdataman-in-ai%2Fthe-intuitions-for-the-discrete-distributions-bernoulli-binomial-beta-dirichlet-distributions-ab538e056f48&source=-----cb8f72e72e65----2-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://medium.com/@will.badr?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://medium.com/@will.badr?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Will Badr"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "Uncovering Anomalies with Variational Autoencoders (VAE): A Deep Dive into the World of\u2026An example use case of using Variational Autoencoders (VAE) to detect anomalies in all types of data"}, {"url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": "\u00b79 min read\u00b7Jan 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b2bce47e2e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9&user=Will+Badr&userId=551ba3f6b67d&source=-----1b2bce47e2e9----3-----------------clap_footer----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9?source=read_next_recirc-----cb8f72e72e65----3---------------------9067e251_42e7_4d62_bf80_72d79016f32b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b2bce47e2e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9&source=-----cb8f72e72e65----3-----------------bookmark_preview----9067e251_42e7_4d62_bf80_72d79016f32b-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----cb8f72e72e65--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}