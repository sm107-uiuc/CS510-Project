{"url": "https://towardsdatascience.com/k-nearest-neighbors-computational-complexity-502d2c440d5", "time": 1683012103.112309, "path": "towardsdatascience.com/k-nearest-neighbors-computational-complexity-502d2c440d5/", "webpage": {"metadata": {"title": "k nearest neighbors computational complexity | by Jakub Adamczyk | Towards Data Science", "h1": "k nearest neighbors computational complexity", "description": "kNN (k nearest neighbors) is one of the simplest ML algorithms, often taught as one of the first algorithms during introductory courses. It\u2019s relatively simple but quite powerful, although rarely\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["kNN (k nearest neighbors) is one of the simplest ML algorithms, often taught as one of the first algorithms during introductory courses. It\u2019s relatively simple but quite powerful, although rarely time is spent on understanding its computational complexity and practical issues. It can be used both for classification and regression with the same complexity, so for simplicity, we\u2019ll consider the kNN classifier.", "kNN is an associative algorithm \u2014 during prediction it searches for the nearest neighbors and takes their majority vote as the class predicted for the sample. Training phase may or may not exist at all, as in general, we have 2 possibilities:", "We focus on the methods implemented in Scikit-learn, the most popular ML library for Python. It supports brute force, k-d tree and ball tree data structures. These are relatively simple, efficient and perfectly suited for the kNN algorithm. Construction of these trees stems from computational geometry, not from machine learning, and does not concern us that much, so I\u2019ll cover it in less detail, more on the conceptual level. For more details on that, see links at the end of the article.", "In all complexities below, times of calculating the distance were omitted, since they are in most cases negligible compared to the rest of the algorithm. Additionally, we mark:", "Prediction time complexity: O(k * n * d)", "Training phase technically does not exist, since all computation is done during prediction, so we have O(1) for both time and space.", "Prediction phase is, as method name suggest, a simple exhaustive search, which in pseudocode is:", "This is a nested loop structure, where the outer loop takes k steps and the inner loop takes n steps. 3rd point is O(1) and 4th is O(# of classes), so they are smaller. Additionally, we have to take into consideration the numer of dimensions d, more directions mean longer vectors to compute distances. Therefore, we have O(n * k * d) time complexity.", "As for space complexity, we need a small vector to count the votes for each class. It\u2019s almost always very small and is fixed, so we can treat it as a O(1) space complexity.", "Training time complexity: O(d * n * log(n))", "Training space complexity: O(d * n)", "Prediction time complexity: O(k * log(n))", "During the training phase, we have to construct the k-d tree. This data structure splits the k-dimensional space (here k means k dimensions of space, don\u2019t confuse this with k as a number of nearest neighbors!) and allows faster search for nearest points, since we \u201cknow where to look\u201d in that space. You may think of it like a generalization of BST for many dimensions. It \u201ccuts\u201d space with axis-aligned cuts, dividing points into groups in children nodes.", "Constructing the k-d tree is not a machine learning task itself, since it stems from computational geometry domain, so we won\u2019t cover this in detail, only on conceptual level. The time complexity is usually O(d * n * log(n)), because insertion is O(log(n)) (similar to regular BST) and we have n points from the training dataset, each with d dimensions. I assume the efficient implementation of the data structure, i. e. it finds the optimal split point (median in the dimension) in O(n), which is possible with the median of medians algorithm. Space complexity is O(d * n) \u2014 note that it depends on dimensionality d, which makes sense, since more dimensions correspond to more space divisions and larger trees (in addition to larger time complexity for the same reason).", "As for the prediction phase, the k-d tree structure naturally supports \u201ck nearest point neighbors query\u201d operation, which is exactly what we need for kNN. The simple approach is to just query k times, removing the point found each time \u2014 since query takes O(log(n)), it is O(k * log(n)) in total. But since the k-d tree already cuts space during construction, after a single query we approximately know where to look \u2014 we can just search the \u201csurroundings\u201d around that point. Therefore, practical implementations of k-d tree support querying for whole k neighbors at one time and with complexity O(sqrt(n) + k), which is much better for larger dimensionalities, which are very common in machine learning.", "The above complexities are the average ones, assuming the balanced k-d tree. The O(log(n)) times assumed above may degrade up to O(n) for unbalanced trees, but if the median is used during the tree construction, we should always get a tree with approximately O(log(n)) insertion/deletion/search complexity.", "Training time complexity: O(d * n * log(n))", "Training space complexity: O(d * n)", "Prediction time complexity: O(k * log(n))", "Ball tree algorithm takes another approach to dividing space where training points lie. In contrast to k-d trees, which divides space with median value \u201ccuts\u201d, ball tree groups points into \u201cballs\u201d organized into a tree structure. They go from the largest (root, with all points) to the smallest (leaves, with only a few or even 1 point). It allows fast nearest neighbor lookup because nearby neighbors are in the same or at least close \u201cballs\u201d.", "During the training phase, we only need to construct the ball tree. There are a few algorithms for constructing the ball tree, but the one most similar to k-d tree (called \u201ck-d construction algorithm\u201d for that reason) is O(d * n * log(n)), the same as k-d tree.", "Because of the tree building similarity, the complexities of the prediction phase are also the same as for k-d tree.", "To summarize the complexities: brute force is the slowest in the big O notation, while both k-d tree and ball tree have the same lower complexity. How do we know which one to use then?", "To get the answer, we have to look at both training and prediction times, that\u2019s why I have provided both. The brute force algorithm has only one complexity, for prediction, O(k * n). Other algorithms need to create the data structure first, so for training and prediction they get O(d * n * log(n) + k * log(n)), not taking into account the space complexity, which may also be important. Therefore, where the construction of the trees is frequent, the training phase may outweigh their advantage of faster nearest neighbor lookup.", "Should we use k-d tree or ball tree? It depends on the data structure \u2014 relatively uniform or \u201cwell behaved\u201d data will make better use of k-d tree, since the cuts of space will work well (near points will be close in the leaves after all cuts). For more clustered data the \u201cballs\u201d from the ball tree will reflect the structure better and therefore allow for faster nearest neighbor search. Fortunately, Scikit-learn supports \u201cauto\u201d option, which will automatically infer the best data structure from the data.", "Let\u2019s see this in practice on two case studies, which I\u2019ve encountered in practice during my studies and job.", "The more \u201ctraditional\u201d application of the kNN is the classification of data. It often has quite a lot of points, e. g. MNIST has 60k training images and 10k test images. Classification is done offline, which means we first do the training phase, then just use the results during prediction. Therefore, if we want to construct the data structure, we only need to do so once. For 10k test images, let\u2019s compare the brute force (which calculates all distances every time) and k-d tree for 3 neighbors:", "As you can see, the performance gain is huge! The data structure method uses only a tiny fraction of the brute force time. For most datasets this method is a clear winner.", "Machine Learning is commonly used for image recognition, often using neural networks. It\u2019s very useful for real-time applications, where it\u2019s often integrated with cameras, alarms etc. The problem with neural networks is that they often detect the same object 2 or more times \u2014 even the best architectures like YOLO have this problem. We can actually solve it with nearest neighbor search with a simple approach:", "The crucial part is searching for the closest center of another bounding box (point 2). Which algorithm should be used here? Typically we have only a few moving objects on camera, maybe up to 30\u201340. For such a small number, speedup from using data structures for faster lookup is negligible. Each frame is a separate image, so if we wanted to construct a k-d tree for example, we would have to do so for every frame, which may mean 30 times per second \u2014 a huge cost overall. Therefore, for such situations a simple brute force method works fastest and also has the smallest space requirement (which, with heavy neural networks or for embedded CPUs in cameras, may be important).", "kNN algorithm is a popular, easy and useful technique in Machine Learning, and I hope after reading this article you understand it\u2019s complexities and real world scenarios where and how you can use this method.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science student, ML engineer, Data Science and ML algorithms enthusiast."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F502d2c440d5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----502d2c440d5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jakubadamczyk10?source=post_page-----502d2c440d5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakubadamczyk10?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Jakub Adamczyk"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa2bd22267201&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&user=Jakub+Adamczyk&userId=a2bd22267201&source=post_page-a2bd22267201----502d2c440d5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:KnnClassification.svg", "anchor_text": "source"}, {"url": "https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity", "anchor_text": "https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity"}, {"url": "https://en.wikipedia.org/wiki/K-d_tree", "anchor_text": "https://en.wikipedia.org/wiki/K-d_tree"}, {"url": "https://www.alglib.net/other/nearestneighbors.php#header0", "anchor_text": "https://www.alglib.net/other/nearestneighbors.php#header0"}, {"url": "http://www.cs.utah.edu/~lifeifei/cis5930/kdtree.pdf", "anchor_text": "http://www.cs.utah.edu/~lifeifei/cis5930/kdtree.pdf"}, {"url": "https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940", "anchor_text": "https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----502d2c440d5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/knn?source=post_page-----502d2c440d5---------------knn-----------------", "anchor_text": "Knn"}, {"url": "https://medium.com/tag/knn-algorithm?source=post_page-----502d2c440d5---------------knn_algorithm-----------------", "anchor_text": "Knn Algorithm"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----502d2c440d5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/k-nearest-neighbours?source=post_page-----502d2c440d5---------------k_nearest_neighbours-----------------", "anchor_text": "K Nearest Neighbours"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&user=Jakub+Adamczyk&userId=a2bd22267201&source=-----502d2c440d5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&user=Jakub+Adamczyk&userId=a2bd22267201&source=-----502d2c440d5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F502d2c440d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----502d2c440d5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----502d2c440d5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----502d2c440d5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----502d2c440d5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----502d2c440d5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----502d2c440d5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakubadamczyk10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jakubadamczyk10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jakub Adamczyk"}, {"url": "https://medium.com/@jakubadamczyk10/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "89 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa2bd22267201&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&user=Jakub+Adamczyk&userId=a2bd22267201&source=post_page-a2bd22267201--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F494f9b4fd578&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-computational-complexity-502d2c440d5&newsletterV3=a2bd22267201&newsletterV3Id=494f9b4fd578&user=Jakub+Adamczyk&userId=a2bd22267201&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}