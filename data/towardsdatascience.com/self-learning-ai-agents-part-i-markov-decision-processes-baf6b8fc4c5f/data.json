{"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "time": 1682993804.183598, "path": "towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f/", "webpage": {"metadata": {"title": "Markov Decision Processes in Deep Reinforcement Learning | Towards Data Science", "h1": "Self Learning AI-Agents Part I: Markov Decision Processes", "description": "A Markov Decision Process is the backbone of Deep Reinforcement Learning. It is the best approach we have so far to model the complex environment of an AI."}, "outgoing_paragraph_urls": [{"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 35}], "all_paragraphs": ["This is the first article of the multi-part series on self learning AI-Agents or to call it more precisely\u200a\u2014\u200aDeep Reinforcement Learning. The aim of the series isn\u2019t just to give you an intuition on these topics. Rather I want to provide you with more in depth comprehension of the theory, mathematics and implementation behind the most popular and effective methods of Deep Reinforcement Learning.", "Deep reinforcement learning is on the rise. No other sub-field of Deep Learning was more talked about in the recent years - by the researchers as well as the mass media worldwide. Most outstanding achievements in deep learning were made due to deep reinforcement learning. From Google\u2019s Alpha Go that have beaten the worlds best human player in the board game Go (an achievement that was assumed impossible a couple years prior) to DeepMind\u2019s AI agents that teach themselves to walk, run and overcome obstacles (Fig. 1\u20133).", "Other AI agents exceed since 2014 human level performances in playing old school Atari games such as Breakthrough (Fig. 4). The most amazing thing about all of this in my opinion is the fact that none of those AI agents were explicitly programmed or taught by humans how to solve those tasks. They learned it by themselves by the power of deep learning and reinforcement learning. The goal of this first article of the multi-part series is to provide you with necessary mathematical foundation to tackle the most promising areas in this sub-field of AI in the upcoming articles.", "Deep Reinforcement Learning can be summarized as building an algorithm (or an AI agent) that learns directly from interaction with an environment (Fig. 5). The environment may be the real world, a computer game, a simulation or even a board game, like Go or chess. Like a human the AI Agent learns from consequences of its Actions, rather than from being explicitly taught.", "In Deep Reinforcement Learning the Agent is represented by a neural network. The neural network interacts directly with the environment. It observes the current State of the Environment and decides which Action to take (e.g. move left, right etc.) on basis of the current State and the past experiences. Based on the taken Action the AI Agent receives a Reward. The amount of the Reward determines the quality of the taken Action with regards to solving the given problem (e.g. learning how to walk). The objective of an Agent is to learn taking Actions in any given circumstances that maximize the accumulated Reward over time.", "A Markov Decision Processes (MDP) is a discrete time stochastic control process. MDP is the best approach we have so far to model the complex environment of an AI agent. Every problem that the agent aims to solve can be considered as a sequence of states S1, S2, S3, \u2026 Sn (A state may be for example a Go/chess board configuration). The agent takes actions and moves from one state to an other. In the following you will learn the mathematics that determine which action the agent must take in any given situation.", "A Markov Process is a stochastic model describing a sequence of possible states in which the current state depends on only the previous state. This is also called the Markov Property (Eq. 1). For reinforcement learning it means that the next state of an AI agent only depends on the last state and not all the previous states before.", "A Markov Process is a stochastic process. It means that the transition from the current state s to the next state s\u2019 can only happen with a certain probability Pss\u2019 (Eq. 2). In a Markov Process an agent that is told to go left would go left only with a certain probability of e.g. 0.998. With a small probability it is up to the environment to decide where the agent will end up.", "Pss\u2019 can be considered as an entry in a state transition matrix P that defines transition probabilities from all states s to all successor states s\u2019 (Eq. 3).", "Remember: A Markov Process (or Markov Chain) is a tuple <S, P> . S is a (finite) set of states. P is a state transition probability matrix.", "A Markov Reward Process is a tuple <S, P, R>. Here R is the reward that the agent expects to receive in the state s (Eq. 4). This process is motivated by the fact that for an AI agent that aims to achieve a certain goal e.g. winning a chess game, certain states (game configurations) are more promising than others in terms of strategy and potential to win the game.", "The primary topic of interest is the total reward Gt (Eq. 5) which is the expected accumulated reward the agent will receive across the sequence of all states. Every reward is weighted by so called discount factor \u03b3 \u2208 [0, 1]. It is mathematically convenient to discount rewards since it avoids infinite returns in cyclic Markov processes. Besides the discount factor means the more we are in the future the less important the rewards become, because the future is often uncertain. If the reward is financial, immediate rewards may earn more interest than delayed rewards. Besides animal/human behavior shows preference for immediate reward.", "An other important concept is the the one of the value function v(s). The value function maps a value to each state s. The value of a state s is defined as the expected total reward the AI agent will receive if it starts its progress in the state s (Eq. 6).", "The value function can be decomposed into two parts:", "The decomposed value function (Eq. 8) is also called the Bellman Equation for Markov Reward Processes. This function can be visualized in a node graph (Fig. 6). Starting in state s leads to the value v(s). Being in the state s we have certain probability Pss\u2019 to end up in the next state s\u2019. In this particular case we have two possible next states. To obtain the value v(s) we must sum up the values v(s\u2019) of the possible next states weighted by the probabilities Pss\u2019 and add the immediate reward from being in state s. This yields Eq. 9, which is nothing else than Eq.8 if we execute the expectation operator E in the equation.", "A Markov Decision Process is a Markov Reward Process with decisions. A Markov Decision Process is described by a set of tuples <S, A, P, R >, A being a finite set of possible actions the agent can take in the state s. Thus the immediate reward from being in state s now also depends on the action a the agent takes in this state (Eq. 10).", "At this point we shall discuss how the agent decides which action must be taken in a particular state. This is determined by the so called policy \u03c0 (Eq. 11). Mathematically speaking a policy is a distribution over all actions given a state s. The policy determines the mapping from a state s to the action a that must be taken by the agent.", "Remember: Intuitively speaking the policy \u03c0 can be described as a strategy of the agent to select certain actions depending on the current state s.", "The policy leads to a new definition of the the state-value function v(s) (Eq. 12) which we define now as the expected return starting from state s, and then following a policy \u03c0.", "An other important function besides the state-value-function is the so called action-value function q(s,a) (Eq. 13). The action-value function is the expected return we obtain by starting in state s, taking action a and then following a policy \u03c0. Notice that for a state s, q(s,a) can take several values since there can be several actions the agent can take in a state s. The calculation of Q(s, a) is achieved by a neural network. Given a state s as input the network calculates the quality for each possible action in this state as a scalar (Fig. 7). Higher quality means a better action with regards to the given objective.", "Remember: Action-value function tells us how good is it to take a particular action in a particular state.", "Previously the state-value function v(s) could be decomposed into the following form:", "The same decomposition can be applied to the action-value function:", "At this point lets discuss how v(s) and q(s,a) relate to each other. The relation between these functions can be visualized again in a graph:", "In this example being in the state s allows us to take two possible actions a. By definition taking a particular action in a particular state gives us the action-value q(s,a). The value function v(s) is the sum of possible q(s,a) weighted by the probability (which is non other than the policy \u03c0) of taking an action a in the state s (Eq. 16).", "Now lets consider the opposite case in Fig. 9. The root of the binary tree is now a state in which we choose to take an particular action a. Remember that the Markov Processes are stochastic. Taking an action does not mean that you will end up where you want to be with 100% certainty. Strictly speaking you must consider probabilities to end up in other states after taking the action. In this particular case after taking action a you can end up in two different next states s\u2019:", "To obtain the action-value you must take the discounted state-values weighted by the probabilities Pss\u2019 to end up in all possible states (in this case only 2) and add the immediate reward:", "Now that we know the relation between those function we can insert v(s) from Eq. 16 into q(s,a) from Eq. 17. We obtain Eq. 18 and it can be noticed that there is a recursive relation between the current q(s,a) and next action-value q(s\u2019,a\u2019).", "This recursive relation can be again visualized in a binary tree (Fig. 10). We begin with q(s,a), end up in the next state s\u2019 with a certain probability Pss\u2019 from there we can take an action a\u2019 with the probability \u03c0 and we end with the action-value q(s\u2019,a\u2019). To obtain q(s,a) we must go up in the tree and integrate over all probabilities as it can be seen in Eq. 18.", "The most important topic of interest in deep reinforcement learning is finding the optimal action-value function q*. Finding q* means that the agent knows exactly the quality of an action in any given state. Furthermore the agent can decide upon the quality which action must be taken. Lets define that q* means. The best possible action-value function is the one that follows the policy that maximizes the action-values:", "To find the best possible policy we must maximize over q(s, a). Maximization means that we select only the action a from all possible actions for which q(s,a) has the highest value. This yields the following definition for the optimal policy \u03c0:", "The condition for the optimal policy can be inserted into Eq. 18. Thus provides us with the Bellman Optimality Equation:", "If the AI agent can solve this equation than it basically means that the problem in the given environment is solved. The agent knows in any given state or situation the quality of any possible action with regards to the objective and can behave accordingly.", "Solving the Bellman Optimality Equation will be the topic of the upcoming articles. In the following article I will present you the first technique to solve the equation called Deep Q-Learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbaf6b8fc4c5f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----baf6b8fc4c5f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Part II: Deep Q-Learning"}, {"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "anchor_text": "Part III: Deep (Double) Q-Learning"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20", "anchor_text": "Part IV: Policy Gradients for Continues Action Spaces"}, {"url": "https://www.linkedin.com/in/artem-oppermann-929154199/?locale=en_US", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----baf6b8fc4c5f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----baf6b8fc4c5f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----baf6b8fc4c5f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----baf6b8fc4c5f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----baf6b8fc4c5f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&user=Artem+Oppermann&userId=619319ac8220&source=-----baf6b8fc4c5f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&user=Artem+Oppermann&userId=619319ac8220&source=-----baf6b8fc4c5f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbaf6b8fc4c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----baf6b8fc4c5f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----baf6b8fc4c5f--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}