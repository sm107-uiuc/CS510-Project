{"url": "https://towardsdatascience.com/neural-network-optimization-algorithms-1a44c282f61d", "time": 1682993200.3701031, "path": "towardsdatascience.com/neural-network-optimization-algorithms-1a44c282f61d/", "webpage": {"metadata": {"title": "Neural Network Optimization Algorithms | by Vadim Smolyakov | Towards Data Science", "h1": "Neural Network Optimization Algorithms", "description": "What are some of the popular optimization algorithms used for training neural networks? How do they compare? This article attempts to answer these questions using a Convolutional Neural Network (CNN)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp03/tensorflow_optimizers.ipynb", "anchor_text": "ipython notebook", "paragraph_index": 20}, {"url": "https://github.com/vsmolyakov", "anchor_text": "https://github.com/vsmolyakov", "paragraph_index": 25}], "all_paragraphs": ["What are some of the popular optimization algorithms used for training neural networks? How do they compare?", "This article attempts to answer these questions using a Convolutional Neural Network (CNN) as an example trained on MNIST dataset with TensorFlow.", "SGD updates model parameters (theta) in the negative direction of the gradient (g) by taking a subset or a mini-batch of data of size (m):", "The neural network is represented by f(x(i); theta) where x(i) are the training data and y(i) are the training labels, the gradient of the loss L is computed with respect to model parameters theta. The learning rate (eps_k) determines the size of the step that the algorithm takes along the gradient (in the negative direction in the case of minimization and in the positive direction in the case of maximization).", "The learning rate is a function of iteration k and is a single most important hyper-parameter. A learning rate that is too high (e.g. > 0.1) can lead to parameter updates that miss the optimum value, a learning rate that is too low (e.g. < 1e-5) will result in unnecessarily long training time. A good strategy is to start with a learning rate of 1e-3 and use a learning rate schedule that reduces the learning rate as a function of iterations (e.g. a step scheduler that halves the learning rate every 4 epochs):", "In general, we want the learning rate (eps_k) to satisfy the Robbins-Monroe conditions:", "The first condition ensures that the algorithm will be able to find a locally optimal solution regardless of the starting point and the second one controls oscillations.", "Momentum accumulates exponentially decaying moving average of past gradients and continues to move in their direction:", "Thus step size depends on how large and how aligned the sequence of gradients are, common values of momentum parameter alpha are 0.5 and 0.9.", "Nesterov Momentum is inspired by Nesterov\u2019s accelerated gradient method:", "The difference between Nesterov and standard momentum is where the gradient is evaluated, with Nesterov\u2019s momentum the gradient is evaluated after the current velocity is applied, thus Nesterov\u2019s momentum adds a correction factor to the gradient.", "AdaGrad is an adaptive method for setting the learning rate [3]. Consider two scenarios in the Figure below.", "In the case of a slowly varying objective (left), the gradient would typically (at most points) have a small magnitude. As a result, we would need a large learning rate to quickly reach the optimum. In the case of a rapidly varying objective (right), the gradient would typically be very large. Using a large learning rate would result in very large steps, oscillating around but not reaching the optimum.", "These two situations occur because the learning rate is set independent of the gradient. AdaGrad solves this by accumulating squared norms of gradients seen so far and dividing the learning rate by the square root of this sum:", "As a result parameters that receive high gradients will have their effective learning rate reduced and parameters that receive small gradients will have their effective learning rate increased. The net effect is greater progress in the more gently sloped directions of parameter space and more cautious updates in the presence of large gradients.", "RMSProp modifies AdaGrad by changing the gradient accumulation into an exponentially weighted moving average, i.e. it discards history from the distant past [4]:", "Notice that AdaGrad implies a decreasing learning rate even if the gradients remain constant due to accumulation of gradients from the beginning of training. By introducing exponentially weighted moving average we are weighing recent past more heavily in comparison to distant past. As a result, RMSProp has been shown to be an effective and practical optimization algorithm for deep neural networks.", "Adam derives from \u201cadaptive moments\u201d, it can be seen as a variant on the combination of RMSProp and momentum, the update looks like RMSProp except that a smooth version of the gradient is used instead of the raw stochastic gradient, the full Adam update also includes a bias correction mechanism [5]:", "A simple CNN architecture was trained on MNIST dataset using TensorFlow with 1e-3 learning rate and cross-entropy loss using four different optimizers: SGD, Nesterov Momentum, RMSProp and Adam. The Figure below shows the value of the training loss vs iterations:", "We can see from the plot that Adam and Nesterov Momentum optimizers produce the lowest training loss!", "All code is available in the following ipython notebook.", "We compared different optimizers used in training neural networks and gained intuition for how they work. We found that SGD with Nesterov Momentum and Adam produce the best results when training a simple CNN on MNIST data in TensorFlow.", "[3] Duchi, J. ,Hazan, E. and Singer, Y. \u201cAdaptive subgradient methods for online learning and stochastic optimization\u201d, JMLR, 2011.", "[5] Diederik Kingma and Jimmy Ba, \u201cAdam: A Method for Stochastic Optimization\u201d, ICLR, 2015", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "passionate about data science and machine learning https://github.com/vsmolyakov"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a44c282f61d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vsmolyakov?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Vadim Smolyakov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d7cb5e269f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=post_page-5d7cb5e269f9----1a44c282f61d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp03/tensorflow_optimizers.ipynb", "anchor_text": "ipython notebook"}, {"url": "http://cs231n.github.io/neural-networks-3/", "anchor_text": "http://cs231n.github.io/neural-networks-3/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a44c282f61d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a44c282f61d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----1a44c282f61d---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----1a44c282f61d---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1a44c282f61d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=-----1a44c282f61d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=-----1a44c282f61d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a44c282f61d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a44c282f61d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a44c282f61d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a44c282f61d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a44c282f61d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vadim Smolyakov"}, {"url": "https://medium.com/@vsmolyakov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://github.com/vsmolyakov", "anchor_text": "https://github.com/vsmolyakov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d7cb5e269f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=post_page-5d7cb5e269f9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F55bf1aebf554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-algorithms-1a44c282f61d&newsletterV3=5d7cb5e269f9&newsletterV3Id=55bf1aebf554&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}