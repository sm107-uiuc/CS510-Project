{"url": "https://towardsdatascience.com/quantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5", "time": 1683016565.164001, "path": "towardsdatascience.com/quantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5/", "webpage": {"metadata": {"title": "Quantize Your Deep Learning Model to Run on an NPU | by Jan Werth | Towards Data Science", "h1": "Quantize Your Deep Learning Model to Run on an NPU", "description": "In this article, we will explain in this article which steps you have to take to transform and quantize your model with different TensorFlow versions. We are only looking into post training\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "phyBOARD-Pollux", "paragraph_index": 1}, {"url": "https://www.phytec.de/produkte/system-on-modules/phycore-imx-8m-plus/", "anchor_text": "i.MX 8M Plus", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "TensorFlow website", "paragraph_index": 3}, {"url": "https://github.com/JanderHungrige/tf.keras-vggface/tree/main/Anaconda", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://github.com/JanderHungrige/tf.keras-vggface", "anchor_text": "tf.keras_vggface", "paragraph_index": 9}, {"url": "https://github.com/rcmalli/keras-vggface", "anchor_text": "rcmalli", "paragraph_index": 9}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator", "anchor_text": "tf.keras.preprocessing.image.ImageDataGenerator()", "paragraph_index": 11}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors", "anchor_text": "tf.data.Dataset.from_tensors()", "paragraph_index": 11}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices", "anchor_text": "from.tensor_slices()", "paragraph_index": 11}, {"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "pyhBOARD-Pollux", "paragraph_index": 15}, {"url": "https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125", "anchor_text": "saving", "paragraph_index": 20}, {"url": "https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc", "anchor_text": "freeze graph", "paragraph_index": 20}, {"url": "http://linkedin.com/in/jan-werth", "anchor_text": "linkedin.com/in/jan-werth", "paragraph_index": 23}], "all_paragraphs": ["In this article, we will explain in this article which steps you have to take to transform and quantize your model with different TensorFlow versions. We are only looking into post training quantization.", "We are using the phyBOARD-Pollux to run our model. The phyBOARD-Pollux incorporates an i.MX 8M Plus which features a dedicated neural network accelerator IP from VeriSilicon (Vivante VIP8000).", "As the neural processing unit (NPU) from NXP need a fully int8 quantized model we have to look into full int8 quantization of a TensorFlow lite or PyTorch model. Both libraries are supported with the eIQ library from NXP. Here we will only look into the TensorFlow variant.", "The general overview on how to do post training quantization can be found on the TensorFlow website.", "The operations for floating point are more complex than for integer (arithmetic\u2019s, avoiding overflow). This results in the ability to use only the much simpler and smaller arithmetic units instead of the larger floating-point units. \u200b", "\u200bThe physical space needed for float32 operation much larger than for int8. This results in:\u200b", "To create the code on your PC first, we recommend using Anaconda with a virtual environment running Python 3.6, TensorFlow 2.x, numpy, opencv-python and pandas.", "The environment-file to clone the environment can be found here.", "If you created and trained a model via tf.keras there are three similar ways of quantizing the model.", "The trained TensorFlow model has to be converted into a TFlite model and can be directly quantize as described in the following code block. For the trained model we exemplary use the updated tf.keras_vggface model based on the work of rcmalli. The transformation starts at line 28.", "After loading/training your model you first have to create a representative data set. The representative data set is used by the converter to get the max and min values to be able to estimate the scaling factor. This limits the error introduced by the quantization from float32 to intX. The error comes from the different number-space of float and int. Converting from float to int8 limits the number-space to integer values between -128 to 127. Calibrating the model on the dynamic range of the input limits this error.", "Here you can just loop through your images or create a generator as in our example. We used the tf.keras.preprocessing.image.ImageDataGenerator() to yield images and do the necessary prepossessing on the images. As a generator you can of course also use the tf.data.Dataset.from_tensors() or \u2026 from.tensor_slices(). Just keep in mind to do the same pre-processing on your data here as you did on the data you trained your network with (normalization, resizing, de-noising, \u2026). This can all be packed into the preprocessing_function call of the generator (line 19).", "The conversion starts at line 28. A simple TensorFlow lite conversion would look like this:", "Now if we convert the model using TF2.3 with", "However if we do not set the inference_input_type and inference_output_type we receive following model:", "So the effect is that you can determine which input data type the model accepts and returns. This can be important if you work with an embedded camera, as included with the pyhBOARD-Pollux. The mipi camera returns 8bit values, so if you want to spare a conversion to float32 int8 input can be handy. But be aware, if you use a model without prediction layers to gain e.g., embeddings, an int8 output will result in very poor performance. Here an output of float32 is recommended. This shows that each problem needs a specific solution.", "If you already have your model, you most likely have it saved somewhere either as an Keras h5 file or a TensorFlow protocol buffer pb. We will quickly save our model using TF2.3:", "Following the conversion and quantization is very similar as in Method One. The only difference is how we load the model in with the converter. Either load the model and continue as in Method One.", "Or load the h5 model directly. When using TensorFlow version 2 and above you have to use a compatible converter:", "If you load from a TensorFlow pb file use:", "If you want to convert a model written in TensorFlow version < 1.15.3 using Keras, not all options are available for TFlite conversion and quantization. The best way is to save the model with the TensorFlow version it was created in (e.g., rcmalli keras-vggface was trained in TF 1.13.2). I would suggest not using the \u201csaving and freeze graph\u201d method to create a pb file as the pb files differ between TF1 and TF2. The TFLiteConverter.from_saved_model does not work, creating quite a hassle to achieve quantization. I would suggest using the above mentioned method using Keras:", "Then convert and quantize your model with a TensorFlow version from 1.15.3 onward. From this version on a lot of functions where added in preparation for TF2. I suggest using the latest version. This will result in the same models presented earlier.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hi, I am a carpenter, electrical engineer and have over 10 years of experience in signal processing, machine- and deep learning. linkedin.com/in/jan-werth"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2900191757e5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://janwerth.medium.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": ""}, {"url": "https://janwerth.medium.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Jan Werth"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb51a780f5eae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&user=Jan+Werth&userId=b51a780f5eae&source=post_page-b51a780f5eae----2900191757e5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.nxp.com/products/processors-and-microcontrollers/arm-processors/i-mx-applications-processors/i-mx-8-processors/i-mx-8m-plus-arm-cortex-a53-machine-learning-vision-multimedia-and-industrial-iot:IMX8MPLUS", "anchor_text": "i.MX 8M Plus"}, {"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "phyBOARD-Pollux"}, {"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "phyBOARD-Pollux"}, {"url": "https://www.phytec.de/produkte/system-on-modules/phycore-imx-8m-plus/", "anchor_text": "i.MX 8M Plus"}, {"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "phyBOARD pollux"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "TensorFlow website"}, {"url": "https://github.com/JanderHungrige/tf.keras-vggface/tree/main/Anaconda", "anchor_text": "here"}, {"url": "https://github.com/JanderHungrige/tf.keras-vggface", "anchor_text": "tf.keras_vggface"}, {"url": "https://github.com/rcmalli/keras-vggface", "anchor_text": "rcmalli"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator", "anchor_text": "tf.keras.preprocessing.image.ImageDataGenerator()"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors", "anchor_text": "tf.data.Dataset.from_tensors()"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices", "anchor_text": "from.tensor_slices()"}, {"url": "https://groups.google.com/a/tensorflow.org/g/tflite/c/C7Ag0sUrLYg?pli=1", "anchor_text": "and more"}, {"url": "https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/", "anchor_text": "pyhBOARD-Pollux"}, {"url": "https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125", "anchor_text": "saving"}, {"url": "https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc", "anchor_text": "freeze graph"}, {"url": "https://medium.com/tag/quantization?source=post_page-----2900191757e5---------------quantization-----------------", "anchor_text": "Quantization"}, {"url": "https://medium.com/tag/npu?source=post_page-----2900191757e5---------------npu-----------------", "anchor_text": "Npu"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----2900191757e5---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/tensorflow-lite?source=post_page-----2900191757e5---------------tensorflow_lite-----------------", "anchor_text": "Tensorflow Lite"}, {"url": "https://medium.com/tag/tensorflow2?source=post_page-----2900191757e5---------------tensorflow2-----------------", "anchor_text": "Tensorflow2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&user=Jan+Werth&userId=b51a780f5eae&source=-----2900191757e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&user=Jan+Werth&userId=b51a780f5eae&source=-----2900191757e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2900191757e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2900191757e5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2900191757e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2900191757e5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2900191757e5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2900191757e5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2900191757e5--------------------------------", "anchor_text": ""}, {"url": "https://janwerth.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://janwerth.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jan Werth"}, {"url": "https://janwerth.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "65 Followers"}, {"url": "http://linkedin.com/in/jan-werth", "anchor_text": "linkedin.com/in/jan-werth"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb51a780f5eae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&user=Jan+Werth&userId=b51a780f5eae&source=post_page-b51a780f5eae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F95bbc9c4716&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5&newsletterV3=b51a780f5eae&newsletterV3Id=95bbc9c4716&user=Jan+Werth&userId=b51a780f5eae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}