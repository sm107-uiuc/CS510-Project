{"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "time": 1682994922.383723, "path": "towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083/", "webpage": {"metadata": {"title": "Policy Based Reinforcement Learning, the Easy Way | by Ziad SALLOUM | Towards Data Science", "h1": "Policy Based Reinforcement Learning, the Easy Way", "description": "Update 1: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com Update 2: If you are new to the subject, it might be easier for you to start with\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182", "anchor_text": "Reinforcement Learning Policy for Developers", "paragraph_index": 1}, {"url": "https://medium.com/@zsalloum/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Expected_value", "anchor_text": "Expected Value", "paragraph_index": 19}, {"url": "https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2", "anchor_text": "Introduction to Actor Critic", "paragraph_index": 33}], "all_paragraphs": ["Update 1: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "Update 2: If you are new to the subject, it might be easier for you to start with Reinforcement Learning Policy for Developers article.", "Suppose you are in a new town and you have no map nor GPS, and you need to reach downtown. You can try assess your current position relative to your destination, as well the effectiveness (value) of each direction you take. You can think of this as computing the value function. Or you can ask a local and he tells you to go straight and when you see a fountain you go to the left and continue until you reach downtown. He gave you a policy to follow.Naturally, in this case, following the given policy is much less complicated than computing the value function on your own.", "In another example, consider that you are managing in inventory, and you decided that when the count of each item drops below a certain limit you issue a buy order to refill your stock. This is a policy that is far more simple than studying the activity of the customers, and their buying habits and preferences, in order to predict the impact on your stock\u2026", "Surely enough, value functions will lead to determining a policy as seen in previous articles, but there are other methods that can learn a policy that can select actions using parameters, without consulting a value function (this is not quite correct, since value function is needed to increase accuracy).", "So the main idea is to be able to determine at a state (s) which action to take in order to maximize the reward.", "The way to achieve this objective is to fine tune a vector of parameters noted \ud835\udf3d in order to select the best action to take for policy \ud835\udf0b.The policy is noted \ud835\udf0b(a|s, \ud835\udf3d) = Pr{At = a | St = s, \ud835\udf3dt = \ud835\udf3d}, which means that the policy \ud835\udf0b is the probability of taking action a when at state s and the parameters are \ud835\udf3d.", "At first it is important to note that stochastic does not mean randomness in all states, but it can be stochastic in some states where it makes sense.Usually maximizing reward leads to deterministic policy. But in some cases deterministic policies are not a good fit for the problem, for example in any two player game, playing deterministically means the other player will be able to come with counter measures in order to win all the time. For example in Rock-Cissors-Paper game, if we play deterministically meaning the same shape every time, then the other player can easily counter our policy and wins every game.", "So in this game the optimal policy would be stochastic which will be better than the deterministic one.", "Before delving into the details of math and algorithms, it is useful to have an overview of how to proceed, a kind of blue print:", "Remember that in the blue print above we talked about finding an objective function in the aim of assessing the effectiveness of the policy. In this section we will define the objective function and some of its useful derivation.(More details about the policy gradient can be found in the article Policy Gradient Step by Step).", "When talking about maximizing a function, one of the methods that stands out, is the gradient.", "But how are we going to maximize the rewards based on \ud835\udf3d ?One way to do it, is to find an objective function J(\ud835\udf3d) such that", "Where V\ud835\udf0b\ud835\udf3d is the value function for policy \ud835\udf0b\ud835\udf3d, and s0 is that starting state.", "In short the maximizing J(\ud835\udf3d) means maximizing V\ud835\udf0b\ud835\udf3d(s).It follows that", "According to the Policy Gradient theorem", "Where \ud835\udf7b(s) is the distribution under \ud835\udf0b (meaning the probability of being at state s when following policy \ud835\udf0b), q(s,a) is the action value function under \ud835\udf0b, and \u2207\ud835\udf0b(a|s, \ud835\udf3d) is the gradient of \ud835\udf0b given s and \ud835\udf3d.Finally \ud835\udf70 means proportional.", "So the theorem says that \u2207J(\ud835\udf3d) is proportional to the sum of the q function times the gradient of the policies for all actions at the states that we might be at. However we don\u2019t know \ud835\udf0b(a|s, \ud835\udf3d), how can we find its gradient?", "It turns out that it is possible as the following demonstration shows:", "\u2207Log \ud835\udf0b\ud835\udf03(s,a) is called the score function.Note that the gradient of the policy can be expressed as an expectation. If you are asking yourself why? Check this wikipedia article on Expected Value.", "Since this is a gradient method, the update of the parameters (that we are trying to optimize) will be done the usual way.", "This section explains few standard Gradient Policies such as Softmax and Guassian. We use these policies in RL algorithms to learn the parameters \ud835\udf3d.In practice whenever in the RL algorithm we see reference to \u2207Log \ud835\udf0b\ud835\udf03(s,a) we plug in the formula of the chosen policy.", "The softmax Policy consists of a softmax function that converts output to a distribution of probabilities. Which means that it affects a probability for each possible action.", "Softmax is mostly used in the case discrete actions:", "You can check the full demonstration of the derivation here.", "Gaussian policy is used in the case of continuous action space, for example when driving a car and you steer the wheels or press on the gas pedal, these are continuous actions because these are not few actions that you do since you you can (in theory) decide the rotation degree or the flow amount of gas.", "This section will give some of the algorithms that will take into account the policies and their objective function in order to learn parameters that will give the best agent behavior.", "This algorithm uses Monte-Carlo to create episodes according to the policy \ud835\udf0b\ud835\udf03, and then for each episode, it iterates over the states of the episode and computes the total return G(t). The it uses G(t) and \u2207Log \ud835\udf0b\ud835\udf03(s,a) (which can be Softmax policy or other) to learn the parameter \ud835\udf03.", "We have said that Policy Based RL have high variance. However there are several algorithms that can help reduce this variance, some of which are REINFORCE with Baseline and Actor Critic.", "The idea of the baseline is to subtract from G(t) the amount b(s) called baseline in the purpose of reducing the wide change changes in results.Provided that b(s) does not depend on the action a, it can be shown that the equation of \u2207J(\ud835\udf3d) is still valid.", "So now the question would be how to choose b(s) ?", "One of the choices for the baseline is to compute the estimate of the state value, \u00fb(St,w), where w is a parameter vector learned by some methods such as Monte Carlo.So b(s) = \u00fb(St,w)", "The REINFORCE with Baseline algorithm becomes", "(Detailed explanation can be found in Introduction to Actor Critic article)Actor Critic algorithm uses TD in order to compute value function used as a critic.The critic is a state-value function. It is useful to assess how things go after each action, the critic computes the new state to determine if there has been any improvement or not. That evaluation is the TD error:", "Then \u03b4(t) is then used to adjust the parameters \ud835\udf3d and w.In short both \ud835\udf3d and w are adjusted in the way to correct this error.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8de9a3356083&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----8de9a3356083---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jmrthms?utm_source=medium&utm_medium=referral", "anchor_text": "Jomar"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://towardsdatascience.com/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182", "anchor_text": "Reinforcement Learning Policy for Developers"}, {"url": "https://medium.com/@zsalloum/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://en.wikipedia.org/wiki/Expected_value", "anchor_text": "Expected Value"}, {"url": "https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2", "anchor_text": "Introduction to Actor Critic"}, {"url": "https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182", "anchor_text": "Reinforcement Learning Policy for Developers"}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566", "anchor_text": "Function Approximation in Reinforcement Learning"}, {"url": "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2", "anchor_text": "Introduction to Actor Critic in Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8de9a3356083---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----8de9a3356083---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/policy-gradient?source=post_page-----8de9a3356083---------------policy_gradient-----------------", "anchor_text": "Policy Gradient"}, {"url": "https://medium.com/tag/actor-critic?source=post_page-----8de9a3356083---------------actor_critic-----------------", "anchor_text": "Actor Critic"}, {"url": "https://medium.com/tag/reinforce?source=post_page-----8de9a3356083---------------reinforce-----------------", "anchor_text": "Reinforce"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----8de9a3356083---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----8de9a3356083---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8de9a3356083---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8de9a3356083--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8de9a3356083--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8de9a3356083--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8de9a3356083--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8de9a3356083--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}