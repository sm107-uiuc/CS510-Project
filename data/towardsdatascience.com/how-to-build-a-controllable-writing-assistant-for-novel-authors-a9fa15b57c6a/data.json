{"url": "https://towardsdatascience.com/how-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a", "time": 1683015111.353313, "path": "towardsdatascience.com/how-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a/", "webpage": {"metadata": {"title": "How to build a controllable writing assistant for novel authors | by Duval Alexandre | Towards Data Science", "h1": "How to build a controllable writing assistant for novel authors", "description": "A few years ago, creating a writing tool involved using simple probabilistic models together with carefully designed grammar rules in order to pick the next world given previous ones. In addition to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/openai-api/", "anchor_text": "GPT-3", "paragraph_index": 2}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "Open AI GPT-2", "paragraph_index": 4}, {"url": "https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313", "anchor_text": "Hugging Face\u2019s state-of-the-art conversational AI", "paragraph_index": 5}, {"url": "http://textgen.thomas-lamson.com/", "anchor_text": "check it out", "paragraph_index": 8}, {"url": "https://github.com/WeazelDev/AITextGenerator", "anchor_text": "Github", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/2101.03216", "anchor_text": "link", "paragraph_index": 11}, {"url": "https://grover.allenai.org/", "anchor_text": "Grover", "paragraph_index": 17}, {"url": "http://arxiv.org/abs/1706.03762", "anchor_text": "Transformer", "paragraph_index": 21}, {"url": "https://people.cs.umass.edu/~strubell/doc/lisa-final.key", "anchor_text": "Emma Strubell\u2019s EMNLP slides", "paragraph_index": 22}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Illustrated Transformer", "paragraph_index": 22}, {"url": "https://www.gutenberg.org/", "anchor_text": "Project Gutenberg", "paragraph_index": 38}, {"url": "https://github.com/kamalkraj/BERT-NER", "anchor_text": "BERT NER Large model", "paragraph_index": 39}, {"url": "http://nlpprogress.com/english/summarization.html", "anchor_text": "esources", "paragraph_index": 41}, {"url": "https://huggingface.co/transformers/", "anchor_text": "hugging face", "paragraph_index": 42}, {"url": "https://grover.allenai.org/", "anchor_text": "https://grover.allenai.org/", "paragraph_index": 67}], "all_paragraphs": ["A few years ago, creating a writing tool involved using simple probabilistic models together with carefully designed grammar rules in order to pick the next world given previous ones. In addition to being long and fastidious, the results were extremely limited.", "With the recent progress in deep-learning for NLP, we can now get rid of this petty work and build much more powerful text generation algorithm \ud83c\udf1f as you will see in this tutorial. This progress coincides with the apparition of the Transformer architecture (Vaswani et al.) \ud83c\udfed, which has enabled large scale language models trained on huge text corpora (Devlin et al., 2018; Dai et al., 2019) to reach outstanding performance and produce something close to what humans would write.", "If you don\u2019t believe me, I would advise you to check GPT-3, which has just been released and showcases a multitude of amazing features.", "However, these powerful models present several shortcomings \u26d4. They are very difficult and costly to train. They still struggle to maintain a coherent context across long paragraphs. And finally, users are not yet able to steer the generation and control some specific aspects of the produced text.", "To tackle these limitations, our secret sauce was to use a large-scale pre-trained language model, Open AI GPT-2 \ud83e\udd84 [1], combined with an innovative Transfer Learning fine-tuning technique \u2014 for controlled and contextualised text generation.", "From this perspective, our work is similar to Hugging Face\u2019s state-of-the-art conversational AI, which does an outstanding job at contextualising dialog on both previous history and the bot\u2019s \u2019personality.", "More concretely, our objective \ud83c\udfc6 is to make users, at any point when writing a novel, able to automatically generate new sections that are consistent with the rest of the writing, especially previous and following paragraphs. We leave them the opportunity to select entities (characters, locations, etc.) \ud83d\udc64 that they have introduced in their novel and that they want to include in the new section. Similarly, they can specify the size of the desired text \ud83d\udcdd, its content via a small summary or keywords \ud83d\udcac and even the genre of the book they are writing \ud83d\udcd7.", "The lack of open source plateforms motivated us to integrate our tool into a user-friendly and interactive open-source web-service that allows any author to benefit from it \u2014 easily, intuitively and for free.", "Be sure to check it out ! \ud83c\udfae Update: we have closed the server as we are running out of money \u2014 sorry \ud83d\ude05But you can still run it locally on your computer.", "In the end, we propose an easy-to-use writing assistant that automatically makes several suggestions that users can choose from and edit. The aim being to produce creative outputs that give ideas to the writers while respecting the following constraints:", "\ud83d\udcc6 Here is what we will learn and play with today:", "Together with this post, we released our code ! Check the Github repo here \u2708\ufe0fWe also wrote a paper accepted at EACL, link.", "So here we are, let\u2019s dive in \ud83d\ude80", "Before delving into the details of the process, let\u2019s make clear the intuition behind our approach. \ud83d\udca1", "Ideally, we would like to train a model from scratch to generate paragraphs that are consistent with the context and the particular inputs of the user. In practice, we would teach our model to re-generate each book\u2019s paragraph (P2) using the previous and following paragraphs (P1 and P3) as well as additional information given by the user.", "Of course, this implies using a specific dataset \ud83d\udcc1 with hundreds of novels split into paragraphs, together with information about their size, the entities they showcase, a summary of their content and the genre of the book they belong to. But we will come back to it later.", "However, this would be a major challenge \ud83d\ude48 ! It would involve training a huge language model with millions of parameters enough time to make it learn to produce consistent and grammatically correct sentences, which should additionally be contextualised and controllable.", "Grover [3] for instance \u2014an outstanding fake news articles generator \u2014 with its 1.5 billion parameters, required a training time of two weeks with 256 TPUs, which cost 35k$. \ud83d\udcb8", "To tackle this problem, we\u2019ll take a path that gathered tremendous interest over the last months / years: Transfer Learning \ud83d\udcaf. The idea behind this approach is quite simple:", "Since pre-training a language model is an expensive operation, it\u2019s usually better to start from a model that has already been pre-trained and open-sourced. In this project, we have decided to use the existing language model Open AI GPT2, already trained to generate fluent sentences, and to fine tune it in a specific fashion to produce more contextualised and controlled outputs.", "Let\u2019s have a quick look at it \ud83d\udd0e", "In 2018 and 2019, Alec Radford, Jeffrey Wu and their co-workers at OpenAI open-sourced two language models trained on a very large amount of data: GPT and GPT-2 (where GPT stands for Generative Pretrained Transformer). They are two very similar Transformer [2] based language models, called decoder models because they use the left context to predict the next word.", "Many papers and blog posts describe Transformers models and how they use attention mechanisms to process sequential inputs so I won\u2019t spend time presenting them in details. A few pointers if you are not familiar with these models: Emma Strubell\u2019s EMNLP slides are my personal favourite and Jay Alammar\u2019s \u201cIllustrated Transformer\u201d is a very detailed introduction.", "For our purpose, a language model will just be a model that takes as input a sequence of tokens and generates a probability distribution over the vocabulary for the next token following the input sequence. Language models are usually trained in a parallel fashion, by predicting the token following each token in a long input sequence.", "A classic language model is trained with a single input: a sequence of words. But as we saw earlier, our model will have to use several types of contexts to generate an output sequence:", "How can we build an input for our model from these various contexts?", "A simple answer is just to concatenate the context segments in a single sequence, putting the true paragraph at the end. We can then generate a completion of the paragraph token by token by continuing the sequence. So each datapoint will have the following form:", "where [P 1], [P 2], [P 3], [Sum], [T] and [Ent] indicate the type of input received by the model (special tokens). Note that the order of the input is not essential. You simply need to stick to it. We only put P1 at the end so that GPT-2 can continue from there, as it has been trained to do so. More concretely, we have something like this:", "Technical note: Usually, during training, GPT-2 takes as input an entire text file that it tokenizes and splits into blocks of size = block size, the maximum input size of the small GPT-2 model, 1024 tokens. It then keeps them in memory in a torch dataset and loads them through a dataloader. Quite obviously, we have to proceed differently here. We do not want to feed to GPT-2 a continuous text that would be split according to its maximum input capacity but instead the blocks that we specified above, one at a time. Since they would most probably not fill in perfectly GPT-2\u2019s input space, we pad on the right when necessary. When the input sample is too big, we truncate P1 on the left and P3 on the right, so as to stay around P2. This is not done evenly as we allocate 2/3 of the remaining space to P1 and 1/3 to P3, as we consider P1 to be more important than P3.", "There are two issues with this simple setup:", "An easy way to add this information is to build three parallel input sequences for word, position and segments, and fuse them in a single sequence, summing three types of embeddings:", "First, we\u2019ll add special tokens \ud83d\udca5 to our vocabulary for delimiters and segment indicators ([P1], [S], [T]\u2026). These tokens were not part of our model\u2019s pre-training so we will need to create and train new embeddings for them.", "These special-tokens methods respectively add our special tokens to the vocabulary of the tokenizer and create five additional embeddings in the model.", "Overall, we tokenize the whole input passed to the model using GPT-2 Byte-Pair-Encoding (BPE) Tokenizer [4]. This tokenized input of length n thus has three different representations/embeddings, all of shape (1, n, 768) where 768 is the embedding dimension for a small GPT-2, that we add together to create a single input embedding.", "W e have now initialised our pre-trained model \ud83d\udc7b and built our training inputs, all that remains is to choose a loss to optimize \ud83c\udfaf during the fine-tuning.", "To this end, note that we also gave the network a label vector of dimension (1, n, 768) that equals -100 everywhere except for the tokens belonging to P2. This is ultimately used to compute the cross entropy loss function only between generated and original P2, token by token. Indeed, we do not train the model to reproduce the full input but only P2, our paragraph of interest, as shown by the figure above. The idea being that the model utilises the context information provided to learn a correct reconstruction of the paragraph of interest P2.", "Now that we have described the framework, we need to train our model \ud83c\udfcb\ud83c\udffb\u200d\u2642\ufe0f. Let\u2019s thus come back to a crucial part of the project: the dataset. Indeed, as mentioned above, we need to create a very specific dataset to tune the model in such fashion.", "Let\u2019s emphasise some key aspects of the adequate data generation phase since the contextualisation of the pre-trained GPT-2 strongly depends on it.", "Novels data: We use the famous open book library Project Gutenberg to train the model. For each book existing on Gutenberg, we create a json file with its cleaned textual content and its related metadata: author, title, theme and genre. Note that a unique genre per book is manually defined from unstructured thematic information. We then split the text of each book into paragraphs of different lengths, with a minimum and maximum bound, being careful not to cut a sentence in the middle, nor to separate core parts like chapters or even to split big paragraphs into uneven pieces. We store the size of each paragraph as a number of characters, which will then be used to categorise them as either Small, Medium or Large.", "Entity extraction: Once each book is processed as detailed above, we detect entities for each paragraph using a pre-trained BERT NER Large model [5]. They are sorted into four categories: persons, locations, organisations and miscellaneous. Training the model with this data makes it capable of generating text that contains the established entities. It allows authors to specify the ones that they want to incorporate in the generation.", "Summary: Similarly, in order for authors to be able to guide the generation by giving information on the desired content, we derive four very different summaries of each paragraph using state-of-the-art summarisers:", "Using various summary types tends to make our model more robust to the possible ways authors could provide this type of information. See resources on different types of summarisation.", "Once the dataset is built, we fine-tuned \ud83c\udfcb\ud83c\udffb\u200d\u2642\ufe0f the pre-trained GPT2LMHeadModel (small -117M) from hugging face \u2014 a GPT-2 model transformer with a language modelling head on top of it, that is a linear layer with weights tied to the input embeddings \u2014 using a customised version of their training script.", "We trained it on the 313 pre-processed books, using all of Hugging Face\u2019s training settings \u2699\ufe0f. As mentioned, we were limited in terms of resources and could not really run this project at a larger scale. Training was performed using CUDA on an AWS\u2019s p3.2xlarge instance (incorporating one NVidia Tesla V100 GPU) and cost about 100$. In total, the model received 134k samples for each epoch, and there were 10 epochs. However, other metrics we used indicated that fewer epochs could be enough to reach great performances. \ud83d\udcaf", "The amazing thing about text generation model is that you can play with it \ud83e\udd17", "To interact with our model, we need to add one thing: a decoder that will build full sequences from the next token predictions of our model.", "Technical note: Unlike in training, we do not input P2. Nevertheless, we need to leave sufficient space for it to be generated since the model\u2019s output is equal to the model\u2019s input plus the generated sequence. Hence, the input cannot exceed a certain limit, smaller than 1024 tokens, that we determine based on confidence intervals. If the input is too big, we truncate it, similarly to what was done in training.", "There have been very interesting developments in decoders over the last few months and I wanted to present them quickly here to get you up-to-date.", "The two most common decoders for language generation used to be greedy-decoding and beam-search. \u269c\ufe0f\u269c\ufe0f\u269c\ufe0f", "Greedy-decoding is the simplest way to generate a sentence: at each time step, we select the most likely next token according to the model until we reach end-of-sequence tokens. One risk with greedy decoding is that a highly probable token may be hiding after a low-probability token and be missed.", "Beam-search try to mitigate this issue by maintaining a beam of several possible sequences that we construct word-by-word [10]. At the end of the process, we select the best sentence among the beams. Over the last few years, beam-search has been the standard decoding algorithm for almost all language generation tasks. Overall, it leads to a more fluent output but is often quite repetitive, which is particularly undesirable in story generation.", "On top of that, a recent trend of work is the study recently published by Ari Holtzman et al. [11] which showed that the distributions of words in texts generated using beam-search and greedy decoding is very different from the distributions of words in human-generated texts. Clearly, beam-search and greedy decoding fail to reproduce some distributional aspects of human texts and have thus been replaced by top-k and nucleus (or top-p) sampling, the current two most promising approaches. The general principle of these two methods is to sample from the next-token distribution after having filtered this distribution to keep only the top k tokens (top-k) or the top tokens with a cumulative probability just above a threshold (nucleus/top-p).", "In other words, top-k sampling builds on sampling, it simply selects the k most probable words and re-scales the probability mass distribution across the k selected words. This approach yields very nice results (Radford et al., 2019). Its sole limit is that k is fixed whether there is a narrow or wide distribution, while we might want to distinguish between those two cases.", "This is why nucleus sampling was created as the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word\u2019s probability distribution.", "Note that lowering the temperature \ud83c\udf21\ufe0f enables to sharpen the probability mass distribution. Increasing it fosters diversity and suits us better since we want creative outputs that give ideas to the writer, even if some mistakes are made.", "We are now ready to play with our model \ud83d\ude80", "For this project, we also wanted to innovate and give a proposition of what an AI enhanced interface would look like in terms of user experience. So we designed an interface and linked it to elastic instances in the back-end. We then opened it to a small public to test our model. \ud83c\udfac", "We have now closed it due to limited resources \u2014 yeah it\u2019s expensive. But you can still run it on your local computer and use it.", "Technical note: To gain in flexibility in the choice of instances to perform the heavy computations and to allow load balancing on several instances, we uncoupled the master instance \u2014 serving the javascript front-end and general data \u2014 from the computational instances \u2014 performing NER and text generation on demand. It is also possible for the client to run the servers locally to avoid delays and server overloads. The figure below gives an idea of the general architecture of our service.", "In this interface, \ud83c\udfdd\ufe0f \ufe0fusers are invited to write some text in a simple editor. Named entities like characters, locations, organisations and others are detected on the fly by the NER backend, and are displayed on the left panel. It is made easy for users to edit them manually if needed.", "Users have the possibility to select several options: length of desired paragraph, genre of their writing and list of entities they want to see appear in the generation. They can also highlight a small part of the text that will act as a summary (or a list of keywords). When they are ready, they simply need to press the Generate button and let the magic happen.", "In summary, we presented an end-to-end pipeline from Project Gutenberg library\u2019s raw text file \ud83d\udcda to a web-service intended for book writers. The latter embeds a controllable and contextualised GPT-2 for text generation specific to novels, which was fine-tuned following our pipeline on a few hundreds of novels during ten epochs \ud83c\udfb3. Overall, despite having limited computational resources, we have managed to build a final model that is able to take into consideration the context specified by the user.", "With the constant improvement of computing capacities and recent research trend aimed at reducing model size without damaging generation capacities, we strongly believe such controllable generative framework will be easily accessible in the future and will greatly enhance the creativity of writers.", "Some tracks to go even further:", "We\u2019ve come to the end of this post describing how you can build a state-of-the-art controlled and contextualized writing tool aimed for novel authors using transfer learning and a large-scale language model like OpenAI GPT2.", "Be sure to check out the associated demo and code:", "Written with Thomas Lamson and Gael de L\u00e9s\u00e9leuc", "[3] Defending against neural fake news, by Rowan Zellers, Ari Holtzman et al. (2019) https://grover.allenai.org/", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Graph ML | More broadly passionate about AI \u2014 Mathematics \u2014Philosophy \u2014 Sustainability \u2014 Sports."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa9fa15b57c6a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@Alexandre_Duval?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@Alexandre_Duval?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Duval Alexandre"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F394330cc12cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&user=Duval+Alexandre&userId=394330cc12cf&source=post_page-394330cc12cf----a9fa15b57c6a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "GPT-3"}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "Open AI GPT-2"}, {"url": "https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313", "anchor_text": "Hugging Face\u2019s state-of-the-art conversational AI"}, {"url": "http://textgen.thomas-lamson.com/", "anchor_text": "check it out"}, {"url": "https://unsplash.com", "anchor_text": "Unplash"}, {"url": "https://github.com/WeazelDev/AITextGenerator", "anchor_text": "Github"}, {"url": "https://arxiv.org/abs/2101.03216", "anchor_text": "link"}, {"url": "https://grover.allenai.org/", "anchor_text": "Grover"}, {"url": "http://arxiv.org/abs/1706.03762", "anchor_text": "Transformer"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "The Illustrated GPT-2 (Visualizing Transformer Language Models"}, {"url": "https://people.cs.umass.edu/~strubell/doc/lisa-final.key", "anchor_text": "Emma Strubell\u2019s EMNLP slides"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Illustrated Transformer"}, {"url": "https://www.gutenberg.org/", "anchor_text": "Project Gutenberg"}, {"url": "https://github.com/kamalkraj/BERT-NER", "anchor_text": "BERT NER Large model"}, {"url": "https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34", "anchor_text": "here"}, {"url": "http://nlpprogress.com/english/summarization.html", "anchor_text": "esources"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "hugging face"}, {"url": "http://textgen.thomas-lamson.com/#/", "anchor_text": "here"}, {"url": "https://github.com/WeazelDev/AITextGenerator", "anchor_text": "here"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf", "anchor_text": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "https://arxiv.org/pdf/1706.03762.pdf"}, {"url": "https://grover.allenai.org/", "anchor_text": "https://grover.allenai.org/"}, {"url": "https://arxiv.org/pdf/1508.07909.pdf", "anchor_text": "https://arxiv.org/pdf/1508.07909.pdf"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "https://arxiv.org/abs/1810.04805"}, {"url": "https://arxiv.org/abs/1910.13461", "anchor_text": "https://arxiv.org/abs/1910.13461"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "https://arxiv.org/abs/1910.10683"}, {"url": "https://arxiv.org/abs/1903.10318", "anchor_text": "https://arxiv.org/abs/1903.10318"}, {"url": "https://www.aclweb.org/anthology/W04-3252/", "anchor_text": "https://www.aclweb.org/anthology/W04-3252/"}, {"url": "https://arxiv.org/abs/1811.00907", "anchor_text": "https://arxiv.org/abs/1811.00907"}, {"url": "https://arxiv.org/abs/1904.09751", "anchor_text": "https://arxiv.org/abs/1904.09751"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a9fa15b57c6a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/text-generation?source=post_page-----a9fa15b57c6a---------------text_generation-----------------", "anchor_text": "Text Generation"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----a9fa15b57c6a---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&user=Duval+Alexandre&userId=394330cc12cf&source=-----a9fa15b57c6a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&user=Duval+Alexandre&userId=394330cc12cf&source=-----a9fa15b57c6a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa9fa15b57c6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a9fa15b57c6a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a9fa15b57c6a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@Alexandre_Duval?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@Alexandre_Duval?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Duval Alexandre"}, {"url": "https://medium.com/@Alexandre_Duval/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F394330cc12cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&user=Duval+Alexandre&userId=394330cc12cf&source=post_page-394330cc12cf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb84f141be685&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-controllable-writing-assistant-for-novel-authors-a9fa15b57c6a&newsletterV3=394330cc12cf&newsletterV3Id=b84f141be685&user=Duval+Alexandre&userId=394330cc12cf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}