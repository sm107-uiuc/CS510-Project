{"url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "time": 1682993552.663701, "path": "towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12/", "webpage": {"metadata": {"title": "Applications of Reinforcement Learning in Real World | by Gary Chan | Towards Data Science", "h1": "Applications of Reinforcement Learning in Real World", "description": "While Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) are becoming more important for businesses due to their applications in Computer Vision (CV) and Natural Language Processing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process", "paragraph_index": 4}, {"url": "https://github.com/aikorea/awesome-rl", "anchor_text": "awesome-rl", "paragraph_index": 8}, {"url": "http://www.argmin.net/2018/06/25/outsider-rl/", "anchor_text": "argmin", "paragraph_index": 8}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "dennybritz", "paragraph_index": 8}, {"url": "https://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf", "anchor_text": "[2]", "paragraph_index": 10}, {"url": "https://github.com/hongzimao/deeprm", "anchor_text": "Github", "paragraph_index": 11}, {"url": "http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf", "anchor_text": "[3]", "paragraph_index": 12}, {"url": "https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf", "anchor_text": "[10]", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1504.00702.pdf", "anchor_text": "[11]", "paragraph_index": 14}, {"url": "http://ranger.uta.edu/~jrao/papers/ICDCS09.pdf", "anchor_text": "[5]", "paragraph_index": 15}, {"url": "https://pubs.acs.org/doi/full/10.1021/acscentsci.7b00492", "anchor_text": "[4]", "paragraph_index": 18}, {"url": "https://github.com/lightingghost/chemopt", "anchor_text": "The application", "paragraph_index": 20}, {"url": "http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf", "anchor_text": "[1]", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1802.09756.pdf", "anchor_text": "[6]", "paragraph_index": 24}, {"url": "https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf", "anchor_text": "[12]", "paragraph_index": 28}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "[13]", "paragraph_index": 28}, {"url": "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf", "anchor_text": "[7]", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1507.06527.pdf", "anchor_text": "[8]", "paragraph_index": 31}, {"url": "https://arxiv.org/pdf/1804.01118.pdf", "anchor_text": "[9]", "paragraph_index": 32}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "post", "paragraph_index": 35}, {"url": "http://www.princeton.edu/~yael/ICMLTutorial.pdf", "anchor_text": "And this is exactly how human learns to make a decision", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Market_structure", "anchor_text": "market structure", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Externality", "anchor_text": "externality", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Information_asymmetry", "anchor_text": "information asymmetry", "paragraph_index": 37}], "all_paragraphs": ["While Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) are becoming more important for businesses due to their applications in Computer Vision (CV) and Natural Language Processing (NLP), Reinforcement Learning (RL) as a framework for computational neuroscience to model decision making process seems to be undervalued. Besides, there seems to be very little resources detailing how RL is applied in different industries. Despite the criticisms about RL\u2019s weaknesses, RL should never be neglected in the space of corporate research given its huge potentials in assisting decision making. As Koray Kavukcuoglu, the director of research at Deepmind, said at a conference,", "\u201cIf one of the goals that we work for here is AI then it is at the core of that. Reinforcement Learning is a very general framework for learning sequential decision making tasks. And Deep Learning, on the other hand, is of course the best set of algorithms we have to learn representations. And combinations of these two different models is the best answer so far we have in terms of learning very good state representations of very challenging tasks that are not just for solving toy domains but actually to solve challenging real world problems.\u201d", "Therefore, this article aims to 1)investigate the breadth and depth of RL applications in real world; 2)view RL from different aspects; and 3)persuade the decision makers and researchers to put more efforts on RL research.", "The rest of the article is organized as follows. Section I is a general introduction. Section II presents the applications of RL in different domains and a brief description of how it was applied. Section III summarizes the things one would need to apply RL. Section IV is the intuition from other disciplines and Section V is about how RL could be useful in the future. Section VI is conclusion.", "RL, known as a semi-supervised learning model in machine learning, is a technique to allow an agent to take actions and interact with an environment so as to maximize the total rewards. RL is usually modeled as a Markov Decision Process (MDP).", "Imagine a baby is given a TV remote control at your home (environment). In simple terms, the baby (agent) will first observe and construct his/her own representation of the environment (state). Then the curious baby will take certain actions like hitting the remote control (action) and observe how would the TV response (next state). As a non-responding TV is dull, the baby dislike it (receiving a negative reward) and will take less actions that will lead to such a result(updating the policy) and vice versa. The baby will repeat the process until he/she finds a policy (what to do under different circumstances) that he/she is happy with (maximizing the total (discounted) rewards).", "The study of RL is to construct a mathematical framework to solve the problems. For example, to find a good policy we could use valued-based methods like Q-learning to measure how good an action is in a particular state or policy-based methods to directly find out what actions to take under different states without knowing how good the actions are.", "However, the problems we face in the real world can be extremely complicated in many different ways and therefore a typical RL algorithm has no clue to solve. For example, the state space is very large in the game of GO, environment cannot be fully observed in Poker game and there are lots of agents interact with each other in the real world. Researchers have invented methods to solve some of the problems by using deep neural network to model the desired policies, value functions or even the transition models, which therefore is called Deep Reinforcement Learning. This article makes no distinction between RL and Deep RL.", "There are lots of good stuffs about RL online and interested readers can visit awesome-rl, argmin and dennybritz.", "This part is written for general readers. At the same time, it will be of greater value for readers with some knowledge about RL.", "Designing algorithms to allocate limited resources to different tasks is challenging and requires human-generated heuristics. The paper \u201cResource Management with Deep Reinforcement Learning\u201d [2] showed how to use RL to automatically learn to allocate and schedule computer resources to waiting jobs, with the objective to minimize the average job slowdown.", "State space was formulated as the current resources allocation and the resources profile of jobs. For action space, they used a trick to allow the agent to choose more than one action at each time step. Reward was the sum of (-1/duration of the job) over all the jobs in the system. Then they combined REINFORCE algorithm and baseline value to calculate the policy gradients and find the best policy parameters that give the probability distribution of actions to minimize the objective. Click here to view the code on Github.", "In the paper \u201cReinforcement learning-based multi-agent system for network traffic signal control\u201d[3], researchers tried to design a traffic light controller to solve the congestion problem. Tested only on simulated environment though, their methods showed superior results than traditional methods and shed a light on the potential uses of multi-agent RL in designing traffic system.", "Five agents were put in the five-intersection traffic network, with a RL agent at the central intersection to control traffic signaling. The state was defined as eight-dimensional vector with each element representing the relative traffic flow of each lane. Eight choices were available to the agent, each representing a phase combination, and the reward function was defined as reduction in delay compared with previous time step. The authors used DQN to learn the Q value of the {state, action} pairs.", "There are tremendous work on applying RL in Robotics. Readers are referred to [10] for a survey of RL in Robotics. In particular, [11] trained a robot to learn policies to map raw video images to robot\u2019s actions. The RGB images were fed to a CNN and outputs were the motor torques. The RL component was the guided policy search to generate training data that came from its own state distribution.", "There are more than 100 configurable parameters in a web system and the process of tuning the parameters requires a skilled operator and numerous trail-and-error tests. The paper \u201cA Reinforcement Learning Approach to Online Web System Auto-configuration\u201d [5] showed the first attempt in the domain on how to do autonomic reconfiguration of parameters in multi-tier web systems in VM-based dynamic environments.", "The reconfiguration process can be formulated as a finite MDP. The state space was the system configuration, action space was {increase, decrease, keep} for each parameter, and reward was defined as the difference between the given targeted response time and measured response time. The authors used the model-free Q-learning algorithm to do the task.", "Although the authors used some other technique like policy initialization to remedy the large state space and computational complexity of the problem instead of the potential combinations of RL and neural network, it is believed that the pioneering work has paved the way for future research in this area.", "RL can also be applied in optimizing chemical reactions.[4] showed that their model outperformed a state-of-the-art algorithm and generalized to dissimilar underlying mechanisms in the paper \u201cOptimizing Chemical Reactions with Deep Reinforcement Learning\u201d.", "Combined with LSTM to model the policy function, the RL agent optimized the chemical reaction with the Markov decision process (MDP) characterized by {S, A, P, R}, where S was the set of experimental conditions (like temperature, pH, etc), A was the set all possible actions that can change the experimental conditions, P was the transition probability from current experiment condition to the next condition, and R was the reward which is a function of the state.", "The application is a great one to demonstrate how RL can reduce time-consuming and trial-and-error work in a relatively stable environment.", "Previous work of news recommendations faced several challenges including the rapid changing dynamic of news, users get bored easily and Click Through Rate cannot reflect the retention rate of users. Guanjie et al. have applied RL in news recommendation system in a paper titled \u201cDRN: A Deep Reinforcement Learning Framework for News Recommendation\u201d to combat the problems [1].", "In practice, they constructed four categories of features, namely A)user features and B)context features as the state features of the environment, and C)user-news features and D)news features as the action features. The four features were input to the Deep Q-Network(DQN) to calculate the Q-value. A list of news were chosen to recommend based on the Q-value, and the user\u2019s click on the news was a part of the reward the RL agent received.", "The authors also employed other techniques to address other challenging problems, including memory replay, survival models, Dueling Bandit Gradient Descent and so on. Please refer to the paper for details.", "Researchers from Alibaba Group published a paper \u201cReal-Time Bidding with Multi-Agent Reinforcement Learningin Display Advertising\u201d [6] and claimed that their distributed cluster-based multi-agentbidding solution (DCMAB) has achieved promising results and thus they plan to conduct a live test in Taobao platform.", "The details of the implementation are left to users to investigate. Generally speaking, Taobao ad platform is a place for merchants to place a bid in order to display ad to the customers. This could be a multi-agent problem because the merchants are bidding against each other and their actions are interrelated. In the paper, merchants and customers were clustered into different groups to reduce computational complexity. The state space of the agents indicated the cost-revenue status of the agents, action space was the bid (continuous), and reward was the revenue caused by the customer cluster.", "Other questions, including the impact of different reward settings (self-interested vs coordinate) on agents\u2019 revenue were also studied in the paper.", "RL is so well-known these days because it is the mainstream algorithm used to solve different games and sometimes achieve super-human performance.", "The most famous one must be AlphaGo[12] and AlphaGo Zero[13]. AlphaGo, trained with countless human games, already achieved super-human performance by using value network and Monte Carlo tree search (MCTS) in its policy network. Yet, the researchers later on thought back and tried a purer RL approach \u2014 train it from scratch. The researchers let the new agent, AlphaGo Zero, played with itself and finally beat AlphaGo 100\u20130.", "More and more attempts to combine RL and other deep learning architecture can be seen recently and they showed impressive results.", "One of the most influential work in RL is the pioneering work of Deepmind to combine CNN with RL [7]. By doing so, the agent has the ability to \u201csee\u201d the environment through high-dimensional sensory and then learn to interact with it.", "RL and RNN is another combinations people used to try new idea. RNN is a type of neural network that has \u201cmemories\u201d. When combined with RL, RNN gives the agents\u2019 ability to memorize things. For example, [8] combined LSTM with RL to create Deep Recurrent Q-Network(DRQN) to play Atari 2600 games. [4] also used RNN and RL to solve chemical reaction optimization problem.", "Deepmind showed [9] how to use generative models and RL to generate programs. In the model, the adversarially trained agent used the signal as rewards to improve the actions, instead of propagating the gradients to the input space as in the GAN training.", "III. What you need to know before applying RL to your problem", "There are several things needed before RL can be applied:", "To stay objective and fair, you are also warned about the shortcomings of RL and here is a great post about it.", "RL has a very close relationship with psychology, biology and neuroscience. If you think about it, what a RL agent does is just trial-and-error: it learns how good or bad its actions are based on the rewards it receives from the environment. And this is exactly how human learns to make a decision. Besides, the exploration and exploitation problem, credit assignment problem, attempts to model the environment are also something we face in our everyday life.", "The Economics theory can also shed some light on RL. In particular, the analysis of multi-agent reinforcement learning (MARL) can be understood from the perspectives of game theory, which is a research area developed by John Nash to understand the interactions of agents in a system. In addition to game theory, MARL, Partially Observable Markov Decision Process (POMDP) could also be useful to understand other economic topics like market structure (e.g.monopoly, oligopoly, etc), externality and information asymmetry.", "V. What could RL possibly achieve in the future", "RL still has lots of problems and cannot be used easily. Yet, as long as more efforts are put in solving the problems, RL would be influential and impactful in the following ways:", "This article just showed some of the examples of RL applications in various industries. They should not limit your RL use case and as always, you should use first principle to understand the nature of RL and your problem.", "If you are a decision maker of a company, I hope this article is enough to persuade you to rethink about your business and see if RL can be potentially used. If you are a researcher, I hope you would agree with me that although RL still has different shortcomings, it also means it has lots of potentials to improve and lots of research opportunities.", "What are your thoughts? Can you think of any problem that RL could solve?", "[2] H.Mao, Alizadeh, M. Alizadeh, Menache, I.Menache, and S.Kandula. Resource Management With deep Reinforcement Learning. In ACM Workshop on Hot Topics in Networks, 2016.", "[3] I. Arel, C. Liu, T. Urbanik, and A. Kohls, \u201cReinforcement learning-basedmulti-agent system for network traffic signal control,\u201dIET IntelligentTransport Systems, 2010.", "[4] Z. Zhou, X. Li, and R. N. Zare. Optimizing Chemical Reactions with Deep Reinforcement Learning. ACSCentral Science3, 2017.", "[5] X. Bu, J. Rao, C. Z. Xu. A reinforcement learning approach to online web systems auto-configuration. In Distributed Computing Systems, 2009. ICDCS\u201909.29th IEEE International Conference on. IEEE , 2019.", "[8] M. J. Hausknecht and P. Stone. Deep Recurrent Q-Learning For Partially Observable MDPs. Proc. of Conf. on Artificial Intelligence, AAAI, 2015.", "[9] Y. Ganin, T. Kulkarni, I. Babuschkin, S. Eslami and O. Vinyals. Synthesizing Programs For Images Using Reinforced Adversarial Learning. arXiv preprintarXiv:1804.01118.", "[13] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen,T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge.Nature, 2017.", "A journey of human learning\u2026Data Scientist | Developer | Thinker | Writer | Dreamer | Learner | Idealist | Realist | Environmentalist |"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a94955bcd12&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@Garychl?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Gary Chan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe75d60a48e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=post_page-e75d60a48e51----1a94955bcd12---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a94955bcd12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=-----1a94955bcd12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a94955bcd12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=-----1a94955bcd12---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process"}, {"url": "http://incompleteideas.net/book/bookdraft2017nov5.pdf", "anchor_text": "Reinforcement Learning:An Introduction"}, {"url": "https://github.com/aikorea/awesome-rl", "anchor_text": "awesome-rl"}, {"url": "http://www.argmin.net/2018/06/25/outsider-rl/", "anchor_text": "argmin"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "dennybritz"}, {"url": "https://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf", "anchor_text": "[2]"}, {"url": "https://github.com/hongzimao/deeprm", "anchor_text": "Github"}, {"url": "http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf", "anchor_text": "[3]"}, {"url": "http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf", "anchor_text": "Source"}, {"url": "https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf", "anchor_text": "[10]"}, {"url": "https://arxiv.org/pdf/1504.00702.pdf", "anchor_text": "[11]"}, {"url": "http://ranger.uta.edu/~jrao/papers/ICDCS09.pdf", "anchor_text": "[5]"}, {"url": "https://pubs.acs.org/doi/full/10.1021/acscentsci.7b00492", "anchor_text": "[4]"}, {"url": "https://github.com/lightingghost/chemopt", "anchor_text": "The application"}, {"url": "http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf", "anchor_text": "[1]"}, {"url": "https://arxiv.org/pdf/1802.09756.pdf", "anchor_text": "[6]"}, {"url": "https://arxiv.org/pdf/1802.09756.pdf", "anchor_text": "https://arxiv.org/pdf/1802.09756.pdf"}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "here"}, {"url": "https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf", "anchor_text": "[12]"}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "[13]"}, {"url": "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf", "anchor_text": "[7]"}, {"url": "https://arxiv.org/pdf/1507.06527.pdf", "anchor_text": "[8]"}, {"url": "https://arxiv.org/pdf/1804.01118.pdf", "anchor_text": "[9]"}, {"url": "https://www.youtube.com/watch?v=N5oZIO8pE40", "anchor_text": "source"}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "post"}, {"url": "http://www.princeton.edu/~yael/ICMLTutorial.pdf", "anchor_text": "And this is exactly how human learns to make a decision"}, {"url": "https://en.wikipedia.org/wiki/Market_structure", "anchor_text": "market structure"}, {"url": "https://en.wikipedia.org/wiki/Externality", "anchor_text": "externality"}, {"url": "https://en.wikipedia.org/wiki/Information_asymmetry", "anchor_text": "information asymmetry"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1a94955bcd12---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1a94955bcd12---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a94955bcd12---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a94955bcd12---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----1a94955bcd12---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a94955bcd12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=-----1a94955bcd12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a94955bcd12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=-----1a94955bcd12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a94955bcd12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe75d60a48e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=post_page-e75d60a48e51----1a94955bcd12---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe98877c5ce71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&newsletterV3=e75d60a48e51&newsletterV3Id=e98877c5ce71&user=Gary+Chan&userId=e75d60a48e51&source=-----1a94955bcd12---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Written by Gary Chan"}, {"url": "https://medium.com/@Garychl/followers?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "456 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe75d60a48e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&user=Gary+Chan&userId=e75d60a48e51&source=post_page-e75d60a48e51----1a94955bcd12---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe98877c5ce71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplications-of-reinforcement-learning-in-real-world-1a94955bcd12&newsletterV3=e75d60a48e51&newsletterV3Id=e98877c5ce71&user=Gary+Chan&userId=e75d60a48e51&source=-----1a94955bcd12---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-compare-machine-learning-algorithms-ccc266c4777?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Gary Chan"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-compare-machine-learning-algorithms-ccc266c4777?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "How to Compare Machine Learning Algorithms\u201cWhen you change the way you look at things, the things you look at change.\u201d \u2015Wayne Dyer"}, {"url": "https://towardsdatascience.com/how-to-compare-machine-learning-algorithms-ccc266c4777?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "5 min read\u00b7Jan 23, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccc266c4777&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compare-machine-learning-algorithms-ccc266c4777&user=Gary+Chan&userId=e75d60a48e51&source=-----ccc266c4777----0-----------------clap_footer----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-compare-machine-learning-algorithms-ccc266c4777?source=author_recirc-----1a94955bcd12----0---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccc266c4777&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compare-machine-learning-algorithms-ccc266c4777&source=-----1a94955bcd12----0-----------------bookmark_preview----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1a94955bcd12----1---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1a94955bcd12----1-----------------bookmark_preview----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1a94955bcd12----2---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1a94955bcd12----2-----------------bookmark_preview----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/coinmonks/i-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Gary Chan"}, {"url": "https://medium.com/coinmonks?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "Coinmonks"}, {"url": "https://medium.com/coinmonks/i-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "I quit my job to learn AI. Here\u2019s why and how you can get started.Leaving a job is never an easy decision, especially if you are leaving not to get a new one nor to start a new business, but to learn a new\u2026"}, {"url": "https://medium.com/coinmonks/i-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": "9 min read\u00b7Jun 19, 2017"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcoinmonks%2F84bdfc35a3c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoinmonks%2Fi-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3&user=Gary+Chan&userId=e75d60a48e51&source=-----84bdfc35a3c3----3-----------------clap_footer----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/coinmonks/i-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3?source=author_recirc-----1a94955bcd12----3---------------------7fadc853_afed_40a4_a845_edb3ea62ebd8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84bdfc35a3c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoinmonks%2Fi-quit-my-job-to-learn-ai-heres-why-and-how-you-can-get-started-84bdfc35a3c3&source=-----1a94955bcd12----3-----------------bookmark_preview----7fadc853_afed_40a4_a845_edb3ea62ebd8-------", "anchor_text": ""}, {"url": "https://medium.com/@Garychl?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "See all from Gary Chan"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----1a94955bcd12----0-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----1a94955bcd12----1-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Mark Riedl"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "A Very Gentle Introduction to Large Language Models without the Hype[This is a work in progress]"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "38 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&user=Mark+Riedl&userId=7247bdeb9655&source=-----5f67941fa59e----0-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----1a94955bcd12----0---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "53"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&source=-----1a94955bcd12----0-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Aaron Master"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Please Stop Drawing Neural Networks WrongThe Case for GOOD Diagrams"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "12 min read\u00b7Mar 21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77----1-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----1a94955bcd12----1---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "33"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----1a94955bcd12----1-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Piotr Krosniak"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Vaccine Supply Chain Optimization with AI-Powered Capacitated Vehicle Routing Problem(CVRP)- Part 1The world is facing a global health crisis, and one of the most important challenges is to ensure an efficient and timely distribution of\u2026"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "6 min read\u00b7Jan 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&user=Piotr+Krosniak&userId=b791abcfafd5&source=-----ca79519e9ad7----2-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----1a94955bcd12----2---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&source=-----1a94955bcd12----2-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----3-----------------clap_footer----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----1a94955bcd12----3---------------------54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----1a94955bcd12----3-----------------bookmark_preview----54bd0b0e_c6c6_4552_8672_fc6cb64b1c31-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1a94955bcd12--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}