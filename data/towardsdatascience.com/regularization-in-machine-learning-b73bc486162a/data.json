{"url": "https://towardsdatascience.com/regularization-in-machine-learning-b73bc486162a", "time": 1683006471.557293, "path": "towardsdatascience.com/regularization-in-machine-learning-b73bc486162a/", "webpage": {"metadata": {"title": "Regularization in Machine Learning | by Eugen Lindwurm | Towards Data Science", "h1": "Regularization in Machine Learning", "description": "In this article, I will discuss the concept of regularization, a taxonomy of regularization methods, and specific examples of regularization strategies. Essentially, regularization adds bias to the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.deeplearningbook.org/contents/regularization.html", "anchor_text": "Goodfellow et al.\u2019s", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al.", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al.", "paragraph_index": 6}, {"url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0", "anchor_text": "data augmentation in image recognition", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "training with adversarial examples", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1207.0580", "anchor_text": "dropout", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1502.03167v3", "anchor_text": "batchnorm", "paragraph_index": 8}, {"url": "http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Fukushima1988.pdf", "anchor_text": "convolutional layers", "paragraph_index": 12}, {"url": "https://www.deeplearningbook.org/contents/mlp.html", "anchor_text": "softmax layers", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "attention layers", "paragraph_index": 12}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.541", "anchor_text": "making DNNs applicable", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "natural language task", "paragraph_index": 13}, {"url": "https://pdfs.semanticscholar.org/df27/dde10589455d290eeee6d0ae6ceeb83d0c6b.pdf", "anchor_text": "class imbalance independent loss", "paragraph_index": 13}, {"url": "https://www.semanticscholar.org/paper/Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan/4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "anchor_text": "weight decay", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "Tikhonov regularization", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950", "anchor_text": "gradients on the inputs", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1206.5538.pdf", "anchor_text": "functional properties of the input-output mapping", "paragraph_index": 16}, {"url": "https://www.nature.com/articles/s41467-020-14663-9", "anchor_text": "interesting recent results", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1611.03530", "anchor_text": "explain why deep neural networks generalize as well as they do", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al.", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1611.03530", "anchor_text": "Understanding deep learning requires rethinking generalization", "paragraph_index": 19}], "all_paragraphs": ["In this article, I will discuss the concept of regularization, a taxonomy of regularization methods, and specific examples of regularization strategies.", "Essentially, regularization adds bias to the model fitting process in order to combat overfitting. The intuition behind regularization is that if we have a very rich model class, such as large neural networks, we want to restrict our learning procedure to only search over a subset of this class that contains models with desired properties (e.g. some level of simplicity). This restriction is a bias. The hope is that if we pick the subset wisely (and we do that via appropriate regularization), we will find a model that generalizes better than had we looked at the entire model class.", "Imagine the model class we are considering is the class of linear regression and we are trying to fit a dataset of fewer samples than we have input features. Since the number of parameters we want to fit (= number of features) is larger than the number of samples, we have an ill-specified problem and there are potentially many models in our class that would solve the regression problem. We can restrict this class by imposing a L2 regularizer, i.e. adding w\u00b2 to the error term where w are our regression weights. This way, we will pick the solution with the smallest weights over any equally optimal accurate set of weights.", "Regularization is a concept much older than deep learning and an integral part of classical statistics. It has arguably been one of the most important collections of techniques fueling the recent machine learning boom. Still, it is often not entirely clear what we mean when using the term \u201cregularization\u201d and there exist several competing definitions (e.g. Goodfellow et al.\u2019s). In this article I will adopt perhaps one of the broadest ones - by Kuka\u010dka et al.:", "Definition 1: Regularization is any supplementary technique that aims at making the model generalize better, i.e. produce better results on the test set.", "This might at first seem too general to be useful, but the authors also provide a taxonomy to make sense of the wealth of regularization approaches that this definition encompasses. An overview of this taxonomy follows.", "When looking at the model fitting process, Kuka\u010dka et al. identified the following parts that each are associated with a way to bias learning, i.e. to regularize.", "Based on the formula of empirical risk optimization above, we can regularize:1. The training set2. The model family3. The error term4. The regularization term5. The optimization procedure", "Any preprocessing or augmentation of our data (stochastic or deterministic) that fulfills Definition 1 from above (i.e. helps to achieve better test error) belongs in this category. Prominent examples are over- or undersampling of imbalanced datasets, data augmentation in image recognition, but also training with adversarial examples, dropout, and batchnorm. The latter two might seem surprising, but they can be seen as transformations of the data in the representation space created by intermediate network layers.", "Augmenting the training set can be a simple way to get rid of specific biases in the data without having to come up with a more sophisticated model \u2014 and image recognition literature shows that it really works! The downsides are that these techniques often increase the training set size, which means that more resources need to be spent on training, and that one has to be careful not to create samples irrelevant to the task.", "Selecting the right structure of our model is perhaps not commonly referred to as regularization, but it also is a way to improve generalization, so it fits the definition.", "If you have ever worked with neural networks, you will know that hyperparameter tuning is maybe the most tedious part of getting a good model. Selecting the number of layers and neurons per layer, or even just defining over which different settings to search can be seen as regularization. You are essentially restricting the search space of all possible neural networks to just the configurations that make sense to you. The human intuition at play here is perhaps a reason why deep learning is sometimes compared to art or black magic.", "More exciting than just tuning the number of elements in the network is defining the structures the network is made up of: convolutional layers, LSTMs, softmax layers, attention layers, \u2026 the list is endless. Let\u2019s not forget that convnets played a crucial role in making DNNs applicable to image recognition!", "Choosing the right error term, of course, also influences model performance on the test set. Mean squared error is perhaps the most popular error term, but cross entropy is also popular, e.g. for natural language tasks, or some class imbalance independent loss. These error terms encapsulate assumptions about the task, data, and what a good solution looks like.", "The regularization term is probably what most people mean when they talk about regularization. It is a term that modifies the error term without depending on data. This independence of data means that the regularization term only serves to bias the structure of model parameters.", "Probably the most popular regularization term is the L2 norm on the network or regression weights, aka weight decay in deep learning and defining feature of ridge regression in classical statistics. This L2 regularization, a special form of Tikhonov regularization is effectively a zero-mean Gaussian prior on the model weights, meaning that we favour solutions that on few features rather than many and don\u2019t rely on any feature too much. This potentially eliminates redundant features and results in a simpler model. To further increase sparsity in the weights, the L1 norm (lasso regression) can be used.", "Other regularization terms target e.g. gradients on the inputs, or functional properties of the input-output mapping.", "The regularization term is often an elegant method of defining what you want your model to look like because they can be used in a plug-and-play fashion and work the same on any dataset. They often increase the hyperparameter tuning effort by a bit because you generally have to define a parameter that weights the contribution of the regularization term versus the error term.", "The optimization procedure is an interesting and often overlooked way of regularization. Changing the optimizer and learning rate clearly has an effect on what model is learned, but what this effect is is generally not intuitively clear. Some interesting recent results show that SGD actually might be an important regularizer that might help explain why deep neural networks generalize as well as they do (despite being vastly over-parametrized).", "That\u2019s it for this article, I hope you learned something useful! Make sure to check out Kuka\u010dka et al.\u2019s paper that introduced the taxonomy, and if you\u2019re interested in generalization and regularization also take a look at the very accessible \u201cUnderstanding deep learning requires rethinking generalization\u201d by Zhang et al.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Machine Learning. Interested in social and environmental issues."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb73bc486162a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b73bc486162a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----b73bc486162a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00----b73bc486162a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@daiga_ellaby?utm_source=medium&utm_medium=referral", "anchor_text": "Daiga Ellaby"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.deeplearningbook.org/contents/regularization.html", "anchor_text": "Goodfellow et al.\u2019s"}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al."}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al."}, {"url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0", "anchor_text": "data augmentation in image recognition"}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "training with adversarial examples"}, {"url": "https://arxiv.org/abs/1207.0580", "anchor_text": "dropout"}, {"url": "https://arxiv.org/abs/1502.03167v3", "anchor_text": "batchnorm"}, {"url": "http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Fukushima1988.pdf", "anchor_text": "convolutional layers"}, {"url": "https://www.deeplearningbook.org/contents/mlp.html", "anchor_text": "softmax layers"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "attention layers"}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.541", "anchor_text": "making DNNs applicable"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "natural language task"}, {"url": "https://pdfs.semanticscholar.org/df27/dde10589455d290eeee6d0ae6ceeb83d0c6b.pdf", "anchor_text": "class imbalance independent loss"}, {"url": "https://www.semanticscholar.org/paper/Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan/4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "anchor_text": "weight decay"}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "Tikhonov regularization"}, {"url": "https://towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950", "anchor_text": "gradients on the inputs"}, {"url": "https://arxiv.org/pdf/1206.5538.pdf", "anchor_text": "functional properties of the input-output mapping"}, {"url": "https://www.nature.com/articles/s41467-020-14663-9", "anchor_text": "interesting recent results"}, {"url": "https://arxiv.org/abs/1611.03530", "anchor_text": "explain why deep neural networks generalize as well as they do"}, {"url": "https://arxiv.org/abs/1710.10686", "anchor_text": "Kuka\u010dka et al."}, {"url": "https://arxiv.org/abs/1611.03530", "anchor_text": "Understanding deep learning requires rethinking generalization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b73bc486162a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b73bc486162a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/regularization?source=post_page-----b73bc486162a---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b73bc486162a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b73bc486162a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----b73bc486162a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----b73bc486162a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb73bc486162a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b73bc486162a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b73bc486162a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b73bc486162a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b73bc486162a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b73bc486162a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b73bc486162a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/@pflaenzchen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "135 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56b4244ba55f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-machine-learning-b73bc486162a&newsletterV3=cb9e52bc6a00&newsletterV3Id=56b4244ba55f&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}