{"url": "https://towardsdatascience.com/algorithms-from-scratch-decision-tree-1898d37b02e0", "time": 1683011489.9868212, "path": "towardsdatascience.com/algorithms-from-scratch-decision-tree-1898d37b02e0/", "webpage": {"metadata": {"title": "Algorithms from Scratch: Decision Tree | by Kurtis Pykes | Towards Data Science", "h1": "Algorithms from Scratch: Decision Tree", "description": "Those of you familiar with my earlier writings would recall that I once wrote an overview of the Random Forest algorithm. A solid foundation on Decision trees is a prerequisite to understanding the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/tagged/algorithms-from-scratch", "anchor_text": "Algorithms from Scratch", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/algorithms-from-scratch-linear-regression-c654353d1e7c", "anchor_text": "Linear Regression", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/algorithms-from-scratch-logistic-regression-7bacdfd9738e", "anchor_text": "Logistic Regression", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Recursion_(computer_science)#:~:text=Recursion%20in%20computer%20science%20is%20a%20method%20of,one%20of%20the%20central%20ideas%20of%20computer%20science.", "anchor_text": "Wikipedia", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Binary_splitting", "anchor_text": "Binary splitting", "paragraph_index": 11}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html", "anchor_text": "Documentation", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain", "anchor_text": "Wikipedia", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain", "anchor_text": "Wikipedia", "paragraph_index": 20}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html", "anchor_text": "Documentation", "paragraph_index": 29}], "all_paragraphs": ["Those of you familiar with my earlier writings would recall that I once wrote an overview of the Random Forest algorithm. A solid foundation on Decision trees is a prerequisite to understanding the inner workings of Random Forest; The Random forest builds multiple Decision tree\u2019s and outputs the average over the predictions of each tree for regression problems, and in classification problems it outputs the relative majority of the predictions from each tree.", "To build on the above story, I will be focusing much more on the Decision tree learning algorithm as it is a fundamental algorithm in the Machine Learning space. Many models base their structure on the Decision tree model such as the Random Forest and Gradient Boosted tree\u2019s. Additionally, I will be doing a Python implementation of this algorithm from scratch to further expand our intuition on what is happening within our algorithm.", "Popular due to its intelligibility and simplicity, the Decision tree is one of the easiest algorithms to visualize and interpret which is handy when presenting results to a non-technical audience, as is often required in industry. If we simply consider a tree in a flowchart-like state, from root to leaves where the path to a leaf from the root defines decision rules on the features, then we already have a good level of intuition required to understand Decision tree learning.", "Unlike the first two algorithms we covered in the Algorithms from Scratch series (Linear Regression and Logistic Regression), the Decision tree algorithm is a non-parametric algorithm meaning that it does not make an assumption about the data or population. This does have an effect on our model since we are trading bias for variance in the model during training making the Decision tree much more prone to overfitting.", "In the field of Machine Learning there are two main Decision tree models. The one we use depends on the type of target variable we are attempting to predict:", "Classification Tree: A tree model employed to predict a target variable that takes a discrete value. Thereby, the leaf node represents a class and the branches represent conjunctions of the features that lead to those class labels.", "Regression Tree: A tree model employed to predict a target variable that takes a continuous value. Contrary to the classification tree, in the Regression tree, each leaf node contains a continuous value (i.e. a house price); The branches represent conjunctions of the features that lead to each continuous variable.", "Note: An umbrella term to refer to both procedures is Classification and Regression Tree (CART), first introduced by Breiman et al. in 1984.", "In Figure 1 we can see the structure that is followed by CART algorithms. Though this structure is set for both trees, there are some subtle differences between classification and regression trees such as the output from each tree; The classification tree returns mode class of the leaf node and the regression tree returns the mean.", "Another significant difference between the two algorithms is the criterion that we try to minimize when partitioning the feature space. Generally, we want to select the feature, j, and split-point, s, that best splits the feature space into 2 regions, but how this is measured in a regression tree and classification tree differs as is shown in Figure 2.", "Note: We will be building a Decision tree classifier with gini impurity as the criterion for the split.", "The pseudocode above demonstrates a phenomena known as recursion in computer science: A method of solving a problem where the solution depends on solutions to smaller instances of the same problem (Source: Wikipedia), and Binary splitting hence in some illustrations step 1 \u2014 2 is referred to as recursive binary splitting.", "For this implementation we will be leveraging the following frameworks:", "The dataset we will use is the iris dataset from Scikit learn \u2014 See Documentation", "We are going to need to split our data into true and false index in response to the decision rule at a specific branch. If the condition of the decision rule is met, we say that branch is true (which we will denote as the left) and false (denoted as right).", "To check if our function works correctly, we will perform a split on all of the data and pass it the best column and value manually to see if our data is separated with respect to the graph above.", "Next, we need a criterion that we want to minimize. In this implementation we will be minimizing the gini impurity using a function called gini_impurity. Without getting to technical gini impurity simply measures how mixed our data is at a node; To help grasp this concept think of gold. When gold is impure it refers to the mixture of other substances within it, however when it is pure we can say there are 0 impurities (this isn't exactly true as refined gold up to 99.99% pure so technically there are still some impurities).", "The ideal is to nodes that are pure meaning that the target labels are separated into separate nodes. \u2014 To go into the technical details of gini impurity see here", "If you scroll back up to Figure 4, you will see that the impurity at the root node is 0.663. Therefore, to determine whether our gini_impurity function is working correctly, we should see this number on output", "To split at a feature (and value) we need to a way of quantifying what would result in the best outcome if we were to split at that point. Information gain is a useful way to quantify what feature and feature value to split on at each node. For each node of the tree, the information value \u201crepresents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node\u201d. (Source: Wikipedia)", "The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. (Source: Wikipedia)", "Based on the above statement, we can now see why petal length (cm) with the value 2.45 was selected as the first split.", "The above helper functions are now going to be brought into play. We had to manually select the feature and value before right? The next function will now automatically search the feature space and find the feature and feature value the best splits the data.", "Great, we have all the components we need for our algorithm to work. However, the above functions on only perform one split on our training data (the stump/root).", "Here are some classes that we will use to store specific data from our Decision tree and print our tree.", "For the algorithm to work we will need the splits to happen recursively until we meet a stopping criterion \u2014 in this case it\u2019s until each leaf node is pure.", "Super! Now you\u2019ve seen how to implement a Decision tree from scratch and we have trained it on our training data. It does not stop there though, the purpose of building the algorithm in the first place is to automate the classification of new observations. The next section is dedicated to inference\u2026", "To check if our function is operating correctly I will use one example observation.", "When we apply our predict function to the example we should hope that the observation traverses the tree accordingly and outputs setosa, lets check\u2026", "Supreme!! However, that is just to one example. If we want to apply this function to every observation in our test set we can use df.apply \u2014 see Documentation", "Okay, here comes the moment of truth. We need to check if our algorithm returns the same predictions as the scikit learn model as a way of checking if we have implemented our algorithm correctly. We do this by simply doing sklearn_y_preds == X_true[\"predictions\"] which returns a boolean array for each observation \u2014 in our case they are all true.", "Establishing a good foundation of Decision trees will go a long way in understanding many other important Machine Learning algorithms. It is a very powerful algorithm that is often used as an ensemble model to win various Data Science competitions. Although its very easy to conceptualize, the decision tree is quite difficult to construct from scratch, hence why I\u2019d always advocate for using established Machine Learning frameworks whenever possible.", "Thank you for reading to the end of the article! If you\u2019d like to keep in contact with me, I am most accessible on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1898d37b02e0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kurtispykes.medium.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": ""}, {"url": "https://kurtispykes.medium.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Kurtis Pykes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ba760786877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&user=Kurtis+Pykes&userId=5ba760786877&source=post_page-5ba760786877----1898d37b02e0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@johannsiemens?utm_source=medium&utm_medium=referral", "anchor_text": "Johann Siemens"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/tagged/algorithms-from-scratch", "anchor_text": "Algorithms From Scratch"}, {"url": "https://towardsdatascience.com/random-forest-overview-746e7983316", "anchor_text": "Random Forest OverviewA conceptual overview of the Random Foresttowardsdatascience.com"}, {"url": "https://towardsdatascience.com/tagged/algorithms-from-scratch", "anchor_text": "Algorithms from Scratch"}, {"url": "https://towardsdatascience.com/algorithms-from-scratch-linear-regression-c654353d1e7c", "anchor_text": "Linear Regression"}, {"url": "https://towardsdatascience.com/algorithms-from-scratch-logistic-regression-7bacdfd9738e", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/u/60a50d133053?source=post_page-----746e7983316----------------------", "anchor_text": "Stacey Ronaghan"}, {"url": "https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3", "anchor_text": "The Mathematics of Decision Trees, Random Forest and Feature Importance in Scikit-learn and Spark"}, {"url": "https://en.wikipedia.org/wiki/Recursion_(computer_science)#:~:text=Recursion%20in%20computer%20science%20is%20a%20method%20of,one%20of%20the%20central%20ideas%20of%20computer%20science.", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Binary_splitting", "anchor_text": "Binary splitting"}, {"url": "https://github.com/kurtispykes/ml-from-scratch/blob/master/decision_tree.ipynb", "anchor_text": "kurtispykes/ml-from-scratchPermalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage\u2026github.com"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html", "anchor_text": "Documentation"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain", "anchor_text": "Wikipedia"}, {"url": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb", "anchor_text": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb"}, {"url": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb", "anchor_text": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb"}, {"url": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb", "anchor_text": "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html", "anchor_text": "Documentation"}, {"url": "https://www.linkedin.com/in/kurtispykes/", "anchor_text": "Kurtis Pykes - AI Writer - Towards Data Science | LinkedInView Kurtis Pykes' profile on LinkedIn, the world's largest professional community. Kurtis has 1 job listed on their\u2026www.linkedin.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1898d37b02e0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1898d37b02e0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1898d37b02e0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/algorithms-from-scratch?source=post_page-----1898d37b02e0---------------algorithms_from_scratch-----------------", "anchor_text": "Algorithms From Scratch"}, {"url": "https://medium.com/tag/programming?source=post_page-----1898d37b02e0---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&user=Kurtis+Pykes&userId=5ba760786877&source=-----1898d37b02e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&user=Kurtis+Pykes&userId=5ba760786877&source=-----1898d37b02e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1898d37b02e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1898d37b02e0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1898d37b02e0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1898d37b02e0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1898d37b02e0--------------------------------", "anchor_text": ""}, {"url": "https://kurtispykes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kurtispykes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kurtis Pykes"}, {"url": "https://kurtispykes.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.3K Followers"}, {"url": "https://www.fullstackfreelancershub.com/wisdom-wednesday", "anchor_text": "https://www.fullstackfreelancershub.com/wisdom-wednesday"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ba760786877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&user=Kurtis+Pykes&userId=5ba760786877&source=post_page-5ba760786877--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffde3d752d24c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithms-from-scratch-decision-tree-1898d37b02e0&newsletterV3=5ba760786877&newsletterV3Id=fde3d752d24c&user=Kurtis+Pykes&userId=5ba760786877&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}