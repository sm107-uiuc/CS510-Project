{"url": "https://towardsdatascience.com/transformers-at-neurips-2019-3c8e76e3ab42", "time": 1683002591.7107022, "path": "towardsdatascience.com/transformers-at-neurips-2019-3c8e76e3ab42/", "webpage": {"metadata": {"title": "Transformers at NeurIPS 2019. Papers related to transformers at\u2026 | by Paul Gladkov | Towards Data Science", "h1": "Transformers at NeurIPS 2019", "description": "After my previous story about BERT at EMNLP 2019, it would be a crime not to make a short recap of papers about transformers at NeurIPS 2019. The Conference and Workshop on Neural Information\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/bert-at-emnlp-2019-46db6c2e59b2", "anchor_text": "BERT at EMNLP 2019", "paragraph_index": 0}], "all_paragraphs": ["After my previous story about BERT at EMNLP 2019, it would be a crime not to make a short recap of papers about transformers at NeurIPS 2019. The Conference and Workshop on Neural Information Processing Systems was held in Vancouver from December 8th through the 14th. As usual, there were a lot of breathtaking ideas and advanced researches. Here are some of them.", "There was introduced a novel approach for learning task-agnostic joint representations of image content and natural language.", "ViLBERT (Vision-and-Language BERT) consists of two parallel BERT-style models operating over image regions and text segments. Each stream is a series of transformer blocks and novel co-attentional transformer layers which were introduced to enable information exchange between modalities. Each image is represented as a set of region features by extracting bounding boxes and their visual features from a pre-trained object detection network (Faster R-CNN).", "There were two pre-training tasks: masked multi-modal modeling and multi-modal alignment prediction. The masked multi-modal modeling task follows from the masked language modeling task in standard BERT \u2014 masking approximately 15% of both words and image region inputs and tasking the model with reconstructing them given the remaining inputs. In the multi-modal alignment task, the model has presented an image-text pair and must predict whether the image and text are aligned, i.e. whether the text describes the image.", "ViLBERT model outperforms state-of-the-art models in many established vision-and-language tasks: visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval.", "Transformers, undoubtedly, have great achievements in many tasks, but training them might be a long and expensive process. One possible way of solving this issue is parallelization.", "Model parallelization is still an open question when the model is too large to fit in a single device when training. When a model becomes too large to fit on a single computing device, the simplest solution is to distribute model layers across multiple devices.", "There was proposed a novel model-parallel algorithm to parallelize the training of Transformer-based language models. This algorithm can yield a significant speedup without loss of accuracy.", "In this paper, the authors try to find proves that transformers have a set of intermediate representations of both semantic and syntactic information. For finding the syntactic information they trained linear model at the top of attention vectors. This model has to predict the existence and type of dependency relation between two words. The binary probe achieved an accuracy of 85.8%, and the multiclass probe achieved an accuracy of 71.9%. The success of this simple linear probe suggests that syntactic information is encoded in the attention vectors.", "The second part is about semantic information. It is natural to speculate that transformers capture the particular shade of meaning of a word as used in a particular sentence.", "For a given word with n senses, they make a nearest-neighbor classifier where each neighbor is the centroid of a given word sense\u2019s BERT-base embeddings in the training data. To classify a new word they find the closest of these centroids, defaulting to the most commonly used sense if the word was not present in the training data. The simple nearest-neighbor classifier achieves an F1 score of 71.1, higher than the current state of the art, with the accuracy monotonically increasing through the layers. This is a strong signal that context embeddings are representing word-sense information.", "Transformers work well with sequential modeling tasks. But in some cases using purely sequence-oriented methods may result in losing valuable structural information, for example, when we want to represent the hierarchical structure. This work introduces novel positional encodings for tree-structured data. This can potentially extend the transformer to settings ranging from natural language parse trees to program abstract syntax trees.", "There were two evaluation methods for this approach: synthetic translation tasks and translating between generated CoffeeScript and JavaScript code. Experiments have shown that a model with this encoding can outperform sequence-transformers in tree-oriented tasks.", "I think this is the most interesting work. Maybe not by result, but the fact of using tensor decomposition for reducing model complexity.", "As you know, it is very important to consider how to reduce the amount of memory and computing transformers need. Existing model compression methods are mainly divided into parameter pruning and sharing, low-rank approximation, knowledge transfer, transferred convolutional filters and tensor decomposition methods.", "In this work, the authors focus on the compression of the multi-head attention in transformer-based on the idea of parameter sharing. At the same time, they also combine the low-rank approximate method to reduce parameters and computation complexity. It was proposed a novel self-attention method \u2014 Multi-linear attention.", "In all experiments, new architecture achieved results comparable with SoTA while having fewer parameters.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3c8e76e3ab42&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pgladkov?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Paul Gladkov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f29aecc8221&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&user=Paul+Gladkov&userId=6f29aecc8221&source=post_page-6f29aecc8221----3c8e76e3ab42---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/bert-at-emnlp-2019-46db6c2e59b2", "anchor_text": "BERT at EMNLP 2019"}, {"url": "http://arxiv.org/abs/1908.02265", "anchor_text": "http://arxiv.org/abs/1908.02265"}, {"url": "http://arxiv.org/abs/1909.06695", "anchor_text": "http://arxiv.org/abs/1909.06695"}, {"url": "http://arxiv.org/abs/1906.02715", "anchor_text": "http://arxiv.org/abs/1906.02715"}, {"url": "https://openreview.net/pdf?id=SJerEhR5Km", "anchor_text": "https://openreview.net/pdf?id=SJerEhR5Km"}, {"url": "http://arxiv.org/abs/1906.09777", "anchor_text": "http://arxiv.org/abs/1906.09777"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3c8e76e3ab42---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3c8e76e3ab42---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&user=Paul+Gladkov&userId=6f29aecc8221&source=-----3c8e76e3ab42---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&user=Paul+Gladkov&userId=6f29aecc8221&source=-----3c8e76e3ab42---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3c8e76e3ab42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3c8e76e3ab42---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3c8e76e3ab42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgladkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Gladkov"}, {"url": "https://medium.com/@pgladkov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f29aecc8221&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&user=Paul+Gladkov&userId=6f29aecc8221&source=post_page-6f29aecc8221--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5c3f23294075&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-at-neurips-2019-3c8e76e3ab42&newsletterV3=6f29aecc8221&newsletterV3Id=5c3f23294075&user=Paul+Gladkov&userId=6f29aecc8221&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}