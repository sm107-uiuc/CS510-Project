{"url": "https://towardsdatascience.com/efficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff", "time": 1683009340.596746, "path": "towardsdatascience.com/efficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff/", "webpage": {"metadata": {"title": "EfficientNet: Scaling of Convolutional Neural Networks done right | by Armughan Shahid | Towards Data Science", "h1": "EfficientNet: Scaling of Convolutional Neural Networks done right", "description": "Ever since Alex net won the 2012 ImageNet Challenge, Convolutional Neural Networks have become ubiquitous in the world of Computer Vision. They have even found their applications in natural language\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Ever since Alex net won the 2012 ImageNet Challenge, Convolutional Neural Networks have become ubiquitous in the world of Computer Vision. They have even found their applications in natural language processing, with state of the art models using convolution operations to retain context and provide better predictions. However, one of the key issues in designing CNNs, as with all other neural networks, is model scaling i.e deciding how to increase the model size so as to provide better accuracy.", "This is a tedious process, requiring manual hit and trial until a sufficiently accurate model is produced that satisfies the resource constraints. The process is resource and time consuming and often yields models with sub-optimal accuracy and efficiency.", "Taking this issue in consideration, Google released a paper in 2019 that dealt with a new family of CNNs i.e EfficientNet . These CNNs not only provide better accuracy but also improve the efficiency of the models by reducing the parameters and FLOPS (Floating Point Operations Per Second) manifold in comparison to the state of art models such as GPipe. The main contributions of this paper are:", "The compound scaling method can be generalized to existing CNN architectures such as Mobile Net and ResNet. However, choosing a good baseline network is critical for achieving the best results since the compound scaling method only enhances the predictive capacity of the networks by replicating base network\u2019s underlying convolutional operations and network structure.", "To this end, the authors use Neural Architecture Search to build an efficient network architecture, EfficientNet-B0. It achieves 77.3% accuracy on ImageNet with only 5.3M parameters and 0.39B FLOPS. (Resnet-50 provides 76% accuracy with 26M parameters and 4.1B FLOPS).", "The main building block of this network consists of MBConv to which squeeze-and-excitation optimization is added. MBConv is similar to the inverted residual blocks used in MobileNet v2. These form a shortcut connection between the beginning and end of a convolutional block. The input activation maps are first expanded using 1x1 convolutions to increase the depth of the feature maps. This is followed by 3x3 Depth-wise convolutions and Point-wise convolutions that reduce the number of channels in the output feature map. The shortcut connections connect the narrow layers whilst the wider layers are present between the skip connections. This structure helps in decreasing the overall number of operations required as well as the model size.", "The code for this block can be surmised as :", "A convolutional neural network can be scaled in three dimensions: depth, width, resolution. The depth of the network corresponds to the number of layers in a network. The width is associated with the number of neurons in a layer or more pertinently, the number of filters in a convolutional layer. The resolution is simply the height and width of the input image. Figure 2 above, gives a clearer picture of scaling across these 3 dimensions.", "Increasing the depth, by stacking more convolutional layers, allows the network to learn more complex features. However deeper networks tend to suffer from vanishing gradients and become difficult to train. Although new techniques such as batch normalization and skip connections are effective in resolving this problem, empirical studies suggest that the actual accuracy gains by only increasing the depth of the network quickly saturate. For instance Resnet-1000 provides the same accuracy as Resnet-100 despite all the extra layers.", "Scaling the width of the networks allows layers to learn more fine grained features. This concept has been used extensively in numerous works such as Wide ResNet and Mobile Net. However, as is the case of increasing depth, increasing width prevents the network from learning complex features , resulting in diminishing accuracy gains.", "Higher input resolution provides a greater detail about the image and hence enhances the model\u2019s ability to reason about smaller objects and extract finer patterns. But like the other scaling dimensions, this too provides limited accuracy gains on its own.", "This leads to an important observation:", "Observation 1: Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.", "This implies that the scaling of network for increase in accuracy should be contributed in part by a combination of the three dimensions. This is corroborated by empirical evidence in Figure 4 , where the networks\u2019s accuracy is modeled with an increasing width for various depth and resolution settings.", "The results depict that scaling only one dimension (width) quickly stagnates the accuracy gains. However, coupling this with an increase in number of layers (depth) or input resolution enhances the models predictive capabilities.", "These observations are somewhat expected and can be explained by intuition. For instance, if the spatial resolution of the input image is increased , the number of convolutional layers should also be increased so that the receptive field is large enough to span the entire image that now contains more pixels. This leads to the second observation :", "Observation 2: In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.", "A convolutional neural network can be thought of as stacking or composition of various convolutional layers. Furthermore these layers can be partitioned into different stages e.g ResNet has five stages, and all layers in each stage have the same convolutional type. Therefore, a CNN can be represented mathematically as:", "where N depicts the network, i represents the stage number, F \u1d62 represents the convolution operation for the i-th stage, and L \u1d62 represents the number of times F \u1d62 is repeated in stage i. H \u1d62 , W \u1d62 and C \u1d62 simply denote the input tensor shape for stage i.", "As can be deduced from the equation 1, L \u1d62 controls the depth of the network, C \u1d62 is responsible for the width of the network whereas H \u1d62 and W \u1d62 affect the input resolution. Finding a set of good coefficients to scale these dimensions for each layer is impossible, since the search space is huge. So, in order to restrict the search space, the authors lay down a set of ground rules.", "With these rules established , equation 1 can be parameterized as:", "where w, d, r are coefficients for scaling network width,depth, and resolution; F\u0302 \u1d62 , L\u0302 \u1d62 , \u0124 \u1d62 , \u0174 \u1d62 , \u0108 \u1d62 are predefined parameters in baseline network.", "The authors propose a simple, albeit effective scaling technique that uses a compound coefficient \u0278 to uniformly scale network width, depth, and resolution in a principled way:", "\u0278 is a user-defined, global scaling factor (integer) that controls how many resources are available whereas \u03b1, \u03b2, and \u03b3 determine how to assign these resources to network depth, width, and resolution respectively. The FLOPS of a convolutional operation are proportional to d, w\u00b2, r\u00b2, since doubling the depth will double the FLOPS while doubling width or resolution increases FLOPS almost by four times. So ,scaling the network using equation 3 will increase the total FLOPS by (\u03b1 * \u03b2\u00b2 * \u03b3\u00b2) ^\u0278 . Hence, in order to make sure that the total FLOPS don\u2019t exceed 2^\u03d5, the constraint (\u03b1 * \u03b2\u00b2 * \u03b3\u00b2) \u2248 2 is applied. What this means, is that if we have twice the resources available we can simply use compound coefficient of 1 to scale the FLOPS by 2\u00b9.", "The parameters - \u03b1, \u03b2, and \u03b3- can be determined using grid search by setting \u0278=1 and finding parameters that result in the best accuracy. Once found, these parameters can then be fixed , and the compound coefficient \u0278 can be increased to get larger but more accurate models. This was how EfficientNet-B1 to EfficientNet-B7 are constructed , with the integer in the end of the name indicating the value of compound coefficient.", "This technique allowed the authors to produce models that provided accuracy higher than the existing ConvNets and that too with a monumental reduction in overall FLOPS and model size.", "This scaling method is generic and can be used with other architectures to effectively scale Convolutional Neural Networks and provide better accuracy.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Deep Learning Engineer who is eager towards learning about the new trends in this field and engaging in its development."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3fde32aef8ff&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marmughanshahid?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marmughanshahid?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Armughan Shahid"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F624dbaf70db1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&user=Armughan+Shahid&userId=624dbaf70db1&source=post_page-624dbaf70db1----3fde32aef8ff---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lidyanada?utm_source=medium&utm_medium=referral", "anchor_text": "Lidya Nada"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://Rethinking Model Scaling for Convolutional Neural Networks", "anchor_text": "EfficientNet"}, {"url": "https://arxiv.org/abs/arXiv:1801.04381", "anchor_text": "MobileNet v2"}, {"url": "http://papers.nips.cc/paper/8305-gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism", "anchor_text": "GPipe"}, {"url": "https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet", "anchor_text": "https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet"}, {"url": "https://medium.com/tag/convolution-neural-net?source=post_page-----3fde32aef8ff---------------convolution_neural_net-----------------", "anchor_text": "Convolution Neural Net"}, {"url": "https://medium.com/tag/optimization?source=post_page-----3fde32aef8ff---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/efficientnet?source=post_page-----3fde32aef8ff---------------efficientnet-----------------", "anchor_text": "Efficientnet"}, {"url": "https://medium.com/tag/model-scaling?source=post_page-----3fde32aef8ff---------------model_scaling-----------------", "anchor_text": "Model Scaling"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----3fde32aef8ff---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&user=Armughan+Shahid&userId=624dbaf70db1&source=-----3fde32aef8ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&user=Armughan+Shahid&userId=624dbaf70db1&source=-----3fde32aef8ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3fde32aef8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3fde32aef8ff---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3fde32aef8ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marmughanshahid?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marmughanshahid?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Armughan Shahid"}, {"url": "https://medium.com/@marmughanshahid/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F624dbaf70db1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&user=Armughan+Shahid&userId=624dbaf70db1&source=post_page-624dbaf70db1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F624dbaf70db1%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff&user=Armughan+Shahid&userId=624dbaf70db1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}