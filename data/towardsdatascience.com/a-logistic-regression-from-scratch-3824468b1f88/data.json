{"url": "https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88", "time": 1683007545.262569, "path": "towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88/", "webpage": {"metadata": {"title": "A logistic regression from scratch | by Dennis Bakhuis | Towards Data Science", "h1": "A logistic regression from scratch", "description": "In my previous blog post we worked on artificial neural networks and developed a class to build networks with arbitrary numbers of layers and neurons. While the blog referenced to the previous\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d", "anchor_text": "previous blog post", "paragraph_index": 0}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/Logistic_Regression", "anchor_text": "Jupyter Notebook", "paragraph_index": 1}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/Python_Conda_Pip_Environments", "anchor_text": "here is a short explanation", "paragraph_index": 2}, {"url": "https://www.kaggle.com/azeembootwala/titanic", "anchor_text": "https://www.kaggle.com/azeembootwala/titanic", "paragraph_index": 55}], "all_paragraphs": ["In my previous blog post we worked on artificial neural networks and developed a class to build networks with arbitrary numbers of layers and neurons. While the blog referenced to the previous notebook explaining the prerequisites, there was not yet the accompanying article, which is this blog post.", "This post is also available as a Jupyter Notebook on my Github, so you can code along while reading.", "If you are new to Python and Jupyter, here is a short explanation on how I manage my Python environment and packages.", "A short overview of the topics we will be discussing:", "When we hear or read about deep learning we generally mean the sub-field of machine learning using artificial neural networks (ANN). These computing systems are quite successful in solving complex problems in various fields, examples are, image recognition, language modelling, and speech recognition. While the name ANN implies that they are related to the inner workings of our brain, the truth is that they mainly share some terminology. An ANN generally consists of multiple interconnected layers, which on itself are build using neurons (also called nodes). An example is shown in figure 1.", "In this example, we have one input layer, consisting of four individual inputs nodes. This input layer is \u2018fully connected\u2019 to the first hidden layer, i.e. Fully connected means that each input is connected to each node. The first hidden layer is again fully connected to another \u2018hidden\u2019 layer. The term hidden indicates that we are not directly interact with these layers and these are kind of obscured to the user. The second hidden layer is on its turn fully connected two the final output layer, which consists of two nodes. So in this example we feed the model four inputs and we will receive two outputs.", "Let\u2019s now focus on a single neuron from our previous example. This neuron still is connected to all inputs, also called features. Using these features, the neuron calculates a single response (or output). A diagram of such a system is depicted in figure 2.", "The input features are named \ud835\udc53\u00b9, \ud835\udc53\u00b2, \ud835\udc53\u00b3, and \ud835\udc53\u2074 and are all connected to the single neuron. This neuron executes two operations. The first operation is a multiplication of the features with a weight vector \ud835\udc4a and adding the bias term \ud835\udc4f. The second operation is a so called activation function, indicated here by \ud835\udf0e. The output of the neuron is a probability between zero and one. The single neuron acts like a small logistic regression model and therefore, an ANN can be seen as a bunch of interconnected logistic regression models stacked together. While this idea is pretty neat, the underlying truth is a bit more subtle. There are many different architectures for ANNs and they can use various building blocks that act quite different than in this example.", "The linear operation in our single neuron is nothing more than a linear regression. Therefore, to understand logistic regression, the first step is to have an idea how linear regression works. The next section will show a step by step example as a recap.", "Linear regression in its simplest form (also called simple linear regression), models a single dependent variable \ud835\udc66 using a single independent variable \ud835\udc65. This may sound daunting, but was this means is that we want to solve the following equation:", "In the context of machine learning, \ud835\udc65 represents our input data, \ud835\udc66 represents our output data, and by solving we mean to find the best weights (or parameters), represented by \ud835\udc64 and \ud835\udc4f in the linear regression equation. A computer can help us find the best values for \ud835\udc64 and \ud835\udc4f, to have the closest match for \ud835\udc66 using the input variable \ud835\udc65.", "For the next examples, let us define the following values for x and y:", "The values for \ud835\udc65 and \ud835\udc66 have a linear relation so we can use linear regression to find (or fit) the best weights and solve the problem. Maybe, by staring long enough at these values, we can discover the relation, however it is much easier to use a computer to find the answer.", "If you have stared long enough or just want to know the answer, the relation between \ud835\udc65 and \ud835\udc66 is the following:", "In the next section we will use Tensorflow to create our single neuron model and try to \u2018solve\u2019 the equation.", "Before we start with Tensorflow, we should first organize our input data (\ud835\udc65) and output data (\ud835\udc66). For this we are going to use Numpy:", "In the next step, we will import Tensorflow. It is always a good practice to check which version we are using:", "Now we can create a model using Keras, which is now a part of Tensorflow. To do this, we will use the Sequential class, which can stack various layers \u2018sequentially\u2019 after each other. We use the Dense class from Keras to create a \u2018fully connected\u2019 layer, which consists of a single neuron (unit).", "The Dense function is used to create layers of many fully connected neurons (logistic units). The parameter units is used to set the amount of neurons. We only use a single unit and therefore, we will set it to one. As this is the first \u2018layer\u2019 of our model, we need to tell Tensorflow what shape it can expect as an input. This is only necessary for the first layer.", "Now that we have defined the model, we need to use the Compile() method to configure the model for training. The method requires at least two parameters, a loss function and an optimizer. The loss function is a measure for how well the model predicts the actual value. For this example we will use the mean squared error (average of the squared difference between the predicted and the actual value of \ud835\udc66). The \u2018learning algorithm\u2019 will try to minimize the loss my adjusting (optimizing) the parameters (weights and bias) for each step. The optimizer defines a method to perform this Optimizing step and a common method is Gradient Descent, or in our case Stochastic Gradient Descent (SGD). This method will become more clear in the next section where we will implement it in plain Numpy.", "Next, we use the Fit() method to let the algorithm learn the best parameters. While Tensorflow has many smart ways to address this problem, the recipe is more or less like this:", "Do not worry to much if this is not completely clear. We will code each step in plain Numpy, with a thorough derivation of all the math used in the next section.", "We have trained the model in Tensorflow using our training data and we are ready to give it a spin. Now we can use our model to \u2018predict\u2019 values it has never seen. This is sometimes also called inference. Let\u2019s try the value of 12. We know that it should be 25.", "Why is the value not exactly 25?", "The model calculates the difference between the actual value and the predicted value and creeps slowly towards the actual value. Running the fit method for a longer time will get you closer to 25.", "This was not that hard, but it might feel like some dark Jedi power. Therefore, in the next section we will implement this algorithm in plain Python (with the help of Numpy).", "In the previous section we gave a rough overview what Tensorflow is doing under the hood:", "We will now implement exactly this in plain Python and hopefully come to a similar result as Tensorflow.", "First we define the model parameters. These are the weights \ud835\udc4a, which is just a single value, because we only have a single input. Also we need to define the bias term \ud835\udc4f.", "This are all the trainable parameters in our model, a single scalar for the weight and a single scalar for the bias.", "Next we will define a function that makes a prediction using our current model parameters. In deep learning terms, this is called a forward pass. The variable of the predicted value is generally name \u0177 (or A but you can learn this in my next article).", "The function is named forward and uses the input vector \ud835\udc4b and multiplies it with the weight parameter \ud835\udc4a and adds the bias term \ud835\udc4f. Exactly as we described in the before-mentioned equation.", "We can now test the function with the current parameters. Again, we will input a value of 12.0 but of course, it will return gibberish as the weights are randomly initialized.", "Indeed, -0.212261\u2026 is not really what we were looking for, but we still have to train our model before it can make proper predictions. Before we can do that we need to calculate the current loss. As a loss we used the mean squared error of the predicted value \u0177 and the actual value \ud835\udc66.", "Hopefully the math does not scare you, but if you take the time, it is not that hard. The variable \ud835\udc5a here is the amount of examples (points in the dataset). Our \ud835\udc4b holds eight values and therefore, \ud835\udc5a=8. When we have a 1/\ud835\udc5a followed by as sum (\u2211) over all \ud835\udc5a values, it is nothing more than an average of the values that are \ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52 the summation. Here, we take the average over all (\ud835\udc66\u2212\u0177)\u00b2 which is the difference between the true value \ud835\udc66 and the predicted value \u0177 squared. The square is important because a negative difference and a positive difference would cancel each other out if we would not square the difference. Now that we fully understand the mean squared error, we can implement it in code:", "To see what the loss is between our previously calculated value, we can do this:", "As we can see, the loss is pretty large and our weights are quite off. Therefore, we need to update our trainable parameters to make better predictions. To do so, we need to first calculate the gradients \ud835\udeffLoss/\ud835\udeff\ud835\udc4a and \ud835\udeffLoss/\ud835\udeff\ud835\udc4f. Maybe your differential skills are a bit rusty. The trick is to apply the chain rule. Another benefit is that the average operator (the sum) is linear and therefore, we can ignore it in the differential, and put it back later.", "To calculate \ud835\udeffLoss/\ud835\udeff\ud835\udc4f we only need to repeat the last step, with respect to \ud835\udc4f:", "We will implement this in the \u2018backward pass\u2019 function. As variable names get a bit long and we are a bit lazy, we will just call them \ud835\udc51\ud835\udc4a and \ud835\udc51\ud835\udc4f.", "So now we can get the gradient of our previous test example:", "The last function we need, before we can compose our training loop is the update function. This function will \u2018optimize\u2019 our weights one step. This is the actual gradient descent in which we subtract (descent) the gradient from our current weights. Gradient descent is defined as follows:", "Here we have a new parameter \ud835\udefc which is called the learning rate. This value will set the speed in which we try to converge to the minimum loss. For this example we will set it to 0.01. There is no golden value for the learning rate and this value needs to be fine-tuned for each problem. Our code for the update function is as follows:", "To update our current model parameters we simply do:", "Alright, we have updated our weights for the first time. To improve the weights, we have to repeat this process many times. For this we will write a loop:", "We are in luck and the loss, i.e. the difference between our model prediction and the actual value, is decreasing. How would we now predict, when we input a value of 12.0?", "I hope that this dark magic in Tensorflow is now a bit more clear. In the next section we will use our newly acquired knowledge to solve a binary logistic regression problem.", "The differences between a linear regression and a logistic regression are not that major. There are two differences from the previous code we created. First, our linear regression model only had a single feature, which we inputted with \ud835\udc65, meaning that we only had a single weight. In logistic regression, you generally input more than one feature, and each will have its own weight. Technically, you could have a single feature, but it would be nothing more than an if-statement (think about it). Increasing the number of features will change the previous simple multiplication to a matrix multiplication (dot product). Secondly, we will add a so called activation function to map this value between 0 or 1. Let\u2019s remind ourselves again of our simple model:", "By convention in Tensorflow (from what I have understood), the input vector has columns for features, and rows for examples. If we would have 2 datapoints the input matrix would look like this:", "The superscript shows the feature number, the subscript indicates the example.", "Each of these inputs are associated with their own weights. The node itself has two distinct operations. The first is the dot product between the weights vector and the input vector. The second is the Sigmoid function. The weight vector \ud835\udc4a in this example has four weights:", "In the node we first compute the linear part:", "For our example this will look like this:", "Notice that the result is only a single value for each example.", "For all values of \ud835\udc65, which can be from \u2212\u221e to +\u221e (all real numbers), the Sigmoid function maps \ud835\udc65 between 0 and 1. Values for \ud835\udc65 close to zero have the largest effect, as these are in the \u2018linear regime\u2019. Very large, or very small are only clipped to 1 and 0 respectively. The mathematical definition of the Sigmoid function is:", "This is all there is to the logistic unit. The Sigmoid function gives the node its non-linear character. Many of these units together can do almost magical things. In the next section we will first make a logistic regression model in Tensorflow.", "Before we can start we first need some data to do a logistic regression. I downloaded the titantic dataset from Azeem Bootwala from Kaggle to have a play for this example. It can be downloaded from here:https://www.kaggle.com/azeembootwala/titanic", "First, always inspect the columns and data types:", "I expect that Azeem did not save the set using index=False option and therefore we have a \u2018Unnamed: 0\u2019 column. This one is redundant with our current index so we can remove it. Also the PassengerId is not very useful for our model, let us remove that column too. After that, let us sample the dataset to get an idea:", "Azeem already did some preprocessing. The target variable \ud835\udc4c is \u2018Survived\u2019, all other columns are features. A short description of the features:- Sex: 0 or 1 -> male or female- age: value rescaled between 0 and 1- fare: ticket price rescaled between 0 and 1- Pclass_1 .. Pclass_3: One-hot encoded Passenger class- family size: rescaled value between 0 and 1 of family size.- title_1 .. title_4: mr, mrs, master, miss one-hot encoded- emb_1 .. emb_3: Embark location one-hot encoded.", "In total we will have 14 features.", "For this example, the data will suffice and I will not go into detail on how this data has become what it is. Honestly, I do not know myself and have just downloaded it from Kaggle ;-).", "Lets put these variables in the format we defined before (\ud835\udc4b and \ud835\udc4c). Here \ud835\udc4c corresponds to the label if a person has survived. We have a total of 792 examples. Therefore, the shape for \ud835\udc4c is (\ud835\udc5a,1) where \ud835\udc5a = 792. For \ud835\udc4b we expect (\ud835\udc5a, 14), where the columns are the features.", "Now that we have prepared the data we can create a model in Tensorflow:", "The model in Tensorflow is very similar to our linear regression model. The input has changed from 1 to 14 features and we added the Sigmoid activation function. Next, we must again compile our model:", "In this example I changed the loss into \u2018binary_cross_entropy\u2019. This is another loss function which works better for binary logistic regression problems. If you are interested in the inner workings I recommend wikipedia. We added an additional metric called \u2018accuracy\u2019 which is now calculated for each epoch. Now we are ready to train our model:", "Well, these were all steps required for training a binary logistic classifier in Tensorflow. We achieved an accuracy of ~80% which is not too bad for the effort we put in.", "We can extract the weights \ud835\udc4a and the bias \ud835\udc4f from our model. These values we can later compare to our own implementation of the logistic regression:", "Plotting the loss is also quite easy:", "Don\u2019t mind the nice XKCD wobble :-). It is quite impressive how few steps are required to get to such a result. In the final section we will unveil the Dark Jedi arts being performed by Tensorflow.", "Well, the general recipe has not changed:", "The steps itself, need minor modification. The forward pass will be the more general dot product and we need to add the activation function. The loss function is binary cross entropy, which is of course different from the mean square error. The backward pass will calculate the gradient of the new loss function with respect to \ud835\udc4a and \ud835\udc4f. As we now also have an activation function, we will have an additional step. The update-weights function is unchanged, the final loop is also very similar.", "In Numpy, and in math in general as far as I know, the dot product needs the shapes of the vectors (and matrices) to be compatible:", "This means that the number of columns of \ud835\udc4b must be equal to the number of rows in \ud835\udc4c. To make ourselves a bit easier, we will Transpose our input vector \ud835\udc4b and flip the vector. This will result that the rows will be features and the columns will be examples.", "For testing, let us just select two examples, to make it a bit more readable:", "Lets define our weights. As we have 14 features, our vector \ud835\udc4a will have 14 values. The bias is a constant for the whole node, and is only a single value.", "Next, we need to define the Sigmoid function:", "Now lets redefine our forward function, and make it use the dot product and the activation function. We can split these in two steps:\ud835\udc4d=\ud835\udc4a\ud835\udc4b+\ud835\udc4fA = \ud835\udf0e(\ud835\udc4d)", "Note that \ud835\udc4a\ud835\udc4b is a dot product.", "Now that we have actual predictions, we can write our loss function to measure how well our predictions are. This is done with the binary cross entropy, for which I will simply give the equation:", "It might happen we try to calculate a log(0) which is of course not defined. To avoid the warning, we will add a tiny value to our loss. As it is super small, the difference is not noticeable, but does help suppress the warning. One less warning a day keeps the \u2026.", "Next is the backwards pass. For this, We would need to differentiate the Loss function with \ud835\udc4a and \ud835\udc4f. Not to bore you guys, I have provided these functions, but I will not stop you from calculating the differentials yourself:", "In Python, this looks like this:", "Almost there, next we need to update the weights. The function has not changed from our previous example:", "To compare the results, we can calculate the accuracy. However our activation function returns a probability between 0 and 1. By definition, values <= 0.5 are rounded to 0 and values > 0.5 are rounded to 1. This is slightly different from the regular round function so we will make our own function for this:", "Now we have everything to create our training loop. We will store some metrics to plot afterwards:", "We trained the network and got a final accuracy of just above 80%, very similar results as when using Tensorflow. Of course, with a bit more effort, but also with a bit more fun. Let\u2019s plot our metrics:", "Of course, we have calculated the metrics on the training data. To get a proper idea on the model, this should be done using the test data. This, I leave open to the reader ;-).", "Well, this was it for the tutorial on a Logistic regression. Hopefully you got an idea on how Logistic regression work and that Tensorflow is not only black magic. I found it a great exercise to write these from scratch and as you have seen, it is not that difficult either.", "Please let me know if you have any comments or suggestions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a passion for natural language processing and deep learning. Python and open source enthusiast. Background in fluid dynamics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3824468b1f88&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb----3824468b1f88---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d", "anchor_text": "previous blog post"}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/Logistic_Regression", "anchor_text": "Jupyter Notebook"}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/Python_Conda_Pip_Environments", "anchor_text": "here is a short explanation"}, {"url": "https://www.kaggle.com/azeembootwala/titanic", "anchor_text": "https://www.kaggle.com/azeembootwala/titanic"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3824468b1f88---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3824468b1f88---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3824468b1f88---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3824468b1f88---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3824468b1f88---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----3824468b1f88---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----3824468b1f88---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3824468b1f88&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3824468b1f88---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3824468b1f88--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3824468b1f88--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3824468b1f88--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3824468b1f88--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3824468b1f88--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://dennisbakhuis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc167ef22c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-logistic-regression-from-scratch-3824468b1f88&newsletterV3=5b8617eb89bb&newsletterV3Id=c167ef22c4d5&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}