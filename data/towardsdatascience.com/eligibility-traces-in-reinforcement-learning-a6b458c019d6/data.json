{"url": "https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6", "time": 1682996496.929367, "path": "towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6/", "webpage": {"metadata": {"title": "Eligibility Traces in Reinforcement Learning | by Ziad SALLOUM | Towards Data Science", "h1": "Eligibility Traces in Reinforcement Learning", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com In short and a straight forward manner, Eligibility Traces is a kind of mathematical trick that\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference", "paragraph_index": 3}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero", "anchor_text": "Wikipedia article", "paragraph_index": 17}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "In short and a straight forward manner, Eligibility Traces is a kind of mathematical trick that improves the performance of Temporal Difference methods, in Reinforcement Learning.", "Here are the benefits of Eligibility Traces:", "Remember that in Temporal Difference and Monte Carlo methods update a state based on future rewards. This is done either by looking directly one step ahead or by waiting the episode to finish.", "This approach is called the Forward View.", "In TD(0) we look one step ahead, while in Monte Carlo we look ahead until the episode is terminated and we collect the discounted results.However there is a middle ground, in which we look n-steps ahead.", "As explained in the previous section, looking ahead can vary from one step ahead to the end of the episode as the case of Monte Carlo.So, n-steps is some kind of middle ground.Remember that in Monte Carlo we execute the episodes, get their returns Gi and average those returns to compute the state value.Note that length (number of steps) of each episode may vary from one episode to the other. It is not constant!", "Similarly we will do the same with n-steps look-ahead. As in Monte Carlo the number of steps is not necessarily the same on each episode.NB. in this section we no longer refer to an episode as a number of steps that must terminate by reaching a terminal state, but simply as a number of steps (the terminal state is no more a requirement).", "So let\u2019s define an average return for all these iterations like the following:", "Where G( \ud835\udecc, t) is the weighted average of all returns G(t,t+n) which are the returns of individual episodes where each episode starts at t and ends at t+n, for n going from 1 to infinity. \ud835\udecc is a weight that has a value between [0, 1].As in all weighted average, the sum of the weights must be one, which is the case since", "Important Remark: it is easy to notice that for episodes with large n, the \ud835\udecc to the power n becomes small, and the contribution of G(t+n) will be small.But hey! We already know that from the use of the discount factor gamma (\u0263) that is in the return definition:", "But bear in mind this is a different issue, in Monte Carlo for example V(s) is the average of all returns Gi computed from the episodes that has been played. All Gi contributed equally to the value of V(s), even though the rewards are discounted depending on their distance from state s.", "In here, the issue is different, all Gi do not contribute the same way to V(s) but they are weighted, and each weight gets smaller following the number of steps within each episode.", "Below is a picture showing how each episode is weighted according to its length.", "This pictures shows how the weight becomes smaller as the time (or n-steps) increases.", "In short if an episode terminates after 3 steps the weight associated with its return is far greater than an episode that terminates at T steps (where T is much greater than 3).It is also important to notice that the weight decreases exponentially.", "Let\u2019s rewrite the return when the episodes terminates at time T.Note that the return after time step T is always Gt (which is the return at time step T) simply because there are no more states, and last return seen was at T.", "When \ud835\udecc is 0, the expression of Gt shows the formula of TD(0) (note that zero to the power zero in this context is 1 by convention, see this Wikipedia article for more info), and when \ud835\udecc is 1 the Gt becomes the formula of Monte Carlo.", "All these details but still we didn\u2019t solve the problem\u2026", "Forward views are somehow complex to implement because the update of each state depends on later events or rewards that are not available at current time.", "But so far we have been doing just that! We are looking n-steps ahead\u2026However this is going to change by adopting a new approach: The Backward View.", "Suppose an agent randomly walking in an environment and finds a treasure. He then stops and looks backwards in an attempt to know what led him to this treasure ?Naturally the steps that are close to the treasure have more merits in finding it than the steps that are miles away. So closer locations are more valuable than distant ones and thus they are assigned bigger values", "How does this materialize, is through a vector E called eligibility traces.Concretely, the eligibility traces is a function of state E(s) or state action E(s,a) and holds the decaying values of the V(s).", "So how do we transit from Forward View to Backward View and what is the role of eligibility traces in that?", "Remember what we said about Forward View, that the contribution of each episode to the current state is attenuated exponentially (\ud835\udecc to the power n) following the number of steps (n) in the episode.Using the same logic, when we are at state s, instead of looking ahead and see the decaying return (Gt) of an episode coming towards us, we simply use the value we have and throw it backward using the same decaying mechanism.", "For example in TD(0) we defined the TD error as:", "This error will be propagated backwards but in a decaying manner.Similar to the voice that fades away with the distance.The way we implement this, is by multiplying \ud835\udf39 by the eligibility trace at each state.", "Where Et(s) is updated as follows:", "The notation 1(St = s) means that we assign the full value when we are at the state s, and as it gets propagated backwards it gets attenuated exponentially.", "The eligible trace update starts by E(s) = 0 for all states, then as we pass by each state (due to performing an action) we increment E(s) to boost the value of the state, then we decay E(s) by \u0263\ud835\udecc (E(s) = \u0263\ud835\udecc E(s)) for all s.", "The main advantage of eligibility traces over n-step forward view is that only one single trace vector is required rather than a store of the last n feature vectors.", "The following are few algorithms that uses Eligibility Traces.", "Eligibility traces is a way of weighting between temporal-difference \u201ctargets\u201d and Monte-Carlo \u201creturns\u201d. Meaning that instead of using the one-step TD target, we use TD(\u03bb) target. In other words it fine tunes the target to have a better learning performance.", "Reinforcement Learning: An Introduction, by Sutton & Barto", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa6b458c019d6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----a6b458c019d6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lampe_91?utm_source=medium&utm_medium=referral", "anchor_text": "Didier Provost"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference"}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo"}, {"url": "https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero", "anchor_text": "Wikipedia article"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a6b458c019d6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----a6b458c019d6---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a6b458c019d6---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/temporal-difference?source=post_page-----a6b458c019d6---------------temporal_difference-----------------", "anchor_text": "Temporal Difference"}, {"url": "https://medium.com/tag/monte-carlo?source=post_page-----a6b458c019d6---------------monte_carlo-----------------", "anchor_text": "Monte Carlo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----a6b458c019d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----a6b458c019d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa6b458c019d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a6b458c019d6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a6b458c019d6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a6b458c019d6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a6b458c019d6--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feligibility-traces-in-reinforcement-learning-a6b458c019d6&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}