{"url": "https://towardsdatascience.com/how-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60", "time": 1683003190.850666, "path": "towardsdatascience.com/how-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60/", "webpage": {"metadata": {"title": "How to Easily Use Gradient Accumulation in Keras Models | by Raz Rotenberg | Towards Data Science", "h1": "How to Easily Use Gradient Accumulation in Keras Models", "description": "A deep dive into Keras optimizers, and how to implement a generic gradient accumulation wrapper."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa?source=friends_link&sk=28226e1d0ffa7e450d7dffa8d5b9cff6", "anchor_text": "another article", "paragraph_index": 0}, {"url": "http://www.run.ai", "anchor_text": "Run:AI", "paragraph_index": 1}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub", "paragraph_index": 2}, {"url": "https://github.com/run-ai/runai/tree/master/examples/ga/keras", "anchor_text": "examples", "paragraph_index": 2}, {"url": "https://pypi.org/project/runai/", "anchor_text": "Run:AI Python library", "paragraph_index": 3}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py#L60-L154", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py#L183-L209", "anchor_text": "GitHub", "paragraph_index": 25}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py", "anchor_text": "optimizers.py", "paragraph_index": 33}, {"url": "https://github.com/run-ai/runai/blob/master/runai/ga/keras/optimizers.py", "anchor_text": "GitHub", "paragraph_index": 44}, {"url": "https://github.com/run-ai/runai/blob/master/runai/ga/keras/hooks.py", "anchor_text": "GitHub", "paragraph_index": 58}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub", "paragraph_index": 66}], "all_paragraphs": ["In another article, we covered what is gradient accumulation in deep learning and how it can solve issues when running neural networks with large batch sizes.", "In this article, we will first see how you can easily use the generic gradient accumulation tool we implemented and used at Run:AI. Then, we will deep-dive into Keras optimizers and the way we have implemented such a generic tool.", "The code is available on GitHub along with examples you can use right out of the box.", "Adding gradient accumulation support to your Keras models is extremely simple. First of all, install the Run:AI Python library using the command:", "Then, import the gradient accumulation package into your Python code:", "Now, you can choose one of two options. You can either wrap an existing optimizer with the generic gradient accumulation wrapper or create a gradient accumulation version of any of the built-in optimizers. The two options require specifying the number of steps you want to accumulate gradients over (passed as STEPS in the examples below).", "Use the next line to wrap an existing optimizer (where optimizer is your optimizer):", "Or, use the next line to create a gradient accumulation version of any of the built-in optimizers (\u201cAdam\u201d is used in the next example):", "And that\u2019s it! you have successfully added gradient accumulation support to your Keras model.", "After seeing how easy it is to use, let\u2019s now see what\u2019s going on under the hood. First, we are going to examine the concept of optimizers in Keras and discuss their responsibility and their implementation. Then, we will deep-dive into how we implemented such a generic mechanism.", "Optimizers in Keras are responsible for implementing the optimization algorithm \u2014 the mathematical formula responsible for minimizing the loss function. They receive all the model parameters \u2014 weights and biases \u2014 as inputs, compute their gradients and use them to generate updates for the model parameters. The updates for the model parameters are not the gradients themselves and are calculated using the gradients, as well as other parameters.", "Every optimization algorithm has parameters. Some optimizers have similar parameters (e.g. \u201cepsilon\u201d, \u201cbeta_1\u201d, \u201cbeta_2\u201d, etc\u2026), some may have unique ones (e.g. \u201camsgrad\u201d in Adam), and they all support \u201clearning rate\u201d (and learning rate decay).", "While building the model, Keras will call the optimizer and will pass two arguments. The first argument will be all the trainable variables \u2014 weights and biases \u2014 of the model. The second argument will be the loss value. Note that the trainable variables and the loss are tensors and not the actual values of the tensors.", "The optimizer then takes these input tensors and adds the optimization algorithm to the model. It first calculates the gradients of the trainable variables with respect to the loss (by calling tf.gradients()), and then generates the tensors representing the mathematical formula.", "Keras will then evaluate these tensors \u2014 that were generated by the optimizer \u2014 every step. By evaluating those tensors, the gradients will be calculated, and then the variable updates will be calculated and assigned to the model variables \u2014 the weights and biases.", "Keras optimizers are Python classes, and they all inherit a base class called Optimizer (can be seen here). The base class implements a few methods and declares other (virtual) methods that must be implemented in subclasses. Let\u2019s briefly examine the methods in Optimizer:", "We will be focusing on get_updates(). This method is only declared (virtual) in the base class; it is not implemented in it and must be implemented in all subclasses. Its implementation is the main difference between one optimizer to another.", "As we said, Keras will call get_updates() as part of building the model. The method receives two arguments: loss and params, which both are tensors. and returns a list of tensors, which are the \u201cAssign\u201d ops \u2014 the tensors that actually assign the variable updates upon evaluation.", "Let\u2019s examine a simplified version of the implementation of SGD\u2019s get_updates(). Note that this is a simplified version and not the actual code in Keras:", "First, at line 2, the optimizer calculates the gradients of loss with respect to params by calling self.get_gradients().", "Then, in lines 4\u20136, the optimizer iterates through all the trainable variables of the model, with their respective calculated gradients. For every parameter, it calculates (line 5) the new value of the variable (new_p) using the gradient and the learning rate (self.lr is a tensor initialized in __init__()). Then, it creates a tensor that will assign the variable\u2019s new value (K.update(p, new_p)) and adds it to the list of such tensors (self.updates).", "Finally, in line 8, the optimizer returns the list of the \u201cAssign\u201d tensors (self.updates), which will be evaluated by Keras in every step.", "This method is called exactly once when the model is being built. It may take some time to digest, but be sure to understand that the arguments and the results are tensor objects and not the actual tensor values. Note that line 5 may be misleading and may seem like it calculates actual values, but this is not the case and is just syntactic sugar (TensorFlow\u2019s operator overloading) on tensor objects.", "After examining a simplified version, it\u2019s time to figure out what happens in reality.", "In addition to their plain algorithm, most of the optimizers support learning rate decay, which means they modify the learning rate throughout the training of the model, instead of having a constant value for it. In order to support modifying the learning rate over time, the concept of time should be defined. As expected, the learning rate is not modified as a function of the actual time that has passed since the beginning of the training phase, but as a function of the step number, and to support this, optimizers count steps (called iterations in their implementations).", "Moreover, SGD in Keras supports momentums and the Nesterov momentum algorithm, which complicates things a bit more. Let\u2019s take a look at the actual implementation of SGD\u2019s get_updates() (can be seen on GitHub also):", "Let\u2019s go through the additions over the simplified version we examined before.", "In line 3, the optimizer creates an \u201cAssign\u201d tensor to increase self.iterations by 1 in every step. self.iterations is the step counter (a tensor object) and is created in __init__() with an initial value of 0.", "In lines 5\u20137 we can see the learning rate decay formula as a function of the step counter (self.iterations) and the decay value (an argument passed to the optimizer upon creation, and set to self.decay).", "In line 11 it creates variables for the momentums. It declares another variable, for every trainable variable, with the same shape, and initializes it to 0 (the call to K.zeros(shape)).", "Then, inside the iteration over the trainable variables, it calculates the new value for every momentum (line 14) and creates an \u201cAssign\u201d tensor to update the momentum with the new value (line 15).", "Then, in lines 17\u201320, it calculates the new value for the parameter \u2014 depending on whether configured to apply Nesterov momentum or not \u2014 and applies the necessary constraints on the parameter (if there are such) in lines 23\u201324.", "The rest of the lines are similar to the simplified version from before.", "You can wander around optimizers.py and read the implementations of the different optimizers of Keras.", "Now, after deep-diving into what exactly Keras optimizers are and how they are implemented, we are ready to discuss the different implementation alternatives for a gradient accumulation mechanism.", "It is possible to rewrite any optimizer to support gradient-accumulation. Gradients should be accumulated over a few steps and only then should the optimizer use them for updating the model parameters. This is not optimal as gradient accumulation is a generic approach and should be optimizer-independent, and there are several flaws to this approach:", "A preferable approach is to design the gradient accumulation model so that it can wrap any Keras optimizer regardless of its optimization algorithm.", "By having a generic gradient accumulation mechanism, changes in the original optimizers will not require code updates.", "In order to design and implement a generic gradient accumulation mechanism, there are some things that need to be taken into consideration.", "Running the optimization algorithm on every mini-batch will not result in the same updates for the model parameters. In other words, we cannot just evaluate the optimization algorithm in every step \u2014 on every mini-batch. Otherwise, there was no need for gradient accumulation, and we could have just used a smaller batch size.", "If we were to use the global batch, all the gradients would have been calculated using the same values of the model parameters \u2014 the weights and biases. When splitting the global batch into several mini-batches, evaluating the optimization algorithm every step will cause the model parameters to be updated after every mini-batch. This means that the gradients of all mini-batches will not be calculated using the same values of the weights and biases.", "In addition, optimizers use various parameters as part of their formula, and those parameters are updated as part of the evaluation of the optimization algorithm. Updating those parameters every step \u2014 after every mini-batch \u2014 will result in changes in the state of the optimization algorithm between different mini-batches.", "Our wrapper is a Python class that inherits Keras\u2019s base Optimizer class. We receive the original optimizer as an argument upon creation (in __init__()), as well as the number of steps we want to accumulate gradients over.", "We define all the methods exposed by optimizers (i.e. get_gradients(), set_weights(), get_weights(), etc\u2026) and transparently call the original optimizer\u2019s respective methods. The main logic lives \u2014 as expected \u2014 in get_updates().", "Let\u2019s start examining get_updates() (can be seen on GitHub as well), and deep-dive into the algorithm and implementation:", "The first line (2) should seem familiar, where we calculate the gradients in the same way as other optimizers do. Note that grads will hold the value of the gradients of every mini-batch.", "In line 5, we declare a step counter \u2014 called iterations \u2014 with an initial value of 0 (pretty similar to other optimizers). We use the step counter to tell if we are either at the first or the last step of the accumulation. To do so, we declare two tensors: first and last (lines 6\u20137).", "first will be set to True every time we passed exactly self.steps steps. Technically, this is when iterations % self.steps will be equal to 0. For example, if we accumulate over five steps, this will be the case at the first step (indexed 0), the sixth step (indexed 5), the eleventh step (indexed 10), etc\u2026 In those cases, at the beginning of the step, we want to reset the accumulated gradients to 0 and start accumulating them once again.", "last will be set to True every step we want to update the variables. Technically, this is when iterations % self.steps will be equal to self.steps \u2014 1. Continuing the example from before, this will be the case at the fifth step (indexed 4), tenth step (indexed 9), etc\u2026", "In line 10, we declare variables to hold the values accumulated gradients between steps. We declare such a variable for every model parameter \u2014 for every trainable weight or bias \u2014 with the shape and type of the parameter, and initialize them with zeros.", "Using those variables, in line 13 we declare tensors \u2014 agrads \u2014 to hold the values of accumulated gradients in every step. We use first to tell whether we should start accumulating the gradients from now on or use the gradients accumulated in the previous steps. If first is True \u2014 meaning we start accumulating from now on \u2014 we use the gradients of the current mini-batch alone. If first is False \u2014 meaning we should use the gradients accumulated over the past steps \u2014 we add the gradients of the current mini-batch to vagrads. This control flow (checking the value of first) is generated into the model using K.switch().", "As a generic wrapper, we don\u2019t implement any optimization algorithm. The original optimizer is responsible for that. As we covered, every optimizer implements its mathematical formula in get_updates(). There, the optimizer manages and uses all the needed parameters for the formula (e.g. step counter, learning rate, momentums, etc\u2026). The optimizer stores the values of the parameters in dedicated variables, and every time a parameter needs to be updated, it assigns the new value to its dedicated variable.", "The method get_updates() is called once, generating \u201cAssign\u201d tensors that will be evaluated in every step. Some of them are the updates of the model parameters, and the other ones are updates to the optimizer parameters.", "As long as we are accumulating gradients, we don\u2019t want any of these updates to happen. We don\u2019t want the model parameters to be updated in order for all the mini-batches to start from the exact same point, in terms of having the same values for the weights and biases. We don\u2019t want the optimizer parameters to be updated in order for the optimizer to advance in the pace as if it were to run on the global batch. For example, we want the step counter to increase only after all mini-batches passed, so the learning rate will be modified at the correct rate.", "We want all the updates to take place only after all the mini-batches have passed. Technically, this means we want the updates to occur at the last step of the accumulation \u2014 when last is True.", "So, if we could have just called the original optimizer\u2019s get_updates(), while (1) making it use the accumulated gradients, and (2) causing all the variable updates to take place only at the last steps of the accumulation, we would have achieved what we wanted.", "Fortunately, replacing (hooking) methods is really easy in Python, and by replacing a few methods with a different implementation we can easily achieve exactly that.", "Optimizers call their get_gradients() from get_updates() to calculate the gradients of the parameters with respect to the loss. Therefore, we replace the optimizer\u2019s get_gradients() with a function that does nothing but returning the accumulated gradients (agrads \u2014 the tensors we generated in line 13). This will cause the original optimizer to refer to the accumulated gradients in its algorithm and will solve (1). Let\u2019s take a look at a simplified implementation for such a replacement method:", "Regarding (2), variables in Keras can be assigned using 3 methods: K.update(), K.update_add(), and K.update_sub(). Optimizers use these methods for all updates \u2014 for the model parameters as well as for the optimizer parameters. We replace all three of them (can be seen on GitHub). We want all tensors created using those methods to assign values only in the last mini-batch and to do nothing otherwise. Therefore, in our methods \u2014 that replace the three \u2014 we wrap every value being assigned with a conditional switch and pass this switch to the respective method. If this is the last mini-batch (last is True), we assign the actual value to the variable, otherwise, we assign a value that does not affect the variable. For K.update_add() and K.update_sub() we assign zero, causing the variable to not actually increase or decrease. For K.update() we assign the current value of the variable, causing the variable to keep its current value. Let\u2019s take a look at a simplified implementation for such replacement methods:", "Back to our get_updates(), in lines 15\u201318 we actually replace all those methods. We use helper classes \u2014 subclasses of runai.utils.Hook \u2014 to do so.", "In line 19 we call the original optimizer\u2019s get_updates(). With all those methods replaced, we (1) make it refer to the accumulated gradients, and (2) cause all updates (\u201cAssign\u201d ops) to take place only when last is True.", "We have two more things we have to do at the end of every step.", "First, we have to update our variables to hold the current value of the accumulated gradients. This is done in line 33.", "Second, we have to advance our step counter \u2014 self.iterations (line 36). To make sure this happens at the end of the step we generate the tensors under control dependencies of all other \u201cAssign\u201d ops. This causes the step counter update to take place only after all other updates have already taken place.", "This article along with previous ones was meant to describe a few things in details:", "We hope we were able to shed some light on those subjects and help you understand them a bit better.", "An open-source gradient accumulation tool, as well as usage examples and more resources, are available on GitHub.", "Please share with us if you found this interesting and helpful in running your own neural networks!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Programmer. I like technology, music, and too many more things."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa02c0342b60&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327----fa02c0342b60---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@grohsfabian?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Fabian Grohs"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa?source=friends_link&sk=28226e1d0ffa7e450d7dffa8d5b9cff6", "anchor_text": "another article"}, {"url": "http://www.run.ai", "anchor_text": "Run:AI"}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub"}, {"url": "https://github.com/run-ai/runai/tree/master/examples/ga/keras", "anchor_text": "examples"}, {"url": "https://pypi.org/project/runai/", "anchor_text": "Run:AI Python library"}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py#L60-L154", "anchor_text": "here"}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py#L183-L209", "anchor_text": "GitHub"}, {"url": "https://github.com/keras-team/keras/blob/2.2.4/keras/optimizers.py", "anchor_text": "optimizers.py"}, {"url": "https://github.com/run-ai/runai/blob/master/runai/ga/keras/optimizers.py", "anchor_text": "GitHub"}, {"url": "https://github.com/run-ai/runai/blob/master/runai/ga/keras/hooks.py", "anchor_text": "GitHub"}, {"url": "https://pypi.org/project/runai/", "anchor_text": "Run:AI Python library"}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fa02c0342b60---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fa02c0342b60---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fa02c0342b60---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gpu?source=post_page-----fa02c0342b60---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/keras?source=post_page-----fa02c0342b60---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&user=Raz+Rotenberg&userId=4f915c067327&source=-----fa02c0342b60---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&user=Raz+Rotenberg&userId=4f915c067327&source=-----fa02c0342b60---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffa02c0342b60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fa02c0342b60---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa02c0342b60--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fa02c0342b60--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fa02c0342b60--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/@raz.rotenberg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F68478d8b203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60&newsletterV3=4f915c067327&newsletterV3Id=68478d8b203&user=Raz+Rotenberg&userId=4f915c067327&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}