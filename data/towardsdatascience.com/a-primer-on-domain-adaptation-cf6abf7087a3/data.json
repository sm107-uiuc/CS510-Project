{"url": "https://towardsdatascience.com/a-primer-on-domain-adaptation-cf6abf7087a3", "time": 1683003534.615444, "path": "towardsdatascience.com/a-primer-on-domain-adaptation-cf6abf7087a3/", "webpage": {"metadata": {"title": "A Primer on Domain Adaptation. This article is a summary of a joint\u2026 | by Ivan Panico | Towards Data Science", "h1": "A Primer on Domain Adaptation", "description": "Machine learning (ML) aims at making statistical predictions for some phenomena given appropriate data in sufficient quantity. In supervised machine learning this data comes as a set of observations\u2026"}, "outgoing_paragraph_urls": [{"url": "http://linkedin.com/in/pirminlemberger", "anchor_text": "Pirmin Lemberger", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/ivan-panico/", "anchor_text": "Ivan Panico", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2001.09994", "anchor_text": "technical article", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/2001.09994", "anchor_text": "more", "paragraph_index": 30}], "all_paragraphs": ["This article is a summary of a joint work by Pirmin Lemberger and Ivan Panico linked at the bottom of this post.", "Disclaimer : As subscript support in unicode is still lacking on Medium we chose to use the following notation for subscript: p_S = p subscript S. For more readable notations we refer to the arXiv paper.", "Machine learning (ML) aims at making statistical predictions for some phenomena given appropriate data in sufficient quantity. In supervised machine learning this data comes as a set of observations where some target quantity, or category, or label, is dependent on some measurable features. In the standard setting it is assumed that all observations are independent and sampled from one and the same distribution. The role of an ML algorithm is then to convert this training data into a function that accurately predicts the labels of unseen observations given their features.", "Classical theory of ML assumes that the new observations from the test set, for which we want to make predictions are drawn from the same population as those from the training set. This, however, is an ideal situation rarely met in practice.", "The training set is then said to be biased with respect to the test set. Depending on the situation, this bias can either be known or unknown as we shall see. Domain adaptation (DA), which is the subject of this review, is a collection of methods that aims at compensating somehow for the statistical asymmetry between the train set and the test set. As a matter of fact, domain adaptation is everything but a new topic in ML. There are several reasons however that make it difficult for a newcomer to build a coherent overview of domain adaptation:", "Therefore, rather than aiming at completeness, which leads to exhibiting a tedious catalog of methods, this pedagogical review aims at a coherent presentation of four important special cases: prior shift, covariate shift, concept shift and subspace mapping that we shall all define shortly. The goal of this introductory review is to help the reader to build an intuition about this vast domain. For a more in-depth discussion, you can refer to our technical article.", "Let us now move on to the description of the four special cases of DA we consider. Henceforth, the source domain will refer to the population from which training observations are drawn while the target domain will refer to the population from which test observations are drawn. Objects related to the source domain will be \u201csubscripted\u201d _S while object related to the target domain will be \u201csubscripted\u201d _T.", "Prior shift refers to a situation in which the source distribution p_S used for picking the training observations is biased with respect to the target distribution p_T because the prior distribution of the labels y_i in both domains are different. We will focus here on classification where {\u03c9_1, \u2026, \u03c9_n} is a finite set of labels. The most interesting and difficult case is one in which the prior distribution of the labels in the target is unknown while observations in the source are selected according to their label using some known strategy. Stratified sampling for instance selects an equal number of observations (y_j = \u03c9_k, x_j) in each class \u03c9_k. The class conditional distributions p_S(x|y) = p_T(x|y) on the other hand are supposed to be the same in both domains. Figure 1 illustrates the situation.", "Without getting into technicalities, a nice method to deal with this kind of situation is based on the expectation maximization (EM) algorithm within the maximum likelihood framework of ML. Recall that this approach to ML assumes that the unknown distribution p_T(x,y) is parameterized by parameters \u03b8 and then looks for the parameters \u03b8_ML that maximize the probability of observations in a given train set. The EM algorithm on the other hand is an iterative method that finds \u03b8_ML in the particular case when the probability p_T results by marginalization over unobserved (or latent) variables. The idea then, when using EM to deal with the prior shift, is to treat the unobserved categorical variables y_i in the target observations as the latent variables and the list of unknown prior probabilities of the categories p_T(\u03c9_j) as the parameters \u03b8_j. Once EM has produced estimate for these prior probabilities, we can use them to compute posterior probabilities p_T(y=\u03c9_j|x) in the target and thus make predictions. Intuitively, at each step of the procedure, the current predictor is used to update the prior probability of the target which, in turn, is used to correct the predictor by a factor depending on it. See Figure 2 for a visual illustration.", "One nice thing with the method we describe is that it comes with a statistical test which can discriminate between situation in which you should or should not use it.", "Covariate shift, also sometimes termed \u201creal concept drift\u201d, describes a situation where the source distribution is biased because the training observations were sampled depending on their features in proportions p_S(x) that do not match the target distribution p_T(x). This should be contrasted with the prior shift we discussed earlier where a bias existed in the source distribution because observations were selected according to their label y in proportions p_S(y) that did not match p_T(y). Covariate shift assumes however that the dependence of the response y on the features x, as described by the conditional probability p_S(y|x) in the source, is the same as in the target population p_T(y|x). The situation is depicted in Figure 3 for the cases where x is either 1D or 2D.", "We emphasize that the target feature distribution p_T(x) is assumed to be known in the presence of a covariate shift. This again is in contrast with the prior shift where the target label distribution p_T(y) was assumed to be unknown. In other words, covariate and prior shift are not symmetric cases that would result by simply swapping features x with labels y.", "A covariate shift typically occurs when the cost or the difficulty of picking an observation with given features x strongly impacts the probability of selecting an observation (x,y) thus making it practically impossible to replicate the target feature distribution p_T(x) in the training set. A typical example could be a survey were some categories of individuals are more difficult to reach than others while the model should be able to classify all categories equally well.", "In contrast with the prior shift, the method we present in our article is based on the PAC formulation of ML. Very roughly, PAC theory aims at using the data available in the source to find a predictor that will (probably and approximately) minimize the prediction risk in the target domain. As we know that the marginal p_S(x) is biased with respect to p_T(x), the strategy is to introduce a reweighting factor w(x) so that the corrected version w(x) p_S(x) of the source distribution is close, in a sense to be made precise, to the target p_T(x). Various discrepancy measures between distributions are available. We chose to dive into the so called MMD measure (Maximal Mean Discrepancy) both for its elegance and because it nicely illustrates kernel methods. Minimizing the MMD measure between w(x) p_S(x) and p_T(x) with respect to w is known as KMM (Kernel Mean Matching). It eventually leads to a simple quadratic optimization problem for finding w(x) for which many solvers are available.", "In this case no statistical test is able to precisely indicate if the method will be helpful or not, but the main guidance is that p_S(x) and p_T(x) should not be too different.", "Concept shift, or real concept shift as it is sometimes referred to, characterizes a situation in which the dependence of the target variable y on the features x is different in the source and in the target. In other words, the conditional distributions p_S(y|x) \u2260 p_T(y|x) differ. The prior distributions over features are however supposed to coincide, p_S(x)=p_T(x). Figure 4 depicts the situation symbolically.", "Just like for the prior shift we discussed earlier, we generally know nothing about the target labels here either. Therefore we cannot simply deal with a concept shift by inserting a reweighting factor w := p_T(y|x) / p_S(y|x) in the definition of the true risk that we want to minimize.", "Concept shift often takes the form of a concept drift, where the conditional probability p_T(y|x) depends explicitly on the time, in other words it occurs in non-stationary environments, for instance in aging systems whose dynamics changes progressively.", "Algorithms that deal with drift are globally termed adaptive algorithms as they are designed to dynamically adapt to evolving settings. The core of the difficulty one faces in such dynamic environments is to be able to distinguish a random outlier of the response variable y from a genuine drift in the relationship which binds x and y. Here are a few basic strategies that can be used to deal with a concept shift:", "Organizing the wealth of existing methods that deal with the concept shift problem is a challenge by itself. One way to do this considers that any such adaptive system has four parts as figure shows:", "An in-depth description of all options for these four modules is outside the scope of this introduction.", "The number of applications that deal with concept drift is so large that whole studies have been devoted solely to categorize such tasks into a coherent framework. Three groups of applications, that have different goals and use different kind of data have been identified:", "Each of these applications in each category generally use different techniques to deal with the concept shift they face.", "The mathematical framework, based on an extension of the PAC theory, we describe in the article address the case where the drift is continuous and slow enough.", "Let\u2019s not deviate from tradition and imagine that we want to train a classifier to recognize pictures of cats and dogs. Assume that our training set contains pictures shot under specific lighting conditions which include, say, exposure, color balance and a specific kind of background. How then should we optimize our classifier to classify images of these same pets shoot under very different lighting conditions? This is typically a problem of subspace mapping. More generally, subspace mapping deals with situations where the source and target examples are selected likewise but there is an unknown twist, or coordinate change T, between the features x describing these in the source and those describing them in the target x\u2019=T(x). More precisely, in terms of the joint probability distributions we assume that for a subspace mapping:", "By marginalizing and conditioning the above equality we respectively obtain:", "In words: predictions y match for observations described by x in the source and by T(x) in the target. In particular, the first equation implies that p_T(x) \u2260 p_S(x) and the second implies p_T(y|x) \u2260 p_S(y|x) . Thus, another way to look at subspace mapping is as a composition of a covariate shift with a concept shift both characterized by a change of coordinates T on features. There is a wide range of methods that try to deal with a subspace mapping. We chose to focus on the optimal transport approach illustrated in Figure 6:", "Optimal transport (OT) is currently the most generic, elegant and powerful method for dealing with subspace mapping. In short, optimal transport looks for a mapping T that morphs the source joint distribution p_S into the target joint distribution p_T while using minimal effort to move mass from one to the other. Of course, we don\u2019t know the labels y_i in the target (this is why we want to predict them) and thus we don\u2019t have a direct access to p_T. We can however use a predictor h(x_i) as a proxy to these labels. A clear formulation of OT along these lines leads to a double minimizing procedure where the effort to move p_S towards p_T is minimized simultaneously on the transport mapping T and on the prediction function h. A simple minimization procedure alternates between optimizing h with fixed T and the optimizing T for fixed h. Optimizing for h can be done using any ML API while optimizing for T can leverage recent progress in OT optimizers.", "Although the mathematics of OT has already been known for decades, it has long been ignored by data scientists, mainly due to its heavy computational cost. But things have changed lately due to various computational tricks that allow for fast optimization.", "The need for systematic procedures for dealing with situations where a training data set does not faithfully reflect the real world is still understated by many practitioners despite its practical importance in machine learning. We analyze that this is mainly due to the myriad of methods available and to the lack of a unified and well accepted terminology for describing the different DA scenarios. For these reasons data scientists most often favor various tricks, like guessing an appropriate resampling ratio or generating fictitious data, to build appropriate training sets.", "We hope this review can make some contribution in clarifying and spreading awareness of this important topic of data science to a wider audience of data scientists. If we succeeded, you probably want to know more.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist & Big Data Architect at onepoint."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf6abf7087a3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@iv.panico?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iv.panico?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Ivan Panico"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdb668fa3a56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&user=Ivan+Panico&userId=bdb668fa3a56&source=post_page-bdb668fa3a56----cf6abf7087a3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://linkedin.com/in/pirminlemberger", "anchor_text": "Pirmin Lemberger"}, {"url": "https://www.linkedin.com/in/ivan-panico/", "anchor_text": "Ivan Panico"}, {"url": "https://arxiv.org/abs/2001.09994", "anchor_text": "technical article"}, {"url": "https://arxiv.org/abs/2001.09994", "anchor_text": "more"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cf6abf7087a3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cf6abf7087a3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/domain-adaptation?source=post_page-----cf6abf7087a3---------------domain_adaptation-----------------", "anchor_text": "Domain Adaptation"}, {"url": "https://medium.com/tag/optimal-transport?source=post_page-----cf6abf7087a3---------------optimal_transport-----------------", "anchor_text": "Optimal Transport"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&user=Ivan+Panico&userId=bdb668fa3a56&source=-----cf6abf7087a3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&user=Ivan+Panico&userId=bdb668fa3a56&source=-----cf6abf7087a3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcf6abf7087a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cf6abf7087a3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cf6abf7087a3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iv.panico?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iv.panico?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ivan Panico"}, {"url": "https://medium.com/@iv.panico/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdb668fa3a56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&user=Ivan+Panico&userId=bdb668fa3a56&source=post_page-bdb668fa3a56--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fbdb668fa3a56%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-domain-adaptation-cf6abf7087a3&user=Ivan+Panico&userId=bdb668fa3a56&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}