{"url": "https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866", "time": 1682996000.5248158, "path": "towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866/", "webpage": {"metadata": {"title": "An intuitive understanding of the LAMB optimizer | by Ben Mann | Towards Data Science", "h1": "An intuitive understanding of the LAMB optimizer", "description": "In software engineering, decreasing cycle time has a super-linear effect on progress. In modern deep learning, cycle time is often on the order of hours or days. The easiest way to speed up training\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1904.00962.pdf", "anchor_text": "recent paper", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Alternative_definition", "anchor_text": "signal to noise ratio", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1708.03888.pdf", "anchor_text": "LARS", "paragraph_index": 16}, {"url": "https://github.com/cybertronai/pytorch-lamb", "anchor_text": "implemented", "paragraph_index": 26}, {"url": "https://medium.com/u/5511064b4364?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Yaroslav Bulatov", "paragraph_index": 39}, {"url": "https://medium.com/u/c9d4f242a2cb?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Sarah Jane Hong", "paragraph_index": 39}, {"url": "https://medium.com/u/34ab754f8c5e?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Jeremy Howard", "paragraph_index": 39}], "all_paragraphs": ["In software engineering, decreasing cycle time has a super-linear effect on progress. In modern deep learning, cycle time is often on the order of hours or days. The easiest way to speed up training, data parallelism, is to distribute copies of the model across GPUs and machines and have each copy compute the loss on a shard of the training data. The gradients from these losses can then be accumulated using a single parameter server or something fancier like ring all-reduce (default in pytorch). After back-propagating the gradients, repeat with the updated model. But as batch size grows, the gradients can become unstable, causing training to diverge.", "A recent paper decreased training time of one of the biggest and most popular language models, BERT, from 3 days to 76 minutes by calculating the gradient in a more nuanced way, allowing the authors to scale the batch size up ~100X to 65K. While the paper describes its contributions in reference to its previous iteration, LARS, in this post I\u2019ll explain the necessary background starting from stochastic gradient descent (SGD), which LARS is built on.", "First, a review of the basics. Recall that for SGD, the standard update rule is", "Breaking this down, it\u2019s saying the new model weights are the existing ones minus learning rate times the gradient of the existing weight given an example or the average over a batch. Problems:", "In Adam, we keep a moving average of the gradients and their variance:", "where \ud835\udcc2 is the moving mean, \ud835\udccb is the moving uncentered variance, \u03b2\u2081 is the interpolation constant for the mean, and \u03b2\u2082 is the interpolation constant for the uncentered variance, and \u2207L is the gradient of the loss. The parentheses in the exponents mean it\u2019s not actually an exponent, it\u2019s the time step. This looks kind of scary, but the important thing to notice is that both \ud835\udcc2 and \ud835\udccb are simply linear interpolations (\u03b2 * x\u2080 + (1 \u2014 \u03b2) * x\u2081) of the gradients and their variances, which gives us a moving average of each. The higher the beta, the less we update the moving average for each new sample, thus smoothing our estimate of the mean and variance of the gradient across batches. Here\u2019s a visualization of how much smoothing we get on a noisy dataset for different betas.", "If we have a small batch, our estimates of the gradient at each step might be noisier, so we\u2019ll need a higher beta. If we\u2019re using a huge batch size with consistent data, we probably need less beta.", "The problem with the moving averages above is that when the algorithm first initializes, the moving averages are 0. This causes the summary statistics to be closer to 0 than they should be for the first couple of timesteps if beta is close to 1 because we are taking most of the mass from the previous step. This effect is particularly visible in the beta=0.99 graph above.", "The problem doesn\u2019t go away, but it\u2019s much better.", "To plug some numbers in, if \u03b2 = 0.9, on the first iteration the debias will multiply the value by 1 / 1\u20130.9\u00b9 = 10. Then when we linearly interpolate, \u03b2\ud835\udccd\u2080 + (1 \u2014 \u03b2)\ud835\udccd\u2081, the first term \u03b2\ud835\udccd\u2080 = 0. The debias factor, 10, will cancel out (1 \u2014 \u03b2) = 0.1 in the second term, so we entirely use the new value, \ud835\udccd\u2081. After just a few steps, the debias factor will converge to 1. The graphs below show how many steps it takes for the debias term to disappear (note the difference in the y axis):", "Now the final parameter update is", "The numerator says \u201cfor every parameter, take a step in the direction of the gradient for that parameter.\u201d The denominator says \u201cnormalize the step by its standard deviation.\u201d", "The intuitive interpretation is that when we first start updating parameters, we\u2019ll probably be way off. If the gradients are all pointing in different directions (high variance), we\u2019ll take a small, cautious step. Conversely, if all the gradients are telling us to move in the same direction, the variance will be small, so we\u2019ll take a bigger step in that direction. Either way, if the scale of all the gradients is large, the constant factor will cancel out when we divide since we\u2019re using the uncentered variance. As training stabilizes and loss gets closer to 0, the mean will approach 0, so updates will automatically get finer.", "The \u03b5 in the denominator says \u201cok, we may think we\u2019ve got no noise at all, but let\u2019s not go too crazy here and just take one step at a time.\u201d This effectively sets an upper bound on the size of the step you take as the noise variance approaches zero.", "This ratio m/sqrt(v) might look like \u03bc/\u03c3, which is the signal to noise ratio, but that interpretation only applies to scalars.", "As batch size grows, the number of iterations per epoch decreases. To converge in the same number of dataset iterations, we can compensate by increasing the learning rate. However as learning rate increases, training becomes more unstable. The SOTA was to use a learning rate warm up, but this is only helped up to a certain point, at which the learning would start diverging anyway. The warmup was a patch over the real issue: the gradients must be noisy.", "The authors of Layerwise Adaptive Rate Scaling (LARS) explain their trick to solve this problem:", "To analyze the training stability with large LRs we measured the ratio between the norm of the layer weights and norm of gradients update. We observed that if this ratio is too high, the training may become unstable. On the other hand, if the ratio is too small, then weights don\u2019t change fast enough.", "They call this ratio the \u201ctrust ratio\u201d. When it\u2019s higher, the gradients change faster and vice versa. Since we can now be more confident of each step, the cautionary warm-up often used in learning rate schedules is no longer necessary and we can scale to much bigger batch sizes without diverging.", "In English: the layer-wise learning rate \u03bb is the global learning rate \u03b7 times the ratio of the norm of the layer weights to the norm of the layer gradients. If we use weight decay, we can just add it in the denominator. When we plug this into SGD, the denominator ends up normalizing the gradients to have unit norm, which helps avoid divergence.", "The numerator is the norm of the weights because as networks deepen, it becomes important to have zero mean, unit variance (ZMUV) weights. This is because at each layer, these weights are multiplied together, so if it diverges from ZMUV, the values may explode or vanish. When weights are small, we take a small step. When weights are large we take a bigger step. Combined with weight decay this helps us stably step towards ZMUV weights.", "Let\u2019s get a sense of what\u2019s going on here. At the beginning of training, layers are supposed to output ZMUV, so the numerator above will be 0 or close to it. Any steps we take are likely to be small. In contrast, the denominator will probably be large since when everything is wrong, the gradients are large. In this way we naturally warm up as the weights increase. As we approach 0 loss, the gradients will be small, so the trust ratio will keep the learning rate up to 10x (due to clipping) higher than without the trust ratio, keeping us from giving up on reaching the optimum too early.", "LAMB stands for \u201cLayer-wise Adaptive Moments optimizer for Batch training.\u201d It makes a few small changes to LARS", "So in full, the trust ratio in LAMB is", "The final line is the layer-wise LAMB update rule. \ud835\udcc7\u2082 is the norm of the Adam update rule with weight decay, \u03b7\u1d38 is the layer-wise learning rate adjusted by the trust ratio. So overall this method can be summarized as LARS applied to Adam, since it\u2019s just multiplying the old update step by the trust ratio.", "The authors don\u2019t report whether LAMB improves ImageNet training performance over LARS, and they don\u2019t compare LARS to LAMB for BERT, so it\u2019s a bit hard to say how much difference these changes make, but implementation is pretty simple.", "To get a better sense of what\u2019s going on, I implemented LAMB in Pytorch. I ran a bunch of experiments on MNIST and found that where Adam diverges, LAMB keeps chugging. I chose MNIST because it\u2019s tiny enough to try on CPU, but it means we can\u2019t see any convergence improvements. I\u2019ll publish another post exploring LAMB applied to big transformers soon!", "I visualized some of the experiments.", "Below, I compare Adam (blue below) and LAMB (red below) with learning rate 0.01 and betas .9, .99. They\u2019re pretty similar, but LAMB generalizes on test accuracy better.", "To find out what\u2019s going on under the hood, I wanted to look at the layer-wise components of the trust ratio, so I logged every value of r, r\u2081, and r\u2082 as a histogram after every few batches. For Adam, I calculate the values, but don\u2019t use them anywhere.", "How to interpret the chart below: the Y axis shows what timestep, with the first at the top, X axis is histogram buckets, Z axis is histogram frequency.", "You can see that on both sides, r starts significantly below 1. On the LAMB side, this creates a natural warm up period across all layers. Then, as some of the layers start gaining larger weights and stabler gradients, r encourages them to take larger steps. This exaggerates the norms relative to the Adam baseline.", "For the next experiment, I compared LAMB to itself across learning rates 0.1 and 0.01. Adam converges normally at learning rate .01 and at 0.1 doesn\u2019t learn at all, so I won\u2019t compare it here. On the left (blue) learning rate = .01, on the right (green) learning rate = 0.1. On the right, it converges almost instantly during the warmup, but then a few layer weights start to explode (see difference in X axis scale) and it diverges.", "To address the weights running away, I added weight decay 0.01 below right. Training didn\u2019t diverge! Generally the trust ratio kept learning slow at less than 1, whereas in the more comfortable regime above left, it got as high as 4.5.", "Vanilla SGD becomes unstable as learning rate increases.", "LARS adjusts the SGD learning rate by a layer-wise trust ratio that normalizes the gradients and weights.", "Adam modulates updates with debiased means normalized by debiased variances.", "LAMB adjusts Adam\u2019s learning rate by a more accurate layer-wise, clipped trust ratio.", "Combining all these techniques allows us to train on large batches with a high learning rate, decreasing wall time by 100X for BERT!", "Thanks to Yaroslav Bulatov and Sarah Jane Hong for edits and Jeremy Howard/fast.ai part 2 for inspiration", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software engineer, tinkerer, aspiring mad scientist"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46f8c0ae4866&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://8enmann.medium.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": ""}, {"url": "https://8enmann.medium.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Ben Mann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33ebef5d1079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&user=Ben+Mann&userId=33ebef5d1079&source=post_page-33ebef5d1079----46f8c0ae4866---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1904.00962.pdf", "anchor_text": "recent paper"}, {"url": "https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/09_optimizers.ipynb", "anchor_text": "fast.ai"}, {"url": "https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/09_optimizers.ipynb", "anchor_text": "fast.ai"}, {"url": "https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Alternative_definition", "anchor_text": "signal to noise ratio"}, {"url": "https://arxiv.org/pdf/1708.03888.pdf", "anchor_text": "LARS"}, {"url": "https://github.com/titu1994/keras-LAMB-Optimizer/blob/master/keras_lamb.py", "anchor_text": "some code"}, {"url": "https://github.com/cybertronai/pytorch-lamb", "anchor_text": "implemented"}, {"url": "https://medium.com/u/5511064b4364?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Yaroslav Bulatov"}, {"url": "https://medium.com/u/c9d4f242a2cb?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Sarah Jane Hong"}, {"url": "https://medium.com/u/34ab754f8c5e?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Jeremy Howard"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----46f8c0ae4866---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----46f8c0ae4866---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----46f8c0ae4866---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----46f8c0ae4866---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----46f8c0ae4866---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&user=Ben+Mann&userId=33ebef5d1079&source=-----46f8c0ae4866---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&user=Ben+Mann&userId=33ebef5d1079&source=-----46f8c0ae4866---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F46f8c0ae4866&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----46f8c0ae4866---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----46f8c0ae4866--------------------------------", "anchor_text": ""}, {"url": "https://8enmann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://8enmann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ben Mann"}, {"url": "https://8enmann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "950 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33ebef5d1079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&user=Ben+Mann&userId=33ebef5d1079&source=post_page-33ebef5d1079--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F869b10d3566e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866&newsletterV3=33ebef5d1079&newsletterV3Id=869b10d3566e&user=Ben+Mann&userId=33ebef5d1079&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}