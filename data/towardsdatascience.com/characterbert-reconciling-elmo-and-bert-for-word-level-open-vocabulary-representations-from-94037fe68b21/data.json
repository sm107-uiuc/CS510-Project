{"url": "https://towardsdatascience.com/characterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21", "time": 1683015697.142476, "path": "towardsdatascience.com/characterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21/", "webpage": {"metadata": {"title": "CharacterBERT: a Word-Level Open-Vocabulary BERT | Towards Data Science", "h1": "CharacterBERT", "description": "CharacterBERT is a variant of BERT that uses a CharacterCNN (just like ELMo) instead of relying on a WordPiece system."}, "outgoing_paragraph_urls": [{"url": "https://github.com/helboukkouri/character-bert#pre-trained-models", "anchor_text": "pre-trained models are available", "paragraph_index": 20}, {"url": "https://helboukkouri.github.io/", "anchor_text": "Hicham El Boukkouri", "paragraph_index": 22}, {"url": "https://helboukkouri.github.io/", "anchor_text": "https://helboukkouri.github.io/", "paragraph_index": 24}], "all_paragraphs": ["CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].", "The next figure shows the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.", "Let\u2019s imagine that the word \u201cApple\u201d is an unknown word (i.e. it does not appear in BERT\u2019s WordPiece vocabulary), then BERT splits it into known WordPieces: [Ap] and [##ple], where ## are used to designate WordPieces that are not at the beginning of a word. Then, each subword unit is embedded using a WordPiece embedding matrix, producing two output vectors.", "On the other hand, CharacterBERT does not have a WordPiece vocabulary and can handle any* input token as long as it is not unreasonably long (i.e. under 50 characters). Instead of splitting \u201cApple\u201d, CharacterBERT reads it as a sequence of characters: [A], [p], [p], [l], [e]. Each character is then represented using a character embedding matrix, producing a sequence of character embeddings. This sequence is then fed to multiple CNNs, each responsible for scanning the sequence n-characters at a time, with n =[1..7]. All CNN outputs are aggregated into a single vector that is then projected down to the desired dimension using Highway Layers [3]. This final projection is the context-independent representation of the word \u201cApple\u201d, which will be combined with position and segment embeddings before being fed to multiple Transformer Layers as in BERT.", "CharacterBERT acts almost as a drop in replacement for BERT that", "The first point is clearly desirable as working with single embeddings is far more convenient than having a variable number of WordPiece vectors for each token. As for the second point, it is particularly relevant when working in specialized domains (e.g. medical domain, legal domain, \u2026). In fact, the common practice when building specialized versions of BERT (e.g. BioBERT [4], BlueBERT [5] and some SciBERT [6] models) is to re-train the original model on a set of specialized texts. As a result, most SOTA specialized models keep the original general-domain WordPiece vocabulary which is not suited for specialized domain applications.", "The table below shows the difference between the original general-domain vocabulary and a medical WordPiece vocabulary that was built on medical corpora: MIMIC [7] and PMC OA [8].", "We can clearly see that BERT\u2019s vocabulary is not suited for specialized terms (e.g. \u201ccholedocholithiasis\u201d is split into [cho, led, och, oli, thi, asi, s]). The medical wordpiece works better however it has its limits as well (e.g. \u201cborborygmi\u201d into [bor, bor, yg, mi]).", "Therefore, in order to avoid any biases that may come from using the wrong WordPiece vocabulary, and in an effort to got back to conceptually simpler models, a variant of BERT was proposed: CharacterBERT.", "BERT and CharacterBERT are compared in a classic scenario where a general model is pre-trained before serving as an initialisation for the pre-training of a specialized version.", "Note: we focus here on the English language and the medical domain.", "To be as fair as possible, both BERT and CharacterBERT are pre-trained from scratch in exactly the same conditions. Then, each pre-trained model is evaluated on multiple medical tasks. Let\u2019s take an example.", "i2b2/VA 2010 [9] is a competition that consists in multiple tasks including the clinical concept detection task which was used to evaluate our models. The goal is to detect three types of clinical concepts: Problem, Treatment and Test. An example is given in the far left section of the figure above.", "As usual, we evaluate our models by first training on the training set. At each iteration, the model is tested on a separate validation set, allowing us to save the best iteration. Finally, after going through all the iterations, a score (here a strict F1 score) is computed on the test set using the model from the best iteration. This whole procedure is then repeated 9 more times using different random seeds, which allows us to account for some of the variance and report final model performances as: mean \u00b1 standard-deviation.", "Note: more details are available in the paper [2].", "In most cases, CharacterBERT outperformed its BERT counterpart.", "Note: The only exception is the ClinicalSTS task where the medical CharacterBERT got (on average) a lower score than the BERT version. This may be due to the task dataset being small (1000 examples vs. 30,000 on average for other tasks) and should be investigated.", "Besides pure performance, another interesting aspect is whether the models are robust to noisy inputs. In fact, we evaluated BERT and CharacterBERT on noisy versions of the MedNLI task [10] where (put simply) the goal is to say whether two medical sentences are in contradiction with each other. Here, a noise level of X% means that each character in the text if either replaced or swapped with a X% probability. The results are displayed on the figure below.", "As you can see, the medical CharacterBERT model seems to be more robust than medical BERT: the initial gap between the two models of ~1% accuracy grows to ~3% when adding noise to all splits, and ~5% when surprising the models with noise only in the test set.", "The main downside of CharacterBERT is its slower pre-training speed.This is due to:", "/!\\ : However, CharacterBERT is just as fast as BERT during inference (actually, it is even a bit faster) and pre-trained models are available so you can skip the pre-training step altogether \ud83d\ude0a!", "All in all, CharacterBERT is a simple variant of BERT that replaces the WordPiece system with a CharacterCNN (just like ELMo). Evaluation results on multiple medical tasks show that this change is beneficial: improved performance & improved robustness to misspellings. Hopefully, this model will motivate more research towards word-level open-vocabulary transformer-based language models: applying the same idea to ALBERT [11], ERNIE [12]\u2026", "Work done by: Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum and Junichi Tsujii", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student working on Natural Language Processing for the medical domain. (https://helboukkouri.github.io/)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F94037fe68b21&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://helboukkouri.medium.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": ""}, {"url": "https://helboukkouri.medium.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Hicham EL BOUKKOURI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcddab3e218b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&user=Hicham+EL+BOUKKOURI&userId=cddab3e218b0&source=post_page-cddab3e218b0----94037fe68b21---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/helboukkouri/character-bert#pre-trained-models", "anchor_text": "pre-trained models are available"}, {"url": "https://arxiv.org/abs/2010.10392", "anchor_text": "https://arxiv.org/abs/2010.10392"}, {"url": "https://github.com/helboukkouri/character-bert", "anchor_text": "https://github.com/helboukkouri/character-bert"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "Deep contextualized word representations."}, {"url": "https://arxiv.org/abs/2010.10392", "anchor_text": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters."}, {"url": "https://arxiv.org/abs/1505.00387", "anchor_text": "Highway networks."}, {"url": "https://arxiv.org/abs/1901.08746", "anchor_text": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining."}, {"url": "https://arxiv.org/abs/1906.05474", "anchor_text": "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets."}, {"url": "https://arxiv.org/abs/1903.10676", "anchor_text": "SciBERT: A pretrained language model for scientific text."}, {"url": "https://doi.org/10.13026/C2XW26", "anchor_text": "https://doi.org/10.13026/C2XW26"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/", "anchor_text": "https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/", "anchor_text": "2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text."}, {"url": "https://doi.org/10.13026/C2RS98", "anchor_text": "https://doi.org/10.13026/C2RS98"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT: A lite BERT for self-supervised learning of language representations."}, {"url": "https://arxiv.org/abs/1907.12412", "anchor_text": "ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding."}, {"url": "https://helboukkouri.github.io/", "anchor_text": "Hicham El Boukkouri"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----94037fe68b21---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----94037fe68b21---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ai?source=post_page-----94037fe68b21---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----94037fe68b21---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bioinformatics?source=post_page-----94037fe68b21---------------bioinformatics-----------------", "anchor_text": "Bioinformatics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&user=Hicham+EL+BOUKKOURI&userId=cddab3e218b0&source=-----94037fe68b21---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&user=Hicham+EL+BOUKKOURI&userId=cddab3e218b0&source=-----94037fe68b21---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F94037fe68b21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----94037fe68b21---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----94037fe68b21--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----94037fe68b21--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----94037fe68b21--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----94037fe68b21--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----94037fe68b21--------------------------------", "anchor_text": ""}, {"url": "https://helboukkouri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://helboukkouri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hicham EL BOUKKOURI"}, {"url": "https://helboukkouri.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "216 Followers"}, {"url": "https://helboukkouri.github.io/", "anchor_text": "https://helboukkouri.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcddab3e218b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&user=Hicham+EL+BOUKKOURI&userId=cddab3e218b0&source=post_page-cddab3e218b0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4eea2682a887&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcharacterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21&newsletterV3=cddab3e218b0&newsletterV3Id=4eea2682a887&user=Hicham+EL+BOUKKOURI&userId=cddab3e218b0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}