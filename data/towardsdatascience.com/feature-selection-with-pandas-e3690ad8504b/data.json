{"url": "https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b", "time": 1682994957.364099, "path": "towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b/", "webpage": {"metadata": {"title": "Feature Selection with sklearn and Pandas | by Abhini Shetye | Towards Data Science", "h1": "Feature Selection with sklearn and Pandas", "description": "Feature selection is one of the first and important steps while performing any machine learning task. A feature in case of a dataset simply means a column. When we get any dataset, not necessarily\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient", "anchor_text": "Pearson correlation", "paragraph_index": 4}, {"url": "https://www.statsdirect.com/help/basics/p_values.htm", "anchor_text": "pvalue", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "Recursive Feature Elimination", "paragraph_index": 16}, {"url": "https://www.linkedin.com/in/abhinishetye/", "anchor_text": "https://www.linkedin.com/in/abhinishetye/", "paragraph_index": 25}], "all_paragraphs": ["Feature selection is one of the first and important steps while performing any machine learning task. A feature in case of a dataset simply means a column. When we get any dataset, not necessarily every column (feature) is going to have an impact on the output variable. If we add these irrelevant features in the model, it will just make the model worst (Garbage In Garbage Out). This gives rise to the need of doing feature selection.", "When it comes to implementation of feature selection in Pandas, Numerical and Categorical features are to be treated differently. Here we will first discuss about Numeric feature selection. Hence before implementing the following methods, we need to make sure that the DataFrame only contains Numeric features. Also, the following methods are discussed for regression problem, which means both the input and output variables are continuous in nature.", "Feature selection can be done in multiple ways but there are broadly 3 categories of it:1. Filter Method 2. Wrapper Method 3. Embedded Method", "We will be using the built-in Boston dataset which can be loaded through sklearn. We will be selecting features using the above listed methods for the regression problem of predicting the \u201cMEDV\u201d column. In the following code snippet, we will import all the required libraries and load the dataset.", "As the name suggest, in this method, you filter and take only the subset of the relevant features. The model is built after selecting the features. The filtering here is done using correlation matrix and it is most commonly done using Pearson correlation.", "Here we will first plot the Pearson correlation heatmap and see the correlation of independent variables with the output variable MEDV. We will only select features which has correlation of above 0.5 (taking absolute value) with the output variable.", "The correlation coefficient has values between -1 to 1 \u2014 A value closer to 0 implies weaker correlation (exact 0 implying no correlation) \u2014 A value closer to 1 implies stronger positive correlation \u2014 A value closer to -1 implies stronger negative correlation", "As we can see, only the features RM, PTRATIO and LSTAT are highly correlated with the output variable MEDV. Hence we will drop all other features apart from these. However this is not the end of the process. One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. If these variables are correlated with each other, then we need to keep only one of them and drop the rest. So let us check the correlation of selected features with each other. This can be done either by visually checking it from the above correlation matrix or from the code snippet below.", "From the above code, it is seen that the variables RM and LSTAT are highly correlated with each other (-0.613808). Hence we would keep only one variable and drop the other. We will keep LSTAT since its correlation with MEDV is higher than that of RM.", "After dropping RM, we are left with two feature, LSTAT and PTRATIO. These are the final features given by Pearson correlation.", "A wrapper method needs one machine learning algorithm and uses its performance as evaluation criteria. This means, you feed the features to the selected Machine Learning algorithm and based on the model performance you add/remove the features. This is an iterative and computationally expensive process but it is more accurate than the filter method.", "There are different wrapper methods such as Backward Elimination, Forward Selection, Bidirectional Elimination and RFE. We will discuss Backward Elimination and RFE here.", "As the name suggest, we feed all the possible features to the model at first. We check the performance of the model and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.", "The performance metric used here to evaluate feature performance is pvalue. If the pvalue is above 0.05 then we remove the feature, else we keep it.", "We will first run one iteration here just to get an idea of the concept and then we will run the same code in a loop, which will give the final set of features. Here we are using OLS model which stands for \u201cOrdinary Least Squares\u201d. This model is used for performing linear regression.", "As we can see that the variable \u2018AGE\u2019 has highest pvalue of 0.9582293 which is greater than 0.05. Hence we will remove this feature and build the model once again. This is an iterative process and can be performed at once with the help of loop. This approach is implemented below, which would give the final set of variables which are CRIM, ZN, CHAS, NOX, RM, DIS, RAD, TAX, PTRATIO, B and LSTAT", "The Recursive Feature Elimination (RFE) method works by recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance. The RFE method takes the model to be used and the number of required features as input. It then gives the ranking of all the variables, 1 being most important. It also gives its support, True being relevant feature and False being irrelevant feature.", "Here we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of number \u20187\u2019 was random. Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest.", "As seen from above code, the optimum number of features is 10. We now feed 10 as number of features to RFE and get the final set of features given by RFE method, as follows:", "Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.", "Here we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes it\u2019s coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken.", "Here Lasso model has taken all the features except NOX, CHAS and INDUS.", "We saw how to select features using multiple methods for Numeric Data and compared their results. Now there arises a confusion of which method to choose in what situation. Following points will help you make this decision.", "In the next blog we will have a look at some more feature selection method for selecting numerical as well as categorical features.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Data Analyst @ Nium | LinkedIn: https://www.linkedin.com/in/abhinishetye/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe3690ad8504b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@abhini.shetye?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhini.shetye?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Abhini Shetye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F155472da8dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&user=Abhini+Shetye&userId=155472da8dfc&source=post_page-155472da8dfc----e3690ad8504b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient", "anchor_text": "Pearson correlation"}, {"url": "https://www.statsdirect.com/help/basics/p_values.htm", "anchor_text": "pvalue"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "Recursive Feature Elimination"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e3690ad8504b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----e3690ad8504b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e3690ad8504b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----e3690ad8504b---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&user=Abhini+Shetye&userId=155472da8dfc&source=-----e3690ad8504b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&user=Abhini+Shetye&userId=155472da8dfc&source=-----e3690ad8504b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe3690ad8504b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e3690ad8504b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e3690ad8504b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e3690ad8504b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e3690ad8504b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhini.shetye?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhini.shetye?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhini Shetye"}, {"url": "https://medium.com/@abhini.shetye/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "819 Followers"}, {"url": "https://www.linkedin.com/in/abhinishetye/", "anchor_text": "https://www.linkedin.com/in/abhinishetye/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F155472da8dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&user=Abhini+Shetye&userId=155472da8dfc&source=post_page-155472da8dfc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba07cd9115ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-with-pandas-e3690ad8504b&newsletterV3=155472da8dfc&newsletterV3Id=ba07cd9115ed&user=Abhini+Shetye&userId=155472da8dfc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}