{"url": "https://towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c", "time": 1682993793.853513, "path": "towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c/", "webpage": {"metadata": {"title": "Word Embeddings and Document Vectors: Part 2. Order Reduction | by Ashok Chilakapati | Towards Data Science", "h1": "Word Embeddings and Document Vectors: Part 2. Order Reduction", "description": "In the previous post Word Embeddings and Document Vectors: Part 1. Similarity we laid the groundwork for using bag-of-words based document vectors in conjunction with word embeddings (pre-trained or\u2026"}, "outgoing_paragraph_urls": [{"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "Word Embeddings and Document Vectors: Part 1. Similarity", "paragraph_index": 0}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github", "paragraph_index": 1}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github repo", "paragraph_index": 2}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix", "anchor_text": "Compressed Sparse Row", "paragraph_index": 12}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github", "paragraph_index": 13}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "earlier post", "paragraph_index": 16}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com", "paragraph_index": 17}], "all_paragraphs": ["In the previous post Word Embeddings and Document Vectors: Part 1. Similarity we laid the groundwork for using bag-of-words based document vectors in conjunction with word embeddings (pre-trained or custom-trained) for computing document similarity, as a precursor to classification. It seemed that document+word vectors were better at picking up on similarities (or the lack) in toy documents we looked at. We want to carry through with it and apply the approach against actual document repositories to see how the document+word vectors do for classification. This post focuses on the approach, the mechanics, and the code snippets to get there. The results will be covered in the next post in this series.", "The outline for the article is as follows. The code for full implementation can be downloaded from github.", "The code snippets shown in this post are what they are \u2014 snippets, snipped from the full implementation, and edited for brevity to focus on a few things. The github repo is the reference. We will briefly detail tokenization and word-vector generation steps above before getting to the full process pipeline.", "While the document vectorizers in SciKit can tokenize the raw text in a document, we would like to potentially control it with custom stop words, stemming and such. Here is a snippet of code that tokenizes the 20-news corpus saving to an elasticsearch index for future retrieval.", "In Line 10 above, we remove all punctuation, remove tokens that do not start with a letter, and those that are too long (> 14 characters) or short (< 2 characters). Tokens are lowercased, stopwords removed (line 14), and stemmed (line 18). In Line 36 we remove the headers, footers etc\u2026 info from the each post, as those would be a dead give away as to which news group the article belongs to. Basically we are making it harder to classify.", "The following code snippet processes the published fasttext word-vectors into an elasticsearch index.", "In line 22 above we read the pre-trained vectors. Line 23 indexes them into elasticsearch. We can also generate custom word-vectors from any text corpus at hand. Gensim provides handy api for that as well.", "In lines 35 and 41 the models are trained with the tokens (stopped or stemmed) obtained from the corpus index we created in section 1. The chosen length for the vectors is 300. The min_count in line 30 refers to the minimum number of times a token has to occur in the corpus, for that token to be considered.", "We vectorize the documents in the repo, transform and reduce the order of the model if word embeddings are to be employed, and apply a classifier for fitting and prediction as shown in Figure 1 earlier. Let us look at each one of them in turn.", "We said earlier that we could use SciKit\u2019s count/tf-idf vectorizers. They yield a document-term matrix X for sure, but our word-embedding step in the pipeline needs the vocabulary/words obtained by that vectorizer. So we write a custom wrapper class around SciKit\u2019s vectorizer and augment the transform response with the vocabulary.", "The wrapper is initialized in line 1 with the actual SciKit vectorizer along with min_df (the minimum frequency across the repository required for a token to be considered in the vocabulary) set to 2. Line 8 uses the fit procedure of the chosen vectorizer, and the transform method in Line 12 issues out a response with both X and the derived vocabulary V that the second step needs.", "We have m documents and n unique words among them. The core part of the work here is the following.", "p is of course the length of the word-vector, the projection of the original 1-hot n-dimensional vector to this fake p-word-space. We should be careful with matrix multiplication however as X comes in from the vectorizer as a Compressed Sparse Row matrix, and our W is a normal matrix. A bit of index jugglery will do it. Here is a code snippet around this step in the pipeline.", "Line 1 initializes the transformer with a wordvector object (check github for the code) that has the methods to get the vectors from the index. Line 15 gets a sorted word list from the vocabulary passed from the vectorizer step. The csr X matrix uses the same order for its non-zero entries and we need to obtain W in the same order of words as well. This is done in line 16, and finally the sparse matrix multiplication in line 17 yields the reduced order matrix Z that we are after.", "This is easy. The classifier gets the m x p matrix Z where each row is a document. It also gets the m x 1 vector of labels when fitting the model. We will evaluate three classifiers \u2014 naive bayes, support vector machines, and neural nets. We run them without tweaking any of the default SciKit parameters. In the case of neural nets we try a few different number of hidden layers (1, 2 or 3) and neurons within (50, 100, and 200) as there are no good defaults for that.", "The method getNeuralNet in line 1 generates the tuple we need for initializing the neural net with the hidden layers and neurons. We prepare a suite of classifiers that are applied against the various combinations of vectorizers and transformers.", "In the earlier post we studied document similarity with word embeddings. In this post we have laid out the machinery to use those concepts against document repositories to obtain reduced order document-word matrix. In the next post we will run the simulations and study the impact of different tokenization schemes, word-vector algorithms, pre-trained Vs custom word-vectors, on the quality and performance of different classifiers.", "A modified version of this article was originally published at xplordat.com on September 27, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2d11c3b5139c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29----2d11c3b5139c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "Word Embeddings and Document Vectors: Part 1. Similarity"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github"}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "large movie review data set"}, {"url": "https://www.elastic.co/", "anchor_text": "Elasticsearch"}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing", "anchor_text": "Word2Vec/SGNS"}, {"url": "https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip", "anchor_text": "FastText"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "Gensim"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "pipeline"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer", "anchor_text": "TfidfVectorizer"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html", "anchor_text": "Multinomial Naive Bayes"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "anchor_text": "Linear Support Vectors,"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html", "anchor_text": "Neural Nets"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github repo"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "previous post."}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix", "anchor_text": "Compressed Sparse Row"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "earlier post"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2d11c3b5139c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----2d11c3b5139c---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----2d11c3b5139c---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/tag/elasticsearch?source=post_page-----2d11c3b5139c---------------elasticsearch-----------------", "anchor_text": "Elasticsearch"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----2d11c3b5139c---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----2d11c3b5139c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----2d11c3b5139c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2d11c3b5139c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2d11c3b5139c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2d11c3b5139c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/@ashok.chilakapati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ab4b71672c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c&newsletterV3=cc37b40eae29&newsletterV3Id=5ab4b71672c9&user=Ashok+Chilakapati&userId=cc37b40eae29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}