{"url": "https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb", "time": 1682996316.2382772, "path": "towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb/", "webpage": {"metadata": {"title": "Linear Classifiers: An Overview. This article discusses the mathematical\u2026 | by Micha\u0142 Oleszak | Towards Data Science", "h1": "Linear Classifiers: An Overview", "description": "This article is based on a chapter from the excellent Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New\u2026"}, "outgoing_paragraph_urls": [{"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear models for regression can be regularized", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear models for regression can be regularized", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "Scikit-learn\u2019s logistic regression documentation", "paragraph_index": 19}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles", "paragraph_index": 23}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com", "paragraph_index": 25}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal", "paragraph_index": 25}], "all_paragraphs": ["This article is based on a chapter from the excellent Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.", "A popular class of procedures for solving classification tasks are based on linear models. What this means is that they aim at dividing the feature space into a collection of regions labeled according to the values the target can take, where the decision boundaries between those regions are linear: they are lines in 2D, planes in 3D, and hyperplanes with more features.This article reviews popular linear models for classification, providing the descriptions of the discussed methods as well as Python implementations. We will cover the following approaches:", "For demonstrative purposes, we will apply each discussed method to the spam data set, in which the task is to classify emails as either spam or not spam based on a set of features describing word frequencies used in the emails. The data set, as well as some descriptions of the variables, can be found on the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook, in the Data section.", "Let\u2019s start by importing all the packages used throughout this tutorial and loading the data.", "The first method to be discussed is the Linear Discriminant Analysis (LDA). It assumes that the joint density of all features, conditional on the target's class, is a multivariate Gaussian. This means that the density P of the features X, given the target y is in class k, are assumed to be given by", "where d is the number of features, \u03bc is a mean vector, and \u03a3_k the covariance matrix of the Gaussian density for class k.", "The decision boundary between two classes, say k and l, is the hyperplane on which the probability of belonging to either class is the same. This implies that, on this hyperplane, the difference between the two densities (and hence also the log-odds ratio between them) should be zero.", "An important assumption in LDA is that the Gaussians for different classes share the same covariance matrix: the subscript k from \u03a3_k in the formula above can be dropped. This assumption comes in handy for the log-odds ratio calculation: it makes the normalization factors and some quadratic parts in the exponent cancel out. This yields a decision boundary between k and l that is linear in X:", "To calculate the density of the features, P(X|y=k), one just has to estimate the Gaussian parameters: the means \u03bc_k as the sample means and the covariance matrix \u03a3 as the empirical sample covariance matrix. Having calculated this, the probability of the target belonging to class k can be obtained from the Bayes rule:", "where P(y=k) is the prior probability of belonging to class k and can be estimated by the proportion of k-class observations in the sample.", "Note that LDA has no hyperparameters to tune. It takes just a few lines of code to apply it to the spam data.", "LDA\u2019s assumption that the Gaussians for different classes share the same covariance matrix is convenient, but might be incorrect for particular data. The left column in the picture below shows how LDA performs for data that indeed come from a multivariate Gaussians with a common covariance matrix (upper panel) versus when the data for different classes have different covariances (lower panel).", "Hence, one might want to relax the common covariance assumption. In this case, there is not one, but k covariance matrices to be estimated. If there are many features, this can lead to a dramatic increase in the number of parameters in the model. On the other hand, the quadratic terms in the Guassians\u2019 exponents do not cancel out anymore and the decision boundaries are quadratic in X, giving the model more flexibility: see the picture above. This approach is referred to as Quadratic Discriminant Analysis (QDA).", "Thanks to scikit-learn, the Python implementation of QDA is as easy as that of LDA.", "Just like linear models for regression can be regularized to improve accuracy, so can linear classifiers. One can introduce a shrinking parameter \u03b1 that shrinks the separate covariance matrices of QDA towards a common LDA matrix:", "The shrinkage parameter can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches. The best value of \u03b1 can be chosen based on cross-validation. To do this in Python, we need to pass the shrinkage argument to the LDA function, as well as specify the computation algorithm to be least squares, as other computation methods do not support shrinkage.", "Another approach to linear classification is the logistic regression model, which, despite its name, is a classification rather than a regression method. Logistic regression models the probabilities of an observation belonging to each of the K classes via linear functions, ensuring these probabilities sum up to one and stay in the (0, 1) range. The model is specified in terms of K-1 log-odds ratios, with an arbitrary class chosen as reference class (in this example it is the last class, K). Consequently, the difference between log-probabilities of belonging to a given class and to the reference class is modeled linearly as", "where G stands for the true, observed class. From here, the probabilities of an observation belonging to each of the classes can be calculated as", "which clearly shows that all class probabilities sum up to one.", "Logistic regression models are typically estimated by maximum likelihood, which is taken care of by scikit-learn. Just like linear models for regression can be regularized to improve accuracy, so can logistic regression. In fact, L2 penalty is the default setting in scikit-learn. It also supports L1 and Elastic Net penalties (to read more on these, check out the link above), but not all of them are supported by all solvers. Scikit-learn\u2019s logistic regression documentation describes it in detail.", "Although logistic regression is mostly used as an inference tool in tasks where the goal is to understand the role of input variables in explaining the outcome (it produces easily interpretable coefficients, just like linear regression does), it can also prove to be of significant predictive power, as the example below demonstrates.", "This article discussed a couple of linear classifiers:", "Thanks for reading! I hope you have learned something useful that will benefit your projects \ud83d\ude80", "If you liked this post, try one of my other articles. Can\u2019t choose? Pick one of these:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer & Data Science Instructor | Top Writer in AI & Statistics | michaloleszak.com | Book 1:1 @ hiretheauthor.com/michal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe121135bd3bb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaloleszak.medium.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----e121135bd3bb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook"}, {"url": "https://scikit-learn.org/stable/modules/lda_qda.html", "anchor_text": "https://scikit-learn.org/stable/modules/lda_qda.html"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear models for regression can be regularized"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear models for regression can be regularized"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "Scikit-learn\u2019s logistic regression documentation"}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "A Comparison of Shrinkage and Selection Methods for Linear RegressionA detailed look at 7 popular shrinkage & selection methods.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/boost-your-grasp-on-boosting-acf239694b1", "anchor_text": "Boost your grasp on boostingDemystifying the famous competition-winning algorithm.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/model-selection-assessment-bb2d74229172", "anchor_text": "Model Selection & AssessmentGoing beyond the train-val-test splittowardsdatascience.com"}, {"url": "https://scikit-learn.org/stable/modules/lda_qda.html", "anchor_text": "https://scikit-learn.org/stable/modules/lda_qda.html"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e121135bd3bb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e121135bd3bb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----e121135bd3bb---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/classification-algorithms?source=post_page-----e121135bd3bb---------------classification_algorithms-----------------", "anchor_text": "Classification Algorithms"}, {"url": "https://medium.com/tag/python?source=post_page-----e121135bd3bb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----e121135bd3bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----e121135bd3bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe121135bd3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e121135bd3bb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e121135bd3bb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e121135bd3bb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e121135bd3bb--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://michaloleszak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38bf302f5b56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-classifiers-an-overview-e121135bd3bb&newsletterV3=c58320fab2a8&newsletterV3Id=38bf302f5b56&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}