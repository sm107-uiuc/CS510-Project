{"url": "https://towardsdatascience.com/ug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef", "time": 1683007257.064692, "path": "towardsdatascience.com/ug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef/", "webpage": {"metadata": {"title": "The Ultimate Guide to Video Object Detection | by Yu Tong Liu | Towards Data Science", "h1": "The Ultimate Guide to Video Object Detection", "description": "In the past decade, notable work has been done in the field of machine learning, especially in computer vision. From advanced classification algorithms such as Inception by Google to Ian Goodfellow\u2019s\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1602.08465.pdf", "anchor_text": "Seq-NMS", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1712.01111.pdf", "anchor_text": "An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos", "paragraph_index": 6}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Mobile_Video_Object_CVPR_2018_paper.pdf", "anchor_text": "Mobile Video Object Detection with Temporally-Aware Feature Maps", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1903.10172.pdf", "anchor_text": "Looking Fast and Slow: Memory-Guided Mobile Video Object Detection", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1804.02767.pdf", "anchor_text": "YOLOv3", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1905.00641.pdf", "anchor_text": "RetinaFace", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1810.10220.pdf", "anchor_text": "DSFD", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1612.01925.pdf", "anchor_text": "Flownet", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1804.05830.pdf", "anchor_text": "Towards High Performance", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1703.10025.pdf", "anchor_text": "flow-guided feature aggregation (FGDA)", "paragraph_index": 19}], "all_paragraphs": ["In the past decade, notable work has been done in the field of machine learning, especially in computer vision. From advanced classification algorithms such as Inception by Google to Ian Goodfellow\u2019s pioneering work on Generative Adversarial Networks to generate data from noises, multiple fields have been tackled by the many devoted researchers all around the world. Interestingly, in the first half of the decade, the most pioneering work in the field of computer vision have mostly tackled image processing such as classification, detection, segmentation and generation, while the video processing field has been less deeply explored.", "One clear reason for the slight imbalance is because a video is essentially a sequence of images (frames) together. However, this definition cannot encapsulate the whole image of what video processing is, and that is because video processing adds a new dimension to the problem: the temporal dimension. Videos are not only a sequence of images, it is rather a sequence of RELATED images. Though it seems like a minimal difference, researchers are able to exploit this dimension in a multitude of ways that do not apply to single images. Furthermore, due to the complexity of video data (size, related annotations) and the expensive computation of training and inference, it has been more difficult to break through in this field. Recently, however, with the release of ImageNet VID and other massive video datasets during the second half of the decade, more and more video related research papers have surfaced. In this guide, we will mostly explore the researches that have been done in video detection, more precisely, how researchers are able to explore the temporal dimension.", "The first methods that surfaced were modifications applied to the post-processing step of an object detection pipeline. That is because it requires less infrastructure and demands no changes to the architecture of the model. The post-processing methods would still be a per-frame detection process, and therefore have no performance boost (could take slightly longer to process). However, it can achieve a sizeable improvement in accuracy.", "A notable method is Seq-NMS (Sequence Non-Maximal Suppression) that applies modification to detection confidences based on other detections on a \u201ctrack\u201d via dynamic programming. For example, weaker predictions of a positive subject can be caused due to occlusion, motion blur or other defects, but since it will be present in the \u201ctrack\u201d (overlap criterion) extracted from previous frames, the confidence will be boosted. This will effectively minimize the number of wrong detections between frames or random jumping detections, and stabilize the output result.", "From the graph above, the accuracy has been improved a relevant amount: The absolute improvements in mAP (%) using Seq-NMS relatively to single image NMS has increased more than 10% for 7 classes have higher than 10% improvement, while only two classes show decreased accuracy. Though this work was one of the initial works towards better video detection, it did not prove to be the best both in terms of accuracy and performance. However, the visible benefit is that this method does not necessitate training itself and acts more as an add-on that could be plugged in any object detector.", "The first natural instinct of a developer that has experience with image classification, for example, would be thinking about some sort of 3D convolution, based on the 2D convolution that is done on images. The likelihood of such architecture is plausible: iterating through n frames as inputs to the model and output sequential detections on consecutive frames. This is definitely a potential direction for detection as it can extract low-level features for spatio-temporal data, but a Convolutional Neural Network with 3D convolutions has mostly been proven to be useful and fruitful when it comes to processing 3D images such as on the 3D MNIST or MRI scans. That is why these models are more of a breakthrough in the medical imaging field and less relevant for video detection.", "Nonetheless, one example of a research paper that explores using 3D convolution on video processing is An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos. In the research paper, a video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3D CNN features. The tube proposals of different clips are then linked together and spatio-temporal action detection is performed using these linked video proposals. Though the paper mainly talks about segmentation and action detection, a derivative of the architecture could be trained to perform object detection.", "When it comes to performance, due to the high volume of computation with multi-dimensional matrices, the processing time cannot be as fast as real time (30 fps or higher) at the current state. Further improvement and research in this field can change the direction, but the difficulty to extend the performance of 3D convolution is not an easy task. When it comes to accuracy, I believe it can definitely be affected positively. The stability, as well as the precision of the detections, can be improved by the 3D convolution as the architecture can effectively leverage the temporal dimension altogether (aggregation of features between frames). However, it is currently just a speculation based on other state-of-the-art 3D convolutional models. There has yet to be a research paper that goes in depth with video detection.", "For others that have more experience with sequential data, one might incline to think about using a recurrent neural network such as LSTM. RNN are special types of networks that were created to handle sequential including temporal data. A field that has greatly benefited from this architecture is that of natural language processing. For example, AWD-LSTM is shown to perform on par with the state-of-the-art BERT transformer model while having a lot less parameters. Then, does it apply to video detection where frames are literally sequential? There have been quite some advances with the likes of Mobile Video Object Detection with Temporally-Aware Feature Maps and Looking Fast and Slow: Memory-Guided Mobile Video Object Detection.", "In the former, the paper combines fast single-image object detection with convolutional long short term memory (LSTM) layers called Bottleneck-LSTM to create an interweaved recurrent-convolutional architecture. The LSTM layer reduces computational cost while still refine and propagate feature maps across frames. The paper is designed to run in real-time on low-powered mobile and embedded devices achieving 15 fps on a mobile device.", "In the latter, the researchers propose to exploit the \u201cgist\u201d (rich representation of a complex environment in a short period of time) of a scene by relying on relevant prior knowledge which is inspired by how humans are able of recognize and detect objects. The architecture of the model is by interleaving conventional feature extractors with lightweight ones which only need to recognize the gist of the scene (minimal computation). This effectively creates a long term memory for the architecture from a key frame that captures the \u201cgist\u201d which guides the small network on what to detect. The paper also incorporates reinforcement learning algorithms to achieve an adaptive inference policy. The paper offers promising results such as 70 fps on a mobile device while still achieving state-of-the-art results for small neural networks on ImageNet VID.", "One key takeaway is that the architecture is end-to-end meaning that it takes an image and outputs the masked data and training needs to be done on the whole architecture. Because we are dealing with video data, the model will need to be trained on a massive amount of data.", "Why can\u2019t we use image object detectors on videos? Well, we can. Another possible way of processing video detection would be by applying state-of-the-art image detectors such as YOLOv3 or face detectors like RetinaFace and DSFD to every frame of a video file. Every single frame will be used as input to the model and the video results can be as accurate as their average precision on images. However, directly applying these detectors on every single frame of a video file faces challenges from two aspects:", "Therefore, applying the detectors on every single file is not an efficient method of tackling the video detection challenge. However, by exploring the temporal dimension of a video, there are different possible methods that we can implement to tackle one or both of the issues.", "Optical Flow Estimation is a method of estimating the apparent motion of objects between two frames of a video caused by either the camera (background) or the movement of a subject. The output is usually a 2D vector field where each vector represents the displacement vector of a pixel from the first frame to the second frame.", "Optical Flow has been a field of study in computer vision that was explored since the 1980s that has recently resurfaced as an interesting field in deep learning pioneered by Flownet. Before that, the original methods were differential For example, the Lucas-Kannade method assumes that the flow is essentially constant in a local neighbourhood of the pixel under consideration, and solves the basic optical flow equations for all the pixels in that neighbourhood, by the least squares criterion. But with new advances and new optical flow datasets like Sintel, more and more architectures are surfacing, one faster and more accurate than the other.", "There are multiple architectures that can leverage this technology. For example, Towards High Performance and many others that use optical flow to establish correspondence across frames (sparse feature propagation). Optical flow is currently the most explored field to exploit the temporal dimension of video object detection, and so, for a reason. The results of optical flow are getting faster and more accurate.", "The architecture functions with the concept of a sparse key frame. Since an optical flow network can be relatively small, the processing time and computational power required for such networks are less than the object detectors. Therefore, the pipeline functions as a cycle of n frames. The first frame is called a key frame. This is the frame that gets detected by the object detector. Since, now, the detectors gives an accurate detection of all the subjects, the detections will be subject to the optical flow algorithms. After getting the displacement vectors, the detection of the next n-1 frames are known, and the cycle repeats.", "A method to improve accuracy in video detection is multi-frame feature aggregation. There are different ways of implementing it, but all revolve around one idea: densely computed per-frame detections while feature warping from neighboring frames to the current frame and aggregating with weighted averaging. The current frame will therefore benefit from the immediate frames as well as some further frames to get a better detection. This could then solve the issues with motion and cropped subjects from a video frame.", "One such example is the research paper flow-guided feature aggregation (FGDA). Flow-guided feature aggregation aggregates feature maps from nearby frames, which are aligned well through the estimated flow. The architecture is an end-to-end framework that leverages temporal coherence on a feature level.", "After introducing all these methods, we can aggregate the gist of the methods through a comparison table to help understand how the methods compare to a simple frame-by-frame method applied using an image detector:", "Though the methods presented are what is currently published, there is definitely much more research being done at the moment as video object detection become a much more accessible topic. Hopefully, with the upcoming conferences, more and more breakthrough can be observed. The hopes are up for the new decade starting in 2020 for better vision! Cheers!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F816a76073aef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----816a76073aef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yutong.liu?source=post_page-----816a76073aef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yutong.liu?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Yu Tong Liu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc9d0b8fa4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&user=Yu+Tong+Liu&userId=bdc9d0b8fa4e&source=post_page-bdc9d0b8fa4e----816a76073aef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral", "anchor_text": "Ibrahim Rifath"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1602.08465.pdf", "anchor_text": "link"}, {"url": "https://arxiv.org/pdf/1602.08465.pdf", "anchor_text": "Seq-NMS"}, {"url": "https://arxiv.org/pdf/1602.08465.pdf", "anchor_text": "link"}, {"url": "https://vcg.seas.harvard.edu/publications/parallel-separable-3d-convolution-for-video-and-volumetric-data-understanding", "anchor_text": "https://vcg.seas.harvard.edu/publications/parallel-separable-3d-convolution-for-video-and-volumetric-data-understanding"}, {"url": "https://arxiv.org/pdf/1712.01111.pdf", "anchor_text": "An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Mobile_Video_Object_CVPR_2018_paper.pdf", "anchor_text": "Mobile Video Object Detection with Temporally-Aware Feature Maps"}, {"url": "https://arxiv.org/pdf/1903.10172.pdf", "anchor_text": "Looking Fast and Slow: Memory-Guided Mobile Video Object Detection"}, {"url": "https://arxiv.org/pdf/1804.02767.pdf", "anchor_text": "YOLOv3"}, {"url": "https://arxiv.org/pdf/1905.00641.pdf", "anchor_text": "RetinaFace"}, {"url": "https://arxiv.org/pdf/1810.10220.pdf", "anchor_text": "DSFD"}, {"url": "http://sintel.is.tue.mpg.de/", "anchor_text": "http://sintel.is.tue.mpg.de/"}, {"url": "https://arxiv.org/pdf/1612.01925.pdf", "anchor_text": "Flownet"}, {"url": "https://arxiv.org/pdf/1804.05830.pdf", "anchor_text": "Towards High Performance"}, {"url": "https://arxiv.org/pdf/1611.07715.pdf", "anchor_text": "https://arxiv.org/pdf/1611.07715.pdf"}, {"url": "https://arxiv.org/pdf/1703.10025.pdf", "anchor_text": "https://arxiv.org/pdf/1703.10025.pdf"}, {"url": "https://arxiv.org/pdf/1703.10025.pdf", "anchor_text": "flow-guided feature aggregation (FGDA)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----816a76073aef---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----816a76073aef---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----816a76073aef---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/research?source=post_page-----816a76073aef---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&user=Yu+Tong+Liu&userId=bdc9d0b8fa4e&source=-----816a76073aef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&user=Yu+Tong+Liu&userId=bdc9d0b8fa4e&source=-----816a76073aef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----816a76073aef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F816a76073aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----816a76073aef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----816a76073aef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----816a76073aef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----816a76073aef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----816a76073aef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----816a76073aef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yutong.liu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yutong.liu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yu Tong Liu"}, {"url": "https://medium.com/@yutong.liu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc9d0b8fa4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&user=Yu+Tong+Liu&userId=bdc9d0b8fa4e&source=post_page-bdc9d0b8fa4e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8bfab13fecf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fug-vod-the-ultimate-guide-to-video-object-detection-816a76073aef&newsletterV3=bdc9d0b8fa4e&newsletterV3Id=b8bfab13fecf&user=Yu+Tong+Liu&userId=bdc9d0b8fa4e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}