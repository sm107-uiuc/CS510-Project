{"url": "https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665", "time": 1682994551.860209, "path": "towardsdatascience.com/soft-actor-critic-demystified-b8427df61665/", "webpage": {"metadata": {"title": "Soft Actor-Critic Demystified. An intuitive explanation of the theory\u2026 | by Vaishak V.Kumar | Towards Data Science", "h1": "Soft Actor-Critic Demystified", "description": "Soft Actor-Critic, the new Reinforcement Learning Algorithm from the folks at UC Berkley has been making a lot of noise recently. The algorithm not only boasts of being more sample efficient than\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html", "anchor_text": "OpenAI\u2019s tutorial", "paragraph_index": 0}, {"url": "https://github.com/higgsfield/RL-Adventure-2", "anchor_text": "higgsfield\u2019s", "paragraph_index": 17}, {"url": "https://github.com/vaishak2future/sac/blob/master/sac.ipynb", "anchor_text": "The full code can be found here", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1801.01290", "anchor_text": "[1] T. Haarnoja et al. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1812.05905", "anchor_text": "[2] T. Haarnoja et al. 2018. Soft Actor-Critic Algorithms and Applications.", "paragraph_index": 33}], "all_paragraphs": ["Soft Actor-Critic, the new Reinforcement Learning Algorithm from the folks at UC Berkley has been making a lot of noise recently. The algorithm not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence. In this blog post, we\u2019ll dive deep into how the algorithm works and also implement it in PyTorch. This tutorial assumes that you are familiar with the problem specification and terminology of Reinforcement Learning. If you are not familiar with this or need a refresher, check out OpenAI\u2019s tutorial.", "Before we begin, let\u2019s just take a quick look at why we care.", "Not only does the Minotaur Robot learn in a really short time duration but it also learns to generalize to conditions that it hasn\u2019t seen during training! SAC thus brings us ever so close to using Reinforcement Learning in non-simulation environments for applications in robotics and other domains.", "Some of the most successful RL algorithms in recent years such as Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO) and Asynchronous Actor-Critic Agents (A3C) suffer from sample inefficiency. This is because they learn in an \u201con-policy\u201d manner, i.e. they need completely new samples after each policy update. In contrast, Q-learning based \u201coff-policy\u201d methods such as Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3PG) are able to learn efficiently from past samples using experience replay buffers. However, the problem with these methods is that they are very sensitive to hyperparameters and require a lot of tuning to get them to converge. Soft Actor-Critic follows in the tradition of the latter type of algorithms and adds methods to combat the convergence brittleness. Let\u2019s see how.", "SAC is defined for RL tasks involving continuous actions. The biggest feature of SAC is that it uses a modified RL objective function. Instead of only seeking to maximize the lifetime rewards, SAC seeks to also maximize the entropy of the policy. The term \u2018entropy\u2019 has a rather esoteric definition and many interpretations depending on the application but I\u2019d like to share an intuitive explanation here. We can think of entropy as how unpredictable a random variable is. If a random variable always takes a single value then it has zero entropy because it\u2019s not unpredictable at all. If a random variable can be any Real Number with equal probability then it has very high entropy as it is very unpredictable. Why do we want our policy to have high entropy? We want a high entropy in our policy to explicitly encourage exploration, to encourage the policy to assign equal probabilities to actions that have same or nearly equal Q-values, and also to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q function. Therefore, SAC overcomes the brittleness problem by encouraging the policy network to explore and not assign a very high probability to any one part of the range of actions.", "Now that we know what we are optimizing for, let us understand how we go about doing the optimization. SAC makes use of three networks: a state value function V parameterized by \u03c8, a soft Q-function Q parameterized by \u03b8, and a policy function \u03c0 parameterized by \u03d5. While there is no need in principle to have separate approximators for the V and Q functions which are related through the policy, the authors say that in practice having separate function approximators help in convergence. So we need to train the three function approximators as follows:", "Don\u2019t get scared by this long error formula. All it\u2019s saying is that across all the states that we sample from our experience replay buffer, we need to decrease the squared difference between the prediction of our value network and the expected prediction of the Q function plus the entropy of the policy function \u03c0 (measured here by the negative log of the policy function).", "We\u2019ll use the below approximation of the derivative of the above objective to update the parameters of the V function:", "2. We train the Q network by minimizing the following error:", "Minimizing this objective function amounts to the following: For all (state, action) pairs in the experience replay buffer, we want to minimize the squared difference between the prediction of our Q function and the immediate (one time-step) reward plus the discounted expected Value of the next state. Note that the Value comes from a Value function parameterized by \u03c8 with a bar on top of it. This is an additional Value function called the target value function. We\u2019ll get into why we need this but for now, don\u2019t worry about it and just think of it as a Value function that we\u2019re training.", "We\u2019ll use the below approximation of the derivative of the above objective is to update the parameters of the Q function:", "3. We train the Policy network \u03c0 by minimizing the following error:", "This objective function looks complex but it\u2019s actually saying something very simple. The DKL function that you see inside the expectation is called the Kullback-Leibler Divergence. I highly recommend that you read up on the KL divergence since it shows up a lot in deep learning research and applications these days. For the purposes of this tutorial, you can interpret it as how different the two distributions are. So, this objective function is basically trying to make the distribution of our Policy function look more like the distribution of the exponentiation of our Q Function normalized by another function Z.", "In order to minimize this objective, the authors use something called the reparameterization trick. This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors. The policy is now parameterized as follows:", "The epsilon term is a noise vector sampled from a Gaussian distribution. We will explain it more in the implementation section.", "Now, we can express the objective function as follows:", "The normalizing function Z is dropped since it does not depend on the parameter \u03d5. An unbiased estimator for the gradient of the above objective is given as follows:", "Now that we understand the theory behind the algorithm, let\u2019s implement a version of it in Pytorch. My implementation is modeled on higgsfield\u2019s but with a critical change: I\u2019ve used the reparameterization trick which makes training converge better due to lower variance. First off, let\u2019s look at the main body of the algorithm so that we understand what is happening at a high level so that we can then dive into the details of individual components.", "First off, we initialize an OpenAI Gym environment in which our agent will play the Reinforcement Learning game. We store information about the dimension of the observations of the environment, the dimension of the action space, and then, set the hyperparameter of how many hidden layers we want in our networks. Then we initialize the three networks that we want to train along with a target V network. You will note that we have two Q networks. We maintain two Q networks to solve the problem of overestimation of Q-values. To combat this we maintain two Q networks and use the minimum of the two to do our policy and V function updates.", "Now, it\u2019s time to explain the whole target V network business. The use of target networks is motivated by a problem in training V network. If you go back to the objective functions in the Theory section, you will find that the target for the Q network training depends on the V Network and the target for the V Network depends on the Q network (this makes sense because we are trying to enforce Bellman Consistency between the two functions). Because of this, the V network has a target that\u2019s indirectly dependent on itself which means that the V network\u2019s target depends on the same parameters we are trying to train. This makes training very unstable.", "The solution is to use a set of parameters which comes close to the parameters of the main V network, but with a time delay. Thus we create a second network which lags the main network called the target network. There are two ways to go about this. The first way is to have the target network copied over from the main network regularly after a set number of steps. The other way is to update the target network by Polyak averaging (a kind of moving averaging) itself and the main network. In this implementation, we use Polyak averaging. We initialize the main and target V networks to have the same parameters.", "We have nested loops here. The outer loop initializes the environment for the beginning of the episode. The inner loop is for the individual steps within an episode. In the inner loop, we sample an action from the Policy network \u2014 or randomly from the action space for the first few time steps\u2014 and record the state, action, reward, next state, and done \u2014 a variable indicating if we entered the terminal state of the episode \u2014 to the replay buffer. We do this till we have a minimum number of observations in the buffer. Then, we do network updates in each run of the inner loop after recording to the buffer.", "The following is the code for the network update:", "First, we update the two Q function parameters by reducing the MSE between the predicted Q value for a state-action pair and its corresponding (reward + (1 \u2014 done) * gamma * target_value).", "For the V network update, we take the minimum of the two Q values for a state-action pair and subtract from it the Policy\u2019s log probability of selecting that action in that state. Then we decrease the MSE between the above quantity and the predicted V value of that state.", "Then, we update the Policy parameters by reducing the Policy\u2019s log probability of choosing an action in a state log(\u03c0(S)) minus the predicted Q-value of that state-action pair. Note here that in this loss, the predicted q value is composed of the policy : Q(S, \u03c0(S)). This is important because it makes the term dependent on the Policy parameters \u03d5.", "Lastly, we update the Target Value Network by Polyak averaging it with the Main Value Network.", "Next, let\u2019s take a quick look at the network structures:", "The Q and V networks are pretty standard so let\u2019s take a closer look at the Policy network. The policy has two outputs : the mean and the log standard deviation \u2014 we use log standard deviations since their exponential always gives a positive number. The log standard deviation is clamped to be in a sane region. Then to get the action, we use the reparameterization trick.", "For this, we sample some noise from a Standard Normal distribution and multiply it with our standard deviation, and then add the result to the mean. Then this number is activated with a tanh function to give us our action. Finally, the log probability is calculated using an approximator of the log likelihood of tanh(mean + std* z).", "That\u2019s all for the important implementation details! The full code can be found here. Make sure you run it and play around with the different hyperparameters to understand how they affect the training. I hope this has been helpful. Please send me any comments, corrections or links to any cool projects that you make using SAC!", "UPDATE: Tuomas Haarnoja let me know over email that there is a new version of the algorithm that uses only a Q function and disposes of the V function. It also adds automatic discovery of the weight of the entropy term called the \u2018temperature\u2019. You can check out the new paper in [2].", "[1] T. Haarnoja et al. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "[2] T. Haarnoja et al. 2018. Soft Actor-Critic Algorithms and Applications.", "Trying to build thinking machines that can help solve humanity\u2019s biggest problems"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8427df61665&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@vaishakvk?source=post_page-----b8427df61665--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Vaishak V.Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61d2676ad14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=post_page-61d2676ad14----b8427df61665---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8427df61665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----b8427df61665---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8427df61665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=-----b8427df61665---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html", "anchor_text": "OpenAI\u2019s tutorial"}, {"url": "https://github.com/higgsfield/RL-Adventure-2", "anchor_text": "higgsfield\u2019s"}, {"url": "https://github.com/vaishak2future/sac/blob/master/sac.ipynb", "anchor_text": "The full code can be found here"}, {"url": "https://arxiv.org/abs/1801.01290", "anchor_text": "[1] T. Haarnoja et al. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"}, {"url": "https://arxiv.org/abs/1812.05905", "anchor_text": "[2] T. Haarnoja et al. 2018. Soft Actor-Critic Algorithms and Applications."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b8427df61665---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----b8427df61665---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/soft-actor-critic?source=post_page-----b8427df61665---------------soft_actor_critic-----------------", "anchor_text": "Soft Actor Critic"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b8427df61665---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8427df61665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----b8427df61665---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8427df61665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----b8427df61665---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8427df61665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=post_page-----b8427df61665--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61d2676ad14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=post_page-61d2676ad14----b8427df61665---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff850dd15a1b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&newsletterV3=61d2676ad14&newsletterV3Id=f850dd15a1b9&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----b8427df61665---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Written by Vaishak V.Kumar"}, {"url": "https://medium.com/@vaishakvk/followers?source=post_page-----b8427df61665--------------------------------", "anchor_text": "199 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61d2676ad14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&user=Vaishak+V.Kumar&userId=61d2676ad14&source=post_page-61d2676ad14----b8427df61665---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff850dd15a1b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsoft-actor-critic-demystified-b8427df61665&newsletterV3=61d2676ad14&newsletterV3Id=f850dd15a1b9&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----b8427df61665---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Vaishak V.Kumar"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Geometric Deep Learning for Pose EstimationTheory and Pytorch Implementation Tutorial to find Object Pose from Single Monocular Image"}, {"url": "https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "\u00b76 min read\u00b7May 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6af45da05922&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-for-pose-estimation-6af45da05922&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----6af45da05922----0-----------------clap_footer----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=author_recirc-----b8427df61665----0---------------------dfc95229_3238_4947_8c89_8a443f711117-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6af45da05922&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-for-pose-estimation-6af45da05922&source=-----b8427df61665----0-----------------bookmark_preview----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b8427df61665----1---------------------dfc95229_3238_4947_8c89_8a443f711117-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----b8427df61665----1-----------------bookmark_preview----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b8427df61665----2---------------------dfc95229_3238_4947_8c89_8a443f711117-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b8427df61665----2-----------------bookmark_preview----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ml-101-linear-regression-bea0f489cf54?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Vaishak V.Kumar"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/ml-101-linear-regression-bea0f489cf54?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "ML 101: Linear RegressionThe simplest but also one of the most effective ML methods"}, {"url": "https://towardsdatascience.com/ml-101-linear-regression-bea0f489cf54?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": "\u00b75 min read\u00b7May 28, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbea0f489cf54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-101-linear-regression-bea0f489cf54&user=Vaishak+V.Kumar&userId=61d2676ad14&source=-----bea0f489cf54----3-----------------clap_footer----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ml-101-linear-regression-bea0f489cf54?source=author_recirc-----b8427df61665----3---------------------dfc95229_3238_4947_8c89_8a443f711117-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbea0f489cf54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-101-linear-regression-bea0f489cf54&source=-----b8427df61665----3-----------------bookmark_preview----dfc95229_3238_4947_8c89_8a443f711117-------", "anchor_text": ""}, {"url": "https://medium.com/@vaishakvk?source=post_page-----b8427df61665--------------------------------", "anchor_text": "See all from Vaishak V.Kumar"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----0-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----b8427df61665----0-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----b8427df61665----1-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b8427df61665----0---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----b8427df61665----0-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----b8427df61665----1---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----b8427df61665----1-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b8427df61665----2---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----b8427df61665----2-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----3-----------------clap_footer----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----b8427df61665----3---------------------c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----b8427df61665----3-----------------bookmark_preview----c153eb09_7a26_4087_9f8f_8be9afcaeeb1-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8427df61665--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----b8427df61665--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}