{"url": "https://towardsdatascience.com/data-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84", "time": 1683000601.0231838, "path": "towardsdatascience.com/data-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84/", "webpage": {"metadata": {"title": "Build your own dataset using Scrapy | by Sagun Shrestha | Towards Data Science", "h1": "Build your own dataset using Scrapy", "description": "One of the common problems that almost every data scientist faces is collecting data. While there are a lot of websites where you can find datasets as mentioned in this post, it\u2019s always not enough\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/top-10-great-sites-with-free-data-sets-581ac8f6334", "anchor_text": "this post", "paragraph_index": 0}, {"url": "https://scrapy.org/", "anchor_text": "Scrapy", "paragraph_index": 0}, {"url": "https://github.com/sagunsh/sofifa", "anchor_text": "github repo", "paragraph_index": 1}, {"url": "https://sofifa.com/?col=oa&sort=desc", "anchor_text": "sofifa", "paragraph_index": 2}, {"url": "https://docs.anaconda.com/anaconda/install/windows/", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://doc.scrapy.org/en/latest/topics/architecture.html", "anchor_text": "Scrapy architecture", "paragraph_index": 11}, {"url": "https://github.com/sagunsh/sofifa", "anchor_text": "github repo", "paragraph_index": 41}, {"url": "https://twitter.com/sagunsh", "anchor_text": "Twitter", "paragraph_index": 45}], "all_paragraphs": ["One of the common problems that almost every data scientist faces is collecting data. While there are a lot of websites where you can find datasets as mentioned in this post, it\u2019s always not enough. Sometimes building your own dataset is the way to go. And web on the other hand, although messy is a massive source of data, we just need to know how to mine it efficiently. In this article, I will walk you through the process of scraping data from web using Scrapy, a powerful web crawling framework written in Python.", "If you want to jump right into the code, head over to this github repo.", "FIFA 20 player ratings is out and being a keen football follower, I find such data fascinating. You can find the data here on sofifa. We will be scraping the data and saving them in a csv file using Scrapy.", "I will be using Python 3 and Scrapy 1.7 for this post. Installing scrapy is fairly simple for Linux and Mac via pip using the following command:", "Windows users will need to install Anaconda. You can find the installation instructions here. After installing Anaconda, open Anaconda prompt and type the following command to install Scrapy:", "Once we have Scrapy in our system, the next step is to create a project. Open up terminal/anaconda prompt and type", "By default, Scrapy will create a bunch of files and directories inside fifa20.", "Inside fifa20 , you will find another fifa20 folder which contains all of our code. The spiders directory contains Scrapy spiders responsible for extracting data from the web. Generally, each spider is responsible for scraping one particular website. A scrapy project can have multiple spiders. For e.g. if you are creating a price monitoring system, you may want to scrape multiple websites like amazon, walmart, newegg, etc. Each of these websites will have a dedicated spider to handle the scraping logic.", "settings.py contains settings for the projects such as how many concurrent threads to run, delays between 2 requests, which middleware or pipeline to use.", "For this project, we will set a real user agent. Go ahead and change these variables in settings.py.", "Your settings.py file will look something like this", "Check out Scrapy architecture to explore more about middlewares, items, pipelines and other settings.", "To create a spider, navigate into the outerfifa20 project directory and type", "This will create a spider inside the spiders directory named sofifa. This is what the file looks like", "Let\u2019s try to understand what is going on here.", "We will use the following url as our starting url", "Don\u2019t worry about the messy looking url, it was generated by selecting certain columns that we want in our final dataset.", "Scrapy shell is a useful command line tool to analyze the structure of the web page and figure out desired xpaths and selectors. Type the following command to get started", "You will see a lot of logs and messages floating around. They are nothing but your project\u2019s settings. What the above command does for us is downloads the HTML content of the given url and will be accessible via response object. To check, type", "If you want to see what the page looks like in a browser, type", "You will be able to view the page in the browser.", "We have the option to use either css selector and/or xpath in Scrapy. If you are new to the concept of css and xpath, check out these tutorials to understand how it works.", "Now let\u2019s inspect the web page and get the data.", "We can see all the data inside a table tag and each player information is inside tbody -> tr. Looping through all the tr tag will help us get close to the data.", "This will print a list of country and player name side by side. Let\u2019s try to understand this snippet.", "For the sake of simplicity, let\u2019s test everything with one player\u2019s data and use it in our script. So in the shell, type", "The difference between using a > and a space while traversing through the tags is", "Let\u2019s just get the first position which is the primary position the player plays.", "extract() returns a list while get() or extract_first() returns the first element", "Similarly, we can get other fields as", "Scrolling to the bottom of the page, we can see a next page link. Each page has 60 players and there are around 18K players listed in this website. So we need to get the pagination link and scrape data until that page where there is no next page which is the last page.", "The above line prints an incomplete url which is a relative url which is a common practice in modern websites. To complete it, scrapy provides a method urljoin.", "This will print the complete url. Now click on the next page and try verifying if our css works for all page. You will find 2 pagination links: Previous and Next and the selector we used will get the previous page link.", "Solution: Use Xpath. Till now, we were able to distinguish the web element using class but that won\u2019t be the case always. The only difference between these 2 pagination links are the text content (Previous and Next). Xpath allows us to precisely select a web element based on text content.", "Again we can use response.urljoin to complete the url. So our code will be something like this.", "What this does is check if the next_page element is present or not. Then it sends a request to that page by joining the url and calls the parse method again. parse is the default callback so you can remove the callback argument if you want. Now the crawler recursively crawls each page while yielding the data concurrently.", "This is what our final code will look like.", "Scrapy has a builtin feature to dump the data directly into json or csv file. Remember the name variable in our SofifaSpider class, we will use that to invoke the spider.", "If you want the data in json", "Scrapy provides a lot of features right out of the box that makes is easy to write scrapers and collect data. We saw some of them in this article. Under 40 lines of code, we managed to create a crawler that will scrape over 18K data in less than 30 minutes.", "You can check out other advanced settings such as using proxies, adding delays, changing concurrent requests and more.", "You can download the code and data from this github repo.", "Every data science project needs data and web is an enormous source of data. Data Collection is the first step in any data science project and the outcome of your analysis depends on the quality of data you have or in simple terms Garbage In Garbage Out. So instead of solely focussing on which model to use or what kind of visualization to produce, if we spent some time on the boring part i.e. collection, cleaning and preparation, we can improve our resulting outcome.", "Next, we will see how to clean the data and make it ready for analysis.", "Here are some of the learning materials that you may find useful:", "Let me know if you have any confusion in the code or need help deducing xpath and css selectors. Also if you have any suggestions on how to analyze these data in the next steps, let me know in the comments. You can find me on Twitter too.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I read a lot and write sometimes"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffe0ad9963d84&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sagunshrestha?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagunshrestha?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Sagun Shrestha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6ca23303391&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&user=Sagun+Shrestha&userId=d6ca23303391&source=post_page-d6ca23303391----fe0ad9963d84---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/top-10-great-sites-with-free-data-sets-581ac8f6334", "anchor_text": "this post"}, {"url": "https://scrapy.org/", "anchor_text": "Scrapy"}, {"url": "https://github.com/sagunsh/sofifa", "anchor_text": "github repo"}, {"url": "https://sofifa.com/?col=oa&sort=desc", "anchor_text": "sofifa"}, {"url": "https://docs.anaconda.com/anaconda/install/windows/", "anchor_text": "here"}, {"url": "https://doc.scrapy.org/en/latest/topics/architecture.html", "anchor_text": "Scrapy architecture"}, {"url": "http://sofifa.com/'", "anchor_text": "http://sofifa.com/'"}, {"url": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir", "anchor_text": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir"}, {"url": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir", "anchor_text": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir"}, {"url": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir'", "anchor_text": "https://sofifa.com/?col=oa&sort=desc&showCol%5B%5D=ae&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=jt&showCol%5B%5D=vl&showCol%5B%5D=wg&showCol%5B%5D=rc&showCol%5B%5D=wk&showCol%5B%5D=sk&showCol%5B%5D=aw&showCol%5B%5D=dw&showCol%5B%5D=ir'"}, {"url": "https://docs.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html", "anchor_text": "https://docs.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html"}, {"url": "https://docs.scrapy.org/en/xpath-tutorial/topics/selectors.html", "anchor_text": "https://docs.scrapy.org/en/xpath-tutorial/topics/selectors.html"}, {"url": "https://www.guru99.com/xpath-selenium.html", "anchor_text": "https://www.guru99.com/xpath-selenium.html"}, {"url": "http://scrapingauthority.com/2016/09/07/xpath-and-css-selectors/", "anchor_text": "http://scrapingauthority.com/2016/09/07/xpath-and-css-selectors/"}, {"url": "https://github.com/sagunsh/sofifa", "anchor_text": "github repo"}, {"url": "https://docs.scrapy.org/en/latest/intro/tutorial.html", "anchor_text": "https://docs.scrapy.org/en/latest/intro/tutorial.html"}, {"url": "https://learn.scrapinghub.com/scrapy/", "anchor_text": "https://learn.scrapinghub.com/scrapy/"}, {"url": "http://scrapingauthority.com/", "anchor_text": "http://scrapingauthority.com/"}, {"url": "https://www.datacamp.com/community/tutorials/making-web-crawlers-scrapy-python", "anchor_text": "https://www.datacamp.com/community/tutorials/making-web-crawlers-scrapy-python"}, {"url": "https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/", "anchor_text": "https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/"}, {"url": "https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3", "anchor_text": "https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3"}, {"url": "https://twitter.com/sagunsh", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/python?source=post_page-----fe0ad9963d84---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/scrapy?source=post_page-----fe0ad9963d84---------------scrapy-----------------", "anchor_text": "Scrapy"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----fe0ad9963d84---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fe0ad9963d84---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-mining?source=post_page-----fe0ad9963d84---------------data_mining-----------------", "anchor_text": "Data Mining"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&user=Sagun+Shrestha&userId=d6ca23303391&source=-----fe0ad9963d84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&user=Sagun+Shrestha&userId=d6ca23303391&source=-----fe0ad9963d84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffe0ad9963d84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fe0ad9963d84---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fe0ad9963d84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagunshrestha?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagunshrestha?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sagun Shrestha"}, {"url": "https://medium.com/@sagunshrestha/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "90 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6ca23303391&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&user=Sagun+Shrestha&userId=d6ca23303391&source=post_page-d6ca23303391--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F860eb2132aab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84&newsletterV3=d6ca23303391&newsletterV3Id=860eb2132aab&user=Sagun+Shrestha&userId=d6ca23303391&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}