{"url": "https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751", "time": 1682995243.703259, "path": "towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751/", "webpage": {"metadata": {"title": "Using word2vec to Analyze News Headlines and Predict Article Success | by Charlene Chambliss | Towards Data Science", "h1": "Using word2vec to Analyze News Headlines and Predict Article Success", "description": "Word embeddings are a powerful way to represent the latent information contained within words, as well as within documents (collections of words). Using a dataset of news article titles, which\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://nbviewer.jupyter.org/", "anchor_text": "nbviewer", "paragraph_index": 2}, {"url": "https://github.com/RaRe-Technologies/gensim-data", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "this excellent, interactive distill.pub article", "paragraph_index": 6}, {"url": "https://github.com/Phlya/adjustText", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://stackoverflow.com/questions/45234310/doc2vec-worse-than-mean-or-sum-of-word2vec-vectors", "anchor_text": "(and sometimes even more effective)", "paragraph_index": 14}, {"url": "https://github.com/sdimi/average-word2vec/blob/master/notebook.ipynb", "anchor_text": "has created a series of functions", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Unix_time", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv", "anchor_text": "this Kaggle article", "paragraph_index": 34}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)", "anchor_text": "l1 norm", "paragraph_index": 45}, {"url": "https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb", "anchor_text": "here", "paragraph_index": 51}, {"url": "https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms", "anchor_text": "here", "paragraph_index": 51}, {"url": "https://www.linkedin.com/in/charlenechambliss/", "anchor_text": "https://www.linkedin.com/in/charlenechambliss/", "paragraph_index": 53}], "all_paragraphs": ["Word embeddings are a powerful way to represent the latent information contained within words, as well as within documents (collections of words). Using a dataset of news article titles, which included features on source, sentiment, topic, and popularity (# shares), I set out to see what we could learn about articles\u2019 relationships to one another through their respective embeddings.", "The goals of the project were:", "The entire notebook is hosted here, using nbviewer.", "Next, we need to load in the pre-trained word2vec model. You can find several such models here. Since this is a news dataset, I used the Google News model, which was trained on about 100 billion words (wow).", "So the model will generate 300-dimensional word vectors, and all we have to do to create a vector is to pass it through the model. Each vector looks like this:", "word2vec (understandably) can\u2019t create a vector from a word that\u2019s not in its vocabulary. Because of this, we need to specify \u201cif word in model.vocab\u201d when creating the full list of word vectors.", "Next, we\u2019re going to squish (read: do dimensionality reduction on) these word vectors using t-SNE, to see if any patterns emerge. If you\u2019re not familiar with t-SNE and its interpretations, check out this excellent, interactive distill.pub article on t-SNE.", "It\u2019s important to play around with the parameters for t-SNE, as different values can produce very different results. I tested several values between 0 and 100 for perplexity, and found that it produced roughly the same shape each time. I tested several learning rates between 20 and 400 as well, and decided to leave the learning rate at its default (200).", "For the sake of visibility (and processing time), I used 400 word vectors instead of the full set of 20,000 or so.", "Now we\u2019re ready to plot our reduced array of word vectors. I used adjust_text to intelligently push words apart, for improved readability:", "If you\u2019re interested in trying out adjust_text for your own plotting needs, you can find it here. Be sure to import it using the camelcase adjustText, and please note that adjustText is currently not compatible with matplotlib 3.0 or higher.", "It\u2019s encouraging to see that even when the vector embeddings have been reduced to 2 dimensions, we see certain items clustering together. For example, we have months in the left/upper left, we have corporate finance terms near the bottom, and we have more generic, non-topical words (like \u2018full\u2019, \u2018really\u2019, \u2018slew\u2019) in the middle.", "Note that if we were to run the t-SNE again with different parameters, we may observe some similarities to this result, but we\u2019re not guaranteed to see the exact same patterns. t-SNE is not deterministic. Relatedly, tightness of clusters and distances between clusters are not always meaningful. It is meant primarily as an exploratory tool, rather than as a decisive indicator of similarity.", "We\u2019ve gotten a sense of how word embeddings work as applied to this dataset. Now we can move on to some more interesting ML applications: finding titles that cluster together, and seeing what patterns emerge.", "Instead of using Doc2Vec, which does not have pre-trained models available and so would require a lengthy training process, we can use a simpler (and sometimes even more effective) trick: averaging the embeddings of the word vectors in each document. In our case, a document refers to a title.", "We\u2019ll need to redo the preprocessing step to keep titles intact \u2014 as we\u2019ll see, this is somewhat more involved than splitting on words. Thankfully, Dimitris Spathis has created a series of functions that I found to work perfectly for this precise use case. Thanks, Dimitris!", "Now we\u2019ll use these to do the processing:", "Now that we\u2019ve successfully created our array of document vectors, let\u2019s see if we can get similarly interesting results when plotting them with t-SNE.", "Pretty interesting! We can see that the t-SNE has collapsed the document vectors into a dimensional space where the documents are spread out based on whether their content has more to do with countries, world leaders, and foreign affairs, or has more to do with technology companies.", "Let\u2019s now explore article popularity. Consensus has it that the more sensationalized or clickbait-y an article\u2019s title is, the more likely it is to be shared, right? Next, we\u2019ll see if there\u2019s evidence for that in this particular dataset.", "First, we need to drop all articles for which we don\u2019t have a popularity measurement or a source. Null measurements for popularity are represented in this data as -1.", "We still have 81,000 articles to work with, so let\u2019s see if we can find an association between sentiment and number of shares.", "It\u2019s a bit hard to make out whether there\u2019s any relationship here, since a few articles are significant outliers in terms of their share counts. Let\u2019s try log-transforming the x-axis to see if we can reveal any patterns. We\u2019ll also use a regplot, so seaborn will overlay a linear regression for each plot.", "Contrary to what we might expect (from our idea of highly emotional, clickbaity headlines), in this dataset we find no relationship between headline sentiment and article popularity as measured by number of shares.", "To get a clearer sense of how popularity looks on its own, let\u2019s make a final plot of log(Popularity) by platform.", "As our final segment of exploration, let\u2019s take a look at sentiment by itself. Does it seem to differ between publishers?", "The distributions look fairly similar, but it\u2019s a little hard to tell how similar when they\u2019re all on different plots. Let\u2019s try overlaying them all on one plot.", "We see that the sources\u2019 Sentiment distributions for article titles are very similar \u2014 it doesn\u2019t look like any one source is an outlier in terms of positive or negative titles. Instead, all 12 of the most common sources have distributions centered around 0 with modestly sized tails. But does that tell the full story? Let\u2019s take one more look at the numbers:", "We can see at a glance that WSJ has both the highest standard deviation and the largest range, with the lowest minimum sentiment compared to any other top source. This suggests that WSJ could be unusually negative in terms of its article titles. To verify this rigorously would require a hypothesis test, which is beyond the scope of this post, but it\u2019s an interesting potential finding and future direction.", "Our first task in preparing the data for modeling is to rejoin the document vectors with their respective titles. Thankfully, when we were preprocessing the corpus, we processed the corpus and titles_list simultaneously, so the vectors and the titles they represent will still match up. Meanwhile, in main_df, we have dropped all of the articles that had -1 popularity, so we'll need to drop the vectors that represent those article titles.", "Training a model on these enormous vectors as-is will not be possible on this computer, but we\u2019ll see what we can do with a little dimension reduction. I\u2019ll also engineer a new feature from publish date: \u201cDaysSinceEpoch\u201d, which is based on Unix time (read more here).", "As we can see, all of these articles were published within about 250 days of each other.", "Now we need to drop non-numeric and non-dummy columns so we can feed the data to a model. We\u2019ll also apply scaling to the DaysSinceEpoch feature, since it is wildly larger in magnitude compared to the reduced word vectors, sentiment, etc.", "Let\u2019s run a non-optimized XGBoost on the data and see how it works out-of-the-box.", "Underwhelming results, to say the least. Can we improve this performance with hyperparameter tuning? I\u2019ve pulled in and repurposed a hyperparameter tuning grid from this Kaggle article.", "According to xgb_grid, our best parameters were as follows:", "Try again with the new parameters:", "It\u2019s better by around 35,000, but I\u2019m not sure that\u2019s saying a whole lot. We might infer, at this point, that the data in its current state seems insufficient for this model to perform. Let\u2019s see if we can improve it with a little more feature engineering: we\u2019ll train some classifiers to separate the two main groups of articles: Duds (0 or 1 share) vs. Not Duds.", "The idea is that if we can give the regressor a new feature (the probability that the article will have extremely low shares), it may perform more favorably on predicting highly-shared articles, thus lowering the residual values for those articles and reducing mean squared error.", "From the log-transformed plots we made earlier, we can note that in general, there are 2 chunks of articles: 1 cluster at 0, and another cluster (the long tail) going from 1 onwards. We can train a few classifiers to identify whether the article will be a \u201cdud\u201d (be in the 0\u20131 shares bin), and then use the predictions of those models a feature for the final regressor, which will predict probability. This is called model stacking.", "Now that we have our dud feature made, we\u2019ll initialize the classifiers. We\u2019ll use a Random Forest, an optimized XGBClassifier, and a K-Nearest Neighbors classifier. I will leave out the part where I tune the XGB, since it looks essentially the same as the tuning we did earlier.", "Test the models, get the classification reports:", "The top performance in terms of f1-score came from the XGC, followed by the RF and finally the KNN. However, we can also note that the KNN actually did the best job in terms of recall (successfully identifying duds). This is why model stacking is valuable \u2014 sometimes even an otherwise excellent model like XGBoost can underperform on tasks like this one, where evidently the function to be identified can be locally approximated. Including the KNN\u2019s predictions should add some much-needed diversity.", "Now we can average out the probability predictions from the three classifiers, and use it as a feature for the regressor.", "This is followed by another round of HP tuning with the new feature included, which I\u2019ll leave out. Let\u2019s see how we do on performance:", "Uh oh! This performance is essentially the same as it was before we even did any model stacking. That said, we can keep in mind that the MSE as an error measurement tends to overweight outliers. In fact, we can also calculate the mean absolute error (MAE), which is used for assessing performance on data with significant outliers. In mathematical terms, the MAE calculates the l1 norm, essentially the absolute value, of the residuals, rather than the l2 norm used by the MSE. We can compare the MAE with the square root of the MSE, also known as the root mean squared error (RMSE).", "The mean absolute error is only about 1/3 of the RMSE! Perhaps our model is not as bad as we might have initially thought.", "As a final step, let\u2019s take a look at each feature\u2019s importance according to the XGRegressor:", "Neat! Our model found prob_dud to be the most important feature, and our custom StandardizedDays feature was the second most important. (Features 0 through 14 correspond to the reduced title embedding vectors.)", "Even though overall performance didn\u2019t improve through this round of model stacking, we can see that we did successfully capture an important source of variability in the data, which the model picked up on.", "If I were to continue expanding on this project to make the model more accurate, I\u2019d probably consider augmenting the data with outside data, including Source as a variable via binning or hashing, running the models on the original 300-dimensional vectors, and using the \u201ctime-sliced\u201d data (a companion dataset to this data) of each article\u2019s popularity at various time points to predict the final popularity.", "If you found this analysis interesting, please feel free to use the code, and to expand on it further! The notebook is here (note that some of the cells may be in a slightly different order than they were presented here), and the original data used for this project is here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer at Primer AI. I\u2019m on Twitter @blissfulchar, and here\u2019s my LinkedIn: https://www.linkedin.com/in/charlenechambliss/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcdeda5f14751&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@blissfulchar?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@blissfulchar?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Charlene Chambliss"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f4a87a7f560&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&user=Charlene+Chambliss&userId=4f4a87a7f560&source=post_page-4f4a87a7f560----cdeda5f14751---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-data-science/in-depth-analysis/home", "anchor_text": "In-Depth Analysis"}, {"url": "https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb", "anchor_text": "here"}, {"url": "https://nbviewer.jupyter.org/", "anchor_text": "nbviewer"}, {"url": "https://github.com/RaRe-Technologies/gensim-data", "anchor_text": "here"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "this excellent, interactive distill.pub article"}, {"url": "https://github.com/Phlya/adjustText", "anchor_text": "here"}, {"url": "https://stackoverflow.com/questions/45234310/doc2vec-worse-than-mean-or-sum-of-word2vec-vectors", "anchor_text": "(and sometimes even more effective)"}, {"url": "https://github.com/sdimi/average-word2vec/blob/master/notebook.ipynb", "anchor_text": "has created a series of functions"}, {"url": "https://en.wikipedia.org/wiki/Unix_time", "anchor_text": "here"}, {"url": "https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv", "anchor_text": "this Kaggle article"}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)", "anchor_text": "l1 norm"}, {"url": "https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb", "anchor_text": "here"}, {"url": "https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cdeda5f14751---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cdeda5f14751---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----cdeda5f14751---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cdeda5f14751---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/in-depth-analysis?source=post_page-----cdeda5f14751---------------in_depth_analysis-----------------", "anchor_text": "In Depth Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&user=Charlene+Chambliss&userId=4f4a87a7f560&source=-----cdeda5f14751---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&user=Charlene+Chambliss&userId=4f4a87a7f560&source=-----cdeda5f14751---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcdeda5f14751&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cdeda5f14751---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cdeda5f14751--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cdeda5f14751--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cdeda5f14751--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@blissfulchar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@blissfulchar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Charlene Chambliss"}, {"url": "https://medium.com/@blissfulchar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "450 Followers"}, {"url": "https://www.linkedin.com/in/charlenechambliss/", "anchor_text": "https://www.linkedin.com/in/charlenechambliss/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f4a87a7f560&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&user=Charlene+Chambliss&userId=4f4a87a7f560&source=post_page-4f4a87a7f560--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F147e2e716dee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751&newsletterV3=4f4a87a7f560&newsletterV3Id=147e2e716dee&user=Charlene+Chambliss&userId=4f4a87a7f560&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}