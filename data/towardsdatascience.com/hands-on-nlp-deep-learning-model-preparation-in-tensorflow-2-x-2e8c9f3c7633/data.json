{"url": "https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633", "time": 1683012576.860884, "path": "towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633/", "webpage": {"metadata": {"title": "Hands-on NLP Deep Learning Model Preparation in TensorFlow 2.X | by Kefei Mo | Towards Data Science", "h1": "Hands-on NLP Deep Learning Model Preparation in TensorFlow 2.X", "description": "Many state-of-the-art results in NLP problems are achieved by using DL (deep learning), and probably you want to use deep learning style to solve NLP problems as well. While there are a lot of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization", "anchor_text": "Tokenization", "paragraph_index": 4}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB movie review dataset", "paragraph_index": 7}, {"url": "https://stackoverflow.com/questions/61760508/how-to-choose-num-words-parameter-for-keras-tokenizer", "anchor_text": "post", "paragraph_index": 10}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer", "anchor_text": "document of Tokenizer", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "stemming", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Lemmatisation", "anchor_text": "lemmatization", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Text_normalization", "anchor_text": "test normalization", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Language_identification", "anchor_text": "language identification", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Code-mixing", "anchor_text": "code-mixing and translation", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Parsing", "anchor_text": "parsing", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Coreference", "anchor_text": "coreference resolution", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Stop_words", "anchor_text": "stop words", "paragraph_index": 14}, {"url": "https://www.tensorflow.org/guide/keras/masking_and_padding", "anchor_text": "TensorFlow documentation", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "word2vec", "paragraph_index": 24}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 24}, {"url": "https://fasttext.cc/", "anchor_text": "FastText", "paragraph_index": 24}, {"url": "https://spacy.io/models/en", "anchor_text": "spaCy documentary", "paragraph_index": 40}], "all_paragraphs": ["Many state-of-the-art results in NLP problems are achieved by using DL (deep learning), and probably you want to use deep learning style to solve NLP problems as well. While there are a lot of materials discussing how to choose and train the \u201cbest\u201d neural network architecture, like, an RNN, selecting and configuring a suitable neural network is just one part of solving a practical NLP problem. The other import part, but often being underestimated, is model preparation. NLP tasks usually require special data treatment in the model preparation stage. In other words, there is a lot of things to do before we can throw the data in the neural networks to train. Unfortunately, there are not many tutorials giving detailed guidance on model preparation.", "Besides, the packages or APIs to support the state-of-the-art NLP theories and algorithms are usually released very recently and are updating at a rapid speed. (e.g., TensorFlow was first released in 2015, PyTorch in 2016, and spaCy in 2015.) To achieve a better performance, many times, you might have to integrate several packages in your deep learning pipeline, while preventing them from crashing with each other.", "That\u2019s why I decided to write this article to give you a detailed tutorial.", "Without further ado, let\u2019s start with the first step.", "In NLP, Tokenization means to brake the raw text into unique unites (a.k.a., tokens). A token can be sentences, phrases, or words. Each token has a unique token-id. The purpose of tokenization is that we can use those tokens (or token-id) to represent the original text. Here is an illustration.", "stage1: create a token dictionary, in this stage,", "stage2: text representation, in this stage,", "We will take a piece of IMDB movie review dataset to demonstrate the pipeline.", "Now, let\u2019s take a look at what we get from the tokenization step.", "Important note: When using TensorFlow Tokenizer, 0-token-id is reserved to empty-token, i.e., the token-id starts at 1,", "NOTES: There is no simple answer to what should be num_words value. But here is my suggestion: to build a pipeline, you can start with a relatively small number, say, num_words=10,000, and come back to modify it after further analysis. (I found this stack overflow post shares some insightful ideas on how to choose the num_words value. Also, check the document of Tokenizer for other parameter settings.)", "Let\u2019s take a look at a common issue in tokenization that is very harmful to both deep learnings and traditional MLs and how we can deal with it. Consider the following example, to tokenize the sequence [\u2018Storys of a woman\u2026\u2019].", "Since the corpus used for training doesn\u2019t consist of words \u201cstorys\u201d or \u201cwoman\u201d, these words are not included in the token dictionary either. This is out of vocabulary (OOV) issue. While OOV is hard to avoid, there is some solution to mitigate the problems:", "The idea of tokenization might seem very simple, but sooner or later, you will realize tokenization can be much more complicated than it seems in this example. The complexity mainly comes from various preprocessing methods. Some of the common practices of preprocessing are lowercasing, removal of punctuation, word singularization, stemming, and lemmatization. Besides, we have optional preprocessing steps, such as test normalization (e.g., digit to text, expand abbreviation), language identification, and code-mixing and translation; as well as advanced preprocessing, like, [Part-of-speech tagging](Part-of-speech tagging) (a.k.a., POS tagging), parsing, and coreference resolution. Depends on what preprocessing steps to take, the tokens can be different, thus the tokenized texts.", "Don\u2019t worry if you don\u2019t know all these confusing names above. Indeed, it is very overwhelming to determine which preprocessing method(s) to include in the NLP pipeline. For instance, it is not an easy decision to make which tokens to include in the text presentation. Integrating a large number of token candidates are storage and computationally expensive. And it is not very clear which tokens are more important: the most appear words like \u201cthe\u201d, \u201ca\u201d are not very informative for text representation, and that\u2019s why we need to handle stop words in preprocessing.", "Though arguably, we have good news here: deep learnings requires relatively less preprocessing than conventional machine learning algorithms. The reason is that deep learnings can take advantage of the neural network architecture for feature extraction that conventional ML models perform in the preprocessing and feature engineering stages. So, here we can keep the tokenization step simple and come back later if more preprocessing and/or postprocessing are desired.", "While most of the deep learning tutorials still use a list or np.array to store the data, I find it more controllable and scalable using DataFrame (e.g., Pandas, or PySpark) to do the work. This step is optional, but I recommend you do it. Here is the example code.", "That\u2019s what you need to know about tokenization. Let\u2019s move on to the next step: padding.", "Most (if not all) of the neural networks require the input sequence data with the same length, and that\u2019s why we need padding: to truncate or pad sequence (normally pad with 0s) into the same length. Here is an illustration of padding.", "Let\u2019s look at the following example code to perform padding in TensorFlow.", "By default, the pad_sequences parameters are set to padding=\u2019pre\u2019, truncating=\u2019pre\u2019. However, according to TensorFlow documentation, it is recommended \u201cusing \u2018post\u2019 padding when working with RNN layers\u201d. (It is suggested that in English, the most important information appears at the beginning. So truncating or pad sequence after can better represent the original text.) Here is the example code.", "Another question is, what should be the maxlen value. The trade-off here is larger maxlen value leads to sequences that maintain more information but takes more storage space and more computationally expensive, while smaller maxlen value can save storage space but result in loss of information.", "Since we store the token sequence data in a data frame, getting sequence length stats are very straightforward, here is the example code:", "Sequence padding should be an easy piece. Let\u2019s move on to the next step, preparing the word2vec word embeddings.", "Word embeddings build the bridge between human understanding of languages and of a machine. It is essential for many NLP problems. And you might have heard the names \u201cword2vec\u201d, \u201cGloVe\u201d, and \u201cFastText\u201d.", "Don\u2019t worry if you are not familiar with word embeddings. I will give a brief introduction of word embedding that should provide enough intuition and apply word embedding in TensorFlow.", "First, let\u2019s understand some key concepts:", "Embedding: For the set of words in a corpus, embedding is a mapping between vector space coming from distributional representation to vector space coming from distributed representation.", "Vector semantics: This refers to the set of NLP methods that aim to learn the word representations based on the distributional properties of words in a large corpus.", "Let\u2019s see some solid examples using spaCy\u2019s pre-trained embedding models.", "Now the word \u201celephant\u201d has been represented by a vector, so what? Don\u2019t look away. Some magic is about to happen.\ud83e\uddd9\ud83c\udffc\u200d\u2642\ufe0f", "Since we can represent words using vectors, we can calculate the similarity (or distance) between words. Consider the following code.", "Word2Vec is very powerful, and it is a pretty new concept (Word2vec was created and published in 2013). There is so much more to talk about, things like", "Word embedding is a very exciting topic, but don\u2019t get stuck here. For readers who are new to word embeddings, the most important thing is to understand", "Since the word \u201celephan\u201d does not exist in the spaCy \u201cen_core_web_md\u201d model we have loaded earlier, spaCy returns a 0-vector. Again, treating OOV is not a trivial task. But we can use either .has_vector or .is_oov to capture the OOV phenomenon.", "Hopefully, you have a pretty good understanding of word embedding now. Let\u2019s come back to the main track and see how we can apply word embeddings in the pipeline.", "Pretrained Word Embeddings are the embeddings learned in one task that is used for solving another similar task. A pre-trained word embedding model is also called a transformer. Using a pre-trained word embedding models can save us the trouble to train one from scratch. Also, the fact that the pre-trained embedding vectors are generated from a large dataset usually leads to stronger generative capability.", "To apply a pre-trained word embedding model is a bit like searching in a dictionary, and we have seen such a process earlier using spaCy. (e.g., input the word \u201celephant\u201d and spaCy returned an embedding vector. ) At the end of this step, we will create an \u201cembedding matrix\u201d with embedding vectors associated with each token. (The embedding matrix is what TensorFlow will use to connect a token sequence with the word embedding representation.)", "Here we have the embedding matrix (i.e., a 2-d array) with the shape of (50, 96). This embedding matrix will be fed into TensorFlow embedding layers in the last step of this NLP model preparation pipeline.", "NOTES: You might notice that all the is_oov values are True. But you will still get non-zero embedding vectors. This happens using the spaCy \u201cen_core_web_sm\u201d model.", "Unlike \u201cen_core_web_md\u201d, which returns a zero-vector when the token is not in the embedding model, the way how \u201cen_core_web_sm\u201d works will make it always return some non-zero vectors. However, according to the spaCy documentary, the vectors returned by \u201cen_core_web_sm\u201d are not \u201cas precise as\u201d larger models like \u201cen_core_web_md\u201d or \u201cen_core_web_lg\u201d.", "Depends on the application, it is your decision to choose the \u201cnot-very-precise\u201d embedding model but always give non-zero vectors or models return \u201cmore precise\u201d vectors but sometimes zero-vectors when seeing OOVs.", "In the demo, I\u2019ve chosen the \u201cen_core_web_sm\u201d model that always gives me some non-zero embedding vectors. A strategy could be by using vectors learned for subword fragments during training, similar to how people can often work out the gist of a word from familiar word-roots. Some people call this strategy \u201cbetter something-not-precise than nothing-at-all\u201d. (Though I am not sure how spaCy assigns non-zero values to OOVs.)", "So far, we have the padded token sequence to represent the original text data. Also, we have created an embedding matrix with each row associated with the tokens. Now it is time to set up the TensorFlow Embedding layers.", "The Embedding layer mechanism is summarized in the following illustration.", "Embedding layer builds the bridge between the token sequences (as input) and the word embedding representation (as output) through an embedding matrix (as weights).", "Notes: The key to modern NLP feature extraction: If everything works, the output of the embedding layers should represent well of the original text, with all the features storing in the word embedding weights; this is the key idea of modern NLP feature extraction. You will see very soon, we can fine-tune this weights by setting trainable=True for embedding layers.", "Also note that, in this example, we explicitly specify the empty token\u2019s word2vec as zero just for demonstration purposes. In fact, once the Embedding layer sees the 0-token-id, it will immediately assign a zero-vector to that position without looking into the embedding matrix.", "The following example code shows how embedding is done In TensorFlow,", "We can check the output of the Embedding layer using a test case. The output Tensor of the Embedding layer should be in the shape [num_sequence, padded_sequence_length, embedding_vector_dim].", "And that\u2019s it. You are ready to train your text data. (You can refer to the notebook to see training using RNN and CNN.)", "We have been through a long way to prepare data for NLP deep learning. Use the following checklist to test your understanding:", "Tokenization: train on a corpus to create a token dictionary and represent the original text with tokens (or token-ids) by referring to the token dictionary created. In TensorFlow, we can use Tokenizer for tokenization.", "Padding: pad or truncate sequences to the same length, i.e., the padded sequences have the same number of tokens (including empty-tokens). In TensorFlow, we can use pad_sequences for padding.", "Word embeddings: the tokens can be mapped to vectors by referring to an embedding model, e.g., word2vec. The embedding vectors possess information that both humans and a machine can understand. We can use spaCy \u201cen_core_web_sm\u201d, \u201cen_core_web_md\u201d, or \u201cen_core_web_lg\u201d for word embeddings.", "Embedding layer in TensorFlow: to take advantage of the pre-trained word embeddings, the inputs of an Embedding layer in TensorFlow include padded sequences represented by token-ids, and an embedding matrix that stores embedding vectors associated with the tokens within the padded sequences. The output is a 3-d tensors with the shape of [num_sequence, padded_sequence_length, embedding_vector_dim].", "If you haven\u2019t checked the notebook, here is the link:", "I hope you like this post. See you next time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML engineer / data story teller / electrical engineer / digital illustrator / 3D modeler. I am writing data science blog with my cousin."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2e8c9f3c7633&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kefeimo.medium.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": ""}, {"url": "https://kefeimo.medium.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Kefei Mo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73964e25a06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&user=Kefei+Mo&userId=73964e25a06&source=post_page-73964e25a06----2e8c9f3c7633---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://drive.google.com/file/d/1E7l_XJ1HnhaGXwUo4jpYv5WZwmULqU92/view?usp=sharing", "anchor_text": "Colab"}, {"url": "https://github.com/kefeimo/DataScienceBlog/blob/master/08_06_2020_tf_embedding/tf_nlp_tokenizer_embedding.ipynb", "anchor_text": "Github"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB movie review dataset"}, {"url": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization", "anchor_text": "Tokenization"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB movie review dataset"}, {"url": "https://stackoverflow.com/questions/61760508/how-to-choose-num-words-parameter-for-keras-tokenizer", "anchor_text": "post"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer", "anchor_text": "document of Tokenizer"}, {"url": "https://keras.io/api/preprocessing/text/", "anchor_text": "Kerras document"}, {"url": "https://spacy.io/usage/linguistic-features#native-tokenizers", "anchor_text": "spaCy"}, {"url": "https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html", "anchor_text": "Gensim"}, {"url": "https://huggingface.co/transformers/main_classes/tokenizer.html", "anchor_text": "PretrainedTokenizer"}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "stemming"}, {"url": "https://en.wikipedia.org/wiki/Lemmatisation", "anchor_text": "lemmatization"}, {"url": "https://en.wikipedia.org/wiki/Text_normalization", "anchor_text": "test normalization"}, {"url": "https://en.wikipedia.org/wiki/Language_identification", "anchor_text": "language identification"}, {"url": "https://en.wikipedia.org/wiki/Code-mixing", "anchor_text": "code-mixing and translation"}, {"url": "https://en.wikipedia.org/wiki/Parsing", "anchor_text": "parsing"}, {"url": "https://en.wikipedia.org/wiki/Coreference", "anchor_text": "coreference resolution"}, {"url": "https://en.wikipedia.org/wiki/Stop_words", "anchor_text": "stop words"}, {"url": "https://www.tensorflow.org/guide/keras/masking_and_padding", "anchor_text": "TensorFlow documentation"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "word2vec"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://spacy.io/models/en", "anchor_text": "spaCy documentary"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Constant", "anchor_text": "tensorflow.keras.initializers.Constant"}, {"url": "https://drive.google.com/file/d/1E7l_XJ1HnhaGXwUo4jpYv5WZwmULqU92/view?usp=sharing", "anchor_text": "tf_nlp_tokenizer_embedding.ipynbColaboratory notebookdrive.google.com"}, {"url": "https://www.oreilly.com/library/view/practical-natural-language/9781492054047/", "anchor_text": "Practical Natural Language Processing: A Comprehensive Guide to Building Real-world Nlp Systems-Oreilly & Associates Inc (2020)"}, {"url": "https://www.manning.com/books/natural-language-processing-in-action", "anchor_text": "Natural Language Processing in Action: Understanding, analyzing, and generating text with Python-Manning Publications (2019)"}, {"url": "https://www.manning.com/books/deep-learning-with-python", "anchor_text": "Deep Learning with Python-Manning Publications (2018)"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2e8c9f3c7633---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----2e8c9f3c7633---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----2e8c9f3c7633---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2e8c9f3c7633---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2e8c9f3c7633---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&user=Kefei+Mo&userId=73964e25a06&source=-----2e8c9f3c7633---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&user=Kefei+Mo&userId=73964e25a06&source=-----2e8c9f3c7633---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2e8c9f3c7633&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2e8c9f3c7633---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2e8c9f3c7633--------------------------------", "anchor_text": ""}, {"url": "https://kefeimo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kefeimo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kefei Mo"}, {"url": "https://kefeimo.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "85 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73964e25a06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&user=Kefei+Mo&userId=73964e25a06&source=post_page-73964e25a06--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba7aa259b62a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633&newsletterV3=73964e25a06&newsletterV3Id=ba7aa259b62a&user=Kefei+Mo&userId=73964e25a06&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}