{"url": "https://towardsdatascience.com/battling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92", "time": 1683016955.086745, "path": "towardsdatascience.com/battling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92/", "webpage": {"metadata": {"title": "Battling label distribution shift in a dynamic world | by Av Shrikumar | Towards Data Science", "h1": "Battling label distribution shift in a dynamic world", "description": "In this tutorial, we will see how we can use a combination of model calibration and a simple iterative procedure to make our model predictions robust to shifts in the class proportions between when\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pypi.org/project/abstention/", "anchor_text": "package", "paragraph_index": 28}, {"url": "https://colab.research.google.com/github/kundajelab/labelshiftexperiments/blob/master/notebooks/demo/blog_colab.ipynb", "anchor_text": "here", "paragraph_index": 29}, {"url": "http://proceedings.mlr.press/v70/guo17a.html", "anchor_text": "http://proceedings.mlr.press/v70/guo17a.html", "paragraph_index": 31}, {"url": "http://proceedings.mlr./", "anchor_text": "http://proceedings.mlr.", "paragraph_index": 33}, {"url": "https://openreview.net/forum?id=rJl0r3R9KX", "anchor_text": "https://openreview.net/forum?id=rJl0r3R9KX", "paragraph_index": 34}], "all_paragraphs": ["by Amr M. Alexandari & Avanti Shrikumar", "In this tutorial, we will see how we can use a combination of model calibration and a simple iterative procedure to make our model predictions robust to shifts in the class proportions between when the model is trained and when it is deployed.", "Say we build a classifier using data gathered in June to predict the probability that a patient has COVID19 based on the severity of their symptoms. At the time we train our classifier, covid positivity was at a comparatively lower rate in the community. Now it\u2019s November, and the rate of covid positivity has increased considerably. Is it still a good idea to use our classifier from June to predict who has covid?", "To understand why the classifier from June might underestimate the prevalence of covid, let\u2019s imagine our classifier is built using a single variable capturing symptoms severity that we will call the \u201cdisease score\u201d (this argument generalizes to classifiers built using many variables, as well as classifiers that have more than 2 classes as the output, but the intuition is easiest in the single variable & two-class case). Here is a toy visualization of what the ground-truth distribution of symptom severity might look like for positive and negative cases, in a situation where 10% of all tested cases in June were positive:", "The red line shows the predicted fraction of positives for an ideal classifier that matches the ground-truth. This ground-truth probability can be calculated by taking the height of the orange bar at a given disease score and dividing by the total height of the blue and orange bars at that disease score added together.", "Now let us visualize the case where the proportion of positives among the tested population has risen to 70% in November (note: this an extreme shift, and we show it solely for ease of visualization). The symptoms of covid have not changed between June and November, which means the overall shape of the blue distribution and orange distribution would stay the same. However, the height of the blue bars relative to orange bars would increase to reflect the greater proportion of positives, giving:", "The solid red line shows the predictions from the classifier trained in June, while the dashed red line shows the true fraction of positives for data gathered in November. As we can see, the ideal classifier for data gathered in June underestimates the probability that a patient has COVID19 when the classifier is deployed in November. This phenomenon is called label shift or prior probability shift. If we knew the labels for the testing data, we could train a new classifier to output the class probabilities \u2014 but in practice, we don\u2019t know the labels for the testing data \u2014 all we observe is the overall distribution of symptom severity for both the covid-positive and covid-negative patients combined, which looks something like this:", "So, how can we get an updated classifier?", "It turns out there is a simple way to adapt our classifier to account for the shift in class proportions. To see how this method works, let us introduce some terminology. The dataset that we train on is called the \u201csource domain\u201d, while the dataset we deploy the model on is called the \u201ctarget domain\u201d. We will use the following notation:", "Let\u2019s first begin by taking stock of which of these quantities we do have information about. We can estimate p(y) by simply calculating the average class proportions in our data from the source domain. We also have an estimate of p(y|x) from building our classifier on the source domain (remember, the classifier was trained to estimate the probability that a person belongs to a particular class \u2014 in this case covid vs. no covid \u2014 given their symptoms). What about p(x|y)? While it is technically possible to build an estimate of p(x|y) using the source-domain data, in practice this can be very hard when x is high-dimensional. Thus, we will not assume that we have access to p(x|y). However, if we assume that the symptoms of covid do not change between June and November, we can assume that p(x|y) = q(x|y). To see why this is useful, let us consider how we can update our classifier if we were given a guess for the value of q(y). From Bayes\u2019 rule, we have:", "If we substitute our assumption that q(x|y) = p(x|y), we get:", "As mentioned before, we don\u2019t always have an estimate of p(x|y) \u2014 however, we can get around this by applying Bayes\u2019 rule again to p(x|y), which gives:", "We\u2019re very close! We have an estimate of p(y|x) and p(y) from the training data, and we assumed we were given a guess for q(y) \u2014 the only term we don\u2019t know is p(x). Fortunately, we can see that p(x) cancels out in the numerator and the denominator, giving us:", "Let\u2019s develop some intuition for this formula. In the numerator, we see that the predictions p(y|x) are being re-weighted by the class ratios q(y)/p(y) to produce q(y|x), and the denominator is simply normalizing the re-weighted predictions to sum to 1 across all classes (in order to get a valid probability distribution). If q(y) were 0 for all except one class, then after applying this re-weighting and normalization, q(y|x) would again be 0 for all except that one class (for which it would be 1).", "We have thus seen that if we are given a guess for q(y), we can get an updated estimate of q(y|x). But how can we obtain a good guess for q(y)? Well, one thing that you (like the authors of this work) might intuitively try to do is to estimate q(y) by (a) making predictions on all the target-domain examples using your original source-domain classifier p(y|x), and (b) averaging the predictions to obtain an initial guess for q(y). When we apply this to our toy covid example from before, we get an estimate of q(y)=34%.", "Using this guess for q(y), we can get an estimate of q(y|x) using Bayes\u2019 rule as shown above. But wait! What if we go back and apply our updated q(y|x) to all the target domain examples to re-estimate q(y)? When we do this, we get an estimate of q(y)=53%, which is even closer to the ground truth. We can now repeat this process: average the value of q(y|x) over all the testing set examples to re-estimate q(y), and then use our updated q(y) to re-estimate q(y|x). If we do this for multiple iterations, we eventually converge to the true value q(y)=70%, as shown below:", "If this iterative procedure reminds you of Expectation Maximization, you\u2019d be correct! The algorithm we have described above is indeed a valid expectation maximization algorithm, as was first shown in 2002 by Saerens et al.\u00b9 Incidentally, that 2002 paper was incorrectly described in several recent works as requiring access to an estimate of p(x|y) \u2014 however, as we have shown, p(x|y) is not used in the EM algorithm.", "One issue that can arise when applying the EM updates is that, sometimes, the predictions p(y|x) that are output by a model might not be calibrated. By calibrated, we mean simply that for all examples where the model outputs a probability of x% for a particular class, there is actually an x% chance that those examples belong to that class. Modern neural networks are often notoriously miscalibrated, as was shown by Guo et al.\u00b2 , and this miscalibration is believed to stem from overfitting to the training set. To visualize this miscalibration, we can bin the predicted probabilities for a given class into intervals of size 0.1 on the x-axis, and for each bin we can plot the actual probability that the example belonged to the class on the y-axis. Any divergence from the x=y line indicates miscalibration. If we do this for a model trained on the CIFAR10 object recognition dataset, here is what we observe for the \u201ccat\u201d class when we make predictions on the testing set:", "As you can see, there is a marked deviation from the x=y line, indicating miscalibration. This suggests that the predicted probabilities p(y|x) are not very reliable, and if we were to apply them as-is in the EM algorithm, we might get poor results. The approach proposed in Guo et al. to correct for miscalibration is called Temperature Scaling (TS). In TS, the predicted probabilities are adjusted by rescaling the softmax logits according to a \u201ctemperature\u201d parameter T. The parameter T is optimized to achieve the best negative-log-likelihood on a held-out validation set. Formally, with TS, the new predicted probabilities become:", "Where z(x) is a function that returns the original logit vector from the classifier. If we use TS to adjust the probabilities in the CIFAR10 case above, here is what we observe for the \u201ccat\u201d class:", "We still notice some issues. There is consistent bias with a higher \u201ctrue\u201d fraction of cat labels in almost every bin; the network seems to be systematically under-estimating the true probability that an observation belongs to the \u201ccat\u201d class. To fix this, we propose introducing explicit class-specific bias correction terms to the TS equation, which we call Bias-Corrected Temperature Scaling (BCTS). BCTS is defined as follows:", "As we can see, the systematic bias in the predicted probabilities has been greatly reduced. In our paper, we found that BCTS consistently tended to give superior results for label shift adaptation compared to TS.", "Let\u2019s summarize our procedure for adapting to label shift. Our algorithm proceeds as follows:", "on each observation from the target domain", "4. Return q(y|x) as the predictions for the target domain.", "This simple algorithm, described in Alexandari et al. (ICML 2020)\u00b3, turns out to achieve state-of-the-art results compared to alternative methods such as BBSL\u2074 and RLLS\u2075. This was independently verified in a paper by Garg et al.\u2076, which states (referring to the maximum likelihood approach as MLLS):", "\u201cAcross all shifts, MLLS (with BCTS-calibrated classifiers) uniformly dominates BBSE, RLLS, \u2026\u201d", "Garg et al. also provided a theoretical analysis of the maximum likelihood approach that confirms the critical importance of good calibration (but also shows that there is more work to be done beyond using BCTS calibration).", "To see this method in action with code, we can use the python abstention package, which implements all of these methods and makes battling label shift as easy as:", "A colab notebook demonstrating this process is available here", "[1]: Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural Comput., 14(1): 21\u201341, January 2002.", "[2]: Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321\u20131330, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/guo17a.html", "[3]: Alexandari, A., Kundaje, A., Shrikumar, A., 2020. Maximum Likelihood with Bias-Corrected Calibration is Hard-To-Beat at Label Shift Adaptation. Proceedings of the thirty-seventh International Conference on Machine Learning, ICML 2020.", "[4]: Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3122\u2013 3130, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/lipton18a.html", "[5]: Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD in Computational Genomics from Stanford. MIT '13. Interested in the truth."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbc1f4c4d2f92&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://avshrikumar.medium.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": ""}, {"url": "https://avshrikumar.medium.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Av Shrikumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11d8a60965a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&user=Av+Shrikumar&userId=11d8a60965a2&source=post_page-11d8a60965a2----bc1f4c4d2f92---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://github.com/nytimes/covid-19-data", "anchor_text": "NYTimes"}, {"url": "https://pypi.org/project/abstention/", "anchor_text": "package"}, {"url": "https://colab.research.google.com/github/kundajelab/labelshiftexperiments/blob/master/notebooks/demo/blog_colab.ipynb", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1901.06852", "anchor_text": "https://arxiv.org/abs/1901.06852"}, {"url": "https://github.com/kundajelab/labelshiftexperiments", "anchor_text": "https://github.com/kundajelab/labelshiftexperiments"}, {"url": "https://youtu.be/ZBXjE9QTruE", "anchor_text": "https://youtu.be/ZBXjE9QTruE"}, {"url": "http://proceedings.mlr.press/v70/guo17a.html", "anchor_text": "http://proceedings.mlr.press/v70/guo17a.html"}, {"url": "http://proceedings.mlr./", "anchor_text": "http://proceedings.mlr."}, {"url": "https://openreview.net/forum?id=rJl0r3R9KX", "anchor_text": "https://openreview.net/forum?id=rJl0r3R9KX"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bc1f4c4d2f92---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bc1f4c4d2f92---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/domain-adaptation?source=post_page-----bc1f4c4d2f92---------------domain_adaptation-----------------", "anchor_text": "Domain Adaptation"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----bc1f4c4d2f92---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----bc1f4c4d2f92---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&user=Av+Shrikumar&userId=11d8a60965a2&source=-----bc1f4c4d2f92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&user=Av+Shrikumar&userId=11d8a60965a2&source=-----bc1f4c4d2f92---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbc1f4c4d2f92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bc1f4c4d2f92---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bc1f4c4d2f92--------------------------------", "anchor_text": ""}, {"url": "https://avshrikumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://avshrikumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Av Shrikumar"}, {"url": "https://avshrikumar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "202 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11d8a60965a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&user=Av+Shrikumar&userId=11d8a60965a2&source=post_page-11d8a60965a2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F989c555b3815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattling-label-distribution-shift-in-a-dynamic-world-bc1f4c4d2f92&newsletterV3=11d8a60965a2&newsletterV3Id=989c555b3815&user=Av+Shrikumar&userId=11d8a60965a2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}