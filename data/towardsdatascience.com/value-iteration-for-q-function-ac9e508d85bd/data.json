{"url": "https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd", "time": 1683009289.898907, "path": "towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd/", "webpage": {"metadata": {"title": "Value Iteration for Q-function. and THE END of this introductory series\u2026 | by Jordi TORRES.AI | Towards Data Science", "h1": "Value Iteration for Q-function", "description": "In this post, we will review the Q-function and present the Value Iteration method that learns the values of the actions to create a policy."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "the post", "paragraph_index": 4}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 6}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 6}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "GitHub code", "paragraph_index": 11}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f", "anchor_text": "See you in the next post!", "paragraph_index": 21}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 22}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 22}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 23}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 24}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 26}], "all_paragraphs": ["In the previous post, we presented how to implement the Value Iteration method for computing the state value, V-function, by solving the Frozen-Lake Environment. In this post, we will review the Q-function and present the Value Iteration method that learns the values of the actions to create a policy.", "As we will see later in this post, Q-values are much more convenient in practice, as for the Agent, it\u2019s much simpler to make decisions about actions based on Q-values than on V-values.", "The Value Iteration method can be used for learning the V-values or the Q-function. That is, store in a table the state value or the action-value. Here we will present how to use Value Iteration method for computing the Q-value instead of the V-value presented in the previous post.", "Based on the previous Value Iteration method implementation for V-function, in the case of action values, only minor modifications to the preceding code are required. The most obvious change is to our value table. In the previous case, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters: state and action, so the key in the value table is now a composite.", "Regarding the update, in the post about the Bellman Equation, we show that we the optimal value of the action-state can be defined as:", "Next, we will solve the Frozen-Lake environment with Q-function.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "Then, we will present in the code that makes up the Value Iteration method with Q-function, the main differences versus the V-function version.", "The central data which will keep our tables and the functions, that we will be using in the training loop, are the same as the previous example of V-function. The main change is into thevalues table, a dictionary that now maps a state-action pair into the calculated value of this action.", "In the previous case, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters: state and action, so the key in the value table is now a composite. This implies that another difference is in the calc_action_value function. We just don\u2019t need it anymore, as our action values are stored in the value table.", "Finally, the most important change in the code is in the Agent\u2019s value_iteration() method. Before, it was just a wrapper around the calc_action_value() call, which did the job of Bellman approximation. Now, as this function has gone and been replaced by a value table, we need to do this approximation in the value_iteration() method.", "Let\u2019s look at the code. As it\u2019s almost the same from the previous implementation, I will jump directly to the main differences and the reader can go for the details in the GitHub code. Let\u2019s start with the main function value_iteration():", "As the reader can notice, the code is equivalent to the code of calc_action_value in the previous implementation. The idea is that for a given state and action, it needs to calculate the action value using the information we collected by the function play_n_random_steps that plays N random steps from the Environment, populating the reward and transits tables with random experiences.", "However, in the previous implementation, we had the V-function stored in the value table, so we just took it from this table. We can\u2019t do this anymore, so we have to call the select_action method, which will choose for us the best action with the largest Q-value, and then we take this Q-value as the value of the target state.", "In fact, this method is implemented differently, as it no longer calls the calc_action_valu method but we just iterate over the actions and look up their values in our values table.", "As you can notice, the code for the learning loop is just the same as in the previous post. And also the code to test the client and plot the results in TensorBoard:", "Finally, the reader can, as before, test with the FrozenLake8x8 environment or test other hyperparameters.", "Acknowledgments: The code presented in this post has been inspired by the code of Maxim Lapan who has written an excellent practical book on the subject.", "Q-values are much more convenient in practice, as for the agent, it\u2019s much simpler to make decisions about actions based on Q-values than on V-values. In the case of Q-values, to choose the action based on the state, the agent just needs to calculate Q-value for all available actions using the current state and choose the action with the largest Q-value.", "To do the same using values of the states, V-value, the agent needs to know not only values but also probabilities for transitions. In practice, we rarely know them in advance, so the agent needs to estimate transition probabilities for every action and state pair.", "In the Value Iteration method for V-function, this dependency on probability adds an extra burden for the agent. That said, it is important to know this method because they are an essential part of advanced methods.", "See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac9e508d85bd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 11"}, {"url": "https://torres-ai.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----ac9e508d85bd---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac9e508d85bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----ac9e508d85bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac9e508d85bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=-----ac9e508d85bd---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24", "anchor_text": "previous post"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc", "anchor_text": "4. Programaci\u00f3n din\u00e1micaAcceso abierto al cap\u00edtulo 4 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "the post"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb", "anchor_text": "GitHub code"}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject"}, {"url": "https://towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f", "anchor_text": "See you in the next post!"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ac9e508d85bd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac9e508d85bd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ac9e508d85bd---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----ac9e508d85bd---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ac9e508d85bd---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac9e508d85bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----ac9e508d85bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac9e508d85bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----ac9e508d85bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac9e508d85bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----ac9e508d85bd---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----ac9e508d85bd---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Written by Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----ac9e508d85bd---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-iteration-for-q-function-ac9e508d85bd&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----ac9e508d85bd---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "The Bellman EquationV-function and Q-function Explained"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "\u00b712 min read\u00b7Jun 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7----0-----------------clap_footer----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----ac9e508d85bd----0---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=-----ac9e508d85bd----0-----------------bookmark_preview----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ac9e508d85bd----1---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----ac9e508d85bd----1-----------------bookmark_preview----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ac9e508d85bd----2---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ac9e508d85bd----2-----------------bookmark_preview----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "Deep Q-Network (DQN)-IIExperience Replay and Target Networks"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": "\u00b714 min read\u00b7Aug 15, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----b6bf911b6b2c----3-----------------clap_footer----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----ac9e508d85bd----3---------------------abdb3c51_1656_41e5_8f81_ee8ee28f5652-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&source=-----ac9e508d85bd----3-----------------bookmark_preview----abdb3c51_1656_41e5_8f81_ee8ee28f5652-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "See all from Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----ac9e508d85bd----0-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----ac9e508d85bd----1-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ac9e508d85bd----0---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ac9e508d85bd----0-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ac9e508d85bd----1---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----ac9e508d85bd----1-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----2-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ac9e508d85bd----2---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----ac9e508d85bd----2-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----3-----------------clap_footer----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----ac9e508d85bd----3---------------------561f186f_9767_4fd0_8f34_4b199778bdd6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----ac9e508d85bd----3-----------------bookmark_preview----561f186f_9767_4fd0_8f34_4b199778bdd6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----ac9e508d85bd--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}