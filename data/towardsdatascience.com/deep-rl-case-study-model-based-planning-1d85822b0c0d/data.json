{"url": "https://towardsdatascience.com/deep-rl-case-study-model-based-planning-1d85822b0c0d", "time": 1683002856.918437, "path": "towardsdatascience.com/deep-rl-case-study-model-based-planning-1d85822b0c0d/", "webpage": {"metadata": {"title": "Deep RL Case Study: Model-based Planning | by Nathan Lambert | Towards Data Science", "h1": "Deep RL Case Study: Model-based Planning", "description": "What is model-based reinforcement learning, what is optimistic about its future, and what does DeepMind mean in a recent paper when it says its agents can learn to plan from scratch via \u201cdreaming\u201d\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1805.12114.pdf", "anchor_text": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1906.08253.pdf", "anchor_text": "When to Trust Your Model: Model-Based Policy Optimization", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1707.06170.pdf", "anchor_text": "Learning model-based planning from scratch", "paragraph_index": 6}, {"url": "https://deepmind.com/research/publications/Mastering-Atari-Go-Chess-and-Shogi-by-Planning-with-a-Learned-Model", "anchor_text": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "paragraph_index": 10}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 16}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 16}], "all_paragraphs": ["What is model-based reinforcement learning, what is optimistic about its future, and what does DeepMind mean in a recent paper when it says its agents can learn to plan from scratch via \u201cdreaming\u201d. Model-based learning is potentially sample optimal \u2014 if we can properly capture the dynamics of an environment, as then an agent can plan precisely to accomplish any task we want (where model-free learning is focused on individual tasks). DeepMind\u2019s recent MuZero algorithm shows the world that MBRL is here to stay, because of its planning capability.", "Many decision making algorithms have been using models of environments to solve problems. Some classic examples include search methods such as A-Star or tree search. The problem with these methods that plan out sequential actions is their lack of ability in continuous environments. What happens when you try to create a tree of action choices with an infinite number of action candidates? This scaling problem is one of many that model-based planning in deep reinforcement learning is trying to cover by converting action choice into a tractable optimization problem \u2014 giving us one action rather than a sample space.", "Model-based Reinforcement Learning (MBRL) follows the approach of an agent acting in its environment, learning a model of said environment, and then leveraging the model to act. It is often characterized with a parametrized dynamics model informing some sort of controller. The loop is illustrated in the diagram with Clank. In MBRL, often the control problem is missing a link between itself and the learning of the dynamics model, which introduces some loss of optimality. This article will focus on how to use some sort of dynamics model to plan a sequence of actions. I start with a review of some past methods, and build into recent work from DeepMind that is gearing up to use model-based planning to solve exciting frontiers in RL.", "MBRL and planning has been substantially on the rise, particularly since a colleague\u2019s paper at the Neural Information Processing conference of 2018: \u201cDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models\u201d (spotlight paper ~4% of accepted papers) from Chua et. al. It describes how to mold the dynamics model into a useful controller, and was the first time that MBRL got attention as being asymptotically optimal (compared to soft actor-critic and other model-free algorithms) and vastly more sample efficient.", "Another (what I will view as fairly seminal in the field of MBRL) paper directly leading up to DeepMind\u2019s recent work is the paper \u201cWhen to Trust Your Model: Model-Based Policy Optimization\u201d from Janner et al. that made two fundamental changes to the older work of MBRL: 1) they successfully showed that a model-based algorithm can use a standard contextual policy (network that returns an action when a state is passed in) as a controller and 2) MBRL can use simulation of the dynamics model offline to improve the current iterations of the control policy.", "This algorithm works by creating a dynamics model, then running really short rollouts of soft actor-critic on the dynamics model in simulation, which generates reliable policies in very few environment interactions, coined Model-based Policy Optimization (MBPO).", "This it older DeepMind paper, \u201cLearning model-based planning from scratch\u201d from Pascanu, Li, and the DeepMind Team, attempts to address the criticism that \u201cwhile a model can be used to evaluate a plan, it does not prescribe how to construct a plan.\u201d In classic DeepMind fashion \u2014 they improve on and integrate many aspects of recent research into an impressive system. They are taking on the MBRL challenge and coined the term \u201cimagination\u201d for the off-line simulation on the dynamics model (very similar to MBPO, which came out two years later). The key difference in this case is that in this new approach Imagination Based Planning (IBP), they also learn when to imagine, and use the context from real and simulated experiences in clever ways.", "Similar to MBPO, this recent work uses contextualized policies for control. The expansion is that, instead of just using the past state to determine the next action, IBP has its own method of generating context. The authors use a long-short term memory (LSTM) block to learn context from recent simulated and real experiences, which enhances performance on control tasks. The paper evaluates the performance of imagining one-step ahead, n-steps ahead, and full tree exploration.Full tree exploration lets the manager explore imagining with different actions to different depths, with a constraint on depth and breadth of exploration via a maximum computation cost. MBPO improved on this setup by using a state of the are model-free algorithm when imagining to get impressive performance.", "The authors propose that this imagination framework could also lend itself nicely to generalization. I see this as a tie to meta-learning, which in this case the manager \u2014 that handles when to act versus when to plan \u2014 could sort of handle the task abstraction, and learn to learn different tasks (in reality and simulation). The manager currently is a discrete action with sparse reward, so it must\u2019ve been tricky to learning \u2014 they currently use the REINFORCE algorithm here.", "This paper is the precursor of some work that DeepMind just released, where they integrate planning into their giant machine learning systems to solve games with huge action spaces and high ceilings.", "\u201cMastering Atari, Go, Chess and Shogi by Planning with a Learned Model\u201d from Schrittwieser et. al is a big moment for model-based RL. Showing that MBRL can solve Atari games directly from pixels (a common struggle point of building a model) as a new state of the art will further propel the exponential rise of MBRL in top reinforcement learning conferences (less then 1% a few years ago, and soon reaching about 10% of RL papers).", "The MuZero algorithm builds a lot of the blocks from MBRL into a system that showed efficacy with model-free algorithms. The items that the algorithm needs to run from pixels rather than physical state observations, so the MBRL need clever encodings to make them perform optimally. Key differences include:", "This planning function gave the learning algorithm increased sample efficiency, because planning ahead can reach the sparse reward of winning a game. Ultimately, that is an underlying reason why planning helps learn faster \u2014 it lets an algorithm search for the reward from it\u2019s current action, rather than relying on stochastic exploration (which also helps and contributes in MBRL).", "(A constraint on MBRL not addressed in this article is the wall-time efficiency of model-based RL being poor \u2014 it can take a week to run a simulation of a running half-cheetah. I hope to engage on this in the future \u2014 essentially all the dynamics model training and imagination steps cause the algorithms to run much slower then model-free counterparts. Some of the algorithms, such as MBPO, even have weird sequencing of events (imagining at each real time step) that prevent it from being able to run on a real robot).", "More? Subscribe to my newsletter on robotics, artificial intelligence, and society!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d85822b0c0d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----1d85822b0c0d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1901.03737", "anchor_text": "talk at IROS 2019"}, {"url": "https://arxiv.org/pdf/1805.12114.pdf", "anchor_text": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"}, {"url": "https://arxiv.org/pdf/1805.12114.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1906.08253.pdf", "anchor_text": "When to Trust Your Model: Model-Based Policy Optimization"}, {"url": "https://arxiv.org/pdf/1906.08253.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1707.06170.pdf", "anchor_text": "Learning model-based planning from scratch"}, {"url": "https://arxiv.org/pdf/1707.06170.pdf", "anchor_text": "IBP"}, {"url": "https://arxiv.org/pdf/1707.06170.pdf", "anchor_text": "source"}, {"url": "https://deepmind.com/research/publications/Mastering-Atari-Go-Chess-and-Shogi-by-Planning-with-a-Learned-Model", "anchor_text": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"}, {"url": "https://deepmind.com/research/publications/Mastering-Atari-Go-Chess-and-Shogi-by-Planning-with-a-Learned-Model", "anchor_text": "source"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d85822b0c0d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----1d85822b0c0d---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1d85822b0c0d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1d85822b0c0d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/robotics?source=post_page-----1d85822b0c0d---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&user=Nathan+Lambert&userId=890b1765e6d&source=-----1d85822b0c0d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&user=Nathan+Lambert&userId=890b1765e6d&source=-----1d85822b0c0d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d85822b0c0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d85822b0c0d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d85822b0c0d--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-model-based-planning-1d85822b0c0d&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}