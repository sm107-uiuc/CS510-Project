{"url": "https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655", "time": 1682995114.5675561, "path": "towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655/", "webpage": {"metadata": {"title": "Too powerful NLP model (GPT-2). What is Generative Pre-Training | by Edward Ma | Towards Data Science", "h1": "Too powerful NLP model (GPT-2)", "description": "OpenAI released generative pre-training model (GPT) which achieved the state-of-the-art result in many NLP task in 2018. GPT is leveraged transformer to perform both unsupervised learning and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "generative pre-training model", "paragraph_index": 0}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "blog", "paragraph_index": 1}, {"url": "https://techcrunch.com/2019/02/17/openai-text-generator-dangerous/?utm_source=tcfbpage&sr_share=facebook&fbclid=IwAR3HuAaJasah3ZsxcFPDg73pNse0dFHtYbIGx8L9TczSEhJoXDfKLrLxhDw", "anchor_text": "noise", "paragraph_index": 3}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners", "paragraph_index": 3}, {"url": "https://www.reddit.com/", "anchor_text": "Reddit", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word level", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10", "anchor_text": "character level", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb", "anchor_text": "BERT", "paragraph_index": 11}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 Github", "paragraph_index": 12}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 Github", "paragraph_index": 14}, {"url": "https://www.youtube.com/watch?v=DZLcADWbZWw", "anchor_text": "lyrics", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=QYh6mYIJG2Y", "anchor_text": "7 rings from Ariana Grande", "paragraph_index": 20}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog", "paragraph_index": 21}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn", "paragraph_index": 21}, {"url": "https://github.com/makcedward", "anchor_text": "Github", "paragraph_index": 21}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/", "paragraph_index": 23}], "all_paragraphs": ["OpenAI released generative pre-training model (GPT) which achieved the state-of-the-art result in many NLP task in 2018. GPT is leveraged transformer to perform both unsupervised learning and supervised learning to learn text representation for NLP downstream tasks.", "To demonstrate the success of this model, OpenAI enhanced it and released a GPT-2 in Feb 2019. GPT-2 is trained to predict next word based on 40GB text. Unlike other model and practise, OpenAI does not publish the full version model but a lightweight version. They mentioned it in their blog:", "Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper.", "Due to this reason, it made lots of noise about no latest model and source code is available for public. Should research open model and source code? OpenAI really trigger a lots of discussion but seems like the majority feedback is negative. Neglected whether it should be open or not, this story will discuss about Language Models are Unsupervised Multitask Learners (Radford et al., 2019) and the following are will be covered:", "Instead of using existing dataset, OpenAI choose to build up a new web scrape which emphasised document quality. All text come from outbound linke from Reddit post and post must be rated at least 3 karma. In other words, it is confirmed by human that it is interesting, educational or meaningful things.", "No preprocessing step is required. In other word, lower casing, tokenization and other step are skipped as authors believe that these pre-processing step restrict the capability of the model and it is able evaluate all language model benchmark.", "Text representations is a good way to represent a word in neural network is undoubtedly true. However, Radford et al., does not apply neither word level nor character level. They choose the middle one which is subword. Subword can be obtained by Byte Pair Encoding (BPE) algorithm.", "BPE is way of compression originally. A list of subword will be calculated by using the following algorithm.", "Taking \u201clow: 5\u201d, \u201clower: 2\u201d, \u201cnewest: 6\u201d and \u201cwidest: 3\u201d as an example, the highest frequency subword will be extracted in each iteration:", "Same as GPT, GPT-2 leverages transformer model. While the difference between GPT and GPT-2 are:", "To cater different scenario, 4 model with different parameters are trained", "GPT-2 use unsupervised learning approach to train the language model. Unlike other model such as ELMo and BERT need 2 stages training which are pre-training and fine-tuning stage. There is no fine-tuning stage for GPT-2.", "No custom training for GPT-2. OpenAI does not release source code of training GPT-2 (as of Feb 15, 2019). Therefore, we can only use the trained model for research or adoption. Meanwhile, the only released trained model is the smallest one which having 117M parameters one. To download this model, you may follow the instruction in GPT-2 Github.", "Radford et al. demonstrated that the largest model (i.e. 1542M parameters) achieve 8 state-of-the-art result while the smallest one achieve 4 state-of-the-art result.", "It is easy to try GPT-2 small model. You only need to follow the simple instruction from GPT-2 Github. After downloading source code and model and installing libraries, you can generate text by using either unconditional sample generation or conditional sample generation.", "The first mode is Unconditional Sample Generation. It means generating text without any condition.", "After a while, some texts will be generated and here is one of the example:", "You can also generate text by changing default configuration. Temperature is the level of randomization. Lower value will have a high chance to output data from WebText\u2019s test set.", "After unconditional text generation, we will try conditional text generation.", "Let try one of the lyrics from Hong Kong\u2019s band (Fama)", "Let try 7 rings from Ariana Grande:", "I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or Github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Focus in Natural Language Processing, Data Science Platform Architecture. https://makcedward.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4cc6afb6655&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@makcedward?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f----4cc6afb6655---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@makcedward?utm_source=medium&utm_medium=referral", "anchor_text": "Edward Ma"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "generative pre-training model"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "blog"}, {"url": "https://techcrunch.com/2019/02/17/openai-text-generator-dangerous/?utm_source=tcfbpage&sr_share=facebook&fbclid=IwAR3HuAaJasah3ZsxcFPDg73pNse0dFHtYbIGx8L9TczSEhJoXDfKLrLxhDw", "anchor_text": "noise"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://www.reddit.com/", "anchor_text": "Reddit"}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word level"}, {"url": "https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10", "anchor_text": "character level"}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT"}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT"}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo"}, {"url": "https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb", "anchor_text": "BERT"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 Github"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 Github"}, {"url": "https://www.youtube.com/watch?v=DZLcADWbZWw", "anchor_text": "lyrics"}, {"url": "https://www.youtube.com/watch?v=QYh6mYIJG2Y", "anchor_text": "7 rings from Ariana Grande"}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog"}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn"}, {"url": "https://github.com/makcedward", "anchor_text": "Github"}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "Generative Pre-Training (GPT)"}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "Word Embeddings"}, {"url": "https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10", "anchor_text": "Character Embeddings"}, {"url": "https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb", "anchor_text": "Bidirectional Encoder Representations from Transformers (BERT)"}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "Embeddings from Language Model (ELMo)"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 Github"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Improving Language Understanding by Generative Pre-Training"}, {"url": "http://aclweb.org/anthology/P16-1162", "anchor_text": "Neural Machine Translation of Rare Words with Subword Units"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4cc6afb6655---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4cc6afb6655---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4cc6afb6655---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----4cc6afb6655---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4cc6afb6655---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&user=Edward+Ma&userId=ba547bff904f&source=-----4cc6afb6655---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&user=Edward+Ma&userId=ba547bff904f&source=-----4cc6afb6655---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4cc6afb6655&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4cc6afb6655---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4cc6afb6655--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4cc6afb6655--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4cc6afb6655--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/@makcedward/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fde3db5912a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-powerful-nlp-model-generative-pre-training-2-4cc6afb6655&newsletterV3=ba547bff904f&newsletterV3Id=de3db5912a7c&user=Edward+Ma&userId=ba547bff904f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}