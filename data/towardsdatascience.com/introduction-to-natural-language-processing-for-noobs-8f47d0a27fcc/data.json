{"url": "https://towardsdatascience.com/introduction-to-natural-language-processing-for-noobs-8f47d0a27fcc", "time": 1683000441.740454, "path": "towardsdatascience.com/introduction-to-natural-language-processing-for-noobs-8f47d0a27fcc/", "webpage": {"metadata": {"title": "Introduction to Natural Language Processing for Noobs | by Vikas Bhandary | Towards Data Science", "h1": "Introduction to Natural Language Processing for Noobs", "description": "This post is my attempt to give an overview of basic concepts which might help noobs. For better understanding, I will be following a completed Kaggle competition Quora Insincere Questions\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/overview", "anchor_text": "Quora Insincere Questions Classification", "paragraph_index": 0}, {"url": "https://www.kaggle.com/vksbhandary/introduction-to-nlp-for-noobs", "anchor_text": "kernel", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/ASCII", "anchor_text": "ASCII", "paragraph_index": 3}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html", "anchor_text": "token", "paragraph_index": 4}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html", "anchor_text": "fillna()", "paragraph_index": 12}, {"url": "http://nlpprogress.com", "anchor_text": "site", "paragraph_index": 26}, {"url": "https://bit.ly/37gMxwb", "anchor_text": "https://bit.ly/37gMxwb", "paragraph_index": 46}], "all_paragraphs": ["This post is my attempt to give an overview of basic concepts which might help noobs. For better understanding, I will be following a completed Kaggle competition Quora Insincere Questions Classification. Here, we are provided 1.31 million questions with labels and 376 thousand questions, whose label we have to predict. This competition is an example of a binary classification problem. I\u2019ve included python code samples which can be used to solve this problem. The code uses the Keras library, which is very easy to understand. The baseline solution for this competition is on Kaggle kernel.", "So let\u2019s start with basics first.", "NLP is a branch of computer science which deals with the interaction between humans and machines in a natural language. It is the intersection of both linguistics and computer science, so it enables machines to understand and reply to humans queries in a natural language. The major problem with NLP is that human languages are ambiguous. Humans are very intelligent to understand the context and the meaning of words but for computers, this problem is on a whole another level since computers understand neither concepts nor context. For computers to understand concepts, they will need a basic understanding of the world, language syntax, and semantics.", "We can understand and read a piece of text but for computers, every text is a sequence of numbers which don\u2019t represent any concept. One simple example, the letter \u201cA\u201d possesses a special meaning in the English language, it is considered as the first letter of all the alphabets. But a computer sees it as 65 (as 65 is ASCII code for letter \u2018A\u2019). ASCII is the traditional encoding system, which is based on the English characters. Collection of such characters is generally referred to as token in NLP.", "A token is an instance of a sequence of characters in some particular text that is grouped together to make some sense in natural language.", "The easiest way to represent any text in an NLP pipeline is by one-hot encoded vector representation. If a sentence contains a certain word then the corresponding entry in the vector is represented as \u201c1\u201d otherwise it\u2019s \u201c0\u201d. For example, let\u2019s consider the following two sentences:", "One hot encoding is a representation of categorical variables as binary vectors. One hot encoding of words is used to encode the text, where every word is represented with zero except the current word. If we have a corpus of one thousand unique words, then each word representation would require a vector of size 1000. Despite the dependence of size on vocabulary, one-hot encoding representation is being used in productions even today.", "Table 1 shows the one-hot encoding of all the words in sentences 1 and 2. Representations shown in Table 2 are called collapsed or \u201cbinary\u201d representation.", "Modern NLP systems have a dictionary of words, where each word is given an integer representation. So the phrase \u201cnatural language processing is the best field!\u201d could be represented as \u201c2 13 6 34 12 22 90\u201d (ignoring punctuations and treating every word as lowercase). This representation is more efficient in terms of memory usage and it also retains the language semantics.", "There are so many tasks in NLP for which we can use the pipeline shown above. Most Kaggle competitions have cleaned data, which doesn't happen in real life, as these datasets are collected and pre-processed by competition coordinators. In a real-world scenario, we must always assume that the data needs some preprocessing. After preprocessing, we need to break up text in tokens and sentences. After breaking up the text, we use pre-trained embeddings to initialize our model.", "For simplicity of this post, I\u2019ve skipped text analysis from our pipeline. Textual analysis is a very important part of any NLP pipeline. Based on the insights of the analysis, pipeline processes can be modified to improve the performance of the application.", "Textual data can be very messy. So most of the times we will need to preprocess the text. Preprocessing may comprise of removing most common spelling mistakes, substituting numbers in the text, replacing slang words, eliminating common grammatical errors, anonymizing data, removing stop words, etc.", "In the above code samples train_df, test_df are pandas dataframe. First, we remove the empty records in our dataset by using fillna() function. Then we declared clean_text function to separate tokens. In code sample 3, we call clean_text function to apply it on our training and testing datasets by using apply() function of pandas dataframe.", "Text normalization is a process of converting text into single canonical form.", "There are many text normalization techniques such as tokenization, lemmatization, stemming, sentence segmentation, spelling correction, etc. Out of these, tokenization is the most used text normalization method.", "Tokenization is the process of converting a piece of text into a list of words or special characters which have meaning in natural language.", "For example text \u201cNLP is the best!\u201d can be converted into the list \u201cNLP\u201d, \u201cis\u201d, \u201cthe\u201d, \u201cbest\u201d and \u201c!\u201d(Note that special character \u201c!\u201d is separated from \u201cbest\u201d because it has a special meaning in the English language). Some languages like Chinese doesn't have words separated by spaces, so tokenization for these languages is even more difficult.", "Lemmatization is the task of determining if two words have the same root. For example, the words \u2018went\u2019 and \u2018gone\u2019 are forms of the verb \u2018go\u2019. Stemming is similar to lemmatization but it just strips from the end of the word. Using lemmatization or stemming means you are throwing away some information from your data. Spelling correction can be used in your NLP systems to eliminate the input error and raise the performance of NLP system. Nevertheless, NLP systems are assumed to be robust to a small variation in input due to unknown tokens.", "In the above code samples, we train a tokenizer by using fit_on_texts() function of a Tokenizer object. This object can be used to convert text into an integer representation of input data. After this, we can limit the size of the input sequence by using pad_sequences() function from Keras package.", "Word embedding is the representation of document vocabulary in a vector space. Embedding vector with size ranging from 50 to 600, can capture both syntactic and semantic information. If enough computational resources are available, then even bigger vector size can be used. The performance of word embeddings can vary either because of the different algorithm used or the size of the embedding vector. But at some point increasing embedding size doesn\u2019t improve the performance.", "Language model (LM) can replace word embedding in NLP systems. LM is different from word embeddings as they can be trained and fine-tuned on a corpus. Word embeddings are treated as a single layer and can\u2019t be tuned further. A statistical language model is a probability distribution over sequences of words. One such example is the N-gram model. The N-gram models calculate the probability of word \u2018w\u2019 appearing after \u2018h\u2019 words.", "To use word embedding in our code, we read and process the embedding file. In each line, the embedding file contains a word and its embedding vector. In the code sample 6, we split the line and create a dictionary which stores the embedding vector for each word.", "After reading the embedding file, we can create a list of used words and embedding matrics. Note that the code sample 8 uses the tokenizer object to access the vocabulary words. Here, we recreate the embedding matrics using the normal distribution. This helps in initializing vectors of words that are not present in the dictionary embeddings_index.", "For a detailed understanding of word embedding and word2vec read above article.", "Models in machine learning pipeline are a sequence of mathematical operations, which can learn to estimate the output on unseen data.Selecting your model is very crucial, as it can decide the performance of your system.", "You can start with very basic models if you are a newbie. But you should experiment with at least a few models to see which parameter settings give you the best results. Once you have six or more models that are working good, you can ensemble them into one huge model.", "Initially, you can choose to use double LSTM or GRU layers and average pooling layer for testing. After experimenting, you can make changes to your model by making it even deeper or using more advanced state-of-the-art models. For tracking NLP progress, you can visit this site and see which models perform better for a specific task.", "The following diagram gives a conceptual representation of the model we have chosen for our first test.", "In the diagram above, you can see the conceptual layout of our model. A layer in a model represents a specific math operation. A model can have a minimum of two layers, the input layer, and output layer. Input layer in Keras is used to instantiate a tensor. The output layer in a model is a neural network with several nodes.", "Embedding layer in a model performs dot product on the input sequence and embedding matrix, which converts every word index into the corresponding embedding vector.", "Dropout layer randomly drops the input by a given percentage. This layer converts the input unit to zero. This process helps in preventing overfitting.", "Gated Recurrent Unit is a type of recurrent layer which doesn\u2019t suffer from vanishing gradient problem. The output of Bidirectional GRU is the combination of the forward and backward GRU.", "Pooling operation decreases the size of the input sequence by performing a mathematical average, maximum, etc. Thus, the average pooling layer performs average pooling also max-pooling layer performs maximum pooling. Concatenation layer combines different input sequences.", "The dense layer is a simple neural network with a fixed number of nodes and a specified activation function.", "For a complete understanding of working of Recurrent Neural Network and GRU Networks, you can read the following article.", "In code sample 9, the input layer takes input of size equal to ques_len. The embedding layer takes the embedding matrix and its dimensions as parameters. The output of the embedding layer is of size ques_len x embedding_matrix.shape[1]. The SpatialDropout layer randomly drops the output of Embedding layer with a fraction of 0.2.", "Next two layers are bidirectional GRU, which is a type of recurrent neural networks. GlobalAveragePooling1D and GlobalMaxPooling1D extract the average and maximum features from the output of the second bidirectional GRU layer. The output, avg_pool, and max_pool is concatenated to feed into the dense layer. The last layer is the output layer which gives a number between 0 and 1.", "Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned.", "In this phase, we try to make use of previous knowledge in our NLP system. Using a separate model trained in a different setting can improve the performance of our model. This process is known as knowledge transfer. The most popular method for initialization of Neural Networks in NLP systems is Word Embeddings.", "Recent developments in NLP has clearly shown that Transfer learning is the way to move forward. Using pre-trained LMs, new state-of-the-art models are getting better and better. LMs are pre-trained on an enormous dataset also they are based on Transformer architecture.", "If you want to read more about transformer models read the following paper.", "In coding sample 9, we initialized our model using embedding_layer which is initialized to the weight of embedding_matrix.", "After you have decided which model you will be using for the first time, you can train your model. You should always start with testing your model on one-tenth of your training dataset, as it makes the testing much faster. There are some problems where different preprocessing methods might give you varying performance.", "In code sample 10, we add the functionality to save the model every time, if its validation accuracy is increased. Then lastly we call the function fit(), to train the model using train_X, train_y (our features and labels) and a bunch of other parameters. We can always increase the batch size if we want to do training at a faster speed. The number of epochs indicates how many times the same training data will be fed to the model.", "After training the model, we must test our model for its accuracy and evaluate how well it does on unseen data. For that purpose, we predict and compare the results with actual labels. If the performance of the model isn\u2019t according to our expectations, we need to make changes in our pipeline. In Kaggle competitions, Leaderboard score can be a great way of evaluating our model.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Full-stack developer, Deep learning & NLP Enthusiast, #dreamer https://bit.ly/37gMxwb"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8f47d0a27fcc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vksbhandary?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Vikas Bhandary"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d3c13d219b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&user=Vikas+Bhandary&userId=3d3c13d219b3&source=post_page-3d3c13d219b3----8f47d0a27fcc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@fabioha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "fabio"}, {"url": "https://unsplash.com/search/photos/ai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/overview", "anchor_text": "Quora Insincere Questions Classification"}, {"url": "https://www.kaggle.com/vksbhandary/introduction-to-nlp-for-noobs", "anchor_text": "kernel"}, {"url": "https://en.wikipedia.org/wiki/ASCII", "anchor_text": "ASCII"}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html", "anchor_text": "token"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html", "anchor_text": "fillna()"}, {"url": "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa", "anchor_text": "Introduction to Word Embedding and Word2VecWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of\u2026towardsdatascience.com"}, {"url": "http://nlpprogress.com", "anchor_text": "site"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9", "anchor_text": "Illustrated Guide to Recurrent Neural NetworksUnderstanding the Intuitiontowardsdatascience.com"}, {"url": "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be", "anchor_text": "Understanding GRU NetworksIn this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You NeedThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an\u2026arxiv.org"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8f47d0a27fcc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8f47d0a27fcc---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----8f47d0a27fcc---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8f47d0a27fcc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8f47d0a27fcc---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&user=Vikas+Bhandary&userId=3d3c13d219b3&source=-----8f47d0a27fcc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&user=Vikas+Bhandary&userId=3d3c13d219b3&source=-----8f47d0a27fcc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8f47d0a27fcc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8f47d0a27fcc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8f47d0a27fcc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vikas Bhandary"}, {"url": "https://medium.com/@vksbhandary/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "69 Followers"}, {"url": "https://bit.ly/37gMxwb", "anchor_text": "https://bit.ly/37gMxwb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d3c13d219b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&user=Vikas+Bhandary&userId=3d3c13d219b3&source=post_page-3d3c13d219b3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcf4958572aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-natural-language-processing-for-noobs-8f47d0a27fcc&newsletterV3=3d3c13d219b3&newsletterV3Id=cf4958572aa0&user=Vikas+Bhandary&userId=3d3c13d219b3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}