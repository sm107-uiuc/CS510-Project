{"url": "https://towardsdatascience.com/the-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a", "time": 1683007171.389226, "path": "towardsdatascience.com/the-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a/", "webpage": {"metadata": {"title": "The Power of Quality Assurance \u2014 Designing Robust QA Processes for SQL Data Analysis Pipelines | by Michael Gendy | Towards Data Science", "h1": "The Power of Quality Assurance \u2014 Designing Robust QA Processes for SQL Data Analysis Pipelines", "description": "Mistakes happen. Bugs can appear seemingly from nowhere. Data sources can falter at a moment\u2019s notice invalidating your entire analysis pipeline. As an analytics team, ultimately it\u2019s your\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/michael-gendy-a04263161/", "anchor_text": "https://www.linkedin.com/in/michael-gendy-a04263161/", "paragraph_index": 34}], "all_paragraphs": ["\u201cA milligram of prevention is worth a kilogram of cure\u201d \u2014 John Robert Colombo", "Mistakes happen. Bugs can appear seemingly from nowhere. Data sources can falter at a moment\u2019s notice invalidating your entire analysis pipeline. As an analytics team, ultimately it\u2019s your responsibility to solve these problems and ensure your reports, deliverables and insights are as accurate as possible. Designing a robust quality assurance (QA) process is paramount to solidifying confidence in your outputs as a team.", "This article will outline some key steps to designing a robust QA process to save time, money and resources; this will ensure that you\u2019re making the most positive impact possible through your analysis.", "There are so many reasons why a robust QA process is invaluable to your business and team. Here\u2019s a handful of the most meaningful outcomes of a complete QA process:", "Business decisions based on false data are costly for all parties involved. It can mean work is repeated, performance is reduced, money is lost or crucially, confidence in your outputs is hit. Confidence in the insights and recommendations you and your team are providing is crucial in gaining the trust of your stakeholders, and making a larger impact with your work. You don\u2019t want one small mistake to affect future performance, both for you and your team. Most larger companies can devote entire teams to the QA process, however in small to medium sized companies that do not have access to such resources, it becomes the responsibility of the analytics team as a collective to ensure that proper infrastructure is built to reduce any unwanted errors in deliverables. Robust QA processes provide tangible, reliable value in the prevention of any such problems leading to efficient and effective data-driven decision making.", "QA is more than just checking over your code. Pipelines with many moving parts, changing volumes and third party data sources can be difficult to manage. Forming a well-rounded and robust QA process that considers any immediate challenges, and accounts for future error scenarios, requires tackling this issue from many angles:", "This article will outline the reason why each of these stages is so important, suggest best-practices to keep your performance high, and ensure your time is spent on providing as much value as possible.", "Often our absolute code efficiency is at the forefront of our coding decision making. Manipulating large volumes of data in complicated queries does require diligent thought to save time and money on processing resource. However, frequently there is the opportunity to trade-off absolute performance with readability to improve how queries are evaluated.", "Using a simple SQL example, let\u2019s say we want to list the names of records I own in my record collection that are vintage. We could solve this using the following query:", "If I owned a very large, entirely impossible collection of billions of records, this is likely close to the best solution, as opposed to using a more computationally heavy sub-query. Yet if I had to hand this over to a friend to run the same query, it doesn\u2019t have that understand-at-a-glance-readability that we might achieve through other means. Consider the questions the new analyst needs to figure out before running the query:", "In these cases, adding milliseconds to the run time of this query in exchange for some more transferable, readable code is an extremely worthwhile trade-off. By rewriting using a sub query and adding a simple comment, we achieve a much clearer result with very similar performance:", "Same result, similar performance and much improved readability. Notice:", "Good readability best-practices include commenting out the purpose of the query, utilising WITH statements (CTEs), capitalising functions, keeping queries short and having clear processing logic. Thanks to our changes, we now have clarity on many of the query\u2019s nuances. We now know it\u2019s purpose and that we\u2019re returning all results from my record collection (not just unique results). We can interpret the code faster thanks to the sub-query, and don\u2019t have to worry about any faulty JOIN logic. Slower but cleaner code is definitely the winner here.", "While it may seem like it saves a small amount of time on a simple query like this, the actual value it derives will continue to pay off every time this is run. When your SQL infrastructure involves complex, multiple thousand line scripts all integrated with one another, the pay-off for writing your code in a clear manner increases exponentially. Not to mention the often overlooked factor in smaller teams \u2014 what happens when the primary analyst for these scripts is on holiday or off sick? Can another analyst pick up the script effectively? An analyst contributes greatly to making business operations more efficient, and so it\u2019s crucial to apply this to their own processes and practices too, in the form of producing readable and transferable code.", "Testing and retesting any production script is the biggest non-negotiable of all. We must both test code in isolated segments (unit testing), and test how well these isolated segments work together (integration testing). For example, testing that a temporary table is created correctly, then ensuring that any queries that use the temporary table interact with it correctly.", "SQL is deceivingly simple, and learning to anticipate duplication and becoming accustomed to diagnosing faulty joins can bring a lot more peace to your day-to-day analysis. Commenting out code to evaluate if each query has produced the intended result (unit testing), and making sure they won\u2019t break any of your other dependent queries (integration testing) is a skill worth developing in itself. At a minimum, you should include unit testing queries and comments which answer the following questions:", "The next step is ensuring these individual queries work in harmony with any other queries in your script. Checking that one section of code is working correctly but failing to test how this query fits into your larger data pipeline can create unnecessary and easily avoided vulnerabilities.", "Take the time and document your thought process \u2014 this will pay off greatly by preventing errors and ensuring bugs are identified and solved swiftly. Give as much detail as you can in a clear, succinct manner. If the result is correct, but counter intuitive \u2014 explain it in a comment! The goal here is to make your outputs reproducible and allow others to easily verify the result. Outlining the above during SQL script writing will soon become second nature, and you\u2019ll develop your own intuition for bugs that arise repeatedly. In addition to building more confidence in our work, we also give our peers the ability to pick up the script, understand the purpose and evaluate its efficacy with minimal hassle.", "It\u2019s genuinely quite incredible how many seemingly insignificant changes to your data ingestion can affect your data pipelines. Here\u2019s a handful of examples:", "Any of these issues can affect multiple queries in seemly obscure ways, from causing JOINS to fail, breaking NOT IN statements and messing with your WHERE clauses. Without clear visibility on your data at every processing stage, often these issues can go unnoticed causing a headache down the line. Odds are, the longer the issue has been present, the bigger the consequences \u2014 catching any of these issues early can be the key to smooth operation.", "Clear data visualisation is the key to solving this. An easy way to do this is by creating a pipeline monitoring table, inserting counts of key metrics at every stage of the data processing, and visualising the outputs in a BI tool:", "This is a very useful, general format of how we want to track our data. Creating visualisations of a table such as qa_pipeline, and tracking important values such as row counts, field population, KPI sums and primary key interactions between queries can enable at-a-glance debugging. For example, in your raw tables, your primary key will always be unique. In your cleaning script where you create a new table and join in extra information, if we now see the count of primary keys increase, we know there\u2019s some duplication caused by the cleaning script. Inversely, if we see the distinct primary key count decrease, we know some have been filtered out:", "The visualisation above demonstrates the power of these qa_pipeline tables. It\u2019s immediately obvious that every metric doubles in the final table, indicating that there\u2019s faulty logic in the script to produce the final_output which is causing duplication. A number of issues can be discovered by simply getting a pair of eyes on the true outputs of each script or query. If we saw a significant decrease in important_kpi_sum, this would suggest that there\u2019s an error when using important_kpi or a mistake in the aggregation logic \u2014 a clear sign that the code should be reviewed. While manually going through entire outputs regularly can demand too much resource for many analytics teams, a quick inspection of these summary visualisations every morning will lead to more proactive solutions to diagnose incorrect SQL scripts.", "Another use case of the same table is making sure your data is being ingested correctly \u2014 the quality of your raw data will mirror the quality of your outputs. Viewing your ingestion table metrics using the qa_pipeline table visualisations allows for easy monitoring that your data is coming in the intended formats. Anomalous results such as low row counts, missing days and high numbers of missing fields can be identified easily and quickly:", "At a quick glance, we can see there are some ingestion issues for our raw_table. On the 3rd of January when the script was run, we saw three times the amount of rows as we see distinct primary keys, indicating that the data may have been ingested three times. In addition, on the 5th of January, we can see our important_field was not completely populated, hinting at some tracking issues. After only a few seconds of looking at the visualisation, we can clearly evaluate the quality of the data and highlight any issues, further improving our ability to react to problems and resolve them efficiently.", "Use your creativity in how you display these counts to suit your business use case. Your intuition is a valuable resource here \u2014 learn to anticipate how your code will affect your data and compare this consistently to the visualisations provided by these qa_pipeline tables. Setting up these INSERT INTO queries at ingestion, during data cleaning and after the deliverables have been produced gives you end-to-end clarity on how your scripts are performing, saving valuable resources on debugging and preventing any errors.", "A competent analyst can work through any problem. In theory, all code should be running perfectly if the quality of the data remains the same. However, when managing the ingestion of multiple data streams from different sources, this data quality is never guaranteed. Possibly the simplest way to maintain good quality data is by taking control of it yourself, allowing full autonomy and greater reactivity when problems arise. Next time you\u2019re ingesting third party data, consider the following:", "\u201cAn international third party is supplying time series data in the form of a daily excel report. Due to daylight savings in their region, the timestamps they have provided are all an hour out, which means you\u2019re missing an hour of data in the report provided. They\u2019re in a completely different timezone, won\u2019t be able to rectify the issue until the morning, and the clients demanding a crucial report be delivered by the end of the day.\u201d", "Not a fun spot to be in. You could make a strong case that the third party is not even complicit, because the process is inherently flawed for many reasons. However, if we had autonomy over the data they provided, by querying their API directly for example; we could simply make another call to correct the time frame within minutes. No more missed deadlines and no more unhappy clients.", "In reality, third parties may not have the same resources, infrastructure, practices or approach that you may be used to. This difference can lead to errors in ingestion, processing or presentation which may be slow to fix if you need to ask a third party to reformat or reproduce the data for any reason. Retaining as much autonomy over your data as possible enables you to be more reactive to changes, find solutions faster and optimise the production to a level that you control.", "A key attribute that\u2019s helped me throughout my time as an analyst is my ability to learn from others. I really enjoy learning why people are successful at what they do and the traits they possess which I can apply to my own life and career. Consistent, constructive feedback from others can really accelerate your learning and challenge you to perform better, and peer review is crucial to this in an analysis environment. Having a colleague review your code can have many benefits:", "A fresh pair of eyes can be incredibly impactful to improving your code. Having someone approach the same problem, think through the same process and come to the same conclusion is so beneficial for someone that\u2019s been staring at the same multi-join for the last 2 hours, not able to figure out if it\u2019s working properly. I can\u2019t remember how many times I\u2019ve walked over to a teammates desk with a bug I\u2019ve been stuck on, only for them to spot it within 30 seconds, or suggest a much more efficient way of achieving the desired result. One of the biggest reasons for this is that often bugs or duplicates appear through the most difficult to notice, obscure and inconspicuous SQL interactions involving the nichest edge cases.", "As an analyst, your ability to identify potential edge cases and account for them, ideally before they even occur, is paramount to preventing any problems before they even materialise. Thinking of every potential edge case alone is extremely difficult and nigh on impossible, so utilise the best resource you have \u2014 productive discussion with your team and peers. Building consistent peer review into your workflow, to ensure that no code goes to production without being seen by at least one colleague, goes a long way to increasing your own confidence as an analyst, and can drastically speed up delivery of work.", "Ensuring your deliverables are as high quality as possible means accounting for as many error scenarios as possible. Once the above infrastructure and QA processes have been applied, you\u2019ll quickly find a vast improvement in bug detection, error management, deliverable quality and stakeholder satisfaction. I hope this article has been useful and that you can apply some of these practices to improve your data pipeline processes.", "If you enjoyed this article and would like to get in touch, feel free to contact me on LinkedIn: https://www.linkedin.com/in/michael-gendy-a04263161/", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Budding data scientist. Contracted data analyst for Amazon."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2b85e9a3928a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@michaelgendy?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelgendy?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Michael Gendy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d45e261014f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&user=Michael+Gendy&userId=7d45e261014f&source=post_page-7d45e261014f----2b85e9a3928a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.linkedin.com/in/michael-gendy-a04263161/", "anchor_text": "https://www.linkedin.com/in/michael-gendy-a04263161/"}, {"url": "https://medium.com/tag/qa-testing?source=post_page-----2b85e9a3928a---------------qa_testing-----------------", "anchor_text": "Qa Testing"}, {"url": "https://medium.com/tag/sql?source=post_page-----2b85e9a3928a---------------sql-----------------", "anchor_text": "Sql"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2b85e9a3928a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/pipeline?source=post_page-----2b85e9a3928a---------------pipeline-----------------", "anchor_text": "Pipeline"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----2b85e9a3928a---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&user=Michael+Gendy&userId=7d45e261014f&source=-----2b85e9a3928a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&user=Michael+Gendy&userId=7d45e261014f&source=-----2b85e9a3928a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2b85e9a3928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2b85e9a3928a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2b85e9a3928a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelgendy?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelgendy?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Gendy"}, {"url": "https://medium.com/@michaelgendy/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d45e261014f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&user=Michael+Gendy&userId=7d45e261014f&source=post_page-7d45e261014f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F653b0e65d28a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-quality-assurance-designing-robust-qa-processes-for-sql-data-analysis-pipelines-2b85e9a3928a&newsletterV3=7d45e261014f&newsletterV3Id=653b0e65d28a&user=Michael+Gendy&userId=7d45e261014f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}