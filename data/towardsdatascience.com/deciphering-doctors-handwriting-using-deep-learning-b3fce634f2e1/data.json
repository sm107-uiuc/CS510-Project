{"url": "https://towardsdatascience.com/deciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1", "time": 1683000538.738351, "path": "towardsdatascience.com/deciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1/", "webpage": {"metadata": {"title": "Deciphering Doctors' Handwriting using Deep Learning | by Ignaz Wanders | Towards Data Science", "h1": "Deciphering Doctors' Handwriting using Deep Learning", "description": "We built a bot that automatically reads doctors handwriting on Belgian death certificates with an accuracy of 47% of certificates of the usable data set correctly predicted. The bot supports\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5", "anchor_text": "https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5", "paragraph_index": 16}, {"url": "https://medium.com/vectrconsulting", "anchor_text": "https://medium.com/vectrconsulting", "paragraph_index": 115}], "all_paragraphs": ["We built a bot that automatically reads doctors handwriting on Belgian death certificates with an accuracy of 47% of certificates of the usable data set correctly predicted. The bot supports government officials with the official death registration and allows for a faster such registration.", "The solution consists of three main components: an image-processing module, a neural net, and a natural-language processing module to output predictions of medical terms.", "When a person deceases, a medical practitioner must certify the deceased state of the person. There is a standard form which the physician fills in. This is done \u201cin the field,\u201d through a handwritten statement on the form, which is subsequently forwarded to other officials under sealed envelope.", "The physician officially records the direct cause of death, and, if known, any secondary causes. An example of such a death certificate is given below.", "(All examples are in the Dutch language, so if you think you can't read anything, that's normal!)", "The example above is fairly straightforward to read, but notoriously difficult-to-read examples also exist.", "Because the physicians record death causes \"in the field,\" a 100% digital solution is not feasible, and a lot of handwriting will exist for years to come.", "The raw data are one-page scans, provided as a PDF.", "The first step is to anonymize the data. Hashes are calculated from document IDs, and a region of interest (ROI) is cut out of the document, which includes the handwriting, but which excludes any personal data, such as the physicians signature, the date and place of decease, etc.", "This yields smaller images than the originals, and there is no link from the images back to the original scans.", "The second step is to clean the images. There is background text from the document template, and there are scan errors. We remove the background, we apply noise reduction and a slight blurring to close small gaps in the handwriting lines while retaining spaces between words.", "The third step is to crop the image to the smallest size possible containing the handwriting.", "The fourth step is to cut between the lines. So, when the text has n lines, we end up with n image segments per original certificate.", "We then apply a neural network (NN) to predict what is written, with a calculated confidence of how certain the NN is of the correctness of the prediction.", "Predictions that include unknown words require additional natural language processing (NLP) to map it to known words. Again, we calculate a confidence level.", "To summarize, the solution for reading the handwriting is a combination of image processing, deep learning, and natural language processing.", "One of the most difficult parts of building a neural network is to decide about its architecture. We decided not to reinvent the wheel and to start with an existing NN architecture. We found that the NN described by H. Scheidl (2018, https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5) worked very well, after a few small modifications.", "The NN by H. Scheidl has a five-layer convolutional neural net (CNN), a two-layer recurrent neural net (RNN), and a connectionist temporal classification (CTC) layer. For the details, we refer to the publication by H. Scheidl.", "This NN trains on character and punctuation-mark recognition, and can therefore also recognize new texts consisting of the same characters and punctuation marks.", "We had to make some modifications to the NN by Scheidl to have it work with the data set under investigation:", "Increase word length. The Scheidl net takes words up to 32 characters. Since we consider full text lines, we need to increase this to 128 characters length. (This is larger than the longest label in the data set.)", "Disable word-beam search. The neural net (NN) can be constrained to a dictionary of words. This makes sense if you need to find all handwritten words. The NN will assign each handwritten word to a known word from the dictionary.", "We tested this with the dictionary of labels available to us, but we found that this produced a lot of false positives. In our business case it is better to not assign a word if the NN cannot really read the handwriting with any confidence. We prefer a human to have the final saying in these cases.", "For these reasons we decided not to use the WBS, but to apply a separate word matching model as a post-processing NLP step.", "Allow multiple words. In medical terms, certain word combinations frequently occur together. By using text lines instead of words only, the NN is able to recognize these, which increases the success rate.", "Before training the NN, we need to examine the quality of the data. We want to train the NN with data for which we have high confidence that the labels are correct.", "The accuracy of the NN is highly dependent on the quality of the data. We have six years of labeled data, but from the business we know that", "Therefore, all certificates with a $ sign in the label are excluded from the training, validation, and test set: these labels are known to be uncertain.", "Other certificates that we exclude are those where the number of identified text lines after segmentation is not equal to the number of labels for the certificate. These are cases where a single death cause is written over more than one line. The human who has labeled the data has contextual domain knowledge to know this, and has labeled this as a single line. The segmentation cannot know if a single death cause is written on a single line or not. Therefore we exclude these certificates; we cannot be certain that a specific label corresponds to a specific handwritten line.", "For technical reasons, lines that are longer than 128 characters are also excluded, but this is an insignificant fraction of less than 0,5%.", "Of the well-labeled certificates, we define 60% as training set, 20% as validation set, and 20% as test set.", "Training occurs on text lines, not on full certificates, and after segmentation, the above certificate numbers result in the following text line numbers.", "The data quality of the labels is unknown at this point. We must assume they are the ground truth, even though it is known that not all labels are exact representations of the handwriting. Visual inspection learns at least the following cases exist in the labeled data set.", "We will see later that we can quantify the data quality to some extent.", "Neural networks are notoriously black-box and hard to understand. Therefore, we define an ensemble of models and compare their outcome. The variation in the ensemble is by which data is included in the training, validation, and test sets.", "The ensemble technique helps during development with the model understanding, and improves the model robustness.", "Our ensemble covers the following data set combinations. The accuracy given is the percentage of text lines in the validation set that was correctly predicted by the neural network. With correctly predicted we mean an exact match of the text string: all characters and punctuation marks match exactly.", "In an early phase in the development of the neural network, the accuracies of two years combined wildly fluctuated between 40% and 75%. As the number of certificates in the data set is nearly the same for each year, this was highly unexpected, and it turned out this was due to a small error which overfitted the model for some data sets. If one does not perform ensemble tests, these overfitted models often go unnoticed. After correction of the error, all accuracies are in line with expectation.", "The variation of accuracy over the years is clear when we plot the results per year of certificates. Shown below are the number of certificates in the training set per year (in blue) and the accuracy the model attains for that year (in red).", "The plot clearly indicates that the accuracy over the first four years (2012\u20132015) is fairly constant with an average accuracy of 36% and 2% standard deviation. We find 2016 significantly lower in accuracy, which is probably due to the lower quality of the labels, as was already known by the business.", "Increasing the data set with labels of less quality does not increase the accuracy of the model. On the contrary, it may even reduce it.", "The best model is the one with the largest data set, namely the combined years 2012\u20132016. Below we show how the accuracy increases when increasing the size of the data set by adding one year at a time.", "Applying the models to the full 2017 data is somewhat less accurate than for the other test sets. This is possibly a result of the less accurate labels for the year 2016 and 2017.", "The best scoring model is the training set from the years 2012\u20132016 combined. The accuracy attained is 53% of exactly correctly predicted text lines. We will examine this best-fit model further.", "The NN model assigns a confidence level to each prediction. For the test set, we know whether or not the prediction exactly matches the label. For each prediction we know whether it falls within the set of correct predictions or the set of incorrect predictions. We can then plot the number of predictions within a confidence range.", "The range of confidences is between 0 and 1. We divide the range into 20 bins, each 5% wide, and count the number of text-line predictions with that confidence level for both the correct and incorrect set of predictions.", "The figure below shows the results for the two sets. Correct predictions are shown in blue, and incorrect predictions are shown in red.", "Predictions with high confidence is what we aim for. These predictions should be accurate. The results show indeed that the number of correct predictions peaks at 95\u2013100% confidence, and is lower at lower confidence levels.", "However, predictions that are known to be incorrect, but which still have a high confidence level show that there exist so-called false positives. These are highly unwanted!", "Predictions with low confidence are mostly incorrect predictions, which is shown by the (red) peak at 0\u20135% confidence level. These are the illegible hand writings. (Illegible for the machine, not necessarily for the human.)", "Predictions with low confidence will always have to be shown to a human who must judge the prediction. The human feedback can be used to improve the model by retraining it with improved data.", "There are also correct predictions for low confidence levels. These are false negatives. The machine will judge them as uncertain and they will also be sent to human inspection.", "Let\u2019s examine some examples from the correct predictions with high confidence.", "The false-positive population is the group of predictions for which the neural network is quite certain, but it does not match the given label.", "In the first example we see that the prediction is actually correct, but the label adds \u201c, COPD\u201d. Hence, our exact-match test makes this fall in the \u2018incorrect\u2019 set. This is an example where the label is not an exact match of the handwritten text.", "In the second example we see a minor difference between label and prediction: \u00eb vs. e. The label is linguistically correct, but the diaeresis on the e is not written by the doctor. It\u2019s only because we are so strict on testing exactness that this prediction falls into the false-positive population.", "The third example shows a human error. It was labeled \u201cuitdroging\u201d, but the prediction \u201cuitputting\u201d has been confirmed by the business as correct. This indicates that humans also make mistakes, thereby creating false positives as well.", "The fourth example is also a human error. Labeled as \u201cverstikking\u201d but it should be \u201cverslikking\u201d, which is correctly predicted by the neural network.", "When manually verifying a random sample of 30 false positives with confidence larger than 60%, all of these show that the neural network was correct, and the label was wrong. For two of these, the human was in error. Sometimes the difference is very small, but due to our strict testing on correctness even the smallest difference between prediction and label results in a false positive.", "The reassuring thing is that the identified population of false positives is already very small, and is probably smaller than the population containing human errors. Human error is estimated to occur 2/30 = 7% in the false-positive group.", "The following is an example of a text line that is correctly predicted, but the neural network had very low confidence in the prediction. Such a case with low confidence level will be flagged for human judgment.", "Then there is a whole group of hand writings that the NN cannot read. All of these will have to be manually judged.", "In the first example, the arrow is not detected correctly, and the last word is not detected either, where it is also questionable that the label is correct. The first word is correctly found, though.", "In the second example we see that parts of the text line above are overlapping with the text. This confuses the recognition of individual letters. But the other words are also poorly readable. It is desirable that these cases receive human judgement.", "The neural network is primarily trained to recognize letters. It doesn\u2019t know about words. This means that NN predictions form a sequence of letters, and although it has some sense of sequential probabilities, some letter combinations might not make sense.", "If the NN prediction exactly matches words from the dictionary, we do not perform further text matching. If there is already an exact match, we would only risk modifying a correct prediction and turn it into something else. So the text matching to words described below is only done when the NN prediction does not exactly match dictionary words.", "Note that this knowledge can also be used to add new words to the dictionary.", "We have experimented to limit the neural network to only predict known words from the medical dictionary, but this resulted in too many false positives. The outcome of such a NN is always a word from the dictionary. It cannot recognize other words.", "Therefore, we decided to train the NN on character recognition only, and perform the matching to the dictionary as a post-processing step.", "Matchings were based on two algorithms.", "The Jaro and Damerau-Levenshtein distance compares predicted letter sequences with known words, and considers the number of \u2018edits\u2019 that are needed to correct the prediction as the distance to a word.", "The distance of the prediction to any word in the dictionary can be calculated, and the word with the shortest distance is assigned as the final prediction.", "The bigram distance is calculated if the text line consists of at least two words. From the dictionary it is known that some words frequently occur together. This helps in identifying such words from predicted character sequences.", "A complexity of the matching is to define how confident the model is about a match. Although the distance of a prediction to a word is a measure of how close the prediction is, it is not a measure of the confidence level.", "As an example, consider the following handwriting, which reads \u201csteekwonde\u201d.", "The NN prediction is \u201cstuikwonde\u201d, which is close, but not exact. In the dictionary we find two look-alike words: \u201cstuitwonde\u201d and \u201csteekwonde\u201d. The first one is only one letter away, and the second is two letters away. But because both are close, the model should be quite uncertain about which one to pick.", "So when we match a NN prediction to a dictionary word, the confidence level should take into account other, close words. The closer the words are to one another, the more uncertain the final result is. If two words are equidistant from the NN prediction, the matching confidence is 50%. If three words are equidistant, the confidence levels falls to 33%.", "We have computed text-matching confidence levels based on such statistics. This is important, because in the final product, both the NN and the text-matching confidence levels must be taken into account in deciding whether or not to have a human intervention.", "The text recognition models operate on individual text lines. On average there are two text lines per certificate, at most there are four text lines on the certificates that are part of the testable data set. (The well-labeled data set.)", "It is expected that the more text lines there are per certificate, the lower fraction of certificates are fully predicted correctly. The figure below shows the number of certificates as a function of number of text lines on the certificate.", "we confirm that with increased number of text lines per certificate, the fraction of fully correctly predicted certificates decreases.", "Let us define the confidence level of predicting a certificate correctly as the minimum confidence level for any text line prediction on the certificate. We can then plot the number of correctly predicted certificates as a function of the confidence level. This is shown in the following plot, where colour-coding is used to indicate the number of lines on the certificate.", "The same plot can be made for certificates that are not correctly predicted.", "We find that for high confidence levels there are very few incorrect predictions on certificate level.", "Excluded from the training and test sets were those certificates which even the human could not read. These have $-signs in the label. As the examples below show, the NN reads some of these correctly, and at least can make useful suggestions to a human trying to decipher the text.", "If we draw the number of text line predictions as a function of NN confidence level, we see that the distribution shows a large peak at 0\u20135% confidence, which is expected. If the human can\u2019t read it, the machine probably can\u2019t, either. Also,the NN was not trained on reading intelligible certificates.", "Note there is a bias in the NN model in the sense that those that cannot be read by a human are most likely the poorest hand writings, and the model could not be trained on these because these are not labeled. By offering human feedback in the system, the machine can slowly be trained on these cases also.", "In the first example the first word could not be read by the human. The NN predicts a word that is close to the final prediction after word matching. (\"PP\" stands for post-processing and includes the text matching.) The final PP prediction is actually correct. So even though the confidence of the prediction is low, the prediction will function as a very good suggestion to the human judge.", "In the second example, the second word was unreadable by the human. The NN prediction is only one letter off: a c instead of an e. The final prediction after word matching is then 100% correct.", "This shows that even though the NN is not trained on these cases, it recognizes sufficient letters to come up with very plausible predictions for illegible words.", "Our final results show that 54% of the neural-network-predicted text lines exactly match the human-defined label. If we add the text matching to the dictionary as a post-processing step, we can increase that to over 60%, and if we take into account that there are approximately two text lines per certificate on average, we have 47% of complete certificates that are predicted correctly.", "From the text lines that were not readable by a human, i.e., those with a $-sign in the label, we asked two business experts to verify our predictions manually. We took a random sample of 100 such text lines. The results were assuring: 20 out of 100 were correctly predicted.", "This gives us confidence that even for illegible certificates, the model algorithms can support humans by giving suggestions of what is written.", "Remains the question how well we can automate the recognition of doctor\u2019s handwriting and integrate it with the existing software systems. This boils down to the question at which confidence level we can define a threshold above which the prediction can be assumed correct, such that no human must intervene.", "To understand this, we have to go back to the distribution of predictions as a function of confidence level. For the neural network, this function was already shown in a previous figure. We repeat it here for clarity.", "Let us define the threshold at 95% confidence. This would mean that all predicted text lines with a confidence of 95\u2013100% would be considered correct: no human intervention needed. This is all text lines in the last bin of the figure above. This turns out to be about 11% of all text lines.", "We do have some false positives as well, but in this case this is only 0.1% of all text lines. Probably lower than human error, and we know from analysis that a lot of the false positives are due to wrong labeling, so the real number of false positives is even less.", "If we set the threshold at 90% instead, we count all text lines in the two highest bins of the plot above. This results in 20% correct, and 0.2% false positives.", "By setting the threshold lower and lower, we add more correct results, but eventually we add intolerably many false positives. All red bins above the threshold count as false positives. Hence there must be some threshold optimum when we have good results with correct predictions, and where the number of false positives is still acceptably low. What is acceptable is a matter of debate, but assuming that humans also make errors and as long as we don\u2019t perform worse than humans, we can accept a small number of false positives.", "So far, we only looked at the confidence levels of the neural network. If we add the post-processing (text matching) to the picture, we effectively move some of the predictions from the incorrect to the correct population. This is shown in the plot below.", "We see that the peak near zero confidence is lowered for the incorrect population, and the correct population is raised, especially for low confidences, of course.", "We can now make a plot of how many predictions above the threshold are correct and incorrect. This is given in the following plot.", "Shown in green is the population of correct predictions, and in red the population of incorrect predictions. The dashed line is the neural network prediction only, and the solid line includes the post-processing as well, which increases the correct population and decreases the incorrect population.", "Drawn as the vertical green bar is the threshold from 70 to 75%. If we set the threshold here, we have 33% correct predictions and less than 1% incorrect predictions (false positives).", "If false positives are not relevant, the maximum number of correct results is achieved at the threshold where the difference between the green and red lines is maximized. This is around 10%, which results in 59% correct and 8% incorrect predictions.", "In our use case, where we need to predict medical terms in the context of death certificates, such high numbers of false positives are unacceptable. Hence, we must operate on the safe side.", "One could start setting the threshold at 95%, and when the humans, who validate any prediction with lower confidence levels, gain confidence with the model predictions and provide feedback to the system, such that the model becomes better, the threshold can be lowered.", "When the model uses human feedback and improves itself, it will predict with higher confidence, which moves the distribution of predictions by confidence level to the right, thereby pushing more predictions above the threshold.", "We can make the same plot for predicting complete certificates instead of just text lines. This is given below.", "The numbers are somewhat lower here, because all text lines on a certificate must be predicted correctly. We now reach more than 30% correct predictions when we set the threshold at 30 to 35% confidence, but the number of false positives reaches 5%.", "We conclude that reading a doctor\u2019s handwriting is feasible using a trained neural network.", "We found that the NN results are very sensitive to the quality of the pre-processing of the data and cutting it into words or text lines. Careful image processing is essential in the success of the project.", "We also found that letting a neural network match texts to dictionary words gives too many false positive results, and it is better to let the neural network only do the reading, and use other NLP techniques to match against a dictionary.", "We can calculate confidence levels for predictions by the neural network and the text matching, which can be used to define a confidence-level threshold for automation purposes.", "We thank the Flemish government, the Vlaams Agentschap Zorg & Gezondheid, who made this project possible, and especially Koenraad, Anne, Rita, and Wilfried for valuable guidance and business expertise, and their challenging critical notes that elevated the project above the average.", "This project is a collaborative team venture from Vectr.Consulting (https://medium.com/vectrconsulting), with special thanks to Bruno and Jony.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb3fce634f2e1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ignazw?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ignazw?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Ignaz Wanders"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F69864af8c458&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&user=Ignaz+Wanders&userId=69864af8c458&source=post_page-69864af8c458----b3fce634f2e1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5", "anchor_text": "https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5"}, {"url": "https://medium.com/vectrconsulting", "anchor_text": "https://medium.com/vectrconsulting"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b3fce634f2e1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b3fce634f2e1---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b3fce634f2e1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/handwriting?source=post_page-----b3fce634f2e1---------------handwriting-----------------", "anchor_text": "Handwriting"}, {"url": "https://medium.com/tag/vectrconsulting?source=post_page-----b3fce634f2e1---------------vectrconsulting-----------------", "anchor_text": "Vectrconsulting"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&user=Ignaz+Wanders&userId=69864af8c458&source=-----b3fce634f2e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&user=Ignaz+Wanders&userId=69864af8c458&source=-----b3fce634f2e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb3fce634f2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b3fce634f2e1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b3fce634f2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ignazw?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ignazw?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ignaz Wanders"}, {"url": "https://medium.com/@ignazw/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "233 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F69864af8c458&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&user=Ignaz+Wanders&userId=69864af8c458&source=post_page-69864af8c458--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccd74e68a1f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeciphering-doctors-handwriting-using-deep-learning-b3fce634f2e1&newsletterV3=69864af8c458&newsletterV3Id=ccd74e68a1f0&user=Ignaz+Wanders&userId=69864af8c458&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}