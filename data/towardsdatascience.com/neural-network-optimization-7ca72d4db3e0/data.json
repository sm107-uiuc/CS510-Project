{"url": "https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0", "time": 1682996861.688288, "path": "towardsdatascience.com/neural-network-optimization-7ca72d4db3e0/", "webpage": {"metadata": {"title": "Neural Network Optimization. Covering optimizers, momentum, adaptive\u2026 | by Matthew Stewart, PhD | Towards Data Science", "h1": "Neural Network Optimization", "description": "This article is the third in a series of articles aimed at demystifying neural networks and outlining how to design and implement them. In this article, I will discuss the following concepts related\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "Convex optimization", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Rosenbrock_function", "anchor_text": "Rosenbrook function", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Truncated_Newton_method", "anchor_text": "truncated-Newton", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Quasi-Newton_method", "anchor_text": "quasi-Newton", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm", "anchor_text": "BFGS", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Condition_number", "anchor_text": "poorly conditioned", "paragraph_index": 18}, {"url": "http://proceedings.mlr.press/v28/sutskever13.pdf", "anchor_text": "Sutskever, Martens et al.\u201dOn the importance of initialization and momentum in deep learning\u201d 2013", "paragraph_index": 34}, {"url": "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf", "anchor_text": "here", "paragraph_index": 54}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "paragraph_index": 66}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "paragraph_index": 72}, {"url": "https://machinelearning.wtf/terms/batch-normalization/", "anchor_text": "Batch normalization", "paragraph_index": 78}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io", "paragraph_index": 93}], "all_paragraphs": ["\u201cThe goal is to hit the sweet spot of maximum value optimization, where foolish risk is balanced against excessive caution.\u201d \u2015 Steven J. Bowen", "This article is the third in a series of articles aimed at demystifying neural networks and outlining how to design and implement them. In this article, I will discuss the following concepts related to the optimization of neural networks:", "You can access the previous articles below. The first provides a simple introduction to the topic of neural networks, to those who are unfamiliar. The second article covers more intermediary topics such as activation functions, neural architecture, and loss functions.", "These tutorials are largely based on the notes and examples from multiple classes taught at Harvard and Stanford in the computer science and data science departments.", "When talking about optimization in the context of neural networks, we are discussing non-convex optimization.", "Convex optimization involves a function in which there is only one optimum, corresponding to the global optimum (maximum or minimum). There is no concept of local optima for convex optimization problems, making them relatively easy to solve \u2014 these are common introductory topics in undergraduate and graduate optimization classes.", "Non-convex optimization involves a function which has multiple optima, only one of which is the global optima. Depending on the loss surface, it can be very difficult to locate the global optima", "For a neural network, the curve or surface that we are talking about is the loss surface. Since we are trying to minimize the prediction error of the network, we are interested in finding the global minimum on this loss surface \u2014 this is the aim of neural network training.", "There are multiple problems associated with this:", "Fortunately, there are methods available that provide ways to tackle all of these challenges, thus mitigating their potentially negative ramifications.", "Previously, local minima were viewed as a major problem in neural network training. Nowadays, researchers have found that when using sufficiently large neural networks, most local minima incur a low cost, and thus it is not particularly important to find the true global minimum \u2014 a local minimum with reasonably low error is acceptable.", "Recent studies indicate that in high dimensions, saddle points are more likely than local minima. Saddle points are also more problematic than local minima because close to a saddle point the gradient can be very small. Thus, gradient descent will result in negligible updates to the network and hence network training will cease.", "An example function that is often used for testing the performance of optimization algorithms on saddle points is the Rosenbrook function. The function is described by the formula: f(x,y) = (a-x)\u00b2 + b(y-x\u00b2)\u00b2, which has a global minimum at (x,y) = (a,a\u00b2).", "This is a non-convex function with a global minimum located within a long and narrow valley. Finding the valley is relatively easy, but it is difficult to converge to the global minimum due to the flat valley \u2014 which thus has small gradients so it is difficult for gradient-based optimization procedures to converge.", "An important problem is the particular form of the error function that represents the learning problem. It has long been noted that the derivatives of the error function are usually ill-conditioned. This ill-conditioning is reflected in error landscapes which contain many saddle points and flat areas.", "To understand this, we can look at the Hessian matrix \u2014 a square matrix of second-order partial derivatives of a scalar-valued function. The Hessian describes the local curvature of a function of many variables.", "The Hessian can be used to determine whether a given stationary point is a saddle point or not. If the Hessian is indefinite at that location, that stationary point is a saddle point. This can also be reasoned in a similar way by looking at the eigenvalues.", "Computing and storing the full Hessian matrix takes O(n\u00b2) memory, which is infeasible for high-dimensional functions such as the loss functions of neural networks. For such situations, truncated-Newton and quasi-Newton algorithms are often used. The latter family of algorithms use approximations to the Hessian; one of the most popular quasi-Newton algorithms is BFGS.", "Often for neural networks, the Hessian matrix is poorly conditioned \u2014 the output changes rapidly for a small change of input. This is an undesirable property as it means that the optimization process is not particularly stable. In these environments, learning is slow despite the presence of strong gradients because oscillations slow the learning process down.", "So far we have only discussed the structure of the objective function \u2014 in this case the loss function \u2014 and its effects on the optimization process. There are additional issues associated with the architecture of the neural network, which is particularly relevant for deep learning applications.", "The above structure is an example of a deep neural network with n hidden layers. As features at the first layer are propagated through the network, they undergo affine transformations followed by an activation function, described as follows:", "The above equations are true for a single layer. We can write the output for an n-layer network:", "Now there are two possible cases for the above formulation, depending on the magnitude of a and b.", "If the values are greater than 1, for a large value of n (a deep neural network), the gradient values will quickly explode as they propagate through the network. Exploding gradients lead to \u201ccliffs\u201d unless gradient clipping is implemented (the gradient is clipped if it exceeds a certain threshold value).", "If the values are less than 1, the gradients will quickly tend to zero. If computers were capable of storing infinitely small numbers then this would not be a problem, but computers only store values to a finite number of decimal places. If the gradient value becomes smaller than this value, it will just be recognized as zero.", "So what are we to do? We have found that neural networks are doomed to have large numbers of local optima, often containing both sharp and flat valleys which result in the stagnation of learning and unstable learning.", "I will now discuss some of the ways we can help to mitigate the issues we have just discussed regarding neural network optimization, starting with momentum.", "One problem with stochastic gradient descent (SGD) is the presence of oscillations which result from updates not exploiting curvature information. This results in SGD being slow when there is high curvature.", "By taking the average gradient, we can obtain a faster path to optimization. This helps to dampen oscillations because gradients in opposite directions get canceled out.", "The name momentum comes from the fact that this is analogous to the notion of linear momentum from physics. An object that has motion (in this case it is the general direction that the optimization algorithm is moving) has some inertia which causes them to tend to move in the direction of motion. Thus, if the optimization algorithm is moving in a general direction, the momentum causes it to \u2018resist\u2019 changes in direction, which is what results in the dampening of oscillations for high curvature surfaces.", "Momentum is an added term in the objective function, which is a value between 0 and 1 that increases the size of the steps taken towards the minimum by trying to jump from a local minimum. If the momentum term is large then the learning rate should be kept smaller. A large value of momentum also means that the convergence will happen fast. But if both the momentum and learning rate are kept at large values, then you might skip the minimum with a huge step. A small value of momentum cannot reliably avoid local minima, and can also slow down the training of the system. Momentum also helps in smoothing out the variations, if the gradient keeps changing direction. A right value of momentum can be either learned by hit and trial or through cross-validation.", "Momentum uses past gradients for updating values, as shown in the formula below. The value v associated with momentum is often called the \u2018velocity\u2019. More weight is applied to more recent gradients, creating an exponentially decaying average of gradients.", "We can see the effects of adding momentum on an optimization algorithm. The first few updates show no real advantage over vanilla SGD \u2014 since we have no previous gradients to utilize for our update. As the number of updates increases our momentum kickstarts and allows faster convergence.", "Another type of momentum that exists is Nesterov momentum, which we will discuss briefly.", "A good discussion of Nesterov momentum is given in Sutskever, Martens et al.\u201dOn the importance of initialization and momentum in deep learning\u201d 2013.", "The main difference is in classical momentum you first correct your velocity and then make a big step according to that velocity (and then repeat), but in Nesterov momentum, you first make a step into velocity direction and then make a correction to a velocity vector based on a new location (then repeat).", "The difference is subtle but in practice, it can make a huge difference.", "This concept can be difficult to comprehend, so below is a visual representation of the difference between the traditional momentum update and Nesterov momentum.", "Oscillations along vertical direction \u2014 Learning must be slower along parameter 2 Use a different learning rate for each parameter?", "Momentum adds updates to the slope of our error function and speeds up SGD in turn. AdaGrad adapts updates to each individual parameter to perform larger or smaller updates depending on their importance.", "One of Adagrad\u2019s main benefits is that it eliminates the need to manually tune the learning rate and results in greater progress along gently sloped directions.", "AdaGrad\u2019s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This, in turn, causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.", "For non-convex problems, AdaGrad can prematurely decrease the learning rate. We can use an exponentially weighted average for gradient accumulation.", "Adam is a combination of RMSprop and momentum (similarly, Nadam refers to a combination of RMSprop and Nesterov momentum). Adam refers to adaptive moment estimation, and it is the most popular optimizer used for neural networks today.", "Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.", "In the previous sections, we looked at how best to navigate the loss surface of the neural network objective function in order to converge to the global optimum (or an acceptable good local optimum). Now we will look at how we can manipulate the network itself in order to aid the optimization procedure.", "The initialization of network weights is an important and often overlooked characteristic of developing neural networks. There are multiple issues associated with poorly initialized networks that can be detrimental to network performance.", "Take for example a network that we initialize with all values of zero. What would happen in this scenario? The network would actually not learn anything at all. Even after a gradient update, all the weights would still be zero, because of the inherent way we are calculating the gradient updates.", "Imagine that we implemented this and worked out it was an issue, and then decided to set our network initialization all to the same value of 0.5. What would happen now? The network would actually learn something, but we have prematurely prescribed some form of symmetry between neural units.", "In general, it is a good idea to avoid presupposing any form of a neural structure by randomizing weights according to a normal distribution. This is often done in Keras by specifying a random state (which provides randomness but ensures that the measurements are repeatable).", "What should be the scale of this initialization? If we choose large values for the weights, this can lead to exploding gradients. On the other hand, small values for weights can lead to vanishing gradients. There is some sweet spot that provides the optimum tradeoff between these two, but it cannot be known a priori and must be inferred through trial and error.", "Xavier initialization is a simple heuristic for assigning network weights. With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to high values or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for both the input and the output.", "The weights are drawn from a distribution with zero mean and a specific variance. For a fully-connected layer with m inputs:", "The value m is sometimes called the fan-in: the number of incoming neurons (input units in the weight tensor).", "It is important to remember that this is a heuristic and therefore has no particular theoretical backing \u2014 it has merely been empirically observed to perform well. You can read the original paper here.", "He normal initialization is essentially the same as Xavier initialization, except that the variance is multiplied by a factor of two.", "In this method, the weights are initialized keeping in mind the size of the previous layer which helps in attaining a global minimum of the cost function faster and more efficiently. The weights are still random but differ in range depending on the size of the previous layer of neurons. This provides a controlled initialization hence the faster and more efficient gradient descent.", "For ReLU units, it is recommended:", "Bias initialization refers to how the biases of the neurons should be initialized. We have already described that weights should be randomly initialized with some form of normal distribution (to break symmetry), but how should we approach the biases?", "To paraphrase the Stanford CS231n course: The simplest and a common way of initializing biases is to set them to zero \u2014 since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant values such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement and it is more common to set biases to zero.", "One main concern with bias initialization is to avoid saturation at initialization within hidden units \u2014 this can be done, for example in ReLU, by initializing biases to 0.1 instead of zero.", "One other method of initializing weights is to use pre-initialization. This is common for convolutional networks used for examining images. The technique involves importing the weights of an already trained network (such as VGG16) and using these as the initial weights of the network to be trained.", "This technique is only really viable for networks which are to be used on similar data to that which the network was trained on. For example, VGG16 was developed for image analysis, if you are planning to analyze images but have few data samples in your data set, pre-initialization might be a tenable method to utilize. This is the underlying concept behind transfer learning, but the terms pre-initialization and transfer learning are not necessarily synonymous.", "Up to this point, we have looked at ways to navigate the loss surface of the neural network using momentum and adaptive learning rates. We have also look at several methods of parameter initialization in order to minimize a priori biases within the network. In this section, we will look at how we can manipulate the data itself in order to aid our model optimization.", "To do this, we will look at batch normalization and some of the ways it can be implemented to aid the optimization of neural networks.", "Feature normalization is exactly what it says, it involves normalizing features before applying the learning algorithm. This involves rescaling the feature and is generally done during preprocessing.", "According to the paper \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d, gradient descent converges much faster with feature scaling than without it.", "There are several ways to scale the data. One common method is min-max normalization, whereby", "The simplest method to scale data is known as min-max normalization and involves rescaling the range of features to scale the range in [0, 1] or [\u22121, 1]. This is done by subtracting each value by the minimum value and then scaling this by the range of values present in the dataset. If the distribution of data is highly skewed this can result in many values clustered in one location. If this occurs it can sometimes be mitigated by taking the logarithm of the feature variable (as this has a tendency to collapse outliers so they have a less profound effect on the distribution).", "Another common method is mean normalization, this is essentially the same as min-max normalization except the average value is subtracted from each value. This is the least common of the three discussed in this article.", "Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (typically those that involve distance-based methods). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next, we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.", "By performing normalization we ameliorate the distortion (such as the elongation of one feature compared to another feature) of the dataset and make it more uniform.", "This idea also comes from the previously mentioned paper \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d.", "The authors define internal covariance shift:", "We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training.", "This may be a bit vague, so I will try to unpack this. In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on. When the parameters of a layer change, the distribution of inputs to subsequent layers also changes.", "These shifts in input distributions can be problematic for neural networks, as it has a tendency to slow down learning, especially deep neural networks that could have a large number of layers.", "It is well established that networks converge faster if the inputs have been whitened (ie zero mean, unit variances) and are uncorrelated and internal covariate shift leads to just the opposite.", "Batch normalization is a method intended to mitigate internal covariate shift for neural networks.", "Batch normalization is an extension to the idea of feature standardization to other layers of the neural network. If the input layer can benefit from standardization, why not the rest of the network layers?", "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.", "Batch normalization allows each layer of a network to learn by itself more independently of other layers.", "However, after this shift/scale of activation outputs by some randomly initialized parameters, the weights in the next layer are no longer optimal. SGD ( Stochastic gradient descent) undoes this normalization if it\u2019s a way for it to minimize the loss function.", "Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a \u201cstandard deviation\u201d parameter (\u03b3) and add a \u201cmean\u201d parameter (\u03b2). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights.", "This procedure is known as the batch normalization transform.", "To illustrate this visually, we can analyze the below figure. We are looking at the first hidden layer, immediately following the input layer. For each of the N mini-batches, we can calculate the mean and standard deviation of the output.", "This is subsequently repeated for all subsequent hidden layers. Following this, we can differentiate the joint loss for the N mini-batches and then backpropagate through the normalization operations.", "Batch normalization reduces overfitting because it has a slight regularization effect. Similar to dropout, it adds some noise to each hidden layer\u2019s activations.", "During test time, the mean and standard deviations are replaced with running averages collected during training time. This is the same as using the population statistics instead of mini-batch statistics as this ensures that the output deterministically depends on the input.", "There are several advantages to using batch normalization:", "This concludes the third part of my series of articles about fully connected neural networks. In the next articles, I will provide some in-depth coded examples demonstrating how to perform neural network optimization, as well as more advanced topics for neural networks such as warm restarts, snapshot ensembles, and more.", "For updates on new blog posts and extra content, sign up for my newsletter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Postdoc @Harvard | Environmental + Data Science PhD @Harvard | ML consultant @Critical Future | Blogger @TDS | Content Creator @EdX. https://mpstewart.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ca72d4db3e0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----7ca72d4db3e0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://videolectures.net/site/normal_dl/tag=983679/deeplearning2015_bengio_theoretical_motivations_01.pdf", "anchor_text": "Yoshua Bengio"}, {"url": "https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c", "anchor_text": "Simple Introduction to Neural NetworksA detailed overview of neural networks with a wealth of examples and simple imagery.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/comprehensive-introduction-to-neural-network-architecture-c08c6d8e5d98", "anchor_text": "Comprehensive Introduction to Neural Network ArchitectureA detailed overview of neural architecture, activation functions, loss functions, output units.towardsdatascience.com"}, {"url": "https://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "Convex optimization"}, {"url": "https://en.wikipedia.org/wiki/Rosenbrock_function", "anchor_text": "Rosenbrook function"}, {"url": "http://Simionescu, P.A. (2014). Computer Aided Graphing and Simulation Tools for AutoCAD users (1st ed.). Boca Raton, FL: CRC Press. ISBN 978-1-4822-5290-3.", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Truncated_Newton_method", "anchor_text": "truncated-Newton"}, {"url": "https://en.wikipedia.org/wiki/Quasi-Newton_method", "anchor_text": "quasi-Newton"}, {"url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm", "anchor_text": "BFGS"}, {"url": "https://en.wikipedia.org/wiki/Condition_number", "anchor_text": "poorly conditioned"}, {"url": "http://proceedings.mlr.press/v28/sutskever13.pdf", "anchor_text": "Sutskever, Martens et al.\u201dOn the importance of initialization and momentum in deep learning\u201d 2013"}, {"url": "http://cs231n.github.io/neural-networks-3/", "anchor_text": "Stanford CS231n class"}, {"url": "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"url": "https://machinelearning.wtf/terms/batch-normalization/", "anchor_text": "Batch normalization"}, {"url": "https://mailchi.mp/6304809e49e7/matthew-stewart", "anchor_text": "Newsletter SubscriptionEnrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain\u2026mailchi.mp"}, {"url": "https://www.coursera.org/course/ml", "anchor_text": "Andrew Ng\u2019s course on machine learning"}, {"url": "https://www.coursera.org/course/neuralnets", "anchor_text": "Coursera Neural Networks for Machine Learning (fall 2012)"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Michael Nielsen\u2019s free book Neural Networks and Deep Learning"}, {"url": "http://www.iro.umontreal.ca/~bengioy/dlbook/", "anchor_text": "book on deep learning"}, {"url": "http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html", "anchor_text": "Hugo Larochelle\u2019s course (videos + slides) at Universit\u00e9 de Sherbrooke"}, {"url": "http://ufldl.stanford.edu/wiki/index.php/Main_Page", "anchor_text": "Stanford\u2019s tutorial (Andrew Ng et al.) on Unsupervised Feature Learning and Deep Learning"}, {"url": "https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/", "anchor_text": "Oxford\u2019s ML 2014\u20132015 course"}, {"url": "https://developer.nvidia.com/deep-learning-courses", "anchor_text": "NVIDIA Deep learning course (summer 2015)"}, {"url": "https://www.udacity.com/course/deep-learning--ud730", "anchor_text": "Google\u2019s Deep Learning course on Udacity (January 2016)"}, {"url": "http://cs224d.stanford.edu/syllabus.html", "anchor_text": "Stanford CS224d: Deep Learning for Natural Language Processing (spring 2015) by Richard Socher"}, {"url": "http://nlp.stanford.edu/courses/NAACL2013/", "anchor_text": "Tutorial given at NAACL HLT 2013: Deep Learning for Natural Language Processing (without Magic) (videos + slides)"}, {"url": "http://cs231n.github.io/", "anchor_text": "CS231n Convolutional Neural Networks for Visual Recognition"}, {"url": "http://karpathy.github.io/neuralnets/", "anchor_text": "Hacker\u2019s guide to Neural Networks"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608014002135", "anchor_text": "Deep learning in neural networks: An overview"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608019300231", "anchor_text": "Continual lifelong learning with neural networks: A review \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608019300784", "anchor_text": "Recent advances in physical reservoir computing: A review \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303332", "anchor_text": "Deep learning in spiking neural networks"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303319", "anchor_text": "Ensemble Neural Networks (ENN): A gradient-free stochastic method \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/0893608089900208", "anchor_text": "Multilayer feedforward networks are universal approximators"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608018303277", "anchor_text": "A comparison of deep networks with ReLU activation function and linear spline-type methods \u2014 Open access"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608097000117", "anchor_text": "Networks of spiking neurons: The third generation of neural network models"}, {"url": "https://www.sciencedirect.com/science/article/pii/089360809190009T", "anchor_text": "Approximation capabilities of multilayer feedforward networks"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608098001166", "anchor_text": "On the momentum term in gradient descent learning algorithms"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7ca72d4db3e0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7ca72d4db3e0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7ca72d4db3e0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7ca72d4db3e0---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7ca72d4db3e0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----7ca72d4db3e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----7ca72d4db3e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7ca72d4db3e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7ca72d4db3e0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7ca72d4db3e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://medium.com/@matthew_stewart/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.5K Followers"}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F20066c159638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-optimization-7ca72d4db3e0&newsletterV3=b89dbc0712c4&newsletterV3Id=20066c159638&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}