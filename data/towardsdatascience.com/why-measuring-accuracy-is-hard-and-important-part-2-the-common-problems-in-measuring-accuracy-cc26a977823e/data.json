{"url": "https://towardsdatascience.com/why-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e", "time": 1682995471.52486, "path": "towardsdatascience.com/why-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e/", "webpage": {"metadata": {"title": "Why Measuring Accuracy Is Hard (and important!) Part 2 | by Bradley Arsenault | Towards Data Science", "h1": "Why Measuring Accuracy Is Hard (and important!) Part 2", "description": "This article is part 2 of a 4 part article series that I am creating on the challenges of measuring accuracy. If you haven\u2019t read the first article, check it out here\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1", "anchor_text": "https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Precision_and_recall", "anchor_text": "https://en.wikipedia.org/wiki/Precision_and_recall", "paragraph_index": 16}, {"url": "https://www.electricbrain.io/", "anchor_text": "https://www.electricbrain.io/", "paragraph_index": 66}], "all_paragraphs": ["This article is part 2 of a 4 part article series that I am creating on the challenges of measuring accuracy. If you haven\u2019t read the first article, check it out here: https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1", "In this article, we are going to discuss the major challenges that occur in measuring accuracy. We\u2019ll try illustrate with examples and graphs where possible.", "The most common thing that makes measuring accuracy difficult is that sometimes the training set that we are using is different then the real world data the algorithm is expected to work on. There might be many different reasons this is the case:", "This problem has come up for me recently on a few different projects, the most salient of which is a chatbot company. This is a bit of a constructed example, but is derived from my real experience when working with the company.", "Imagine we\u2019re developing a chat-bot product for call-centers. Before we can get our first sale, we need to demonstrate that the product works. To get the product to work, we need data. The data we need is just different ways of asking a question, e.g. different ways to say that you want to check on the status of an order, or different ways to order a pizza at a restaurant. We resort to gathering the data the only way we know \u2014 on mechanical turk. However, by just asking people directly to come up with different ways of ordering a pizza, we do get some variety, but we start getting a lot of repeats like:", "They were indeed coming up with lots of different examples, hundreds of them. And indeed, these are all valid sentences that should trigger our chat-bot to order a pizza. But there was not a whole lot of variety here \u2014 not the type of variety we were looking for. This was not representative of the real world data.", "Eventually we resort to putting rules in place like a minimum sentence length, a requirement that the sentence is unique within our entire dataset, a minimum difference from other samples they typed, etc.. and these things do help get us more variety. However, we fear that the dataset was still not really representative of real world data:", "We eventually cobble together a dataset that we like and we achieve a +90% accuracy against it. Seems like a great result.", "We then bring this to clients and show them. But the bot only works 70% of the time in demonstrations at best, often falling down and misunderstanding seemingly straightforward orders. The dataset the algorithm was running on, the real world data, was just plain different from the dataset the algorithm was trained on. The dataset we had technically ticked all the boxes:", "The key problem though was in the nuance of the second point. Although the dataset contained variety, it didn\u2019t contain the same kind of variety as the real world data did. Although we had many different ways of ordering a pizza, we didn\u2019t have the specific ways that real people ordered a pizza to our bot.", "You would be very surprised how often it comes up that we are expected to make models on one type of data, and execute them on a different type of data. It works because machine learning is awesome and incredibly powerful with today\u2019s algorithms. They are able to find general patterns and then transfer those patterns to new environments. But it presents a new problem, which is measuring the accuracy of the system in those new environments.", "It is often the case that your machine learning algorithm can be wrong in different ways. As algorithms continue to get more complex and powerful and used for more things, the specific details of their behaviour become more important.", "In some cases, there is a whole sequence of algorithms that work in concert to create a complete system \u2014 preprocessing, augmentation, multiple machine learning algorithms, post processing, filtering, thresholding, validating \u2014 there are many components to an AI system that make it work. The shape of failures and correct answers that is acceptable at each step is different, making measuring and improving the whole system a complex task.", "In other cases, even the final outputs of the algorithm must take on specific shapes and forms that are hard to reduce down to a single number. In self driving cars, we care about the vehicles performance in edge case situations \u2014 bad weather, other drivers driving badly, etc.. It\u2019s pretty clear from many YouTube videos that self driving cars can work just fine in typical situations.", "I can identify various reasons you care about certain failure cases more than others:", "These types of problems are typically solved by coming up with alternative metrics. Sometimes we use confusion matrices, like this one, to help us understand what is happening:", "Other times we might use alternative metrics, like Precision and Recall, https://en.wikipedia.org/wiki/Precision_and_recall, which omit false negatives and false positives respectively, versus raw accuracy which considers every type of mistake.", "I think another interesting manifestation of this phenomenon is the problem of discriminatory bias in machine learning algorithms. This includes, for example, the problem of facial recognition algorithms working more effectively on white and asian people then people of other races. The problem here is that there can be unlabeled subsets of your dataset that your algorithm performs poorly on. When one is making a facial recognition algorithm, you might just receive a giant dataset of images without any data on who is which race and sex. This could happen if you are, say, a Facebook researcher who receives an anonymized dataset. If this dataset is pulled from Facebook\u2019s USA dataset, it shouldn\u2019t be a surprise to find that it mostly contains images of white caucasians. You train the algorithm, and claim its 95% accurate. But who are the 5% failures of the algorithm? With your anonymized dataset, you can\u2019t really know \u2014 the only solution is hand inspection. Your simplistic measurement of accuracy has failed to capture important socio-political properties of the algorithm.", "Another big problem that gets in the way of measuring accuracy is that there is noise, both noise that is inherent to the model, noise in the dataset, and noise in our measurement of accuracy.", "The basic difficulty can show up in any model. Consider the following. You have a dataset, and you wish to measure the accuracy of the algorithm you have trained to see how well it generalizes the properties of the dataset. You do a what a wise data scientist does and split your data 3 ways \u2014 training, testing and validation:", "But now you\u2019re faced with this question \u2014 which items to you put into which group? What if I use this split instead:", "The naive way might be just to shuffle your dataset randomly and split it 3 ways. But when you train your model, you find that every time you do this, you get different results, even with no changes to your model:", "Frustrated, you might believe its because the dataset is very unbalanced from one run to the next. Maybe you may decide to use the labels in your dataset to construct a training, testing, validation sets that have even numbers of each of your target labels. Effectively dividing your dataset into a bunch of sub-datasets by each label, splitting those datasets 3 ways each randomly, and then recombining them back together to get. Now your dataset is class balanced but still randomized. Now you have reduced the noise, but you still get problems:", "You\u2019ve reduced the spread, but there is still noise in your measurement. You resolve that there are actually harder and easier samples in your dataset for each label, which are still getting shifted around. But what are you to do?", "You might resolve to using a cross-folding method, where, say, you take the average of all 5 possible 80/20 splits within your dataset. This ensures every sample shows up in in the testing set at least once. But this method can be costly for many deep learning models, where GPU power is scarce and you are starving to run as many experiments as possible. Maybe you just lock in a fixed testing and training set and only measure against that. But this can be difficult in dynamic real-world environments where our datasets are continuously evolving and getting larger, effectively requiring us to maintain a database which tags entries as either training, testing or validation.", "The extra infrastructure required to maintain a good, stable, consistent, noise-free measure of the accuracy of the model can grow quickly depending on how much you want to reduce the noise.", "Another big source of problems that I\u2019ve seen is noisy datasets. Noisy and error-filled datasets are the bane of my existence. Having dealt with third-party data outsourcing firms, I\u2019ve seen first hand how difficult it is to get high quality, clean labelled data.", "The problem is inherent in the task \u2014 labelling data sucks. No matter how many fancy tools we develop to assist in labelling, it still remains a mundane task of data entry. Its error prone by nature.", "People take many steps to try and solve this problem:", "No matter how we do it though, the datasets we are working with are working with are often noisy.", "On the machine learning side, as long as the dataset isn\u2019t too noisy, we can often still teach the algorithm to learn the pattern. Deep learning is especially good at generalizing despite a lot noise. But on the measurement and validation side, noise in the dataset gives us a few inherent problems", "Probably even worse then having noise or random mistakes in your dataset, are the consistent errors that are made by your annotation team. If there are errors made in consistent ways, then the machine learning algorithm may attempt to learn those patterns of errors as part of its overall learning.", "I have been burned by this on several real world projects. At a recent project we executed with a large Canadian bank, we had gotten the provided data annotated by a third party. We did not perform sufficient oversight and testing. The samples we received were good, but we failed to check the full dataset. We went ahead and trained our model, and eventually hit very high accuracies. But when presenting the algorithm to the client, they quickly pointed out that the final results appeared to be filled with errors. When diving deep, we realized that not only had much of the dataset been annotated incorrectly, it was consistently annotated incorrectly in the same ways.", "Therefore, it was still possible for us to achieve a high degree of accuracy by every measurement we had available, and for the model to still be wrong. The model had indeed learned the patterns, it learned precisely the patterns of errors which the people annotating the dataset had made. It was clear that we could not just check the \u2018provided samples\u2019 that are emailed to us during the process, as these might have been prepared and scrutinized by managers who correctly understand the requirements. We needed to analyze and validate the final samples that were put together by all of the various low-paid data entry people by this company, who may or may not have received sufficient instruction to annotate the data correctly.", "In my opinion, this is one of the hardest and most frustrating aspects of doing real world AI projects. You can have great accuracy using well validated techniques of measuring accuracy, and still ultimately be wrong.", "Another problem that sometimes comes up when measuring accuracy is that there becomes too much to measure. You might have a dozen different metrics representing different stages of your system.", "This issue came up for me when building a text extraction engine recently. The model took as input a word and its surrounding context, and produced a classification for that word.", "However, our pipeline consisted of two stages. The first machine learning algorithm just decided between null or extract, and the second stage made the final classification. In practice we landed on a four layered system, but the 2 layered system will suffice for explanation. The setup looks like this:", "So now we have two different machine learning algorithms with two independent measurements of accuracy. We also have several different failure cases:", "Additionally, we have the additional complexity that our second stage algorithm is able to partially compensate for errors in the first stage. E.g. a False Positive at the first stage might still be correct predicted as a null in the second stage, and still result in the correct final output. Therefore, the accuracy measurements of these two stages can interact in complex, non-linear ways. Worse accuracy on False Positives in layer #1 does not necessarily result in lower accuracy in the final output.", "In fact, worse accuracy on layer #1 could mean precisely the opposite. We had to tilt the layer #1 towards more false positives in order to get this approach to work and produce a better final accuracy. By default, it would create far more False Negatives, then False Positives, reducing the accuracy versus just a single layered system. But tilting the layer #1 towards more false positives (reducing its accuracy), meant it was only filtering out entries it was highly confident were actually nulls (True Negatives), and everything else could be passed to the layer #2 for a final classification. This resulted an improvement in the final accuracy of the model. Thus, hurting layer #1 in that specific way made the overall model more accurate. The filtering layer (layer #1) helped the model deal with the unbalanced classes and high degree of nulls in the output, but only when measured and used in the right way.", "By itself, there is nothing wrong with having multiple stages of processing, and even multiple measurements of accuracy at these various stages of processing. However, too much of a good thing can create new problems:", "In our experience, when you are in this situation, there is only one solution: focus all R&D on improving your end-to-end or final-result metric, that is, the accuracy exiting the final stage of processing from the system. We don\u2019t mean that you shouldn\u2019t measure the intermediate accuracies \u2014 they can be incredibly useful to understand how the various processing stages are interacting and to come up with ideas on what might work. But when it comes to gauging whether an improvement is actually an improvement, only the final-result accuracy matters.", "This problem is similar to the problem of multiple metrics in your pipeline, but it manifests itself because of a different reason. Let\u2019s consider the problem of extracting data from receipts, where I saw this issue most saliently. The basic problem is to take an image of a receipt and convert it into fully structured data:", "If you are paying attention, you\u2019ll notice that this setup also has multiple metrics across the pipeline. The OCR engine, letter by letter classification, and post-processing could all be making mistakes and could yield differing measurements of accuracy at each stage. They also had different measurements depending on whether their prior input was assumed to be accurate, e.g. we can measure the letter by letter accuracy assuming OCR accuracy was perfect (by inputting ground-truth OCR data), and we can measure the letter-by-letter accuracy on the raw OCR output, including mistakes.", "However, I want to concentrate on another wrench that was thrown in the measurement of accuracy here in the middle section of the system, the letter by letter classification.", "The most obvious way of measuring the accuracy would seem to be to literally measure the number of letter classifications that are correct:", "But the problem with this approach is that most of the characters are to be classified as \u201cnull\u201d, or don\u2019t extract. Under this metric, it was relatively easy for us to hit accuracies that were 97% and 98% accurate. But as we quickly learned, those supposed 2% or 3% of errors mattered a lot.", "Many of the receipts we were analyzing weren\u2019t short starbucks receipts, they were behemoths like these ones:", "In these receipts, almost 95% of the characters were to be categorized as null. However, the remaining 5% of characters were the ones we cared about. According to our traditional metric, these receipts would get a 95% accuracy even if our model did nothing but predict null. The measurement is naturally unstable from one receipt to the next, and didn\u2019t give us adequate insight into what was actually happening.", "So the problem spawns a variety of alternative measurements of accuracy, that are meant to give us more insight into the behavior of the system and the types of errors it was making:", "It was pretty profound to find out that our model with a character level accuracy of 98% had a receipt-level accuracy of 67%. Receipts had on at a minimum 200 characters, so we would expect on average 4 errors per receipt at the character level. You might suppose then, that Receipt Level Accuracy should be 0%, since on average receipts should have had at least 4 character level errors, making them a mistake by the receipt level metric. But most of our receipts went off without a hitch, perfectly classifying every character. Other receipts were complete clusterfucks, with as much as 20% to 30% misclassified.", "When its possible, having metrics with different levels of granularity is helpful to understand how your model is failing. But they can also add confusion and noise to your measurements, making you spin circles when some metrics improve but others become worse. More on what to do in this situation will come in article #4.", "This issue shows up surprisingly often, in fact it showed up for us in the receipt example above as well. You have a machine learning system where you have annotated the most important and difficult part of the learning that needs to take place. But your algorithm has some sort of post-processing that needs to take place before it gets to the final result. And this post-processing is based on some simple heuristics and assumptions that are specific to the problem you are experiencing.", "A salient and simple example of just such a heuristic was when I was doing data extraction on spreadsheets. Each cell in the spreadsheets had to be classified into one of a few categories. The cells would then be grouped together into a set of outputs. The setup was roughly as follows:", "The system needed to be able to handle a variety of different formats of spreadsheets that have been hand prepared by different people, with arbitrary decisions for formatting and structure.", "One reasonable way to construct this system is to get all of the cells classified, something like this:", "Then you create some predictive features, and train an algorithm to predict the classification for each cell. But to get the final result, you must make one more important processing step \u2014 you must group together the cells. In our example here, the method would seem obvious: any cells on the same row should be grouped together into the same output. The nature of 99% of spreadsheets is that people have one entry per line.", "You might then, consider it safe that you only have the classified cells as ground truth data, and you don\u2019t actually have the ground truth data on the final output for your algorithm. You optimize your model, measure your accuracy and get 99%. Hooray! But wait, there\u2019s a problem. Inside your dataset, there were some spreadsheets that looked like this (yes this actually happened to me):", "Now it becomes clear that having ground-truth data only at the cell level is a liability. A key assumption in our post-processing, which is that there is only one output per line, has been violated. The accuracy we measured from our system was excellent. Our code worked flawlessly as designed, and all of our unit tests passed. But the system as a whole was still producing invalid results, and we didn\u2019t know it initially because we only had ground-truth data on an intermediate step in the system, and not on the final result.", "This problem can manifest itself in any situation where you have additional post-processing steps after your machine learning algorithm finishes, but don\u2019t have any way to measure the accuracy of that final output. Although this example is simple and easy to understand, there are much more complicated and nuanced situations that can occur in the stage between your machine learning algorithms output and your final result. Always cough up the time and cash for real ground truth data to measure accuracy against, or you will get burned.", "Measuring accuracy is hard. Many of the best aspects of machine learning systems, like their ability to generalize to new, never before seen examples, also present a massive headache for us to measure. Many of the problems with measuring accuracy arise from the beneficial properties of machine learning:", "Measuring accuracy is hard largely because machine learning algorithms are so good at what they do \u2014 being intelligent. We rarely talk about measuring the accuracy of people, because we acknowledge that it\u2019s very hard to define in advance what correct behaviour is. Increasingly, machine learning models are going to be subject to the same constraint \u2014 accuracy will become even more muddied and more difficult to measure. Increasingly, we are applying machine learning in environments where accuracy is hard to even define, let alone measure \u2014 think about algorithms designed to create music. How do you measure accuracy on that? But yet, we have now created algorithms that are pretty good at generating enjoyable music.", "In the Part 3 of this series of blog articles, I will be discussing some of these more interesting, more nuanced problems in measuring accuracy. What do you do if there is no off the shelf metric to measure accuracy for your problem? What do you do when you can\u2019t measure the outcome you actually care about? What if you don\u2019t have any ground truth data? What if you can\u2019t even define what accuracy is in the context of your problem?", "In Part 4 of this series of blog articles, I will try to present a framework for measuring accuracy better \u2014 how to measure, when to measure, and what to measure. Most importantly, I will show that how you measure accuracy will depend on what you want to do with that accuracy number. Whether its for early stopping or threshhold calibration, for bayesian optimization of a models parameters, for making iterative improvements to a model, for understanding the models failure cases, for communicating the performance to managers, or for communicating the performance to customers \u2014 there are different ways of measuring for different situations. There is no one-size-fits-all solution.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Founder and CEO of Electric Brain (https://www.electricbrain.io/)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcc26a977823e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc26a977823e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@brad.arsenault?source=post_page-----cc26a977823e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brad.arsenault?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Bradley Arsenault"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4cc13146bf71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&user=Bradley+Arsenault&userId=4cc13146bf71&source=post_page-4cc13146bf71----cc26a977823e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1", "anchor_text": "https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1"}, {"url": "https://en.wikipedia.org/wiki/Precision_and_recall", "anchor_text": "https://en.wikipedia.org/wiki/Precision_and_recall"}, {"url": "https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-important-part-2", "anchor_text": "www.electricbrain.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cc26a977823e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cc26a977823e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cc26a977823e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/metrics?source=post_page-----cc26a977823e---------------metrics-----------------", "anchor_text": "Metrics"}, {"url": "https://medium.com/tag/ai?source=post_page-----cc26a977823e---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&user=Bradley+Arsenault&userId=4cc13146bf71&source=-----cc26a977823e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&user=Bradley+Arsenault&userId=4cc13146bf71&source=-----cc26a977823e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcc26a977823e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cc26a977823e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cc26a977823e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cc26a977823e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cc26a977823e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cc26a977823e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cc26a977823e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brad.arsenault?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brad.arsenault?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bradley Arsenault"}, {"url": "https://medium.com/@brad.arsenault/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "53 Followers"}, {"url": "https://www.electricbrain.io/", "anchor_text": "https://www.electricbrain.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4cc13146bf71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&user=Bradley+Arsenault&userId=4cc13146bf71&source=post_page-4cc13146bf71--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4cc13146bf71%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-measuring-accuracy-is-hard-and-important-part-2-the-common-problems-in-measuring-accuracy-cc26a977823e&user=Bradley+Arsenault&userId=4cc13146bf71&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}