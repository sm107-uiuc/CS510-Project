{"url": "https://towardsdatascience.com/first-bayesian-state-space-model-with-pymc3-51cbb06ef8bd", "time": 1683016988.801733, "path": "towardsdatascience.com/first-bayesian-state-space-model-with-pymc3-51cbb06ef8bd/", "webpage": {"metadata": {"title": "First Bayesian State Space Model with PyMC3 | Towards Data Science", "h1": "First Bayesian State-Space Model with PyMC3", "description": "State-Space Models in Bayesian Time Series Analysis with PyMC3"}, "outgoing_paragraph_urls": [{"url": "https://github.com/luisroque/bayesian_time_series", "anchor_text": "GitHub", "paragraph_index": 3}], "all_paragraphs": ["Today, time series forecasting is ubiquitous, and companies' decision-making processes depend heavily on their ability to predict the future. Through a short series of articles, I will present possible approaches to this kind of problems, combining state-space models with Bayesian statistics.", "In the initial articles, I will take some of the examples from the book An Introduction to State Space Time Series Analysis by Jacques J.F. Commandeur and Siem Jan Koopman [1]. It comprises a well-known introduction to the subject of state-space modeling applied to the time series domain.", "This post belongs to a series; feel free to check the previous post to acquire some more context:", "Also, you can always find all the data and code in my GitHub.", "As promised, we will be defining our first state-space model in this article, the deterministic level model. It is the simplest one that we could possibly design. So simple that we can\u2019t solve any real problem with it. Nonetheless, it is a state-space model. It belongs to the local level models bucket of the state space models. In our current context, the level is equivalent to what an intercept is for linear regression. This is why we call it deterministic: it means that the level is static \u2014 i.e., applied globally to all time points.", "for t = {1,\u2026,n}. We can see that we are making our observations dependent on the level value when t=1, keeping the latter constant for all t. The observation disturbance is denoted by \u03b5_t (or sometimes referred to as an irregular component) at time t.", "Let\u2019s code it in two ways, using the standard approach and then using a Bayesian approach.", "Can we implement this by hand? We should get an intuition first on what we are exactly doing. The simplicity of the model gives us some room to go deeper into its mechanics.", "We need to do a quick (probably not the quickest) detour before continuing.", "This is a fundamental concept to understand how these models work. It can be very complex mathematically, but it is quite intuitive to grasp. The Maximum Likelihood Estimation (MLE) is a method that allows us to estimate the parameters of the probability distribution that generated a specific sample of data (for us, it is our time series).", "We will work on a specific use case, the one that we are interested in: the MLE for a Gaussian (or Normal) distribution.", "A Gaussian distribution is a continuous probability distribution, which is very well-known for its bell-shaped probability density. The distribution is completely described with two parameters, the mean \u03bc, and the standard deviation \u03c3. We can write it as,", "The parameters are defined after the semicolon.", "Let\u2019s plot it with different combinations of the parameters.", "Now that we saw the different bell curves that we can create with just two parameters, we have an incredibly flexible power at hand to model real-world processes. We just lack the ability to estimate the best two parameters that generated a specific sample of data. And we need some math to do it.", "To avoid confusion, let\u2019s work on the notation first. This is a frequentist method by nature. You will often find a semicolon instead of a vertical bar | to denote that we are conditioning on the parameters. In the classical perspective, our \u03bc and \u03c3 are unknown parameters and not random variables. If we were using our Bayesian hats (which we are), we would use the vertical bar because our parameters become true random variables. This gives us consistency and also makes it simpler to understand.", "More on notation. We will often use \u03b8 to define a set of parameters. It could represent \u03bc and \u03c3 when defining a Gaussian distribution.", "Having the notation defined, we are going to focus on\u2026 the name. The MLE is the maximum likelihood estimation, so we need to maximize something. In fact, the MLE for \u03b8 is the value of \u03b8 that maximizes the likelihood P(Y|\u03b8).", "Now, we need to assume two things: that our observations are identical and independently distributed. The identical part is quite self-explanatory, as we want to describe them with just one distribution. The independence part allows us to use an important property: their joint probability density functions (pdf) is the product of the individual pdf\u2019s. Nevertheless, this is obviously not the case for our problem. Our points clearly depend on one another. Nevertheless, things get quite messy if we don\u2019t use this assumption, so bear with me for now. We are just building intuition on what is happening.", "We are ready to define it", "We now get to a new problem, multiplying many small probabilities together can be numerically unstable. To overcome this, we can use the log of the same function. The logarithm is a monotonically increasing function, which means that the maximum value in both cases \u2014 the original probability function and the log of the probability \u2014 are the same. It does another very convenient thing for us, it transforms our products into sums.", "Almost there! We will now work on our optimization problem at hand. We can put it as simple as", "The expression derived above can be differentiated to find the maximum. Expanding our parameters, we have log(L(Y|\u03bc, \u03c3)). As it is a function of the two variables, \u03bc and \u03c3, we use partial derivatives to find the MLE.", "Starting with \u03bc (\u03bc hat, in reality, the hat indicates that it is an estimator, i.e., our output), we compute it from", "Setting the expression above equal to zero, we get", "Surprisingly or not (hope it wasn\u2019t), this is the mean of the data! We do exactly the same thing for \u03c3. I\u2019m going to cut some steps.", "And finally, by using the value of \u03bc (hat), we get", "Strangely of not this is the variance of the data. So much work to get the mean and variance of the data! They are the best estimators for our data\u2026 obviously! But now you got the feeling on why mathematically this is.", "Just one more ah ah moment. We already used, in the last post, the least-squares method to estimate our parameters of the linear regression. When the model is assumed to be Gaussian, the MLE estimates are equivalent to the least-squares method.", "Not getting into the math, we can grasp the idea. In the case of the least-squares parameter estimation, we want to find the line that minimizes the total squared distance between the regression line and the data points. On the other hand, in the maximum likelihood estimation, we want to maximize the total probability of the data. In the case of the Gaussian distribution that happens when the data points are close to the mean value. Due to the symmetric nature of the distribution, this is equivalent to minimizing the distance between the data points and the mean value (see more here [2]).", "We discussed the Bayes Theorem in the last post; now it is time to connect it to a new concept: the Maximum a Posteriori (MAP). The MAP is the bayesian equivalent to the MLE.", "Just to remind ourselves, we have the Bayes Theorem below.", "In the equation above, B is the evidence, p(A) is the prior, p(B | A) is the likelihood, and p(A | B) is the posterior. p(A | B) is the probability of A happening if B happened.", "While the equation works for probabilities and probability distributions, for now, let\u2019s forget about it as we are just focused on optimizing a quantity.", "This means that P(A | B) is proportional to P(B | A) P(A), i.e., P(B) is a normalizing constant, and we don\u2019t need to normalize our result. In fact, this is the hardest component to compute in the Bayes equation, and we will see more about it in the future.", "Let\u2019s adapt the equation to our problem.", "Remember that we need to be precise in differentiating P(Y|\u03b8) from P(\u03b8|Y). The first is the likelihood, and the latter is the posterior, and they can be very different. We need to sharpen our eyes to spot the differences constantly.", "Let\u2019s read P(Y|\u03b8): it is the probability of our time series Y (our data) given that our parameters are equal to \u03b8.", "Returning to the problem at hand. The expression above is quite similar to what we saw earlier working our the MLE example. We have one additional element: P(\u03b8). This is our prior knowledge about our parameters and one fundamental idea behind Bayesian statistics. In the MLE case, we were implicitly assuming that all values of our parameters \u03bc and \u03c3 were equally likely, i.e., we didn\u2019t have any information to start with. This is the real difference between MLE and MAP. MLE assumes that all solutions are equally likely beforehand. MAP, on the other hand, allows us to accommodate prior information on our calculations. If we define the MAP with a flat prior, then we are basically performing MLE. When using more informative priors, we add a regularizing effect to our MAP estimation; that is why you often see MAP being framed as a regularization of the MLE.", "It wasn\u2019t a swift detour, but we got somewhere. We know that state-space models maximize a log-likelihood function, and we saw how it is defined as well as two different procedures to do this maximization. Using the MLE, we get two estimators, \u03bc (hat) and \u03c3 (hat). Let\u2019s calculate these estimators for our problem.", "Almost there. Just a small note on the \u03c3 (hat) value. Let\u2019s run the same model with a library that has everything set up for us. We will go through that code, but I need you to see something.", "We are not getting the same result. This happens because NumPy uses by default one formula to compute the variance, but we should use another.", "The one that NumPy uses is", "The one that we should use is", "For small sample sizes, our estimator is unlikely to perfectly represent the data. Using this normalization term is a way of reducing the bias on our estimator. Let\u2019s implement the latter.", "Well, believe it or not, we have our deterministic level equation.", "It makes sense to see how can we implement the deterministic level model with two lines of code using a library such as statsmodels.", "As simple as that, we have our model fitting the data. We can see our two parameters sigma2.irregular (\u03b5_t) and the level component \u03bc_1. We also get several statistical tests that we will learn about in the future.", "As I promised, we will always be performing the same analysis using the bayesian perspective.", "In this estimation, we will not be focusing so much on our Bayesian workflow and using all the good practices that we learned in the last post. But don\u2019t worry, we will get back to it in the future. The reason behind it is our intention of showing that the MLE and the MAP are indeed the same thing, if we use flat priors. We will be using very vague priors (not actually flat, to help our sampler slightly).", "And here we have our parameters. For now, focus only on the columns mean and sd from the table above. As you can see, we have posterior distributions for our parameters \u03bc_1 and \u03c3_\u03b5\u00b2, not only a point estimate. Let\u2019s compare our results with the ones that we got from our implementation using statsmodel.", "Just a quick note on the parameter \u03c3_\u03b5\u00b2. We\u2019ve been estimating the variance of the \u03b5_t term. Nevertheless, when fitting our likelihood, we are using the most usual way to fit a Gaussian distribution with the mean and the standard deviation. That is why I created a Deterministic variable (nothing to do with our deterministic model), which is the way to keep track of a transformed variable in PyMC3.", "Let\u2019s follow the same approach that we did in the last post. But first, we need to compare our Bayesian estimation of the parameters with the frequentist approach that we used earlier. Remember that we can solely compare pointwise estimations since only the Bayesian framework produces posterior distributions.", "Both models yield very similar results. As we saw above, this happens because we are using vague priors, which makes the MAP estimations approximate the MLE estimations.", "Now, it is time to plot our model-fitting against the data and the residuals.", "Not so exciting! The deterministic level model is constant, and thus it does not vary over time as a result. The residuals are clearly not randomly distributed for this case as they result from the deviations of the observed values from their mean.", "The correlogram also shows large autocorrelation in the residuals, which we couldn\u2019t capture with a deterministic model.", "We are not there yet, but we\u2019ve covered an important ground already.", "We did an important job today concerning our understanding of how the parameters' estimations are performed in both perspectives: frequentist and Bayesian. Using that knowledge, we defined our first state-space model from the ground up successfully. Unfortunately, we also saw that it was too simple to capture all the valuable information in the data. As we said in the previous article, there is an underlying dynamic evolution that cannot be observed, and we were again unable to model it.", "We are building momentum and gathering important tools for the journey ahead. I think we are ready to start adding some stochastic behavior to our parameters. See you in the next post!", "[1] J. Commandeur, S. Koopman, An Introduction to State Space Time Series Analysis (2007), Oxford University Press", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Data @ Marley Spoon | Ph.D. Researcher AI @ LIACC | Coordinator DS Masters @ NDS | CoFounder & ex-CEO @ HUUB"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F51cbb06ef8bd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@luisroque?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86----51cbb06ef8bd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://towardsdatascience.com/the-first-step-in-bayesian-time-series-linear-regression-89a64b826a7e", "anchor_text": "The First Step in Bayesian Time Series \u2014 Linear Regression"}, {"url": "https://github.com/luisroque/bayesian_time_series", "anchor_text": "GitHub"}, {"url": "https://www.linkedin.com/in/luisbrasroque/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1", "anchor_text": "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----51cbb06ef8bd---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----51cbb06ef8bd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----51cbb06ef8bd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----51cbb06ef8bd---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----51cbb06ef8bd---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----51cbb06ef8bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----51cbb06ef8bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F51cbb06ef8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----51cbb06ef8bd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----51cbb06ef8bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://medium.com/@luisroque/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "902 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2195f049db86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=post_page-2195f049db86--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd8e0bc4b610d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffirst-bayesian-state-space-model-with-pymc3-51cbb06ef8bd&newsletterV3=2195f049db86&newsletterV3Id=d8e0bc4b610d&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}