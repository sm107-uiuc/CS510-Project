{"url": "https://towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f", "time": 1683006995.6385121, "path": "towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f/", "webpage": {"metadata": {"title": "Exploring the Next Word Predictor! | by Dhruvil Shah | Towards Data Science", "h1": "Exploring the Next Word Predictor!", "description": "How does the keyboard on your phone know what you would like to type next? NLP is concerned with predicting the next word given in the previous words."}, "outgoing_paragraph_urls": [{"url": "https://www.kite.com/python/docs/nltk.word_tokenize", "anchor_text": "word_tokenize", "paragraph_index": 3}, {"url": "https://docs.python.org/2/library/collections.html#collections.defaultdict", "anchor_text": "defaultdict", "paragraph_index": 3}, {"url": "https://docs.python.org/2/library/collections.html#collections.Counter", "anchor_text": "Counter", "paragraph_index": 3}, {"url": "https://github.com/jackfrost1411/next_word_suggestor/tree/master/n-grams", "anchor_text": "this", "paragraph_index": 12}, {"url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Tokenizer API", "paragraph_index": 20}, {"url": "https://keras.io/layers/embeddings/#embedding", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://github.com/jackfrost1411/next_word_suggestor/tree/master/LSTM", "anchor_text": "this", "paragraph_index": 36}, {"url": "http://LinkedIn.com/in/dhruvilshah28", "anchor_text": "LinkedIn.com/in/dhruvilshah28", "paragraph_index": 40}], "all_paragraphs": ["How does the keyboard on your phone know what you would like to type next? Language prediction is a Natural Language Processing - NLP application concerned with predicting the text given in the preceding text. Auto-complete or suggested responses are popular types of language prediction. The first step towards language prediction is the selection of a language model. This article shows different approaches one can adopt for building the Next Word Predictor you have in apps like Whatsapp or any other messaging app.", "There are generally two models you can use to develop Next Word Suggester/Predictor: 1) N-grams model or 2) Long Short Term Memory (LSTM). We will go through every model and conclude which one is better.", "If you\u2019re going down the n-grams path, you\u2019ll need to focus on the \u2018Markov Chains\u2019 to predict the likelihood of each following word or character based on the training corpus. Below is the snippet of the code for this approach. In this approach, the sequence length of one is taken for predicting the next word. This means we will predict the next word given in the previous word.", "Importing necessary modules: word_tokenize, defaultdict, Counter", "Creating the class MarkovChain containing methods:", "When we create an instance of the above class a default dictionary is initialized. There is a method to preprocess the training corpus that we add via the .add_document() method. When we add a document with the help of the .add_document() method, pairs are created for each unique word. Let\u2019s understand this with an example: if our training corpus was \u201cHow are you? How many days since we last met? How are your parents?\u201d our lookup dictionary, after preprocessing and adding the document, would be:", "Each unique word as a key and its following words\u2019 list as a value is added to our lookup dictionary lookup_dict.", "When we input a word it will be looked up in the dictionary and the most common words in its list of following words will be suggested. Here, the maximum number of word suggestions is three like we have in our keyboards. Below is the running example of this approach for the sequence length of one. The output contains suggested words and their respective frequency in the list. When we enter the word \u2018how\u2019, it is looked up in the dictionary and the most common three words from its list of following words are chosen. Here, \u2018many\u2019 word appears 1531 times meaning the word sequence \u2018How many\u2019 appears 1531 times in the training corpus.", "Further, in the above-explained method, we can have a sequence length of 2 or 3 or more. For this, we will have to change some of the code above.", "Let\u2019s break the code. Methods .__generate_2tuple_keys() and .__generate_3tuple_keys() are to store the sequences of length two and three respectively and their following words\u2019 list. Now, our code has the strength to predict words based on up to three previous words. Let\u2019s look at our new lookup dictionary lookup_dict for the example: \u201cHow are you? How many days since we last met? How are your parents?\u201d", "New pairs are added to the dictionary compared to the previous one. The class MarkovChain that we created above handles any length of a sequence we input. If we input one word then the method \u2018oneword\u2019 will be called and this will be the same as the previous one. For input length two or three the methods \u2018twowords\u2019 and \u2018threewords\u2019 will be called respectively. What these methods do is that they look for the most common three words from the lookup dictionary, given the input words. When input words are more than four then the last three will be processed. When encountered an unknown word, that word will be ignored and the rest of the string will be processed. Look at the figure below to clear any doubts. This figure is based on a different training corpus. The left side shows the input and the right side, the output.", "Below is the running output of this approach:", "The above output is based on a different and bigger dataset that was used for this approach. GitHub\u2019s link for this approach is this. You can find the above code there.", "Markov chains do not have memory. There are many limitations to adopting this approach. Take an example, \u201cI ate so many grilled \u2026\u201d next word \u201csandwiches\u201d will be predicted based on how many times \u201cgrilled sandwiches\u201d have appeared together in the training data. As we are getting suggestions based only on the frequency, there are many scenarios where this approach could fail.", "A more advanced approach, using a neural language model, is to use Long Short Term Memory (LSTM). LSTM model uses Deep learning with a network of artificial \u201ccells\u201d that manage memory, making them better suited for text prediction than traditional neural networks and other models.", "\u201c The grass is always \u2026 \u201d", "The next word is simply \u201cgreen\u201d and could be predicted by most models and networks.", "But for the sentence, \u201c It\u2019s winter and there has been little sunlight, the grass is always \u2026 \u201d, we need to know the context from further back in the sentence to predict the next word \u201cbrown\u201d.", "Standard RNNs and other language models become less accurate when the gap between the context and the word to be predicted increases. Here\u2019s when LSTM comes in use to tackle the long-term dependency problem because it has memory cells to remember the previous context. You can learn more about LSTM networks here.", "Let\u2019s start coding and define our LSTM model. In building our model, first, an embedding layer, two stacked LSTM layers with 50 units each are used.", "Keras offers an embedding layer that can be used for neural networks on text data. The Embedding layer is initialized with random weights and learns embeddings for all of the words in the training dataset. It requires the input data in an integer encoded form. This data preparation step can be performed with the help of Tokenizer API also provided by Keras. Learn more about Embedding layer here.", "The two LSTM layers are followed by two fully connected or dense layers. The first layer has 50 units and the second dense layer is our output (softmax) layer and has the number of units equal to the vocabulary size. As for each input, the model will predict the next word from our vocabulary based on the probability. Categorical cross-entropy is used as a loss function.", "For input to the Embedding layer, we first have to use Tokenizer from keras.processing.text to encode our input strings. What we are doing in preprocessing is simple: We first create features dictionary sequences. Then we encode it into the integer form with the help of the Tokenizer.", "Let\u2019s understand what is happening in the code above with an example: \u201cHow are you? How many days since we last met? How are your parents?\u201d. We first clean our corpus and tokenize it with the help of Regular expressions, and word_tokenize from nltk library. What does the \u2018sequences\u2019 dictionary do? Below is the \u2018sequences\u2019 dictionary before using the tokenizer.", "Our \u2018text_sequences\u2019 list keeps all the sequences in our training corpus and it would be:", "After using tokenizer we have the above sequences in the encoded form. The numbers are nothing but the indexes of the respective words from the \u2018sequences\u2019 dictionary before re-assignment.", "Once we have our sequences in encoded form training data and target data is defined by splitting the sequences into the inputs and output labels. As for this example, we are going to predict the next word based on three previous words so in training we use the first three words as input and the last word as a label that is to be predicted by the model. Our \u2018training_inputs\u2019 would now be:", "Then, we convert our output labels into one-hot vectors i.e into combinations of 0\u2019s and 1. One-hot vectors in \u2018train_targets\u2019 would look like:", "For the first target label \u201chow\u201d, the index was \u20181\u2019 in sequence dictionary so in the encoded form you\u2019d expect \u20181\u2019 at the place of index 1 in the first one-hot vector of \u2018train_targets\u2019.", "Note: The above code is explained for the text \u201cHow are you? How many days since we last met? How are your parents?\u201d for a simpler explanation. But in reality, a bigger dataset is used.", "Now we train our Sequential model that has 5 layers: An Embedding layer, two LSTM layers, and two Dense layers. In the input layer of our model i.e. Embedding layer, the input length is set to the size of a sequence that is 3 for this example. (Note: We split the data for training inputs and training targets as 3 to 1, so when we give input to our model for prediction we will have to provide 3 length vector.)", "After our model is trained we can give input in the encoded form and get the three most probable words from the softmax function as shown below.", "In the above code, we use padding because we trained our model on sequences of length 3, so when we input 5 words, padding will ensure that the last three words are taken as an input to our model. What happens when we input less than 3 words? We will not get the best results! The same happens when we input an unknown word as the one-hot vector will contain 0 in that word\u2019s index. What we can do in the future is we add sequences of length 2(inputs) to 1(target label) and 1(input) to 1(target label) as we did here 3(inputs) to 1(target label) for best results.", "Below is the final output of our model predicting the next 3 words based on the previous words.", "The above output shows the vector form of the input along with the suggested words. [6, 4, 3] is the \u2018encoded_text\u2019 and [[6, 4, 3]] is the \u2018pad_encoded\u2019.", "Note: Here we split our data as 3(inputs) to 1(target label). Therefore, we must input three words.", "GitHub\u2019s link for the above code is this. You can find the code of the LSTM approach there.", "Above, we saw that the n-grams approach is inferior to the LSTM approach as LSTMs have the memory to remember the context from further back in the text corpus. While starting a new project, you might want to consider one of the existing pre-trained frameworks by looking on the internet for open-source implementations. This way, you will not have to start from scratch and you don\u2019t need to worry about the training process or hyperparameters.", "Much recent work within Natural Language Processing domain includes the development and training of the neural models to approximate the way our human brains exert towards language. This deep learning approach enables computers to mimic the human language in a far more efficient way.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Enthusiast | Full Stack Developer | LinkedIn.com/in/dhruvilshah28"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5e22aeb85d8f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dhruvilshah28?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhruvilshah28?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Dhruvil Shah"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0cd54e8f51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&user=Dhruvil+Shah&userId=e0cd54e8f51c&source=post_page-e0cd54e8f51c----5e22aeb85d8f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@freestocks?utm_source=medium&utm_medium=referral", "anchor_text": "freestocks"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kite.com/python/docs/nltk.word_tokenize", "anchor_text": "word_tokenize"}, {"url": "https://docs.python.org/2/library/collections.html#collections.defaultdict", "anchor_text": "defaultdict"}, {"url": "https://docs.python.org/2/library/collections.html#collections.Counter", "anchor_text": "Counter"}, {"url": "https://github.com/jackfrost1411/next_word_suggestor/tree/master/n-grams", "anchor_text": "this"}, {"url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "anchor_text": "here"}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Tokenizer API"}, {"url": "https://keras.io/layers/embeddings/#embedding", "anchor_text": "here"}, {"url": "https://github.com/jackfrost1411/next_word_suggestor/tree/master/LSTM", "anchor_text": "this"}, {"url": "https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral", "anchor_text": "Josh Riemer"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5e22aeb85d8f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5e22aeb85d8f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5e22aeb85d8f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----5e22aeb85d8f---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/markov-chains?source=post_page-----5e22aeb85d8f---------------markov_chains-----------------", "anchor_text": "Markov Chains"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&user=Dhruvil+Shah&userId=e0cd54e8f51c&source=-----5e22aeb85d8f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&user=Dhruvil+Shah&userId=e0cd54e8f51c&source=-----5e22aeb85d8f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5e22aeb85d8f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5e22aeb85d8f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5e22aeb85d8f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhruvilshah28?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhruvilshah28?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dhruvil Shah"}, {"url": "https://medium.com/@dhruvilshah28/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "172 Followers"}, {"url": "http://LinkedIn.com/in/dhruvilshah28", "anchor_text": "LinkedIn.com/in/dhruvilshah28"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0cd54e8f51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&user=Dhruvil+Shah&userId=e0cd54e8f51c&source=post_page-e0cd54e8f51c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2bf065f3ed8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-the-next-word-predictor-5e22aeb85d8f&newsletterV3=e0cd54e8f51c&newsletterV3Id=2bf065f3ed8b&user=Dhruvil+Shah&userId=e0cd54e8f51c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}