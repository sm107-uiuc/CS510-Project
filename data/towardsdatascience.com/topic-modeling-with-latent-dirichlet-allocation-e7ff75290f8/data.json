{"url": "https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8", "time": 1683017198.7221498, "path": "towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8/", "webpage": {"metadata": {"title": "Topic Modeling with Latent Dirichlet Allocation | by Haaya Naushan | Towards Data Science", "h1": "Topic Modeling with Latent Dirichlet Allocation", "description": "Topic modeling is a form of unsupervised machine learning that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Unsupervised_learning", "anchor_text": "unsupervised machine learning", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Information_retrieval", "anchor_text": "information retrieval", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "term frequency\u2013inverse document frequency (tf-idf)", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Dimensionality_reduction", "anchor_text": "dimensionality reduction", "paragraph_index": 4}, {"url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9", "anchor_text": "Latent Semantic Indexing (Deerwester et al., 1990)", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "singular value decomposition (SVD)", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Low-rank_approximation", "anchor_text": "low-rank approximation", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_Frobenius_norm)", "anchor_text": "Eckart-Young Theorem", "paragraph_index": 6}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "truncated SVD", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Polysemy", "anchor_text": "polysemy", "paragraph_index": 7}, {"url": "https://www.researchgate.net/publication/2941307_Probabilistic_Latent_Semantic_Indexing", "anchor_text": "Hoffman, 1999", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "expectation-maximization algorithm (EM algorithm)", "paragraph_index": 9}, {"url": "https://www.genetics.org/content/155/2/945", "anchor_text": "2000 researchers developed this model for the study of population genetics", "paragraph_index": 10}, {"url": "https://dl.acm.org/doi/10.5555/944919.944937", "anchor_text": "Blei et al., 2003", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling", "anchor_text": "hierarchical Bayesian model", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Exchangeable_random_variables", "anchor_text": "exchangeability", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/De_Finetti%27s_theorem", "anchor_text": "de Finetti theorem", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag-of-words", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Probability_interpretations#Logical,_epistemic,_and_inductive_probability", "anchor_text": "epistemic probability", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli random variables", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed (i.i.d.)", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Mixture_model", "anchor_text": "mixture model", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Mixture_distribution", "anchor_text": "mixture distribution", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "priors", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian inference", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Plate_notation", "anchor_text": "plate notation", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Simplex#Probability", "anchor_text": "probability simplex", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 Theorem", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Laplace%27s_method", "anchor_text": "Laplace approximation", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov chain Monte Carlo", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Gibbs_sampling", "anchor_text": "Gibbs sampling", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes algorithm", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding", "anchor_text": "t-distributed stochastic neighbor embedding (t-SNE)", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "Sci-Kit Learn", "paragraph_index": 20}, {"url": "https://radimrehurek.com/gensim/models/ldamodel.html", "anchor_text": "Gensim", "paragraph_index": 20}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "fine-tune other parameters such as the number of iterations, the learning method (batch or online), the learning offset, perplexity tolerance, and others", "paragraph_index": 21}, {"url": "https://radimrehurek.com/gensim/models/coherencemodel.html", "anchor_text": "topic coherence", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0", "anchor_text": "plotting the resultant topic coherence trend", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Symmetric_probability_distribution", "anchor_text": "symmetric distribution", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Skewness", "anchor_text": "skewness", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian inference", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior probability", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "prior probability", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood function", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayesian method", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Jensen%27s_inequality", "anchor_text": "Jensen\u2019s inequality", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler (KL) divergence", "paragraph_index": 32}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "blog post about KL divergence", "paragraph_index": 32}, {"url": "http://mallet.cs.umass.edu/", "anchor_text": "Java-based library MALLET", "paragraph_index": 33}, {"url": "https://dl.acm.org/doi/abs/10.1145/1401890.1401960", "anchor_text": "collapsed Gibbs sampling", "paragraph_index": 33}, {"url": "https://radimrehurek.com/gensim/models/wrappers/ldamallet.html", "anchor_text": "Python wrapper through Gensim", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity scores", "paragraph_index": 35}, {"url": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding", "anchor_text": "t-distributed Stochastic Neighbor Embedding (t-SNE)", "paragraph_index": 37}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "personal blog of Laurens van der Maaten", "paragraph_index": 37}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity", "anchor_text": "sklearn to calculate perplexity", "paragraph_index": 39}, {"url": "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94", "anchor_text": "this blog post", "paragraph_index": 39}, {"url": "https://dl.acm.org/doi/10.1145/1964858.1964870", "anchor_text": "Hong and Davison, 2010", "paragraph_index": 41}, {"url": "https://dl.acm.org/doi/10.5555/3044805.3044828", "anchor_text": "Tang et al.", "paragraph_index": 42}, {"url": "https://www.linkedin.com/in/haaya-naushan-a4b5b61a5/", "anchor_text": "Linkedin", "paragraph_index": 43}], "all_paragraphs": ["Topic modeling is a form of unsupervised machine learning that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks such as classification or summarization. The goal of topic modeling is to uncover latent variables that govern the semantics of a document, these latent variables representing abstract topics. Currently, the most popular technique for topic modeling is Latent Dirichlet Allocation (LDA), and this model can be used effectively on a variety of document types such as collections of news articles, policy documents, social media posts or tweets.", "This article will necessarily and briefly mention precursive topic modeling techniques, such as Latent Semantic Indexing (LSI, also referred to interchangeably as Latent Semantic Analysis/LSA) and probabilistic Latent Semantic Indexing (pLSI). The main focus will be a discussion of the LDA model, with an emphasis on understanding the role of hyperparameters and the challenge of inference. Next, I offer a practical introduction to implementation, covering dataset requirements, fine-tuning, and evaluation. Lastly, I conclude with a discussion of the limitations of the LDA model.", "A brief history and a theoretical understanding of the foundational techniques that preceded LDA will explain the importance of the improvements made by this modern technique. Back in the 1980\u2019s the field of information retrieval produced a text representation scheme called term frequency\u2013inverse document frequency (tf-idf) for applying a numerical statistic to a word that would be representative of its importance within a document (Salton and McGill, 1983).", "Tf-idf vectorization is where a normalized term frequency (count of the number of occurrences of a term within a document) is compared to the normalized inverse document frequency (count of the number of occurrences of a term within a corpus) on a log scale. This results in a term-by-document weight matrix that is unfortunately very sparse, noisy and redundant. The process to calculate the tf-idf vector for any word in a document can be represented by the equation below.", "Using tf-idf as a dimensionality reduction technique identifies discriminative words for a collection of documents; however, it does not expose the inter- or intra- document statistical structure. Therefore, Latent Semantic Indexing (Deerwester et al., 1990) was introduced as a way of overcoming the deficits of tf-idf. By using singular value decomposition (SVD) to accomplish dimensionality reduction of the term-document matrix, it is possible to identify the linear subspace within the tf-idf features that captures most of the variance in a collection of documents, allowing LSI to be a generative rather than discriminative process.", "The SVD process can be visualized by the diagram below, where term-document matrix A is factored into a product of three matrices: U, S, and VT. The rows of U represent document vectors in terms of topics and the rows of V represent term vectors in terms of topics.", "LSI, however, requires a special type of SVD, specifically a low-rank approximation of the matrix A is required for efficiency, therefore truncation based on the Eckart-Young Theorem (specifically the proof for the Frobenius norm) is used to build an approximate matrix. In application, a value k is the hyperparameter used to represent the number of topics desired, and truncated SVD is used to select only the k columns of U and V. The selected singular values from the diagonal S matrix are represented by \u201c\u03c3\u2097\u2026\u03c3\u2096\u201d, where \u03c3\u2096 \u2265 0 and \u03c3\u2097 has the highest importance. Truncated SVD of matrix A, can be visualized as follows:", "The derived features of LSI are linear combinations of the original tf-idf features, thereby capturing some aspects of linguistic notions of synonymy and polysemy. Nonetheless, LSI unfortunately requires a very large collection of documents and terms, in addition to lacking interpretability with regards to topic content and sentiment.", "Instead of relying solely on truncated SVD, a probabilistic model of LSI was introduced as an alternative, referred to as pLSI (Hoffman, 1999). Each word in a document is sampled from a mixture model, where the mixture components are multinomial random variables that can be viewed as representative of topics. Hence, each word is generated from a single topic, and different words within a document can be generated from different topics. Essentially, each document is reduced to a probability distribution on a fixed set of topics. This probabilistic approach can be modeled by a set of equivalent equations representing the joint probability of a document with a word, P(D,W). Furthermore, as seen in the diagram below, these equations can be derived from the truncated SVD model of LSI.", "In the set of equations above for P(D,W), the joint probability parameters are multinomial distributions that can be trained by an expectation-maximization algorithm (EM algorithm) for inference of parameter estimates which depend on unobserved, latent variables. Importantly, pLSI only applies a probabilistic treatment to topics and words, not documents. This leads to two problems, firstly the number of parameters for pLSI grows linearly with the size of the corpus, so it is prone to overfitting. Secondly, there are no parameters to model P(D), so it is not evident how probability would be assigned to a new document. These issues led to the development of the LDA model, which allowed for better generalization and will be discussed in the following section.", "LDA has roots in evolutionary biology; back in 2000 researchers developed this model for the study of population genetics. A few years later, LDA was applied to the field of machine learning by Blei et al., 2003, a group that includes the renowned Andrew Ng. LDA is a generative probabilistic model, specifically it is a three-level hierarchical Bayesian model, for a collection of discrete data (such as a text corpora). LDA can be thought of as a Bayesian version of pLSI, that overcomes the weakness of the latter and thus allows for better generalization.", "The theoretical underpinnings of LDA rely on exploiting the concepts of exchangeability with the de Finetti theorem (1990). Exchangeability is a major simplifying assumption of text processing that allows for computationally-efficient methods. Both LSI and pLSI and based on the fundamental probability assumption described by the \u201cbag-of-words\u201d method whereby the order of words in a document can be ignored. This assumption of exchangeability extends to the treatment of documents, where one can assume that the specific order of documents in a corpus is not an important consideration.", "According to de Finetti\u2019s theorem, exchangeable observations are conditionally independent relative to a latent variable, therefore an epistemic probability can be assigned to the latent variable. Furthermore, any collection of exchangeable random variables, also referred to as an exchangeable sequence of Bernoulli random variables, has a representation as a \u201cmixture\u201d distribution, specifically a mixture of sequences of independent and identically distributed (i.i.d.) Bernoulli random variables. The implication of a mixture model is the possibility of probabilistically representing the presence of sub-populations within an overall population without requiring an observed dataset to identify the sub-populations. Essentially, utilizing de Finetti\u2019s theorem, it is possible to capture significant intra-document statistical structure via the mixture distribution.", "The innovation of LDA is in using Dirichlet priors for document-topic and term-topic distributions, thereby allowing for Bayesian inference over a three-level hierarchical model, the third layer being the distinguishing feature in a comparison to a simple Dirichlet multinomial clustering model. With regards to Bayesian inference, plate notation is an intuitive method of graphically representing variables that repeat; a \u201cplate\u201d (ie. box) is used to represent replicates, and edges denote conditional dependencies. As seen in the diagram below, the outer plate represents documents, and the inner plate represents repeated choices of topics and words within a document.", "Dirichlet distributions allow for probability distribution sampling over a probability simplex in which all the numbers add up to 1, and these numbers represent probabilities over K distinct categories. A K-dimensional Dirichlet distribution has k-parameters and represents uncertainty as a probability distribution. The Dirichlet prior parameters \ud835\udec2 and \ud835\udec3 are corpus-level parameters that are sampled once in the process of generating a corpus, and parameter \ud835\udeb9\u2098 is a document-level variable that is sampled once per document. Variables z\u2098\u2099 and w\u2098\u2099 are word-level variables that are sampled once for each word in each document. Lastly, \ud835\uded7\u2096 represents the word probability distribution for a topic k.", "With the LDA model, documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. This generative process for each document within a corpus can be written simply as follows:", "The probability of a corpus (D) is the result of taking the product of the marginal probabilities of single documents, and the marginal distribution for a single document is obtained by integrating over \ud835\udf2d and summing over z topics.", "Understanding the importance of Dirichlet distributions in LDA requires an understanding of Bayes\u2019 Theorem which states that, if the prior is Dirichlet distributed (\ud835\udec2, \ud835\udec3) and the likelihood is multinomial distributed (z\u2098\u2099, w\u2098\u2099), the posterior will be Dirichlet distributed and can therefore be computed. Nevertheless, the posterior distribution is intractable for exact inference, and there are several approximating inference algorithms such as Laplace approximation, Markov chain Monte Carlo (eg. Gibbs sampling) and my choice of a variational Bayes algorithm which will be discussed later in the implementation section covering inference.", "A reasonably large dataset is required to build a LDA model. Minimum necessary size is dependent on the characteristics and average length of the documents. Generally, the larger the dataset the better the results, due to an increase in observations. For example, when working with news articles, the minimum number of articles required for modeling is 600; however, results improve when the dataset includes more than 1,000 articles. For tweets, the minimum size depends on the heterogeneity of the conversation, and since tweets have a short sequence length, the minimum dataset size is significantly larger than what is required for news articles. A dataset of 5,000 to 10,000 tweets is sufficient for modelling, and results improve at a slower rate compared to news articles when the dataset grows. For policy documents, which have a longer average length compared to news articles, fewer samples are required to build a model; minimum size depends on the scope of the collection.", "The following four sections will cover the details necessary to successfully implement LDA for topic modeling of text data. The first section provides a description of select hyperparameters of the LDA model, including the Dirichlet priors. Secondly, it is important to consider the problem of inference; specifically, I focus on the solution of a variational Bayes algorithm. Following that, I provide an explanation of how to heuristically fine-tune the Dirichlet prior parameters with empirical experiments (ie. without relying on an EM algorithm for estimations, the method used in the introductory paper by Blei et al., 2003). Lastly, a practical introduction to a method for model evaluation that relies on a t-distributed stochastic neighbor embedding (t-SNE) visualization and perplexity scores.", "There are several Python libraries with LDA modules. Currently, I prefer using Sci-Kit Learn (sklearn), though Gensim is a very popular choice with a multicore option for parallelization of LDA. Regardless of the choice of package, the hyperparameters remain the same, so the following discussion will be general in nature.", "When implementing LDA it is necessary to fine-tune the hyperparameters, specifically the number of topics (k), the number of features (V, ie. fixed vocabulary size), and the \ud835\udec2 and \ud835\udec3 Dirichlet prior parameters. It is possible to fine-tune other parameters such as the number of iterations, the learning method (batch or online), the learning offset, perplexity tolerance, and others. For the sake of practicality, however, I will focus on the four parameters first mentioned. In my experience, working with a variety of datasets ranging from tweets (short and informal) to policy documents (long and formal), I have found that the best approach is to choose hyperparameters heuristically and then refine them with empirical experiments.", "Often, the most important hyperparameter is the number of topics, the choice of which depends on the characteristics and size of the dataset. For example, the larger the dataset the greater the number of topics, only if the dataset is representative of a diverse collection. However, a collection of a few thousand scientific articles on a particular subject, might not contain more topics if several thousand similar articles are added to the initial dataset.", "The heuristic approach is to leverage knowledge of the content of the dataset to estimate a probable target, and make adjustments based on model evaluation and dimensionality reduction based visualizations. The optimal number of topics can be determined by calculating topic coherence scores over a range of topic numbers and plotting the resultant topic coherence trend. However, this is a computationally expensive task (ie. very time-intensive), therefore it is far more efficient to heuristically estimate a starting value for k and use model evaluation techniques to empirically guide adjustments.", "The same approach can be used for choosing the number of features, which is equivalent to setting a fixed size for the vocabulary. The greater the number of features, the longer the LDA model will take to train; however, a sufficiently-sized vocabulary is necessary to capture the most important words for clustering of topics. Generally, setting the number of features to 10,000 is a good starting point for most models. This value needs to be fine-tuned depending on the size of the dataset and the amount of diversity of the words in the collection.", "Importantly, increasing the number of features has diminishing returns on clustering accuracy. For example, if the total vocabulary for a dataset is 20,000 words, setting the number of features to 10,000 will definitely capture the vast majority of important words. On the other hand, assuming a total vocabulary of 100,000 words, increasing the number of features to 15 or 20,000 is enough to capture the majority of important words, since ordering the 100,000 words by tf-idf vectorization values will result in only a small percent being feature relevant.", "Most LDA models assume symmetric distribution, and the \ud835\udec2 and \ud835\udec3 parameters act as prior to the posterior calculation. This assumption of symmetry would mean that each topic is evenly distributed throughout a document, whereas for an asymmetric distribution (as measured by skewness) certain topics would be favoured over others. As Dirichlet prior concentration parameters, \ud835\udec2 and \ud835\udec3, are representative of document-topic density and topic-word density respectively.", "The \ud835\udec2 parameter will specify prior beliefs about topic sparsity and uniformity; visualized as a matrix each row is a document and each column is a topic. With a high \ud835\udec2 value, documents are assumed to contain more topics. Essentially, this means that each document is likely to contain a mix of many topics and not a single topic specifically. Conversely, a low \ud835\udec2 value assumes that a document will contain a mixture of just a few or a single topic. This happens because as the value of \ud835\udec2 decreases, sparsity increases such that, when the distribution is sampled, most values will be zero or close to zero.", "Additionally, if the distributions are asymmetrical, a high \ud835\udec2 value results in a more specific topic distribution per document. Initially, the \ud835\udec2 parameter can be set to a real number value divided by the number of topics, and the results should reveal a sense of the sparsity and symmetry of the distribution. Therefore, the heuristic approach for choosing an \ud835\udec2 value, is to estimate the topical sparsity of each document on average. Subsequent adjustments should be determined by model evaluation and then tested empirically.", "As mentioned, the \ud835\udec3 parameter represents topic-word density, and it is a matrix where each row represents a topic and each column represents a word. The \ud835\udec3 parameter will specify prior beliefs about word sparsity and uniformity within topics, adjusting for bias that certain topics will favour certain words. With a high \ud835\udec3 value, topics are assumed to be made up of most words in the fixed-sized vocabulary and this results in a more specific word mixture for each topic.", "Conversely, with a low \ud835\udec3 value, a topic may contain a mixture of just a few of the words in the fixed-sized vocabulary. Furthermore, if the distribution is asymmetrical, a high \ud835\udec3 value will result in a more specific word distribution. The topics, however, will be more similar in terms of words contained. Generally, it is sufficient to set the \ud835\udec3 value to 0.01 which is the value commonly used when the word distribution is sparse (usually true); however, it may be necessary to adjust this parameter.", "By definition, Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a likelihood function derived from a statistical model for the observed data. With regards to LDA, the problem is how to compute the posterior distribution of the hidden variables, given a document. Normalizing the posterior distribution by marginalizing over the hidden variables results in a function which is intractable, due to the coupling of \ud835\udeb9 and \ud835\udec3 in summation over latent topics. This joint probability can be modeled with plate notation as seen in Figure 5, and expressed with the following equation:", "In this form, it is clear that integrating over \ud835\udeb9, \ud835\udec3 is intractable, and as mentioned earlier, this makes exact inference of the posterior distribution very difficult. One solution is to use variational inference, specifically variational Bayesian methods that allow for approximating intractable integrals arising from Bayesian inference. Variational inference uses Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood; therefore, variational parameters are chosen by an optimization procedure aimed at finding the tightest possible lower bound. The optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior distribution. I recommend this blog post about KL divergence for an understanding of the role this dissimilarity measure plays in Bayesian inference.", "A variational Bayes algorithm provides a locally-optimal exact-analytical solution to an approximation of the posterior distribution, in other words, it is an extension of the EM algorithm. The advantage of this approach over other options is the speed; therefore, I choose to use SciKit-Learn\u2019s LDA module which by default relies on a variational Bayes algorithm for inference. A slower, but nonetheless popular alternative is to use the Java-based library MALLET, which offers an optimized version of collapsed Gibbs sampling, that can also be accessed with a Python wrapper through Gensim.", "Based on my experience, generalized for working with any dataset, the guidelines for a heuristic approach are as follows:", "Alternately, if knowledge of the dataset is limited or if the distribution is asymmetrical, it is possible to empirically rely on model evaluation to inform fine tuning of the \ud835\udec2 and \ud835\udec3 parameters. This is accomplished by calculating perplexity scores and adjusting for sparsity. The general procedure can be described by the following steps:", "Personally, I favour combining the heuristic approach with empirical experiments. Calculating model perplexity scores is a method of model evaluation, which will be discussed further in the following section.", "The challenge with many machine learning models, including the LDA model is how to interact with the high-dimensional data in a meaningful way that is interpretable for humans. Therefore, my primary method of evaluation is to use t-distributed Stochastic Neighbor Embedding (t-SNE) as a tool to visualize the high-dimensional data. This dimensionality reduction technique was introduced by Laurens van der Maaten and Geoffrey Hinton in 2008. The best resource I have found for implementing t-SNE is the personal blog of Laurens van der Maaten; the FAQ section on the t-SNE page in particular, offers valuable tips for understanding the visualization.", "Essentially, the t-SNE technique works to convert similarities between data points to joint probabilities, and then tries to minimize the KL divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. The dimensionality of the LDA model is determined by the number of features set during training (usually a minimum of 10,000); therefore, t-SNE can be used to reduce the dimensions to a 2-D embedding that offers a visualization of the clustering determined by LDA. To interpret the t-SNE, a simple visual evaluation of the clusters offers insight, as does the KL divergence score, for which the value closest to 0 is optimal. In the image below, created with the Python plotting library Bokeh and a dataset of 67,000 tweets, the differently coloured clusters represent the abstract topics, and positioning is determined by the dimensionality reduction algorithm.", "As mentioned earlier, model perplexity scores can be calculated to evaluate the effect of various hyperparameters for empirical testing. I use sklearn to calculate perplexity, and this blog post provides an overview of how to assess perplexity in language models. When building a LDA model I prefer to set the perplexity tolerance to 0.1 and I keep this value constant so as to better utilize t-SNE visualizations. It is important to note that t-SNE has a non-convex objective function, where the objective function is minimized using a gradient descent optimization that is initiated randomly. Therefore, different initializations will result in different solutions, so it is possible (and often advisable) to run t-SNE multiple times with the same data and perplexity, and then choose the visualization with the lowest KL divergence.", "Practically, an assessment of the LDA model reveals a few weaknesses, namely the necessity of a fixed k value, the inability of Dirichlet distributions in capturing correlations, the static nature does not show the evolution of topics over time, and lastly the simplifying \u201cbag-of-words\u201d exchangeability assumption. Of these limitations, none are sufficient to abandon this topic modeling method, but an awareness is necessary to understand the boundaries of results.", "In my experience, LDA can be used consistently and successfully to model text collections of news articles and policy documents, yet the results can be mixed for unconventional datasets, such as collections of tweets which are short and informal. Generally, a dataset is unsuitable for topic modeling if the length of the documents is too short, the data set is too small, or if there are too many topics within a collection (eg. book). In the past, ad hoc heuristics have been successfully employed to preprocess documents, such as aggregating tweets into longer \u201cdocuments\u201d (Hong and Davison, 2010). These measures, however, can be blind since many of the common assumptions of limitations are not theoretically justified; for example, the deficiency in handling shorter documents has not been explained by theory.", "In a paper from 2014 by Tang et al. an attempt was made to understand the limiting factors of LDA with posterior contraction analysis. It was found that Liebig\u2019s law of the minimum is applicable to LDA, whereby the scarcest resource acts as the limiting factor. Of the four guidelines proposed, the most important one is regarding the number of documents (M); a sufficiently sized dataset is absolutely necessary. Once a viable M value is achieved, further increasing M may not significantly improve performance, unless the document length is also suitably increased. Lastly, when a large number of topics (K) is used to fit an LDA model, the statistical inference may become inescapably inefficient. This is because the convergence rate deteriorates quickly to a non-parametric rate, depending on the number of topics used to fit the LDA model. Therefore, in practice it is very important to avoid selecting an overly large k value.", "In conclusion, I believe that an awareness of the LDA model\u2019s deficiencies along with practical guidelines for choosing datasets and hyperparameters will allow for successful implementation of this method for topic modeling. I welcome all feedback, whether questions or comments, so please feel free to connect with me on Linkedin.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Consultant and Data Scientist. Enthusiastic about machine learning, social justice, video games and philosophy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe7ff75290f8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Haaya Naushan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&user=Haaya+Naushan&userId=68f801f1b50b&source=post_page-68f801f1b50b----e7ff75290f8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Unsupervised_learning", "anchor_text": "unsupervised machine learning"}, {"url": "https://en.wikipedia.org/wiki/Information_retrieval", "anchor_text": "information retrieval"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "term frequency\u2013inverse document frequency (tf-idf)"}, {"url": "https://en.wikipedia.org/wiki/Dimensionality_reduction", "anchor_text": "dimensionality reduction"}, {"url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9", "anchor_text": "Latent Semantic Indexing (Deerwester et al., 1990)"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "singular value decomposition (SVD)"}, {"url": "https://en.wikipedia.org/wiki/Low-rank_approximation", "anchor_text": "low-rank approximation"}, {"url": "https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_Frobenius_norm)", "anchor_text": "Eckart-Young Theorem"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "truncated SVD"}, {"url": "https://en.wikipedia.org/wiki/Polysemy", "anchor_text": "polysemy"}, {"url": "https://www.researchgate.net/publication/2941307_Probabilistic_Latent_Semantic_Indexing", "anchor_text": "Hoffman, 1999"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "expectation-maximization algorithm (EM algorithm)"}, {"url": "https://www.genetics.org/content/155/2/945", "anchor_text": "2000 researchers developed this model for the study of population genetics"}, {"url": "https://dl.acm.org/doi/10.5555/944919.944937", "anchor_text": "Blei et al., 2003"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling", "anchor_text": "hierarchical Bayesian model"}, {"url": "https://en.wikipedia.org/wiki/Exchangeable_random_variables", "anchor_text": "exchangeability"}, {"url": "https://en.wikipedia.org/wiki/De_Finetti%27s_theorem", "anchor_text": "de Finetti theorem"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag-of-words"}, {"url": "https://en.wikipedia.org/wiki/Probability_interpretations#Logical,_epistemic,_and_inductive_probability", "anchor_text": "epistemic probability"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli random variables"}, {"url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed (i.i.d.)"}, {"url": "https://en.wikipedia.org/wiki/Mixture_model", "anchor_text": "mixture model"}, {"url": "https://en.wikipedia.org/wiki/Mixture_distribution", "anchor_text": "mixture distribution"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "priors"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian inference"}, {"url": "https://en.wikipedia.org/wiki/Plate_notation", "anchor_text": "plate notation"}, {"url": "https://en.wikipedia.org/wiki/Simplex#Probability", "anchor_text": "probability simplex"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 Theorem"}, {"url": "https://en.wikipedia.org/wiki/Laplace%27s_method", "anchor_text": "Laplace approximation"}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov chain Monte Carlo"}, {"url": "https://en.wikipedia.org/wiki/Gibbs_sampling", "anchor_text": "Gibbs sampling"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes algorithm"}, {"url": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding", "anchor_text": "t-distributed stochastic neighbor embedding (t-SNE)"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "Sci-Kit Learn"}, {"url": "https://radimrehurek.com/gensim/models/ldamodel.html", "anchor_text": "Gensim"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "fine-tune other parameters such as the number of iterations, the learning method (batch or online), the learning offset, perplexity tolerance, and others"}, {"url": "https://radimrehurek.com/gensim/models/coherencemodel.html", "anchor_text": "topic coherence"}, {"url": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0", "anchor_text": "plotting the resultant topic coherence trend"}, {"url": "https://en.wikipedia.org/wiki/Symmetric_probability_distribution", "anchor_text": "symmetric distribution"}, {"url": "https://en.wikipedia.org/wiki/Skewness", "anchor_text": "skewness"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian inference"}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior probability"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "prior probability"}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood function"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayesian method"}, {"url": "https://en.wikipedia.org/wiki/Jensen%27s_inequality", "anchor_text": "Jensen\u2019s inequality"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler (KL) divergence"}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "blog post about KL divergence"}, {"url": "http://mallet.cs.umass.edu/", "anchor_text": "Java-based library MALLET"}, {"url": "https://dl.acm.org/doi/abs/10.1145/1401890.1401960", "anchor_text": "collapsed Gibbs sampling"}, {"url": "https://radimrehurek.com/gensim/models/wrappers/ldamallet.html", "anchor_text": "Python wrapper through Gensim"}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity scores"}, {"url": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding", "anchor_text": "t-distributed Stochastic Neighbor Embedding (t-SNE)"}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "personal blog of Laurens van der Maaten"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity", "anchor_text": "sklearn to calculate perplexity"}, {"url": "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94", "anchor_text": "this blog post"}, {"url": "https://dl.acm.org/doi/10.1145/1964858.1964870", "anchor_text": "Hong and Davison, 2010"}, {"url": "https://dl.acm.org/doi/10.5555/3044805.3044828", "anchor_text": "Tang et al."}, {"url": "https://www.linkedin.com/in/haaya-naushan-a4b5b61a5/", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e7ff75290f8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----e7ff75290f8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/topic-modeling?source=post_page-----e7ff75290f8---------------topic_modeling-----------------", "anchor_text": "Topic Modeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&user=Haaya+Naushan&userId=68f801f1b50b&source=-----e7ff75290f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&user=Haaya+Naushan&userId=68f801f1b50b&source=-----e7ff75290f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe7ff75290f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e7ff75290f8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e7ff75290f8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e7ff75290f8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e7ff75290f8--------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Haaya Naushan"}, {"url": "https://haaya-naushan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "913 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&user=Haaya+Naushan&userId=68f801f1b50b&source=post_page-68f801f1b50b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e411f1cc489&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&newsletterV3=68f801f1b50b&newsletterV3Id=4e411f1cc489&user=Haaya+Naushan&userId=68f801f1b50b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}