{"url": "https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d", "time": 1683012582.4185169, "path": "towardsdatascience.com/an-adaptive-lasso-63afca54b80d/", "webpage": {"metadata": {"title": "An adaptive lasso for python. How to build an oracle estimator that\u2026 | by \u00c1lvaro M\u00e9ndez Civieta | Towards Data Science", "h1": "An adaptive lasso for python", "description": "This is my second post on the series about penalized regression. In the first one we talked about how to implement a sparse group lasso in python, one of the best variable selection alternatives\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/sparse-group-lasso-in-python-255e379ab892", "anchor_text": "sparse group lasso in python", "paragraph_index": 0}, {"url": "https://github.com/alvaromc317/asgl/blob/master/user_guide.ipynb", "anchor_text": "jupyter notebook", "paragraph_index": 30}, {"url": "https://doi.org/10.1007/s11634-020-00413-8", "anchor_text": "adaptive sparse group lasso in quantile regression.", "paragraph_index": 30}, {"url": "https://doi.org/10.1007/s11634-020-00413-8", "anchor_text": "Mendez-Civieta, A., Aguilera-Morillo, M. C., and Lillo, R. E. (2020). Adaptive sparse group LASSO in quantile regression. Advances in Data Analysis and Classifcation.", "paragraph_index": 31}], "all_paragraphs": ["This is my second post on the series about penalized regression. In the first one we talked about how to implement a sparse group lasso in python, one of the best variable selection alternatives available nowadays for regression models, but today I would like to go one step ahead and introduce the adaptive idea, that can convert your regression estimator into an oracle, something that knows the truth about your dataset.", "Let me start with a brief introduction of lasso regression. Imagine you are working with a dataset in which you know that only a few of the variables are truly related with the response variable but you do not know which ones. Maybe you are dealing with a high dimensional dataset with more variables than observations, in which a simple linear regression model cannot be solved. For example, a genetic dataset formed by thousands of genes but in which just a few genes are related with a disease.", "So you decide to use lasso, a penalization that adds an L1 constraint to the \u03b2 coefficients of the regression model.", "This way, you obtain solutions that are sparse, meaning that many of the \u03b2 coefficients will be sent to 0 and your model will make predictions based on the few coefficients that are not 0.", "You have potentially reduced the prediction error of your model by reducing the model complexity (the number of variables different than 0). But as a side effect, you have increased the bias of your estimation of \u03b2 (this is known as the variance bias tradeoff).", "Lasso provides sparse solutions that are biased, so the variables that lasso selects as meaningful can differ from the truly meaningful variables.", "Other penalizations such as ridge regression and sparse group lasso face the same problem: they provide biased solutions and thus can fail to identify the truly meaningful variables in our model.", "Our objective then is clear: we want a solution that is not biased, so that we can select the variables from our dataset as if we knew in advance which ones were the truly significant variables. Just like if our estimator were an oracle that knew the truth.", "I know, calling \u201coracle\u201d to a regression estimator can sound like something I came up with, but it actually has a mathematical formal definition, proposed by Fan and Li (2001). An estimator is oracle if it can correctly select the nonzero coefficients in a model with probability converging to one, and if the nonzero coefficients are asymptotically normally distributed.", "This means that given a set of p variables {\u03b21,\u2026,\u03b2p}, if we consider two subsets,", "An oracle estimator selects the truly significant variables with probability tending to one. Asymptotically, both subsets coincide.", "So\u2026 how can we obtain our oracle estimator? We can use, for example, an adaptive lasso estimator. This estimator was proposed initially by Zou (2006), and the idea behind it is pretty straightforward: add some weights w that corrects the bias in lasso.", "If a variable is important, it should have a small weight. This way it is lightly penalized and remains in the model. If it is not important, by using a large weight we ensure that we get rid of it and send it to 0.", "But this yields to the last question we will see today:", "There are many alternatives for computing these weights, but today I will go with one of the simplest:", "3. Plug in the weights and solve the adaptive lasso", "And that\u2019s it. Now your estimator is an oracle, and you will obtain much better predictions (both in terms of prediction errors and in terms of subset selection) than the ones you would obtain by using a simple lasso.", "But do not believe me, let\u2019s test this in Python using the asgl package.", "We start by installing the asgl package, which is available both in pip and as a GitHub repository.", "First, let\u2019s import the libraries that we will use. We will test the benefit of using an adaptive lasso estimator on a synthetic dataset generated using the make_regression() function from sklearn . Our dataset will have 100 observations and 200 variables. But among the 200 variables only 10 will be related with the response, and the remaining 190 ones will be noise.", "x is the matrix of regressors, of shape (100, 200), y is the response variable (a vector of length 100) and true_beta is a vector that contains the true value of the beta coefficients. This way we will be able to compare the true betas with the ones provided by lasso and adaptive lasso.", "We will compare a simple lasso model against an adaptive lasso model and see if the adaptive lasso actually reduces the prediction error and provides a better selection of meaningful variables.", "For this, we consider a train/validate/test split for the dataset. We train the models for the different parameter values using the training set. Then, we select the best model using the validation set, and finally, we compute the model error using the test set (that was not involved in both model training and selection). This can be directly done in the asgl package using the TVT class and the train_validate_test() function.", "We will solve a linear model model=lm with a penalization=lasso, and define the values for lambda1, which is the parameter \u03bb associated with the lasso penalization. We will find the optimal model in terms of the minimum mean squared error (MSE), and will use 50 observations for training the model, 25 for validation and the remaining (25) for testing. All this is automatically performed by the train_validate_test() function.", "The prediction error from the best lasso model (in terms of MSE) is stored inlasso_prediction_error, and the coefficients associated to the model are stored in lasso_betas", "Now we solve the adaptive lasso model. For this, we specify the penalization=alasso (that stands for adaptive lasso), and we select the technique used for computing the weights as weight_technique=lasso . As described above, this way we will solve an initial lasso model, compute the weights, and then plug this weight into a second lasso model, which will be our final model.", "Finally, let\u2019s compare the results. We will compare two metrics:", "In the following code snippet, the bool_something variables are used for the computation of the correct selection rate.", "The results obtained by the adaptive lasso are much better than those from simple lasso. We see that the adaptive lasso error is almost 8 times smaller than the lasso error (1.4 from lasso compared to 11.8 from lasso). And in terms of variable selection, while lasso only selected correctly 13% of the 200 variables, the adaptive lasso selected correctly 100% of the variables. This means that the adaptive lasso was able to identify correctly all the meaningful variables as meaningful, and all the noisy variables as noisy.", "And that\u2019s it on this post about the adaptive lasso. Remember, try to use oracle estimators, as they know the truth of your dataset. I hope you enjoyed this post and found it useful. Please, contact me if you have any question/suggestion.", "For a deeper review on what the asgl package has to offer, I recommend reading the jupyter notebook provided in the Github repository, and for a review on oracle estimators, I suggest a recent paper published as part of my Ph.D: adaptive sparse group lasso in quantile regression.", "Mendez-Civieta, A., Aguilera-Morillo, M. C., and Lillo, R. E. (2020). Adaptive sparse group LASSO in quantile regression. Advances in Data Analysis and Classifcation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hello, I am a Ph.D. student at University Carlos III of Madrid. I mainly work in high dimensional statistics, but I am also very interested in deep learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F63afca54b80d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----63afca54b80d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alvaromc317?source=post_page-----63afca54b80d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alvaromc317?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "\u00c1lvaro M\u00e9ndez Civieta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97c9cdfb03c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&user=%C3%81lvaro+M%C3%A9ndez+Civieta&userId=97c9cdfb03c5&source=post_page-97c9cdfb03c5----63afca54b80d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral", "anchor_text": "Pierre Bamin"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/sparse-group-lasso-in-python-255e379ab892", "anchor_text": "sparse group lasso in python"}, {"url": "https://github.com/alvaromc317/asgl/blob/master/user_guide.ipynb", "anchor_text": "jupyter notebook"}, {"url": "https://doi.org/10.1007/s11634-020-00413-8", "anchor_text": "adaptive sparse group lasso in quantile regression."}, {"url": "https://doi.org/10.1198/016214501753382273", "anchor_text": "Fan J, Li R (2001) Variable selection via nonconcave penalized likelihood and its oracle properties. J Am Stat Assoc 96(456):1348\u20131360"}, {"url": "https://doi.org/10.1198/016214506000000735", "anchor_text": "Zou H (2006) The adaptive lasso and its oracle properties. J Am Stat Assoc 101(476):1418\u20131429"}, {"url": "https://doi.org/10.1007/s11634-020-00413-8", "anchor_text": "Mendez-Civieta, A., Aguilera-Morillo, M. C., and Lillo, R. E. (2020). Adaptive sparse group LASSO in quantile regression. Advances in Data Analysis and Classifcation."}, {"url": "https://medium.com/tag/lasso-regression?source=post_page-----63afca54b80d---------------lasso_regression-----------------", "anchor_text": "Lasso Regression"}, {"url": "https://medium.com/tag/lasso?source=post_page-----63afca54b80d---------------lasso-----------------", "anchor_text": "Lasso"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----63afca54b80d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----63afca54b80d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----63afca54b80d---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&user=%C3%81lvaro+M%C3%A9ndez+Civieta&userId=97c9cdfb03c5&source=-----63afca54b80d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&user=%C3%81lvaro+M%C3%A9ndez+Civieta&userId=97c9cdfb03c5&source=-----63afca54b80d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F63afca54b80d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----63afca54b80d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----63afca54b80d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----63afca54b80d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----63afca54b80d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----63afca54b80d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----63afca54b80d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alvaromc317?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alvaromc317?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "\u00c1lvaro M\u00e9ndez Civieta"}, {"url": "https://medium.com/@alvaromc317/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97c9cdfb03c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&user=%C3%81lvaro+M%C3%A9ndez+Civieta&userId=97c9cdfb03c5&source=post_page-97c9cdfb03c5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fec20f9e9838b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-adaptive-lasso-63afca54b80d&newsletterV3=97c9cdfb03c5&newsletterV3Id=ec20f9e9838b&user=%C3%81lvaro+M%C3%A9ndez+Civieta&userId=97c9cdfb03c5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}