{"url": "https://towardsdatascience.com/five-reasons-for-in-database-data-mining-84e9cf8e084a", "time": 1683006276.768241, "path": "towardsdatascience.com/five-reasons-for-in-database-data-mining-84e9cf8e084a/", "webpage": {"metadata": {"title": "Five reasons for in-database data mining | by V\u00edctor L. Fandi\u00f1o | Towards Data Science", "h1": "Five reasons for in-database data mining", "description": "The purpose of this paper is to present the relational database system as a suitable platform to deploy data mining workloads"}, "outgoing_paragraph_urls": [{"url": "https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/AnalyzingData/MachineLearning/MachineLearning.htm", "anchor_text": "Machine Learning for Predictive Analytics", "paragraph_index": 79}, {"url": "https://docs.teradata.com/reader/Tk6W9zbo2NHGakhfyvWmmw/nt3Z~v5KgGV98ELQWJvQNg", "anchor_text": "Machine Learning Engine Analytic Function Reference", "paragraph_index": 80}, {"url": "http://docs.greenplum.org/6-4/analytics/madlib.html", "anchor_text": "Machine Learning and Deep Learning using MADlib", "paragraph_index": 81}, {"url": "https://docs.oracle.com/en/database/oracle/oracle-database/19/dmprg/index.html", "anchor_text": "Oracle Data Mining User\u2019s Guide", "paragraph_index": 82}, {"url": "https://www.ibm.com/support/knowledgecenter/SSCJDQ/com.ibm.swg.im.dashdb.analytics.doc/doc/r_analytic_stored_procedures.html", "anchor_text": "Analytic Stored Procedures", "paragraph_index": 83}, {"url": "https://www.iso.org/standard/38648.html", "anchor_text": "ISO/IEC 13249\u20136:2006 Information technology | Database languages | SQL multimedia and application packages | Part 6: Data mining", "paragraph_index": 84}, {"url": "http://dmg.org/pmml/v4-4/GeneralStructure.html", "anchor_text": "PMML 4.4 General Structure", "paragraph_index": 85}, {"url": "http://github.com/datamininggroup/pfa/releases/download/0.8.1/pfa-specification.pdf", "anchor_text": "PFA 0.8.1 Specification", "paragraph_index": 86}, {"url": "http://archive.ics.uci.edu/ml/datasets/Wine", "anchor_text": "Wine Data Set", "paragraph_index": 87}], "all_paragraphs": ["Data mining can be defined as the set of methodologies, processes and technologies for the non-trivial discovery of relevant information, usually underlying in large volumes of data, and its subsequent application and integration into business operations, in order to improve performance and support decision making.", "The purpose of this paper is to present the relational database system as a suitable platform to deploy data mining workloads, involving both modeling and scoring operations. Because of its unique capabilities in terms of information management, scalability, extensibility and performance, the relational database provides an integrated environment where data and algorithms coexist for the benefit of the business.", "The above data mining definition consists of three parts that must be properly qualified.", "First, non-trivial discovery of relevant information implies the detection of patterns, tendencies and correlations that cannot be exposed through conventional query techniques, either because these are, in fact, inappropriate, or highly inefficient for the complexity of the problem. By contrast, data mining provides methods coming from disciplines such as artificial intelligence (machine learning) and multivariate analysis to address this kind of problems. These methods, based on statistically robust algorithms, can model complex relationships in structured and semi-structured data sets, involving different variable types, high scattering levels, and with no assumption about the underlying data distribution. Data mining modeling methods are usually categorized as supervised learning (classification, regression, time series forecasting) and unsupervised learning (clustering, association rules detection, sequential patterns discovery).", "Secondly, information discovery requires a methodology. It is needed in order to define the problem that must be addressed, the business context and the required analysis framework. The framework covers the variables or features that will be included in the analysis, as well as the sequence of preparation steps and modeling tasks to be conducted until a valid solution is found and can be applied (Figure 1). This methodology is translated into a set of processes that include the initial qualification and preparation of the data sets, the development, assembly and validation of one or several models for knowledge extraction and, finally, and most importantly, the deployment of the preparation-modeling stream into production and its continuous monitoring and recalibration. As in other environments, these processes are implemented using several underlying IT services that execute them.", "Last but not least, the main goal of the extracted information is to improve business performance. First, exposing the new knowledge and insight in order to support strategic plans and tactical decisions. Then, deploying this knowledge into business operations by applying the analytical models and integrating the results inside the informational and transactional processes.", "The business focus, the methodological approach and a service-oriented implementation, make data mining a core business analytics specialization, and not just a bunch of mathematical techniques and algorithms.", "In the last decade, the number of application areas and potential users of data mining has expanded considerably. Something that in the past was considered a restricted domain of highly skilled statistical practitioners now has evolved to be part of business applications, involving data engineers, developers and end-users. This step has been possible to the extent the technological evolution has facilitated and automated the use and application of the modeling techniques. The formalization and standardization of the methodology have also contributed, but not as decisively as the technology has enhanced a new generation of analytical applications where data mining is embedded, enabling business users to solve complex problems and take advantage of new opportunities.", "From a tooling perspective, until the mid-nineties, data mining modeling was mainly developed in a case-by-case basis through programming languages, although the existence of some statistical packages and specialized libraries contributed to facilitate the effort. Then, a set of data mining workbenches, equipped with sophisticated end-user graphical interfaces, appeared in the market. Solutions like SAS Enterprise Miner, IBM Intelligent Miner, Angoss KnowledgeStudio or ISL Clementine (acquired later by SPSS, now SPSS Modeler, part of IBM Analytics portfolio) provided a data mining visual design approach, with extended functionality for database connectivity, data preparation, scripting extensions and model visualization. Although most of these applications operated in stand-alone mode, vendors usually provided client-server architectures to improve scalability, concurrency and performance. In some sense, it is certainly surprising that a large part of new data mining practitioners has embraced again a pure coding approach, not considering the benefits a specialized modeling environment can provide in terms of ease of use, productivity and maintainability.", "While the workbench-based approach includes capabilities for data transformation, model training, testing and recalibration, it still requires a deep knowledge of modeling concepts and thus, specialized staff. Also, model management has been, and still is in many solutions, vendor-specific and hence, proprietary. The integration of data mining functionality within the business application layer has been really challenging, mainly due to the lack of flexible, standardized and easy-to-use model formats and programming interfaces. Although the emergence of producer-agnostic scoring platforms came to facilitate model inference and deployment, the development of fully integrated data mining solutions is still an issue with this kind of applications, particularly for everything about model creation and introspection. This lack of automation capabilities in model life cycle management is behind the wide current use of programming libraries, mostly Python or Scala based, usually supported by distributed computing frameworks, like Apache Spark or Hadoop.", "On the other side, in the middle of the 2000s, a data-centric data mining approach reached the market. As with other in-database analytics offerings, the idea was to bring and concentrate the data mining effort as close as possible to where the core data resides.", "In-database analytics initiatives have their origins in the mid-90s, when providers like Sybase, IBM, Oracle or Teradata equipped their relational database systems with object-oriented extensions. These extensions allowed the creation of user-defined types and methods, so the definition and manipulation of complex data structures inside the database engine became possible. Also, the ability to include C-language and, later, Java routines in stored procedures and functions expanded the SQL procedural language capabilities, so the implementation of rich business logic in the database was not a restriction any longer. At this point, geospatial, image, audio or content management extensions began to be part of subsequent database releases.", "In-database data mining is based on this database extension scope. The idea is to leverage the database platform capabilities and resources to support and expose data mining functionality and, what is most important, enable business applications with embedded data mining, making it more accessible to developers and, in the end, to business users.", "Currently, in-database data mining is a mature path in order to implement integrated solutions with pattern discovery and predictive modeling capabilities. Data warehousing vendors like Vertica (Vertica Analytics Platform) [1], Teradata (Teradata Vantage) [2], Pivotal (Greenplum Database) [3], Oracle (Oracle Data Mining) [4] or IBM (IBM Db2 Warehouse) [5] have been releasing their in-database data mining extensions for years, both in the form of software platforms, appliances or data warehouse-as-a-service offerings (DWaaS) on public or private cloud platforms.", "In the present environment, where software architectures based on decoupled microservices dictate development best practices, and hybrid cloud environments demands greater flexibility when orchestrating changing components, what are the real benefits of placing data mining functionality in the database? What are the key points that differentiate and give value to in-database data mining when compared to other approaches?", "Here is a list of five reasons that give the database a competitive advantage when it is considered as the target environment to implement data mining initiatives for the enterprise. The list is numbered, but the ordering does not represent relevance; it is just a formal way of exposing each topic on top of its predecessors.", "Relational database systems have been at the core of business systems for decades, evolving with them and making technology evolve. They have demonstrated to be the best option for OLTP (OnLine Transaction Processing) applications, where data availability and consistency is a must, but also for OLAP (OnLine Analytical Processing) scenarios, where scalability and query performance for complex and dynamic aggregations are critical factors. Additionally, relational databases provide the main persistence option for CRM (Customer Relationship Management) and MDM (Master Data Management) solutions. This means that most customer behavioral and sociodemographic data, the fuel of business analytics applications, resides natively in relational repositories.", "But a relational database is not just a powerful data store. It is an environment where data-intensive applications, these being batch, real-time or event-driven, can be deployed. And, of course, data mining is a data-intensive process.", "The mechanism by which an application can be implemented in a database is through routines. Database routines are objects that enclose programming logic than can be invoked using SQL. As with the rest of programming approaches, routines simplify code reuse, standardization and maintenance. Routines can be built-in, so they are provided with the database for a variety of support tasks, or created by the user to extend the SQL language. Besides other benefits, like encapsulating access control, database routines improve application performance and SQL execution. This is especially important when building data mining models with big training sets.", "Model building means extensive database interactions, because data has to be repetitively fetched in each training epoch. If the data mining algorithm is running as a database client, this process involves multiple remote SQL statements, which in turn can create many send and receive network operations. This results in high levels of network traffic with the associated increased processor costs. If the training algorithm process is implemented as a routine that is executed in the database, all these SQL statements, including those writing to result tables, are encapsulated into a single send and receive operation. Consequently, network traffic is minimized and training performance improved.", "Additionally, SQL statements inside routines usually provide better performance than those issued from client applications. This is due to routines being executed in the same process than the database, leveraging shared memory for communication. If the routine implements static SQL, which will be the case in data mining tasks touching data from predefined tables, access plans will be generated at precompile time, resulting in no runtime overhead and better performance.", "From a functional point of view, database routines can be classified in stored procedures, functions and methods. A stored procedure is a compiled program that can be invoked by executing a CALL or EXEC statement. It can have input, output and input-output parameters, as well as execute a wide variety of SQL statements and procedural logic. A stored procedure can also return multiple result sets. Functions provide a way to extend and customize the SQL functionality. Unlike stored procedures, functions are invoked inside a SQL statement, such as a SELECT or VALUES expression or a FROM clause. Depending on the kind of result type, a function can be scalar (returns a single value), column (aggregates a scalar result over a set of input values), row (a single row is returned) or table (returns a table to the SELECT statement that references it in the FROM clause). Finally, methods are used to define the behavior for user-defined types, much the same as in other object-oriented approaches.", "From an implementation perspective, a database routine logic can be composed entirely of SQL statements or by an external programming language code. In the former case, routine logic is implemented using SQL procedural language extensions, that usually vary between providers (Sybase Transact-SQL, IBM SQL PL, Oracle PL/SQL, PostgreSQL PL/pgSQL, etc.). These extensions bring statements for variables and condition handlers declaration, control structures, error management and so on.", "External routines can be written in many programming languages, like C/C++, Java, .NET or Python. In any case, the code resides external to the database, but in the same server. At execution time, these routines can run in the same process as the database manager, providing great performance. They significantly extend the logic complexity that can be implemented in a SQL routine. For instance, they can interact with the file system, perform HTTP requests or send e-mail notifications. Although SQL routines offer the best option in terms of performance, security and scalability, C/C++ routines are usually comparable, offering in addition richer programming capabilities.", "External routines are usually the best option to implement modeling algorithms in the database. In particular, external stored procedures are most suitable for training operations, because their execution and invocation manner (a CALL statement to a program) is what is expected in a model building task. Also, model building usually involves creating new data structures and inserting or modifying existing data; this kind of operations are neither expected nor supported in a function, which is always part of a SELECT or other data manipulation statements. In this sense, stored procedures also provide more extensive support for SQL procedural elements than functions. Oppositely, functions are usually preferred for scoring operations because these fit naturally in a SELECT statement.", "The benefits of external stored procedures for modeling operations can be summarized as follows:", "In order to implement these functionalities, routines can be developed in-house or implemented by leveraging specialized built-in data mining modules from the database provider. Of course, a mixed approach is possible and recommended, so the out-of-the-box functionality is customized and adapted to the own environment. Table 1 summarizes the main modeling capabilities for different in-database data mining providers; the idea is not to detail all the supplied algorithms, but to show that different descriptive, transformation and modeling tasks are well supported by all of them. Unfortunately, but not surprisingly, there is no unified offering in terms of standardization for in-database modeling and scoring activities; almost all providers embrace a proprietary, and thus, vendor-locking approach. However, this does not mean a lack of efforts in that sense.", "With the general title Information Technology \u2014 Database languages \u2014 SQL multimedia and application packages (SQL/MM), the ISO/IEC 13249 is an international standard that defines a set of modules of application-specific data types, table structures and related routines. The motivation behind this norm is to enable several content types, like images, text and spatial data, to be stored and manipulated in a database, providing extensions to the ANSI SQL standard. In this direction, and published for the first time in 2002, SQL/MM Part 6: Data Mining [6] covers data mining user-defined types and their associated routines. It defines a standardized interface, in the form of a SQL API, to data mining algorithms that can be tiered on top of a relational database. The goal is to store, manage and retrieve information based on elements such as physical and logical specifications, data mining models, mining run settings and model results.", "Although the specification is really rich in terms of functionality and supported models, the success of this norm has been clearly limited. The only provider that has implemented these interfaces in a commercial or open source product is IBM, with the Db2 Intelligent Miner Modeling and Scoring extenders. However, IBM has deprecated this functionality in Db2 11.1, moving towards a proprietary approach in Db2 Warehouse 11.5.", "More consensus, at least initially, has been achieved on model representation. Currently in version 4.4 (2019), the Predictive Model Markup Language (PMML) [7] is de facto standard for the definition of statistical and data mining models. Based on XML, PMML enables the transmission of models between producer and consumer applications. The Data Mining Group (DMG), the consortium that develops the standards, claims that it is supported by around 30 providers. Although different programming frameworks provide extensions in the form of packages and libraries, PMML has, in fact, lost some attraction in the last years. This is due to the lack of flexibility in the definition of new algorithms and extensions, so interoperability between model producers and scoring engines is sometimes limited.", "To resolve these limitations, the DMG released in 2015 the Portable Format for Analytics (PFA) [8], a complementary JSON-based format more similar to a programming language. PFA defines models by combining building blocks based on a type system, control structures and an extensive library of primitive functions. In this way, a new algorithm pipeline can be easily assembled, with no need to be accepted by any specification, as is the case with PMML. PFA is still young but gaining attention. It is well positioned for general-purpose adoption, especially when compared with more focused open standards, like MLeap, ONNX or NNEF.", "In the current database landscape, all providers implement a proprietary format for model definition. In addition, only Greenplum, Oracle and IBM provide some kind of built-in support to export or import PMML models, but they are losing model amplitude in recent releases. PFA is not supported yet by any database provider.", "As an example of how built-in data mining extensions work in a database, Figure 2 represents a modeling pipeline built in IBM Db2 Warehouse. In this implementation, the data mining framework is composed by a module (SYIBMADM.IDAX) that contains a set of variables, conditions, functions and analytical stored procedures. These routines are supported by a set of four catalog tables storing model metadata.", "These modeling steps deploy a simple clustering scenario based on the wine dataset [9], a well-known example in the field of chemometrics, and frequently used in machine learning to test and showcase different algorithms. The dataset contains the results of the chemical analysis of 13 constituents present in several wines belonging to three different cultivars. Besides building the model, the pipeline features also a scoring (predict) step: the goal is to analyze the mapping between clusters and cultivars.", "Listing 1 details each step of the flow in terms of the stored procedure used, its functionality and the arguments required (step 2 is fictitious and just for completeness: the wine dataset contains no missing values).", "Once the model is built (step 6) it is registered in the catalog tables. These tables contain different metadata, covering model definition, hyperparameters, features properties and related components. These components are tables that are built as a result of the training run, and they depend on the type of model generated. In the case of a K-means clustering model, the following tables are created and populated:", "In short, model insights are ready to be consumed in a relational format by query & reporting tools or interactive dashboards. Finally, the last step exports the model in PMML format for further exploitation inside or outside the database.", "From a model building point of view, data mining is not only about executing training runs based on sophisticated algorithms. The way data can be organized, accessed and manipulated is key in order to ensure a good performance and scalability, especially when dealing with billions of records.", "The following is a quick review of some of the capabilities of relational database systems, especially those involved in how to optimize the access and retrieval of training data, as well as how to build and administer data mining models.", "Database shared-nothing architectures have been there for years, ensuring the performance and capacity of the system by providing increased processing power and storage resources. Scaling out is achieved by adding new partitions, logical or physical, to an existing cluster. In a multi-partition database, a table can be placed in more than one partition, so its rows are distributed along the members of the cluster. In this way, each partition consists of its own resources, data, indexes, configuration files, etc.", "By spreading table rows, a partitioned database provides extended parallelism capabilities, especially for data retrieval. Besides intrapartition parallelism, where a single SQL query is subdivided into multiple parts which can be run in parallel within a single partition, interpartition parallelism means breaking up a query across multiple partitions, with the parallelism degree determined by the number of existing database partitions. Intrapartition and interpartition parallelism can be used simultaneously and transparently, providing a high increase in the speed at which queries are processed. This is especially notorious when dealing with big data sets in the form of star schemas, where fact tables often run into billions of rows and there are many dimension tables and, therefore, a large number of join operations.", "Figure 3 represents a call detail record (CDR) fact table that has been organized in a three-level schema. In the first level, rows have been distributed across three database partitions using the call ID. In this case, this distribution key ensures that records are allocated evenly across all three partitions, so no one will be idle during data access. Then, in each database partition, the table has been partitioned by defined ranges, using the call month in this case. Each range is in a different storage object and is easy to detach and attach new ranges as soon as new data is available. Finally, table data is organized into blocks along one or more dimensions (call hour in this example). Globally, this organization schema supports database designs that exhibit great levels of scalability and performance. In any case, it is important to realize that processing data across multiple partitions and nodes impose some overhead, thus a trade-off between data volume and performance must be met.", "In addition, some database engines support column-organized tables. In this case, table data pages contain compressed column data instead of row data, so I/O is performed only on columns that are part of the query. Analytic workloads based on this kind of tables are orders-of-magnitude faster, providing storage savings and eliminating the need for indexes, materialized views, range partitioning and time-consuming database tuning.", "From a data mining standpoint, these table partition and organization strategies signify the speed up of training tasks in terms of data access and compute time. Routines that implement algorithms that fully exploit these features are based on a split-apply-combine strategy. They run a main process that issues several parallel requests, each one reading a fraction of the training data and computing the respective statistical aggregations and metrics. All these partial results are finally gathered and aggregated globally before the next training iteration. Additionally, several routines can always take advantage of interquery parallelism and be executed simultaneously.", "Partitioning capabilities enable also high-volume, high-speed, parallelized data scoring, providing better concurrency support for real time and batch operations.", "Basically, a federated system provides data virtualization capabilities. That means that a single SQL statement (read & write) can gather data distributed among several data sources, each one from a different vendor. For example, with one SELECT it is possible to join data that is in an Informix table, a MySQL table, a HDFS Parquet file and an Excel sheet, all as if the data were stored locally in a single database. From a data mining point of view, this can simplify data access both for modeling and scoring, avoiding consolidation tasks that may be expensive. It also facilitates the combined access to disparate and heterogeneous data technologies, providing connectors (wrappers) to relational and non-relational data sources. This results in less effort when assembling different data pieces in order to build training and validation sets.", "Database engines that provide these capabilities maintain a federated system catalog, containing information about the objects and statistics in the federated database. When a query is submitted, the SQL compiler uses this information to develop an optimum access plan, decomposing the query into fragments that are pushed down to each data source. As a result, each fragment can run natively, and the results be combined and presented. Also, most federated systems support cache and materialized query tables with remote objects, improving the performance of queries and encapsulating a part of the logic. Federated stored procedures at remote data sources are commonly supported as well.", "The benefits of a federated database are not only visible to in-database data mining deployments, but to any approach that leverages the database as a repository, especially those implementing SQL pushback.", "In the database context, workload management refers to the capability to monitor and control active work in the system. That means that it is possible to divide different workloads into categories, adapting the database server to support different users and applications requirements on the same system. Management activities usually start with the definition of different execution groups and subgroups where database resources can be allocated, as well as thresholds that determine how work is permitted to run on them. These resources include CPU limits, maximum execution time, memory access priority, concurrency control or maximum degree of parallelism for running activities. The execution group definition usually also specifies the kind of data that will be collected for the statistics event monitors.", "The second step is the classification of database activities into workloads, which are then mapped to execution groups. Workloads can be defined based on connection attributes, such as the user ID, user role or application name, or on activity type attributes. For instance, it is possible to separate data definition and manipulation activities in different subgroups, segregate specific intensive read-only queries from the rest, or isolate certain stored procedure calls. Figure 4 shows how to address and monitor different workloads types in a data warehouse environment.", "Workload management is a critical element in order to operate an enterprise business analytics environment. Assigning in-database data mining workloads to specific execution subgroups provides several benefits. For instance:", "The idea behind database backup is simple: anticipate a possible failure of the database by making a copy of the current data, and storing it in a different medium, so it can be restored later when the system is up and running again. Backup can usually be done at database, database partition or tablespace level, covering only specific tables in this last case. Also, backups can be made while the database is either online or offline. The process can be automated as well, so the database manager determines the need to perform a backup based on different measures.", "From a data mining perspective, database backup means taking care of the model catalog tables. These tables can be placed on a dedicated tablespace (which is recommended), so they can be maintained separately with a specific policy. In any case, they will be part of the system plan, probably with a different schedule, so there is no need for a separate backup and recovery mechanism for the data mining models.", "Besides global disaster recovery strategies, including HADR (High Availability & Disaster Recovery), some interesting scenarios can be suggested with simple backup features. One of them is snapshot control with version recovery. In this case, a tablespace can be recovered to a previous version using an image created previously with a backup operation. If a set of models are generated in a recurrent and scheduled mode and there is some need to restore previous versions (for auditing or quality control purposes, for instance), then they can be placed in separated tables in a dedicated tablespace, where the backup and restore operations are made before a new batch of models is generated.", "Authentication and authorization are two common database operations. The first one is usually delegated to an external facility, such as the operating system or a LDAP server, but authorization is intrinsic to the database manager. Authorization consists in checking which database operations can be performed and which objects can be accessed by the authenticated user. For data mining related operations, defining authorization privileges at the user or group level can be useful in the following situations:", "First-class database systems provide row and column access control (RCAC), which complements the table privileges model. RCAC provides two benefits that can be combined in terms of data protection. First, it is possible to restrict user\u2019s access to only a subset of records, based on some predefined condition. That means that records that do not meet such condition are simply ignored in the data retrieval. Second, data can be masked at the column level. Column masks hide data returned to users or applications unless they are permitted. So, it is possible to restrict not only who can access specified columns, but also through which routines can this access be done. RCAC facilitates the implementation of complex business rules, defining which specific data sets can be accessed, by whom and using which modeling and scoring routines. It also enables the definition of fine-grained access control over the model catalog.", "Additional capabilities can be enumerated. For instance, routine activity monitoring and execution scheduling for training and scoring runs can be implemented using out-of-the-box features, as well as auditing tasks based on the unified database logging subsystem. And what is also important, all of them can be managed programmatically using a common and unified SQL administrative interface.", "A modern relational database system should follow the NoSQL model, but NoSQL standing here for Not only SQL. That means that the system provides a hybrid single environment for polyglot persistence, combining a tabular-relations model with other flexible-schema approaches, like key-value, document, columnar or graph stores. This involves managing additional query languages besides SQL, but also expanding it in order to support data types like XML (SQL:2006 revision), JSON (SQL:2016 revision) or RDF triples.", "The first advantage in providing XML and JSON native data types is the capability to use semi-structured sources for data mining model development and scoring.", "Data stored in XML columns can be retrieved by using XQuery, SQL or a combination of both. SQL/XML provides publishing functions that enable XQuery expressions to be executed within a SQL context, so procedures and applications expecting a relational view of the data can query within XML documents. In the same way, XML data can be grouped or aggregated, and XML documents can be joined with other documents or with relational data, supporting the definition of views on top of them. Specific parts of XML documents stored in an XML column can be indexed, improving the performance and efficiency of queries. Also, the SQL optimizer can exploit the statistics gathered over XML data and indexes, producing efficient execution plans when evaluating SQL, XQuery and SQL/XML functions that embed XQuery against XML and relational data.", "As a lightweight data exchange format, language independent and portable, JSON is an alternative to XML for semi-structured data. It eliminates the need for predetermined schema designs, is less verbose, and reduces the need for data transformation. As with XML, both a NoSQL (based on a JSON-oriented query language) and a SQL approach can be provided by the database. For SQL access, JSON documents can usually be stored in columns in either their original format or in binary-encoded format (BSON), which provides faster document traversal. SQL/JSON path expressions can be used to define access to the elements of a JSON document and publish them, for instance, in the form of a SQL view (Listing 2). Also, expression-based index can be created to help queries perform better.", "Besides enabling the processing of semi-structured data, the support of these data types provides excellent capabilities for model analysis and introspection. This is especially useful when using SQL-based visualization and reporting tools to expose the insights present in a PMML model (Listing 3), or when shredding its content in columns of relational tables (for instance, the statistics of the different partitions of a clustering model) as part of a data extraction and transformation job.", "In this sense, PMML models created or imported into the database can be stored directly in XML or as BLOB data type. In the first case, the model is stored in a table in its native hierarchical format, usually alongside other relational data. Depending on the database engine and on the size of the model, this can reside directly in the base table row if it is small (usually less than 32 KB), or in a separate storage object. If the model fits in the table page size, performance usually increases for read and write statements, because fewer I/O operations are required. In both situations, if the database provides data row compression, enabling it will improve I/O efficiency and reduce storage space requirements.", "In the case of a BLOB, the binary format also provides a reduced size due to model compression. It is also code-page independent, so it is possible to transfer the model to a database that is defined with a different code set. However, the PMML model is stored serialized, so this approach is recommended mainly for model archiving. In any case, the PMML content can always be retrieved by parsing the binary object at query time.", "In a similar way, PFA models can be stored and queried exploiting the JSON capabilities of the database. An interesting feature is model format conversion. Relational database engines that offer native XML types usually provide XLST support, so it is possible to implement and automate transformations on PMML models, including publishing to other formats, like JSON or HTML. Since PFA defines models by composing primitives from a function library, any PMML model can be transformed into PFA (the opposite is not always possible, especially for those model types not part of the PMML standard yet). In this case, the PMML model can be converted to PFA by applying an XLS style sheet in a single SELECT statement.", "A final word regarding the use of RDF data sets. Some database engines have the capability to support them on top of relational tables, containing data and metadata about the RDF store: triples, graphs, predicates, statistics, etc. Modeling relationships in the form of triples enables complex graphs, composed by nodes and edges, to be stored in the database and retrieved using RDF query languages, like SPARQL. This kind of representation, and the full set of analytic query operations provided by SPARQL, makes RDF an interesting format to store and analyze association rules and sequential patterns extracted by data mining models.", "Once a data mining model has been trained and tested, the next and most important step is to deploy it to a production environment, so it can be applied by scoring new business data. It is important to remark that the scoring environment, and the supporting database, can be different than the modeling one. It can also be analytical, operational, or even with mixed workload. In fact, predictive models are expected to support and enhance ongoing business operations, so an OLTP application environment will be the most common one. OLAP solutions will also benefit from the automation of scoring operations, so the results can be directly integrated in reports and dashboards.", "Operationalizing a data mining model entails some important considerations:", "Deploying data mining models in the database offers several scoring scenarios. The selection between one or another will be based on the real-time requirements of the application, but also on how volatile the data is and, therefore, how often the scores should be recalculated.", "Some database engines provide a reserved memory cache where models can be stored and executed faster: once the model is in the cache, the loading and parsing stages are already done, and can be skipped in any subsequent scoring tasks. This is especially useful in order to improve the performance of real-time scoring scenarios, where several model instances can serve different concurrent scoring requests.", "Scenarios 1 and 2 are more common in OLAP environments, while 3 and 4 are frequent in OLTP applications. Some precautions must be taken specially in the second case. OLTP applications are usually critical because they support business operations. Triggered-based scoring can impact the performance and throughput of the database system. The trigger is part of a SQL transaction, and that means that while the trigger is firing the transaction is not yet complete and database locks are still in place. Also, if the trigger fails for any reason, the complete transaction is rolled back. Triggers overload transactions; this impact must be considered, and database resources safeguarded through workload management.", "Data mining operationalization not only covers inference tasks, where validated models are deployed to production environments for scoring and continuous monitoring. It should address the complete assembly line, as it appears in the workflow in Figure 1. Although operationalization does not necessarily mean automation, the less intervention in the model factory, the more integration with the corporate information delivery processes.", "In fact, automation is feasible in some scenarios where supervised learning algorithms are involved, but with some unsupervised models as well. Descriptive techniques, such as association rules or sequential patterns discovery, fit very well with the idea of recurrent tasks that can be integrated and scheduled in data manipulation jobs. But this is also the case for some specific clustering model templates, where the focus is in describing rather than discovering.", "Retail industry, and specially grocery stores, provide good examples of this kind of data mining modeling integrations. Figure 6 represents an ELT (Extraction, Loading and Transformation) scenario with the three canonical sections: source systems, data staging area and presentation layer (data marts). The goal is to integrate here a segmentation & product affinity schema at three levels. Challenge is not just to extract and publish new information by modeling the daily basis data flow, but to feedback these insights into that flow and enrich existing business dimensions. This is how it works:", "This example can be easily extended to include additional techniques. For instance, time series modeling can be implemented to enhance product affinity analysis, with support, confidence and lift forecasting.", "This case represents very well how ELT, data mining and multidimensional modeling can be integrated in the same scenario. Moreover, it illustrates how advanced modeling techniques can be completely embedded in regular data flows: just their results are directly exposed to end users in the form of member attributes and business metrics.", "And at the foundation of this integration is the relational database. By leveraging a SQL API for data mining operations, it is possible to automate all the related tasks inside the ELT pipeline, without dependencies on external systems, and with all the benefits that the database server can provide in terms of scalability, reliability and data management capabilities.", "There should be an additional sixth reason to finalize this exposition, and this is quite simple: the relational database system is a valuable resource in the analyst toolbox to build and integrate data mining operations. And there are many business scenarios where this option should always be considered. It is a matter of bringing the algorithm where corporate data resides.", "In present times, where there is some obsession in renaming and reinventing every technology and discipline with more than five years, someone will soon say this is Edge Computing. We will see.", "[1] Vertica Systems, Machine Learning for Predictive Analytics, Vertica Analytics Platform Version 9.2.x Documentation", "[2] Teradata Corporation, Machine Learning Engine Analytic Function Reference, Teradata Vantage Documentation Release Number 1.1, 8.10", "[3] Pivotal Software, Inc., Machine Learning and Deep Learning using MADlib, Greenplum Database Version 6.4 Documentation", "[4] Oracle Corporation, Oracle Data Mining User\u2019s Guide, Oracle Database 19c Help Center", "[5] IBM Corporation, Analytic Stored Procedures, Db2 Warehouse 11.5 Knowledge Center", "[6] ISO/IEC JTC 1/SC 32 Data Management and Interchange, ISO/IEC 13249\u20136:2006 Information technology | Database languages | SQL multimedia and application packages | Part 6: Data mining, International Organization for Standardization", "[7] The Data Mining Group, PMML 4.4 General Structure, PMML Standard", "[8] The Data Mining Group, PFA 0.8.1 Specification, Portable Format for Analytics", "[9] Forina, M. et al, Wine Data Set, UCI Machine Learning Repository", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Consultant at Analyticae | Data Miner | PhD in Chemometrics & Multivariate Analysis | Opinions are my own"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F84e9cf8e084a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vfando?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vfando?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "V\u00edctor L. Fandi\u00f1o"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a4e3d1331b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&user=V%C3%ADctor+L.+Fandi%C3%B1o&userId=96a4e3d1331b&source=post_page-96a4e3d1331b----84e9cf8e084a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/db2-warehouse-data-mining", "anchor_text": "Db2 Warehouse Data Mining"}, {"url": "https://unsplash.com/@perotto?utm_source=medium&utm_medium=referral", "anchor_text": "Alexandre Perotto"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/AnalyzingData/MachineLearning/MachineLearning.htm", "anchor_text": "Machine Learning for Predictive Analytics"}, {"url": "https://docs.teradata.com/reader/Tk6W9zbo2NHGakhfyvWmmw/nt3Z~v5KgGV98ELQWJvQNg", "anchor_text": "Machine Learning Engine Analytic Function Reference"}, {"url": "http://docs.greenplum.org/6-4/analytics/madlib.html", "anchor_text": "Machine Learning and Deep Learning using MADlib"}, {"url": "https://docs.oracle.com/en/database/oracle/oracle-database/19/dmprg/index.html", "anchor_text": "Oracle Data Mining User\u2019s Guide"}, {"url": "https://www.ibm.com/support/knowledgecenter/SSCJDQ/com.ibm.swg.im.dashdb.analytics.doc/doc/r_analytic_stored_procedures.html", "anchor_text": "Analytic Stored Procedures"}, {"url": "https://www.iso.org/standard/38648.html", "anchor_text": "ISO/IEC 13249\u20136:2006 Information technology | Database languages | SQL multimedia and application packages | Part 6: Data mining"}, {"url": "http://dmg.org/pmml/v4-4/GeneralStructure.html", "anchor_text": "PMML 4.4 General Structure"}, {"url": "http://github.com/datamininggroup/pfa/releases/download/0.8.1/pfa-specification.pdf", "anchor_text": "PFA 0.8.1 Specification"}, {"url": "http://archive.ics.uci.edu/ml/datasets/Wine", "anchor_text": "Wine Data Set"}, {"url": "https://medium.com/tag/data-mining?source=post_page-----84e9cf8e084a---------------data_mining-----------------", "anchor_text": "Data Mining"}, {"url": "https://medium.com/tag/database?source=post_page-----84e9cf8e084a---------------database-----------------", "anchor_text": "Database"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----84e9cf8e084a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/business-analytics?source=post_page-----84e9cf8e084a---------------business_analytics-----------------", "anchor_text": "Business Analytics"}, {"url": "https://medium.com/tag/db2-warehouse-data-mining?source=post_page-----84e9cf8e084a---------------db2_warehouse_data_mining-----------------", "anchor_text": "Db2 Warehouse Data Mining"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&user=V%C3%ADctor+L.+Fandi%C3%B1o&userId=96a4e3d1331b&source=-----84e9cf8e084a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&user=V%C3%ADctor+L.+Fandi%C3%B1o&userId=96a4e3d1331b&source=-----84e9cf8e084a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F84e9cf8e084a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----84e9cf8e084a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----84e9cf8e084a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vfando?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vfando?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "V\u00edctor L. Fandi\u00f1o"}, {"url": "https://medium.com/@vfando/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a4e3d1331b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&user=V%C3%ADctor+L.+Fandi%C3%B1o&userId=96a4e3d1331b&source=post_page-96a4e3d1331b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F96a4e3d1331b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffive-reasons-for-in-database-data-mining-84e9cf8e084a&user=V%C3%ADctor+L.+Fandi%C3%B1o&userId=96a4e3d1331b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}