{"url": "https://towardsdatascience.com/word-embedding-using-bert-in-python-dd5a86c00342", "time": 1683002104.071184, "path": "towardsdatascience.com/word-embedding-using-bert-in-python-dd5a86c00342/", "webpage": {"metadata": {"title": "Word Embedding Using BERT In Python | by Anirudh S | Towards Data Science", "h1": "Word Embedding Using BERT In Python", "description": "Words have different meanings in different contexts. With deep learning NLP models, we can capture semantic meanings of words like we humans do!"}, "outgoing_paragraph_urls": [{"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "Google understands search queries better using BERT.", "paragraph_index": 0}, {"url": "https://github.com/google-research/bert#pre-trained-models", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://hackerstreak.com/yolo-made-simple-interpreting-the-you-only-look-once-paper/", "anchor_text": "Object detection", "paragraph_index": 13}, {"url": "https://hackerstreak.com/siamese-neural-network-for-signature-verification/", "anchor_text": "authenticity verification", "paragraph_index": 13}, {"url": "https://hackerstreak.com/posts/", "anchor_text": "more", "paragraph_index": 13}], "all_paragraphs": ["In the world of NLP, representing words or sentences in a vector form or word embedding opens up the gates to various potential applications. This functionality of encoding words into vectors is a powerful tool for NLP tasks such as calculating semantic similarity between words with which one can build a semantic search engine. For example, here\u2019s an application of word embedding with which Google understands search queries better using BERT. Arguably, it\u2019s one of the most powerful language models that became hugely popular among machine learning communities.", "BERT (Bidirectional Encoder Representations from Transformers) models were pre-trained using a large corpus of sentences. In brief, the training is done by masking a few words (~15% of the words according to the authors of the paper) in a sentence and tasking the model to predict the masked words. And as the model trains to predict, it learns to produce a powerful internal representation of words as word embedding. Today, we\u2019ll see how to get the BERT model up and running with little to no hassle and encode words into word embedding.", "There\u2019s a suite of available options to run BERT model with Pytorch and Tensorflow. But to make it super easy for you to get your hands on BERT models, we\u2019ll go with a Python library that\u2019ll help us set it up in no time!", "Bert-as-a-service is a Python library that enables us to deploy pre-trained BERT models in our local machine and run inference. It can be used to serve any of the released model types and even the models fine-tuned on specific downstream tasks. Also, it requires Tensorflow in the back-end to work with the pre-trained models. So, we\u2019ll go ahead and install Tensorflow 1.15 in the console.", "Up next, we\u2019ll install bert-as-a-service client and server. And again, this library doesn\u2019t support Python 2. So, make sure that you have Python 3.5 or higher.", "The BERT server deploys the model in the local machine and the client can subscribe to it. Moreover, one can install the two in the same machine or deploy the server in one and subscribe from another machine. Once the installation is complete, download the BERT model of your choice. And you can find the list of all models over here.", "Now that the initial setup is done, let\u2019s start the model service with the following command.", "The \u201cnum_workers\u201d argument is to initialize the number of concurrent requests the server can handle. However, just go with num_workers=1 as we\u2019re just playing with our model with a single client. If you\u2019re deploying for multiple clients to subscribe, choose the \u201cnum_workers\u201d argument accordingly.", "We can run a Python script from which we use the BERT service to encode our words into word embedding. Given that, we just have to import the BERT-client library and create an instance of the client class. Once we do that, we can feed the list of words or sentences that we want to encode.", "We should feed the words that we want to encode as Python list. Above, I fed three lists, each having a single word. Therefore, the \u201cvectors\u201d object would be of shape (3,embedding_size). In general, embedding size is the length of the word vector that the BERT model encodes. Indeed, it encodes words of any length into a constant length vector. But this may differ between the different BERT models.", "Okay, so far so good! What to do with the vectors which are just some numbers? Well, they\u2019re more than just numbers. As I said earlier, these vectors represent where the words are encoded in the 1024-dimensional hyperspace (1024 for this model uncased_L-24_H-1024_A-16). Moreover, comparing the vectors of different words with some sort of similarity function would help determine how close they are related.", "Cosine similarity is one such function that gives a similarity score between 0.0 and 1.0. Provided that, 1.0 means that the words mean the same (100% match) and 0 means that they\u2019re completely dissimilar. Here\u2019s a scikit-learn implementation of cosine similarity between word embedding.", "You can also feed an entire sentence rather than individual words and the server will take care of it. There are multiple ways in which word embeddings can be combined to form embedding for sentences like concatenation.", "Check out the other articles on Object detection, authenticity verification and more!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Always Believing there's more to learn!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd5a86c00342&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860----dd5a86c00342---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "Google understands search queries better using BERT."}, {"url": "https://github.com/google-research/bert#pre-trained-models", "anchor_text": "here"}, {"url": "https://hackerstreak.com/yolo-made-simple-interpreting-the-you-only-look-once-paper/", "anchor_text": "Object detection"}, {"url": "https://hackerstreak.com/siamese-neural-network-for-signature-verification/", "anchor_text": "authenticity verification"}, {"url": "https://hackerstreak.com/posts/", "anchor_text": "more"}, {"url": "https://hackerstreak.com/", "anchor_text": "https://hackerstreak.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd5a86c00342---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bert?source=post_page-----dd5a86c00342---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/nlp?source=post_page-----dd5a86c00342---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dd5a86c00342---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----dd5a86c00342---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&user=Anirudh+S&userId=1a92cae35860&source=-----dd5a86c00342---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&user=Anirudh+S&userId=1a92cae35860&source=-----dd5a86c00342---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd5a86c00342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dd5a86c00342---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd5a86c00342--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dd5a86c00342--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dd5a86c00342--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/@baakchsu.sprx77/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "127 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff34e096e7feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-using-bert-in-python-dd5a86c00342&newsletterV3=1a92cae35860&newsletterV3Id=f34e096e7feb&user=Anirudh+S&userId=1a92cae35860&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}