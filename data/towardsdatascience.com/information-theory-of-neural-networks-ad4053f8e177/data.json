{"url": "https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177", "time": 1682993306.430308, "path": "towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177/", "webpage": {"metadata": {"title": "Information Theory of Neural Networks | by Mukul Malik | Towards Data Science", "h1": "Information Theory of Neural Networks", "description": "Aim of this blog is not to understand the underlying mathematical concepts behind Neural Network but to visualise Neural Networks in terms of information manipulation. Originally, a concept of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "Encoder is simply compresses the information and decoder expands the encoded information.", "paragraph_index": 3}, {"url": "https://brilliant.org/wiki/markov-chains/", "anchor_text": "Markov Chain", "paragraph_index": 17}, {"url": "http://colah.github.io/posts/2014-10-Visualizing-MNIST/", "anchor_text": "blog", "paragraph_index": 39}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "blog", "paragraph_index": 39}, {"url": "https://www.youtube.com/watch?v=bLqJHjXihK8&t=856s", "anchor_text": "video", "paragraph_index": 42}], "all_paragraphs": ["\u201cInformation: the negative reciprocal value of probability.\u201d \u2014 Claude Shannon", "Aim of this blog is not to understand the underlying mathematical concepts behind Neural Network but to visualise Neural Networks in terms of information manipulation.", "Encoder-Decoder is not two CNNs/RNNs combined together! Neither have to be neural network in fact!", "Originally, a concept of information theory. Encoder is simply compresses the information and decoder expands the encoded information.", "In case of machine learning, both encoding and decoding are both lose-full processes i.e. some information is always lost.", "The encoded output of the encoder is called the context vector and this is the input for the decoder.", "There are two ways to setup an encoder-decoder setting:", "Auto-Encoder: Encoder compressing English text into a vector. Decoder generating original English text from the vector.", "Encoder-Decoder: Encoder compressing English text into a vector. Decoder generating French translation of the original text from the vector.", "Encoder-Decoder: Encoder compressing English text into a vector. Decoder generating image from the content of text.", "Now, if I say every neural network, itself, is an encoder-decoder setting; it would sound absurd to most.", "Let input layer be X and their real tags/classes (present in the training set) be Y. Now we already know Neural Networks find the underlying function between X and Y.", "So X can be seen as High-Entropy distribution of Y. High entropy because X contains the information Y but it also a lot of other information.", "\u201cThis boy is good.\u201d contains enough information to tell us about it\u2019s \u2018positive\u2019 sentiment.", "3. Tense of sentence is present", "Now no-entropy version of this sentence would be \u2018positive\u2019. Yes that\u2019s also the output. We\u2019ll come back to that in a while.", "Now imagine every hidden layer as a single variable H ( so layers will be named as H0, H1 \u2026.. H(n-1) )", "Now every layer becomes a variable and the neural network becomes a Markov Chain. Markov Chain because each variable is only dependent upon the previous layer only.", "So essentially each layer forms a partisan of information.", "Following is a visualisation of a Neural Network as a Markov Chain.", "The last layer Y_ is supposed to result in least entropy output (with respect to \u2018Y\u2019, the original tag/class).", "This process of obtaining Y_ is squeezing the information in X layer as it flows through H layers and retaining only the information most relevant to Y. This is the information bottleneck.", "H(X|Y) -> Conditional Entropy of X given Y", "In other words, H(X|Y) signifies how much uncertainty is removed form X if Y is known.", "In the Markov Representation of Neural Network, every layer becomes a partition of information.", "In Information Theory these partitions are known as Successive Refinement of Relevant Information. You don\u2019t have to worry about the details.", "Another way of seeing this is the input being encoded and decoded into the output.", "Sample Complexity is the number and variety of examples one needs to receive certain accuracy.", "Initially,weights are randomly initialised. So barely anything is known about the correct output. With successive layers the mutual information about input decreases and the information in hidden layers about the output is low as well.", "As we train the Neural Network the plots start moving up, signifying gain of information about the output.", "Plots also start shifting towards the right side, signifying increase of information in latter layers about the input.", "This his the longest phase. Here the density of the plots is maximum and plots are concentrated at the top right. This signifies compression of information about the input in relation to the output.", "This is called the compression phase.", "After the compression phase, the plots starts shifting towards the top but also to left side.", "This signifies, with successive layers, there is loss of information about the input and what\u2019s retained in the last layer is the lowest entropy information about the output.", "The Markov Chain version of Neural Network highlights one more point, learning happens from layer to layer. A layer has all the information it needs to predict the output (plus some noise).", "So we use each layer to predict the output. This helps us peep into the layer-wise knowledge of the so called black box.", "This gives us a perspective about how many layers are required to make an accurate enough prediction of output. If saturation is achieved at earlier layers, the layers succeeding this layer can be pruned/dropped.", "These layers are usually very of hundreds or thousands of dimensions. Our evolution doesn\u2019t allow us to visualise anything beyond the 3-dimensions. So we use dimension reduction techniques.", "There are various methods of performing dimension reduction. Cristopher Olah has a brilliant blog explaining those methods. I won\u2019t go into the details of t-SNE, you can check this blog for details.", "To keep it short, t-SNE tries to reduce dimension retaining the neighbours from higher dimensions in lower dimensions. So this results in pretty accurate 2D and 3D plots.", "Following are the layer plots of a language model with 2 layers.", "I have excluded mathematics from this blog. If you are comfortable enough with the mathematics of Information Theory, Game Theory, Learning Theory etc then do watch this video of the Mastero Naftali Tishby.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Manager Data Science at PublicisSapient (NLP & Deep Learning Specialist)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fad4053f8e177&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mukulmalik?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Mukul Malik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbd3e39707a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&user=Mukul+Malik&userId=bd3e39707a79&source=post_page-bd3e39707a79----ad4053f8e177---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.cs.toronto.edu/~hinton/science.pdf", "anchor_text": "Encoder is simply compresses the information and decoder expands the encoded information."}, {"url": "https://brilliant.org/wiki/markov-chains/", "anchor_text": "Markov Chain"}, {"url": "http://colah.github.io/posts/2014-10-Visualizing-MNIST/", "anchor_text": "blog"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "blog"}, {"url": "https://www.youtube.com/watch?v=bLqJHjXihK8&t=856s", "anchor_text": "video"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ad4053f8e177---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----ad4053f8e177---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ad4053f8e177---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ad4053f8e177---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&user=Mukul+Malik&userId=bd3e39707a79&source=-----ad4053f8e177---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&user=Mukul+Malik&userId=bd3e39707a79&source=-----ad4053f8e177---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fad4053f8e177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ad4053f8e177---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ad4053f8e177--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ad4053f8e177--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ad4053f8e177--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mukulmalik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mukul Malik"}, {"url": "https://medium.com/@mukulmalik/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "656 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbd3e39707a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&user=Mukul+Malik&userId=bd3e39707a79&source=post_page-bd3e39707a79--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3e3c8532f5d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-theory-of-neural-networks-ad4053f8e177&newsletterV3=bd3e39707a79&newsletterV3Id=3e3c8532f5d8&user=Mukul+Malik&userId=bd3e39707a79&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}