{"url": "https://towardsdatascience.com/unpacking-pca-b5ea8bec6aa5", "time": 1682994216.437106, "path": "towardsdatascience.com/unpacking-pca-b5ea8bec6aa5/", "webpage": {"metadata": {"title": "Unpacking (** PCA ). A practical implementation of\u2026 | by Ryota Bannai | Towards Data Science", "h1": "Unpacking (** PCA )", "description": "Despite the fact ample of resources for PCA(Principal component analysis) on the internet, it is intractable to grab whole idea within few hours, let alone implementing that into codes with knowing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/ryota-bannai", "anchor_text": "https://medium.com/ryota-bannai", "paragraph_index": 56}], "all_paragraphs": ["Despite of the accessibility of ample resources on PCA(Principal component analysis) on the internet, it can be done only by Ninja to grab whole ideas within a certain, reasonable amount of time, let alone materialize the ideas into codes at the same time you\u2019re knowing what is actually going on. Having said that, we would very much like the feelings where we\u2019re applying PCA and solving real case scenarios like professionals do.", "In this article, I\u2019ll take a stab at and clearly illustrate the ideas one might find puzzling, which, I assume, that a newbie like me will fall for and stuck in for hours.", "Without further ado, let\u2019s get right into it.", "First off, PCA is a method for dimensionality reduction, more simply, the method for summarizing multiple features(variables) into fewer features. Suppose, we need to classify a new seen animal into \u2018a dog\u2019 or \u2018a cat\u2019, we would measure the animal\u2019s features, such as \u2018height\u2019 and \u2018weight\u2019. We might also classify the unseen animal based on more features(\u2018color\u2019, \u2018shape\u2019, \u2018agility\u2019\u2026 etc.)", "However, the problem is that we cannot classify them more than three features(dimensions), which is visually uninterpretable for us. And another one is that some of features are less important to use, or worse, some can be a noise if they\u2019re random data or an error, either of which certainly disturbs our prediction.", "To be simply, a feature \u2018weight\u2019 might not effective information to separate \u2018a dog\u2019 from \u2018a cat\u2019, therefore we should get rid of \u2018weight\u2019 from our data. And by getting rid of most of unimportant features, we can save computational costs. Imagine that our data have 100 x 100 features, like images or genetic information, it\u2019ll be so much better to reduce dimensions as less as possible.", "We can achieve these desires by using PCA.", "It might be better to implement to get a whole idea. Let\u2019s start by making 5 *10 matrix, and take steps of the process.", "The column are variables (features) and the row are samples(say, \u2018cat\u2019 or \u2018dog\u2019).", "What we want to do with this matrix is to get eigenvalues and eigenvectors, which turns to be new variables(Principal Components) to describe samples. Let matrix X to be n *p size, then p *p covariance matrix C, where C=X\u1d40X/n. Since covariance matrix is hermitian matrix and semi-positive definite matrix, by using spectral theorem, we can get eigenvalues(L: a diagonal matrix with eigenvalues \u03bb\u1d62) and eigenvectors(V) from C=VLV\u1d40. Let\u2019s get a covariance matrix and eigenvalues. But before that, don\u2019t forget to subtract means of each column from the same column.", "We set \u2018rowvar = False\u2019, so that the relationship is transposed: each column represents a variable, while the rows contain observations. Then, we get eigenvalues in w and eigenvectors in v.", "At this point, the eigenvectors is 10 *10 matrix. To perform dimensionality reduction from 10 variables to 2 variables, we need to truncate the matrix.", "Then, we want to transform( project X on the principal axes) the original 10 *10 sample into new space (2 dimensions), we use the equation T=XV.", "We got matrix 5 *2 matrix T, which is projected onto 2 principal components. We finally describe each sample with 2 variables.", "Fun part should be visualizing these transformed sample data. Let\u2019s use bigger data. Starting off by defining 200 *2 matrix X.", "(In this case, eigenvalues are two, therefore we actually don\u2019t need to truncate it into two)", "Finally plot. Let\u2019s not forget to set axis \u2018equal\u2019 , so that we can see an orthogonality of principal components. Otherwise, we will get a non perpendicular eigenvectors.", "Let\u2019s plot transformed samples as well. Because our samples are already 2 dimensional, I have no need to truncate them, but, let\u2019s take steps of dimensionality reduction process.", "As we can see, the two principal components are orthogonal, since they are decorrelated by PCA so that variables are independent each other.", "Also sample data is transformed ( uncorrelated/ rotated ) along with two principal axes.", "To better understand the term \u2018uncorrelated/ rotated\u2019, let\u2019s take a look at the Fig. 1, and compare with Fig. 2. The scattered data is actually exactly the same data, however, we decorrelate(rotate) them so that we make the two variables(axes) independent each other(Fig. 2).", "Why do we use Eigendecomposition(or Singular value decomposition for efficiency), and use some of the largest Eigenvalues and corresponding eigenvectors? This is because the purpose of PCA is to reduce the dimensionality, but at the same time, to describe the samples as much as possible with fewer variables. In other words, we want the variable which varies among each samples, and don\u2019t want the variable which is the same among the samples. We can accomplish this idea by using larger eigenvalue, or larger variance. Based on the information theoretic point of view, the largest variance corresponds to the dimension with largest entropy, therefore, largest variance encodes most information. However, Eigendecomposition is costy. Suppose, we have image samples, say 100 *100 pixels, and that means it has 10,000 variables. Its covariance matrix C will be 10,000 *10,000 dimensions. Therefore, we often prefer SVD(Singular value decomposition) to reduce the dimension size as small as a sample size rather than a variable size. Let\u2019s give it a try, and plot sample with SVD.", "SVD decomposes covariance matrix C, where C=XX\u1d40/n, into C=USV\u1d40, where S is a (rectangle) diagonal matrix containing singular values s\u1d62, U is a unitary matrix, and V are principal directions. In terms of a singular value s\u1d62, we can use \u03bb\u1d62 = (s\u1d62**2)/n, where n is the sample size, \u03bb\u1d62 are Eigenvalues.", "From this results, we see exactly the same principal components by means of SVD and Eigendecomposition.", "Let\u2019s also make sure that Eigenvectors are orthogonal each other. An orthogonal matrix Q is defined as Q\u1d40Q=QQ\u1d40=I, where I is an identity matrix. We got eigenvectors V\u1d40 from Singular value decomposition, V\u1d40 is now like below.", "The result of V.dot(V.T) is an identity matrix.", "In a lot of theories of mathematical concepts, such as Partial Least Squares, the idea of an orthogonality of vectors often shows up. Checking it by this way is useful to make sure that we are on the right track.", "One might like to observe how far each point from the center. As one of ways to do is to draw an equiprobability, or an error eclipse. This is especially helpful to see the sample which is in the middle of two groups, for instance, we can use this for K-means clustering.", "We start with import modules. Then, we want to decompose the covariance matrix of an original sample with SVD.", "The unitary matrix U_ is used for rotation the sample data, now we use U_[0] and U_[1], cosine, sine, respectively, so that we can get tangent for calculating the angle.", "The angle is from x axis. When axes are defined by the standard deviation, the eclipse is defined by the equation, (x/\u03c3\u2081)\u00b2+(y/\u03c3\u2082)\u00b2=s, where \u03c3\u2081 is a standard deviation of x axis, \u03c3\u2082 is a standard deviation of y axis, and s is a variance of the sample.", "x=2\u03c3\u2081\u221as means one standard deviation. We plot three times in the code above. The third circle contains most of the sample data on the image, since it supposed to be a 99.7% of confidence interval.", "We can use Scikit-learn for PCA. Let\u2019s run through the same sample one more time. Staring with sample data as usual.", "Defining the 200 *2 matrix X, we use two components for the plot\u2019s sake.", "The rest of process is the same as before.", "The result is the same. As we can see from the codes, we can access the largest eigenvectors via components_ and eigenvalues via explained_variance_.", "Let\u2019s take a look at the explained_variance_", "Major principal component explains 97.6% [0.7625315/(0.7625315+0.0184779)] of sample data and second principal component does the rest. That means that we can almost describe original data without second principal component. Scikit-learn already calculates the explained variance radio, so we can use via explained_variance_ratio_.", "For better understandings, let\u2019s use 1797 *64 matrix digits data, and reduce those dimensions 64 to 10.", "Now we can see how much these new 10 components can describe original sample data.", "As it turns out, it described 72~73% of sample instead of using 64 dimensions that ensures 100% of accuracy. Notice that in the left graph the first component is index 0, that\u2019s why the graph begins from 14~15% of variance.", "Suppose how many of components should be needed to get 90% of accuracy then? We can blank the PCA function and to plot the graph first.", "We would interpret around 20 components should be good enough for 90% of variance. The simpler way for this is to add the digit into the function directly, such as, pca = PCA(.9)", "For further understandings, let\u2019s inverse the transformed sample data into original data, and apply heatmap to them. We should observe that the more the number of components is, the more precise it creates sample data.", "Clearly the original has a lot more features than inversed data with two principal components. Using forty principal components, we can more precisely inverse the transformed data.", "We actually can show the digits data we are dealing with so far.", "Let\u2019s make these digits images rougher, or more precisely, once perform dimensionality reduction and inverse the digits image to the original size.", "We can also apply PCA to Eigenfaces, since its feature is only brightness like digits image above. In terms of reconstructing the original data, we take a slightly different step from digits reconstruction. Since we need an average face first. After inverting transformed data, we add the average face. Suppose we want an inversed face image X, X is described as X=\u03bc+w\u2081*v\u2081+w\u2082*v\u2082+\u2026+w\u1d62*v\u1d62, where \u03bc is a mean, (w\u2081, w\u2082,\u2026,w\u1d62|w\u1d62\u2208 US) are principal components, and (v\u2081, v\u2082,\u2026,v\u1d62| v\u1d62\u2208 V) are Eigenvectors.", "As we discussed before, SVD well performs when the dimensions are vast. For instance, these image is 62 *47 = 2914 dimensions. We need to compute the 2914 *2914 dimensions of the correlation matrix if we use Eigendecomopsition. Instead, SVD can calculation by sample size * sample size.", "Principal components are given by XV=US. We reduce the dimensions from 500 to 100, now US is a 500 *100 matrix.", "To compare with original face images, we want to project US, and to inverse it to X. We apply the equation X=USV\u1d40, and get 500 * 2914 matrix X.", "The resulting images are not as blur as the prior scikit-learn method. Seeming to be better method, the fewer data we use, the clearer images are constructed. This is because less noises from samples, and the machine uses less filters. This will lead itself to predict low for new images, which the machine has never seen before.", "Eigenvectors that composes these faces are called Eigenfaces, each Eigenface is unique. Let\u2019s check out how they look like, and how the average face of these face images does.", "The average face is a foundation of face images.", "To end, here is the overview of SVD\u2019s process for Eigenfaces.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m a learner of English as a second language, who\u2019s obsessed with informative articles. I always like to learn new things. https://medium.com/ryota-bannai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb5ea8bec6aa5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ryotabannai?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryotabannai?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Ryota Bannai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0649b684033&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&user=Ryota+Bannai&userId=e0649b684033&source=post_page-e0649b684033----b5ea8bec6aa5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca/", "anchor_text": "Relationship between SVD and PCA. How to use SVD to perform PCA?Principal component analysis (PCA) is usually explained via an eigen-decomposition of the covariance matrix. However\u2026stats.stackexchange.com"}, {"url": "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues", "anchor_text": "Making sense of principal component analysis, eigenvectors & eigenvaluesIn today\u2019s pattern recognition class my professor talked about PCA, eigenvectors &amp; eigenvalues. I got the\u2026stats.stackexchange.com"}, {"url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c", "anchor_text": "A One-Stop Shop for Principal Component AnalysisAt the beginning of the textbook I used for my graduate stat theory class, the authors (George Casella and Roger\u2026towardsdatascience.com"}, {"url": "http://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/", "anchor_text": "How to draw an error ellipse representing the covariance matrix?In this post, I will show how to draw an error ellipse, a.k.a. confidence ellipse, for 2D normally distributed data\u2026www.visiondummy.com"}, {"url": "http://jotterbach.github.io/content/posts/pca/2016-03-24-Principal_Component_Analysis/", "anchor_text": "Principal Component Analysis (PCA) for Feature Selection and some of its PitfallsA typical approach in Data Science is what I call featurization of the Universe. What I mean by that is that we extract\u2026jotterbach.github.io"}, {"url": "http://www.scholarpedia.org/article/Eigenfaces#fig:PIE.jpg", "anchor_text": "Eigenfaces \u2014 ScholarpediaThe eigenfaces may be considered as a set of features which characterize the global variation among face images. Then\u2026www.scholarpedia.org"}, {"url": "http://www.visiondummy.com/2014/05/feature-extraction-using-pca/", "anchor_text": "Feature extraction using PCA - Computer vision for dummiesIn this article, we discuss how Principal Component Analysis (PCA) works, and how it can be used as a dimensionality\u2026www.visiondummy.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b5ea8bec6aa5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/cpa?source=post_page-----b5ea8bec6aa5---------------cpa-----------------", "anchor_text": "CPA"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----b5ea8bec6aa5---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/face-recognition?source=post_page-----b5ea8bec6aa5---------------face_recognition-----------------", "anchor_text": "Face Recognition"}, {"url": "https://medium.com/tag/python?source=post_page-----b5ea8bec6aa5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&user=Ryota+Bannai&userId=e0649b684033&source=-----b5ea8bec6aa5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&user=Ryota+Bannai&userId=e0649b684033&source=-----b5ea8bec6aa5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb5ea8bec6aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b5ea8bec6aa5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b5ea8bec6aa5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryotabannai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryotabannai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ryota Bannai"}, {"url": "https://medium.com/@ryotabannai/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "101 Followers"}, {"url": "https://medium.com/ryota-bannai", "anchor_text": "https://medium.com/ryota-bannai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0649b684033&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&user=Ryota+Bannai&userId=e0649b684033&source=post_page-e0649b684033--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe0649b684033%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funpacking-pca-b5ea8bec6aa5&user=Ryota+Bannai&userId=e0649b684033&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}