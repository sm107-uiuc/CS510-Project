{"url": "https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b", "time": 1683005675.1298602, "path": "towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b/", "webpage": {"metadata": {"title": "Ten More AI Papers to Read in 2020 | by Ygor Serpa | Towards Data Science", "h1": "Ten More AI Papers to Read in 2020", "description": "A couple of weeks ago, I released an article with artificial intelligence (AI) papers to read in 2020. If you count all the additional reading suggestions, it sums up to 27 articles. Yet, the list is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "artificial intelligence (AI) papers to read in 2020", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "previous list", "paragraph_index": 2}, {"url": "https://www.aclweb.org/anthology/D14-1162.pdf", "anchor_text": "\u201cGlove: Global vectors for word representation.\u201d", "paragraph_index": 3}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "Transformers", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2Vec", "paragraph_index": 8}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716/", "anchor_text": "biology community", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 8}, {"url": "https://www.sciencedirect.com/science/article/pii/S002200009791504X", "anchor_text": "A decision-theoretic generalization of on-line learning and an application to boosting", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Random_forest", "anchor_text": "Random Forest", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "Gradient Boosting", "paragraph_index": 15}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 15}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "LightGBM", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1710.09829", "anchor_text": "\u201cDynamic routing between capsules.\u201d", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Part-based_models", "anchor_text": "object parts", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Constellation_model", "anchor_text": "databases/ontologies", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention mechanism", "paragraph_index": 22}, {"url": "https://medium.com/u/b60b4c7e3bfc?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Sergei Ivanov", "paragraph_index": 29}, {"url": "https://towardsdatascience.com/top-trends-of-graph-machine-learning-in-2020-1194175351a3", "anchor_text": "new trends in GNNs", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/2003.00152", "anchor_text": "\u201cTraining BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs.\u201d", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "Recent evidence shows this is not the case", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "Lottery Ticket Hypothesis", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1701.07875", "anchor_text": "Wasserstein loss", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Lipschitz_continuity", "anchor_text": "1-Lipschitz", "paragraph_index": 36}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "careful activation function design", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Dropout_(neural_networks)", "anchor_text": "dropout", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1803.08494", "anchor_text": "Group Normalization", "paragraph_index": 39}, {"url": "http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html", "anchor_text": "Adaptive Instance Normalization", "paragraph_index": 39}, {"url": "https://arxiv.org/abs/1603.08155", "anchor_text": "\u201cPerceptual losses for real-time style transfer and super-resolution.\u201d", "paragraph_index": 40}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html", "anchor_text": "topological loss paper", "paragraph_index": 46}, {"url": "https://en.wikipedia.org/wiki/Neural_architecture_search", "anchor_text": "Neural Architecture Search (NAS)", "paragraph_index": 46}, {"url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "anchor_text": "comprehensive guide", "paragraph_index": 46}, {"url": "https://medium.com/u/dc830cac0dcb?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Sowmya Yellapragada", "paragraph_index": 46}, {"url": "https://arxiv.org/abs/1908.03265v1", "anchor_text": "Radam", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1907.08610", "anchor_text": "Lookahead", "paragraph_index": 50}, {"url": "https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer", "anchor_text": "Ranger", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1812.01187", "anchor_text": "Bag of Tricks for Image Classification", "paragraph_index": 56}, {"url": "https://github.com/fchollet/ARC", "anchor_text": "Abstraction and Reasoning Corpus (ARC)", "paragraph_index": 60}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "Youtube", "paragraph_index": 65}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me", "paragraph_index": 72}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing", "paragraph_index": 72}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow", "paragraph_index": 72}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up.", "paragraph_index": 72}, {"url": "https://www.buymeacoffee.com/ygorreboucas", "anchor_text": "buying me a cup of coffee", "paragraph_index": 72}], "all_paragraphs": ["A couple of weeks ago, I released an article with artificial intelligence (AI) papers to read in 2020. If you count all the additional reading suggestions, it sums up to 27 articles. Yet, the list is far from complete. Many gems were left out or were only briefly mentioned. In this article, for your reading pleasure, I am listing ten more suggestions (and several other further reading recommendations) of AI papers to read this year.", "In this list, I focus on articles that are pushing the state-of-the-art without proposing new architectures. This won\u2019t be about the most recent YOLO or ResNet variants. Instead, it highlights recent advancements in loss formulations, theoretical breakthroughs, updated optimizers, and so on.", "As for the previous list, I will focus on computer vision and NLP, as these are the topics I am most familiar with and start with one or two classics. For each paper, I give a summary of its key contributions and a list of reasons for reading it. Finally, I finish each article with specific reading suggestions on the topic, linking it to other recent advancements or similar ideas.", "Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. \u201cGlove: Global vectors for word representation.\u201d Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.", "While today\u2019s community focuses heavily on neural networks, much of early results were obtained by much simpler mathematics. Starting this list with a classic algorithm, GloVe is a word embedding model based on reducing the dimensionality of the word co-occurrence matrix. Unlike previous approaches, GloVe uses an implicit formulation that allows it to scale for massive text corpora.", "Reason #1: If you are starting with Natural Language Processing (NLP), this is a great read to get the basics of word embeddings and why they matter.", "Reason #2: Once upon a time, not everything was based on Transformers. Reading earlier works is an excellent way of finding that one \u201cforgotten idea\u201d that could push the state-of-the-art a bit further.", "Reason #3: Many of the concepts mentioned/proposed in this paper were extended later on by many other authors. Word embeddings are today a staple in the Natural Language Processing (NLP) literature.", "Further Reading: Around the same period, Google released Word2Vec, another well-known model for generating semantic vectors. Soon after, these ideas were adopted by the biology community as ways to signify large protein and gene sequences. Today, BERT is the dominating figure for word representation and semantic understanding.", "Freund, Yoav; Schapire, Robert E (1997). \u201cA decision-theoretic generalization of on-line learning and an application to boosting\u201d.", "Classic machine learning models were anything but flexible. Most formulations had striking limitations, which prevented them from scaling to increasingly more complex tasks. One of the first solutions to this problem was to ensemble the best available models into democratic voting. In 1997, Freund and Schapire proposed the AdaBoost algorithm, a meta-heuristic learner capable of leveraging many \u201cweak\u201d models into a \u201cstrong\u201d classifier.", "Simply put, the algorithm is based on iteratively training more classifiers and re-weighting each training sample as \u201ceasy\u201d or \u201chard.\u201d As the training goes, the ensemble evolves by focusing more on harder to classify samples. The algorithm is so effective that it is prone to overfitting even complex problems.", "Reason #1: It can be argued that neural networks are an ensemble of weak classifiers (neurons/layers). However, the neural network literature has evolved independently of ensembles. Reading a paper on the topic might yield a few insights on why neural networks work so well.", "Reason #2: Many novices treat classic machine learning approaches as old fashioned and \u201cweak,\u201d favoring neural networks for pretty much everything. AdaBoost is an excellent example that classic machine learning is anything but weak. And, unlike networks, these models are highly interpretable.", "Reason #3: How many papers start by considering the story of a gambler that is frustrated by repeatedly losing horse-riding gambles against his friends? I wish I dared to start a paper like that.", "Further Reading: Other popular ensemble methods are the Random Forest classifier, the Gradient Boosting technique, and the acclaimed XGBoost package, renowned for winning several machine learning competitions while being relatively easy to use and tune. The most recent addition to the family is Microsoft\u2019s LightGBM, geared towards massively distributed datasets.", "Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. \u201cDynamic routing between capsules.\u201d Advances in neural information processing systems. 2017.", "The neural network literature started with the perceptron model and arrived at convolutional neural networks (CNNs). The next big leap is a much-debated topic. One of such proposals is the Capsule Network, proposed by Sara Sabour, Nicholas Frosst, and the Turing Award winner Geoffrey Hinton.", "An easy way to understand capsule networks is to replace the word \u201ccapsule\u201d for \u201cobject detector.\u201d Each layer of \u201cobject detectors\u201d tries to identify relevant features in the image, along with its pose (orientation, scale, skew, etc.). By stacking detectors, robust representations of objects can be derived. In essence, capsules are not aggregating local information towards high-level features, as CNNs do. Instead, they are detecting object parts and composing them hierarchically to identify larger structures and relationships.", "Reason #1: As scientists, we should all be on the look for the next big thing. While we can\u2019t say Capsule Networks will be the next rock star, we can tell the problem they try to solve is relevant. And, as for all relevant issues, someone will answer it eventually.", "Reason #2: This paper reminds us that CNNs are not perfect. They are not invariant to rotations and scaling. Although we use data augmentation to alleviate this, no band-aid has ever healed a man.", "Reason #3: Before deep learning becoming mainstream, many object detection methods relied on identifying easy to spot \u201cobject parts\u201d and performing pattern matching against databases/ontologies. What Hinton and his team are doing is modernizing this early approach. That\u2019s why we should all read classics from time to time. Many things could be updated.", "Further Reading: Something that is getting a lot of traction in the past year is the Attention mechanism. While it does not tries to replace or augment convolutions, it does provide an avenue for global reasoning, which is one of the many Aquiles heels of modern networks.", "Part position paper, part review, and part unification, this article summarizes what the Deep Mind team believes is the next big thing in deep learning: graph neural networks (GNNs). In the authors\u2019 own words:", "(\u2026). We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \u201chand-engineering\u201d and \u201cend-to-end\u201d learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. (\u2026)", "Side Note: Inductive biases are all the assumptions a learning algorithm makes about the data. Linear models, for instance, assume the data to be linear. If a model assumes the data to have a specific relationship, it has a relational inductive bias. Thus, graphs are a useful representation.", "Reason #1: Current CNN models are \u201cend-to-end,\u201d meaning they work with raw, mostly unprocessed data. Features are not \u201cengineered\u201d by humans, but automatically \u201clearned\u201d by the algorithm. Most of us were taught that feature learning is better. In this paper, the authors provide an opposing view.", "Reason #2: Most of the early AI literature was concerned with computational reasoning. However, computational intuition prevailed. NNs do not deliberate over inputs; they generate a reasonably accurate mathematical \u201chunch.\u201d Graphs might be a way to bridge this gap towards intuitive reasoning.", "Reason #3: Combinatorial problems are arguably the most critical problems in computer science. Most are at the edge (or beyond) of what we consider tractable or possible. Yet, we humans reason naturally and effortlessly. Could graph neural networks be the answer?", "Further Reading: GNNs is an exciting and growing field. From graph theory, we know that almost anything can be modeled as a graph. Sergei Ivanov has put an excellent list of new trends in GNNs with plenty of paper references from the upcoming 2020 ICLR conference.", "Frankle, Jonathan, David J. Schwab, and Ari S. Morcos. \u201cTraining BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs.\u201d arXiv preprint arXiv:2003.00152 (2020).", "Would you believe the batch normalization layers alone of a ResNet-151 can achieve +60% accuracy on CIFAR-10? In other words, if you lock all other layers at their random initial weights and train the network for fifty or so epochs, it will perform better than random. I had to reproduce the paper to see it for myself. The \u201cmagic\u201d comes from the often forgotten \u03b3 and \u03b2 parameters of Batch Norm:", "Reason #1: This is a crazy enough idea to deserve reading. Out-of-the-box ideas are always welcome.", "Reason #2: You may be asking yourself how can batch norm layers learn anything at all, and you might also wonder why anyone would care about this. As for many things in data science, we take batch norm for granted. We believe it only speeds-up training. Apparently, it can do a lot more.", "Reason #3: This paper might spark some interest in you for checking out which parameters and hyper-parameters all common layers have.", "Further Reading: Most courses teach that batch norm layers act against the so-called internal covariance shift problem. Recent evidence shows this is not the case. Instead, the authors argue that BN layers make the overall loss landscape smoother. Another ingenious idea is the Lottery Ticket Hypothesis, which was also proposed by Frankle et al.", "Within the GAN literature, the Wasserstein loss improves on several key challenges of training GANs. However, it imposes that gradients must have a norm lesser or equal than one (1-Lipschitz ). The original authors of the loss proposed to just clip weights to [-0.01, 0.01] as a way to enforce small gradients. A hack, in the computer science jargon. In response, the Spectral norm was proposed as a smooth alternative to constraining weight matrices to produce at most unitary gradients. A much cleaner solution.", "Reason #1: Normalization is a much bigger topic than most people are aware of. Many special properties can be enforced through specialized normalization and careful activation function design.", "Reason #2: Besides being a norm, this is also a regularization, which is an often overlooked topic in neural network design. It is refreshing to read a successful paper on the matter besides dropout.", "Further Reading: Other recent advances in normalization techniques are the Group Normalization and Adaptive Instance Normalization techniques. The former solves some of batch norm shortcomings with small batch sizes while the latter is one of the critical breakthroughs in arbitrary style transfer.", "Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. \u201cPerceptual losses for real-time style transfer and super-resolution.\u201d European conference on computer vision. Springer, Cham, 2016.", "The driving force behind most neural networks is the loss function. The better a loss is at describing what is good and what is bad, the quicker we converge to useful models. In the literature, most losses are relatively simple and can measure only low-level properties. Beyond that, it is notoriously tricky to capture high-level semantics.", "Instead of hand-engineering a complex loss function, the Perceptual Loss paper argues that pre-trained networks could be used to measure semantic similarity. In practice, the generated and ground truth results are passed through a pre-trained VGG network, and the activations of specific layers are compared. Similar images should have similar activations. Early layers capture broad features while later layers capture more nuanced details.", "Reason #1: Losses are one of the most important aspects of training good models. No optimization process will ever converge without a proper feedback signal. That\u2019s the role of a good teacher: give feedback.", "Reason #2: Successful new losses are often a landmark. The quality achieved by GANs leaped after the perceptual loss was invented. Understanding this work is essential to understanding most of the later literature.", "Reason #3: These neural losses are as mysterious as they are useful. While the authors provide reasonable explanations to the workings of these models, many of their aspects are still open, as are most things in neural networks.", "Further Reading: A fascinating aspect of neural networks is their composability. This work uses neural networks to solve neural network problems. The topological loss paper extends this idea to the image segmentation problem. The Neural Architecture Search (NAS) literature uses neural networks to find new neural networks. As for other losses in Computer Vision, here is a comprehensive guide. Thanks to Sowmya Yellapragada for putting this great list together :)", "Most of us are familiar with terms such as SGD, Adam, and RMSprop. Some are also aware of less familiar names, such as AdaGrad, AdaDelta, and AdaMax. However, few are the ones that have dedicated some time to understand what these names mean and why Adam is the default choice nowadays. Tensorflow bundles Nadam, which improves upon Adam, yet most users are unaware of it.", "Reason #1: This technical report provides a comprehensive and straight-forward explanation of most neural network optimizers. Each one is presented as a direct improvement over the others. Rare are the papers that can cover such a math-heavy topic in two and a half pages.", "Reason #2: We all take optimizers for granted. Understanding their rationale can be very useful in improving neural networks. This is why we replace Adam for RMSprop when it doesn\u2019t converge, and later for SGD.", "Further Reading: Many other improvements to optimizers have been proposed since 2016. Some will be merged into mainstream libraries at some point. Check out Radam, Lookahead, and Ranger for some new ideas.", "Conventional wisdom says small models underfit and larger models overfit. Yet, somewhere over the rainbow, way up high, bigger models still shine.", "In this paper, Nakkiran et al. show evidence that several models exhibit a \u201cdouble descent\u201d phenomenon as they grow in size. Test accuracy goes down, then up, then down again. Additionally, they argue that the inflection point is at the \u201cinterpolation threshold\u201d: a point where the model is big enough to interpolate the data. In other words, when a model is trained beyond everything the field advises against, it starts improving.", "Reason #1: Most courses teach the bias/variance tradeoff. Apparently, this principle only applies to a certain extent \u2014 time to review the basics.", "Reason #2: If increasing the number of epochs also crosses the interpolation point, we should all ditch early stopping and see what happens. Collectively, we can all do science.", "Reason #3: This and #5 are good reminders that there is still much we don\u2019t know. Not all we learned is right, and not all that is intuitive is correct.", "Further Reading: A much lighter read is the Bag of Tricks for Image Classification paper. In it, you will find several simple and actionable advice for extracting that extra drop of performance from your models.", "Most are trying to go the extra mile, Fran\u00e7ois Chollet is shooting for the moon.", "In this list, all mentioned articles pushed the practical and theoretical state-of-the-art a bit further. Some have had widespread adoption, some advocate for this or that technology, and others provide sound improvements to convergence. Yet, the elephant in the room, intelligence, remains a mysterious and elusive topic, not to say arcane or mystic.", "To this day, the AI field progress towards general intelligence has only been loosely measured by \u201cachievements.\u201d Every once in a while, an algorithm beats humans in a complex task, such as chess, Dota 2, or go. Whenever this happens, we say we are one step closer \ud83d\ude03. However, this falls short of measuring the skill acquisition efficiency component of intelligence.", "In this (long) article, Chollet argues: \u201cTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal.\u201d In other words, we need a proper machine intelligence benchmark. An IQ test of sorts. Thus, the author proposed the Abstraction and Reasoning Corpus (ARC):", "\u201cARC can be seen as a general artificial intelligence benchmark, as a program synthesis benchmark, or as a psychometric intelligence test. It is targeted at both humans and artificially intelligent systems that aim at emulating a human-like form of general fluid intelligence.\u201d", "Reason #1: While data science is cool and trendy, artificial intelligence is the real thing. There wouldn\u2019t be data science if there weren\u2019t AI. And its ultimate goal is not to find insights in data, is to build machines that can have ideas of their own. Take your time to ponder the fundamental questions: what is intelligence, and how can we measure it? This paper is a good start.", "Reason #2: In the past decades, the IA community was dominated by ideas from mathematical logic and deductive reasoning. However, SVMs and Neural Networks, have advanced the field far more than logic-based methods ever could, without any form of explicit reasoning. Will ARC spark a revival of classic techniques?", "Reason #3: If Chollet is right, we are still years away from creating algorithms that can solve the ARC dataset. If you are looking for a dataset to play with on your spare time, here is one that will keep you busy :)", "Further Reading: In 2018, Geoffrey Hinton, Yosha Bengio, and Yan LeCun received the Turing Award for their pioneer work on the foundations of deep learning. This year, at the AAAI conference, they shared their opinions on what will be the future of AI. You can watch it on Youtube:", "I would like to close this article with a quote from Geoffrey Hinton, which I believe summarises it all:", "\u201c The future depends on some graduate student who is deeply suspicious of everything I have said.\u201d", "GloVe subdued the co-occurrence matrix by doing it implicitly. AdaBoost made hundreds of weak classifiers the state-of-the-art. Capsule Networks challenge CNNs while Graph Neural Networks might replace them all. Key advancement may come from normalization, losses, and optimizers while we still find room to question batch norm and to train over-parametrized models", "I wonder how many things are there left to discover about dropout and ReLU.", "I hope this has been an exciting read for you as it was for me. Please let me know of other papers you believe would fit this list. I will be glad to read and consider them for future lists \ud83d\ude03", "Edit: After writing this list, I compiled a third with ten GAN papers to read in 2020. If you enjoyed reading this (and the previous) list, you might enjoy reading the third:", "Feel free to comment or connect with me. If you are new to Medium, I highly recommend subscribing. Medium articles are the perfect pair to StackOverflow for Data and IT professionals, and even more so for new comers. Please consider using my affiliate link when signing up. You can also support me directly by buying me a cup of coffee :)", "Former game developer turned data scientist after falling in love with AI and all its branches."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c6fb4650a9b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://ygorserpa.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Ygor Serpa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----8c6fb4650a9b---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c6fb4650a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=-----8c6fb4650a9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c6fb4650a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=-----8c6fb4650a9b---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@syinq?utm_source=medium&utm_medium=referral", "anchor_text": "Susan Yin"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "artificial intelligence (AI) papers to read in 2020"}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915", "anchor_text": "previous list"}, {"url": "https://www.aclweb.org/anthology/D14-1162.pdf", "anchor_text": "\u201cGlove: Global vectors for word representation.\u201d"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2Vec"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716/", "anchor_text": "biology community"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://www.sciencedirect.com/science/article/pii/S002200009791504X", "anchor_text": "A decision-theoretic generalization of on-line learning and an application to boosting"}, {"url": "https://en.wikipedia.org/wiki/Random_forest", "anchor_text": "Random Forest"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "Gradient Boosting"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "LightGBM"}, {"url": "https://arxiv.org/abs/1710.09829", "anchor_text": "\u201cDynamic routing between capsules.\u201d"}, {"url": "https://en.wikipedia.org/wiki/Part-based_models", "anchor_text": "object parts"}, {"url": "https://en.wikipedia.org/wiki/Constellation_model", "anchor_text": "databases/ontologies"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention mechanism"}, {"url": "https://arxiv.org/pdf/1806.01261.pdf", "anchor_text": "\u201cRelational inductive biases, deep learning, and graph networks.\u201d"}, {"url": "https://medium.com/u/b60b4c7e3bfc?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Sergei Ivanov"}, {"url": "https://towardsdatascience.com/top-trends-of-graph-machine-learning-in-2020-1194175351a3", "anchor_text": "new trends in GNNs"}, {"url": "https://towardsdatascience.com/top-trends-of-graph-machine-learning-in-2020-1194175351a3", "anchor_text": "Top Trends of Graph Machine Learning in 2020The year 2020 has just started but we can already see the trends of Graph Machine Learning (GML) in the latest research\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/2003.00152", "anchor_text": "\u201cTraining BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs.\u201d"}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "Recent evidence shows this is not the case"}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "Lottery Ticket Hypothesis"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "\u201cSpectral normalization for generative adversarial networks.\u201d"}, {"url": "https://arxiv.org/abs/1701.07875", "anchor_text": "Wasserstein loss"}, {"url": "https://en.wikipedia.org/wiki/Lipschitz_continuity", "anchor_text": "1-Lipschitz"}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "careful activation function design"}, {"url": "https://en.wikipedia.org/wiki/Dropout_(neural_networks)", "anchor_text": "dropout"}, {"url": "https://arxiv.org/abs/1803.08494", "anchor_text": "Group Normalization"}, {"url": "http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html", "anchor_text": "Adaptive Instance Normalization"}, {"url": "https://arxiv.org/abs/1603.08155", "anchor_text": "\u201cPerceptual losses for real-time style transfer and super-resolution.\u201d"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html", "anchor_text": "topological loss paper"}, {"url": "https://en.wikipedia.org/wiki/Neural_architecture_search", "anchor_text": "Neural Architecture Search (NAS)"}, {"url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "anchor_text": "comprehensive guide"}, {"url": "https://medium.com/u/dc830cac0dcb?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Sowmya Yellapragada"}, {"url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "anchor_text": "Understanding Loss Functions in Computer Vision!Choosing the right loss function can optimize the model convergence, also help to focus on the right set of features in\u2026medium.com"}, {"url": "http://cs229.stanford.edu/proj2015/054_report.pdf", "anchor_text": "\u201cIncorporating nesterov momentum into adam.\u201d"}, {"url": "https://arxiv.org/abs/1908.03265v1", "anchor_text": "Radam"}, {"url": "https://arxiv.org/abs/1907.08610", "anchor_text": "Lookahead"}, {"url": "https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer", "anchor_text": "Ranger"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "\u201cDeep double descent: Where bigger models and more data hurt.\u201d"}, {"url": "https://arxiv.org/abs/1812.01187", "anchor_text": "Bag of Tricks for Image Classification"}, {"url": "https://arxiv.org/abs/1911.01547", "anchor_text": "\u201cOn the Measure of Intelligence.\u201d"}, {"url": "https://github.com/fchollet/ARC", "anchor_text": "Abstraction and Reasoning Corpus (ARC)"}, {"url": "https://www.youtube.com/watch?v=UX8OubxsY8w", "anchor_text": "Youtube"}, {"url": "https://towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4", "anchor_text": "GAN Papers to Read in 2020Reading suggestions on Generative Adversarial Networks.towardsdatascience.com"}, {"url": "https://www.linkedin.com/in/ygorreboucas/", "anchor_text": "connect with me"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "subscribing"}, {"url": "https://stackoverflow.com/", "anchor_text": "StackOverflow"}, {"url": "https://ygorserpa.medium.com/membership", "anchor_text": "my affiliate link when signing up."}, {"url": "https://www.buymeacoffee.com/ygorreboucas", "anchor_text": "buying me a cup of coffee"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8c6fb4650a9b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c6fb4650a9b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8c6fb4650a9b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----8c6fb4650a9b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c6fb4650a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=-----8c6fb4650a9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c6fb4650a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=-----8c6fb4650a9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c6fb4650a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----8c6fb4650a9b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3644829c90d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&newsletterV3=857d8734c7da&newsletterV3Id=3644829c90d9&user=Ygor+Serpa&userId=857d8734c7da&source=-----8c6fb4650a9b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Written by Ygor Serpa"}, {"url": "https://ygorserpa.medium.com/followers?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F857d8734c7da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&user=Ygor+Serpa&userId=857d8734c7da&source=post_page-857d8734c7da----8c6fb4650a9b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3644829c90d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ften-more-ai-papers-to-read-in-2020-8c6fb4650a9b&newsletterV3=857d8734c7da&newsletterV3Id=3644829c90d9&user=Ygor+Serpa&userId=857d8734c7da&source=-----8c6fb4650a9b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2022-c6edd4302247?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Ygor Serpa"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2022-c6edd4302247?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "AI Papers to Read in 2022Reading suggestions to keep you up-to-date with the latest and classic breakthroughs in AI and Data Science."}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2022-c6edd4302247?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "\u00b713 min read\u00b7Mar 4, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6edd4302247&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-papers-to-read-in-2022-c6edd4302247&user=Ygor+Serpa&userId=857d8734c7da&source=-----c6edd4302247----0-----------------clap_footer----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ai-papers-to-read-in-2022-c6edd4302247?source=author_recirc-----8c6fb4650a9b----0---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6edd4302247&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-papers-to-read-in-2022-c6edd4302247&source=-----8c6fb4650a9b----0-----------------bookmark_preview----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c6fb4650a9b----1---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----8c6fb4650a9b----1-----------------bookmark_preview----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----8c6fb4650a9b----2---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----8c6fb4650a9b----2-----------------bookmark_preview----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/memory-efficient-data-science-types-53423d48ba1d?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Ygor Serpa"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/memory-efficient-data-science-types-53423d48ba1d?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "Memory Efficient Data Science: TypesSave up to 90% memory by using the right floats and ints at the right time."}, {"url": "https://towardsdatascience.com/memory-efficient-data-science-types-53423d48ba1d?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": "\u00b78 min read\u00b7Dec 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53423d48ba1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmemory-efficient-data-science-types-53423d48ba1d&user=Ygor+Serpa&userId=857d8734c7da&source=-----53423d48ba1d----3-----------------clap_footer----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/memory-efficient-data-science-types-53423d48ba1d?source=author_recirc-----8c6fb4650a9b----3---------------------d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53423d48ba1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmemory-efficient-data-science-types-53423d48ba1d&source=-----8c6fb4650a9b----3-----------------bookmark_preview----d1db2b22_73a1_4c6d_b20d_f259ffcd7df3-------", "anchor_text": ""}, {"url": "https://ygorserpa.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "See all from Ygor Serpa"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----8c6fb4650a9b----0-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "90"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----8c6fb4650a9b----1-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c6fb4650a9b----0---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----8c6fb4650a9b----0-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----8c6fb4650a9b----1---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----8c6fb4650a9b----1-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/metas-sam-a-unique-model-to-segment-anything-c3a956bf5d62?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://salvatore-raieli.medium.com/?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://salvatore-raieli.medium.com/?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Salvatore Raieli"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/metas-sam-a-unique-model-to-segment-anything-c3a956bf5d62?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "META\u2019S SAM: A Unique Model to Segment AnythingSegmentation needs a foundation model: why is it important?"}, {"url": "https://levelup.gitconnected.com/metas-sam-a-unique-model-to-segment-anything-c3a956bf5d62?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b714 min read\u00b7Apr 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fc3a956bf5d62&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fmetas-sam-a-unique-model-to-segment-anything-c3a956bf5d62&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----c3a956bf5d62----2-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/metas-sam-a-unique-model-to-segment-anything-c3a956bf5d62?source=read_next_recirc-----8c6fb4650a9b----2---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3a956bf5d62&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fmetas-sam-a-unique-model-to-segment-anything-c3a956bf5d62&source=-----8c6fb4650a9b----2-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----3-----------------clap_footer----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----8c6fb4650a9b----3---------------------f0fed783_d40f_4287_8ef0_9c9d14b6d504-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----8c6fb4650a9b----3-----------------bookmark_preview----f0fed783_d40f_4287_8ef0_9c9d14b6d504-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----8c6fb4650a9b--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}