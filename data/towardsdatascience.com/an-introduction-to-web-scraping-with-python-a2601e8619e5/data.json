{"url": "https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5", "time": 1682994163.8153942, "path": "towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5/", "webpage": {"metadata": {"title": "An introduction to web scraping with Python | by Jonathan Oheix | Towards Data Science", "h1": "An introduction to web scraping with Python", "description": "As a data scientist, I often find myself looking for external data sources that could be relevant for my machine learning projects. The problem is that it is uncommon to find open source data sets\u2026"}, "outgoing_paragraph_urls": [{"url": "http://books.toscrape.com/", "anchor_text": "http://books.toscrape.com/", "paragraph_index": 8}, {"url": "http://books.toscrape.com/index.html", "anchor_text": "http://books.toscrape.com/index.html", "paragraph_index": 27}, {"url": "https://www.seleniumhq.org/", "anchor_text": "https://www.seleniumhq.org/", "paragraph_index": 58}, {"url": "https://www.linkedin.com/in/jonathanoheix/", "anchor_text": "https://www.linkedin.com/in/jonathanoheix/", "paragraph_index": 61}], "all_paragraphs": ["As a data scientist, I often find myself looking for external data sources that could be relevant for my machine learning projects. The problem is that it is uncommon to find open source data sets that perfectly correspond to what you are looking for, or free APIs that give you access to data. In this case, web scraping can be one solution to get more data.", "Web scraping consists in gathering data available on websites. This can be done manually by a human user or by a bot. The latter can of course gather data much faster than a human user and that is why we are going to focus on this. Is it therefore technically possible to collect all the data of a website in a matter of minutes this kind of bot. The legality of this practice is not well defined however. Websites usually describe in their terms of use and in their robots.txt file if they allow scrapers or not.", "Web scrapers gather website data in the same way a human would do it: the scraper goes onto a web page of the website, gets the relevant data, and move forward to the next web page. Every website has a different structure, that is why web scrapers are usually built to explore one website. The two important issues that arise during the implementation of a web scraper are the following:", "In order to answer those questions, we need to understand a little how websites work. Websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. HTML elements are separated by tags and they directly introduce content to the web page. Here is what a basic HTML document looks like:", "We can see that the content of the first heading is contained between the \u2018h1\u2019 tags. The first paragraph is contained between the \u2018p\u2019 tags. On a real website, we need to find out between which tags the relevant data is and tell it to our scraper. We also need to specify which links should be explored and where they can be found among the HTML file. With all this information, our scraper should be able to gather the required data.", "In this tutorial we are going to use the Python modules requests and BeautifulSoup.", "Requests will allow us to send HTTP requests to get the HTML files.", "BeautifulSoup will be used to parse the HTML files. It is one of the most used library for web scraping. Its is quite simple to use and has many features that help gathering websites data efficiently.", "We want to scrape the data of an online book store: http://books.toscrape.com/", "This website is fictional so we can scrape it as much as we want.", "In this tutorial we will be gathering the following information about all the products of the website:", "First let\u2019s use the requests module to get the HTML of the website\u2019s main page.", "The result is quite messy! Let\u2019s make this more readable:", "The function prettify() makes the HTML more readable. However we will not use this directly to explore where the relevant data is.", "Let\u2019s define a function to request and parse a HTML web page as we will need this a lot during this tutorial:", "Now let\u2019s start to dive deeper into the subject. In order to get the book data, we need to be able to access their product page. The first step consist in finding the URL of every book product page.", "In your browser, go onto the website main page, right-click on the name of a product and click on inspect. This will show you the HTML part of the web page corresponding to this element. Congratulations, you have found the first book link!", "Note the structure of the HTML code:", "You can try this with every other product on the page: the structure is always the same. The link of the product corresponds to the \u2018href\u2019 attribute of the \u2018a\u2019 tag. This one belongs to an \u2018article\u2019 tag with the a class value \u2018product_pod\u2019. This seems to be a reliable source to spot product URLs.", "BeautifulSoup enables us to find those special \u2018article\u2019 tags. We can wall the find() function in order to find the first occurence of this tag in the HTML:", "We still have too much information.", "Let\u2019s dive deeper in the tree by adding the other child tags:", "Much better! But we only need the URL contained in the \u2018href\u2019 value.", "We can get this by adding .get(\u201chref\u201d) to the previous instruction:", "Ok, we managed to get our first product URL with BeautifulSoup.", "Now let\u2019s gather all the products URLs on the main web page at once using the findAll() function:", "This function is very handy for finding all the values at once, but you have to check that all the information collected is relevant. Sometimes one same tag can contain completely different data. That is why it is important to be as specific as possible when choosing the tags. Here we decided to rely on the tag \u2018article\u2019 with the \u2018product_pod\u2019 class because this seems to be a very specific tag and it is unlikely that we can find data other than product data in it.", "The previous URLs correspond to their relative path from the main page. In order to make them complete, we just need to add before them the URL of the main page: http://books.toscrape.com/index.html (after removing the index.html part).", "Now let\u2019s use this to define a function to retrieve book links on any given page of the website:", "Now let\u2019s try retrieving the URLs corresponding the different product categories:", "By inspecting, we can see that they follow the same URL pattern: \u2018catalogue/category/books\u2019.", "We can tell BeautifulSoup to match the URLs that contain this pattern in order to retrieve easily the categories URLs:", "We managed to retrieve the 50 categories URLs successfully!", "Remember to always check what you fetched to be sure that all the information is relevant.", "Getting the URLs of subsections of a website can be very useful if we want to scrape a specific part of it.", "For the last part of this tutorial, we will finally tackle our main objective: gather data about all the books of the website.", "We know how to get the links of the books within a given page. If all the books were displayed on a same page this would be easy. However this situation is unlikely as it is not very user friendly to display all the catalog to the user on the same page.", "Usually products are displayed on multiple pages or on one page but through scrolling. We can see here at the bottom of the main page that there are 50 products pages and a button \u2018next\u2019 to access to the next product page.", "On the next pages there is also a \u2018previous\u2019 button to come back to the last product page.", "In order to fetch all the products URLs, we need to be able to get through all the pages. To do so, we can go iteratively through all the \u2018next\u2019 buttons.", "The \u2018next\u2019 button contains the pattern \u2018page\u2019. We can use this to retrieve the URLs of the next pages. But let\u2019s be careful: the \u2018previous\u2019 button also contains this pattern!", "If we have two results when matching with \u2018page\u2019, we should take the second one as it will correspond to the next page. For the first and the last pages we will have only one result because we will have either the \u2018next\u2019 button or the \u2018previous\u2019 button.", "We successfully managed to get the 50 pages URLs. What is interesting here is that the URL of those pages is highly predictable. We could have just created this list by incrementing \u2018page-X.html\u2019 until 50.", "This solution could work for this exact example but would not work anymore if the number of pages changed (e.g. if the website decided to print more products per pages, or if the catalog changed).", "One solution could be to increment the value until we get on a 404 page.", "Here we can see that trying to go to the 51th page effectively gets us a 404 error.", "Fortunately the result of a request has a very useful attribute that can show us the return status of the HTML request.", "The 200 code indicates that there is no error. The 404 code tells us that the page was not found.", "We can use this information to get all our pages URLs: we should iterate until we get a 404 code.", "We managed to obtain the same URLs using this simpler method!", "Now the next step consists in fetching all the products URLs for every page. This step is quite simple as we already have the list of all pages and the function to get products URLs from a page.", "Let\u2019s iterate through the pages and apply our function:", "We finally got the 1000 book URLs. This corresponds to the number indicated on the website!", "The last step consist in scraping the data for each product. Let\u2019s explore first how the information is structured on the products pages:", "We can easily retrieve a lot of information for every book:", "We got our data: our web scraping experiment is a success.", "Some data cleaning may be useful before using them:", "We have seen how to get through websites and gather data on each web page using automated web scrapers. One key thing in order to build efficient web scrapers is to understand the structure of the website on which you want to scrape the information. This means that you will probably have to maintain you scraper if you want it to remain useful after websites updates.", "This book store website was an easy example, but in real life you may have to deal with more complex websites that render some of their content using Javascript. You may want to use a browser automator like Selenium for those kind of tasks (https://www.seleniumhq.org/).", "Here is the link to the original Jupyter notebook:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with 4 years of experience in various fields. https://www.linkedin.com/in/jonathanoheix/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa2601e8619e5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jo879344?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jo879344?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Jonathan Oheix"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2e8e06482696&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&user=Jonathan+Oheix&userId=2e8e06482696&source=post_page-2e8e06482696----a2601e8619e5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@darwiiiin?utm_source=medium&utm_medium=referral", "anchor_text": "Darwin Vegher"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://docs.python-requests.org/en/master/", "anchor_text": "http://docs.python-requests.org/en/master/"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"}, {"url": "http://books.toscrape.com/", "anchor_text": "http://books.toscrape.com/"}, {"url": "http://books.toscrape.com/index.html", "anchor_text": "http://books.toscrape.com/index.html"}, {"url": "https://www.seleniumhq.org/", "anchor_text": "https://www.seleniumhq.org/"}, {"url": "https://github.com/jonathanoheix/scraping_basics_with_beautifulsoup", "anchor_text": "jonathanoheix/scraping_basics_with_beautifulsoupContribute to jonathanoheix/scraping_basics_with_beautifulsoup development by creating an account on GitHub.github.com"}, {"url": "https://www.linkedin.com/in/jonathanoheix/", "anchor_text": "https://www.linkedin.com/in/jonathanoheix/"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a2601e8619e5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----a2601e8619e5---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/python?source=post_page-----a2601e8619e5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/beautifulsoup?source=post_page-----a2601e8619e5---------------beautifulsoup-----------------", "anchor_text": "Beautifulsoup"}, {"url": "https://medium.com/tag/html?source=post_page-----a2601e8619e5---------------html-----------------", "anchor_text": "HTML"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&user=Jonathan+Oheix&userId=2e8e06482696&source=-----a2601e8619e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&user=Jonathan+Oheix&userId=2e8e06482696&source=-----a2601e8619e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa2601e8619e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a2601e8619e5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a2601e8619e5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a2601e8619e5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a2601e8619e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jo879344?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jo879344?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jonathan Oheix"}, {"url": "https://medium.com/@jo879344/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "253 Followers"}, {"url": "https://www.linkedin.com/in/jonathanoheix/", "anchor_text": "https://www.linkedin.com/in/jonathanoheix/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2e8e06482696&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&user=Jonathan+Oheix&userId=2e8e06482696&source=post_page-2e8e06482696--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa627ef796d52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-web-scraping-with-python-a2601e8619e5&newsletterV3=2e8e06482696&newsletterV3Id=a627ef796d52&user=Jonathan+Oheix&userId=2e8e06482696&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}