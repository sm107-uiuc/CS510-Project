{"url": "https://towardsdatascience.com/distilling-bert-models-with-spacy-277c7edc426c", "time": 1683000128.081113, "path": "towardsdatascience.com/distilling-bert-models-with-spacy-277c7edc426c/", "webpage": {"metadata": {"title": "Distilling BERT models with spaCy | by Yves Peirsman | Towards Data Science", "h1": "Distilling BERT models with spaCy", "description": "Transfer learning is one of the most impactful recent breakthroughs in Natural Language Processing. Less than a year after its release, Google\u2019s BERT and its offspring (RoBERTa, XLNet, etc.) dominate\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/", "anchor_text": "RoBERTa", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet", "paragraph_index": 0}, {"url": "http://www.nlp.town/", "anchor_text": "NLP Town", "paragraph_index": 0}, {"url": "https://nlp.stanford.edu/pubs/clark2019what.pdf", "anchor_text": "co-reference", "paragraph_index": 1}, {"url": "http://u.cs.biu.ac.il/~yogo/bert-syntax.pdf", "anchor_text": "syntax", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformers", "paragraph_index": 2}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "Facebook\u2019s XLM with 665M parameters", "paragraph_index": 2}, {"url": "https://openai.com/blog/gpt-2-6-month-follow-up/", "anchor_text": "GPT-2 with 774M", "paragraph_index": 2}, {"url": "https://blog.rasa.com/compressing-bert-for-faster-prediction-2/", "anchor_text": "Three possible approaches", "paragraph_index": 3}, {"url": "https://www.linkedin.com/in/simon-lepercq-26a058191", "anchor_text": "Simon Lepercq", "paragraph_index": 4}, {"url": "https://www.aclweb.org/anthology/W02-1011", "anchor_text": "Pang, Lee and Vaithyanathan", "paragraph_index": 4}, {"url": "https://spacy.io/usage/training#textcat", "anchor_text": "text classification model", "paragraph_index": 5}, {"url": "https://spacy.io/api/textcategorizer#architectures", "anchor_text": "stacked ensemble", "paragraph_index": 5}, {"url": "https://github.com/huggingface/pytorch-transformers", "anchor_text": "PyTorch-Transformers", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "model distillation", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1903.12136.pdf", "anchor_text": "Tang et al. (2019)", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1903.12136.pdf", "anchor_text": "Tang et al. (2019)", "paragraph_index": 11}, {"url": "http://www.nlp.town", "anchor_text": "www.nlp.town", "paragraph_index": 16}], "all_paragraphs": ["Transfer learning is one of the most impactful recent breakthroughs in Natural Language Processing. Less than a year after its release, Google\u2019s BERT and its offspring (RoBERTa, XLNet, etc.) dominate most of the NLP leaderboards. While it can be a headache to put these enormous models into production, various solutions exist to reduce their size considerably. At NLP Town we successfully applied model distillation to train spaCy\u2019s text classifier to perform almost as well as BERT on sentiment analysis of product reviews.", "Recently the standard approach to Natural Language Processing has changed drastically. Whereas until one year ago, almost all NLP models were trained entirely from scratch (usually with the exception of their pre-trained word embeddings), today the safest road to success is to download a pre-trained model such as BERT and finetune it for your particular NLP task. Because these transfer-learning models have already seen a large collection of unlabelled texts, they have acquired a lot of knowledge about language: they are aware of word and sentence meaning, co-reference, syntax, and so on. Exciting as this revolution may be, models like BERT have so many parameters they are fairly slow and resource-intensive. For some NLP tasks at least, finetuning BERT feels like using a sledgehammer to crack a nut.", "Most transfer-learning models are huge. BERT\u2019s base and multilingual models are transformers with 12 layers, a hidden size of 768 and 12 self-attention heads \u2014 no less than 110 million parameters in total. BERT-large sports a whopping 340M parameters. Still, BERT dwarfs in comparison to even more recent models, such as Facebook\u2019s XLM with 665M parameters and OpenAI\u2019s GPT-2 with 774M. It certainly looks like this evolution towards ever larger models is set to continue for a while.", "Of course, language is a complex phenomenon. It\u2019s obvious that more traditional, smaller models with relatively few parameters will not be able to handle all NLP tasks you throw at them. For individual text classification or sequence labelling tasks, however, it\u2019s questionable whether all the expressive power of BERT and its peers is really needed. That\u2019s why researchers have begun investigating how we can bring down the size of these models. Three possible approaches have emerged: quantization reduces the precision of the weights in a model by encoding them in fewer bits, pruning completely removes certain parts of a model (connection weights, neurons or even full weight matrices), while in distillation the goal is to train a small model to mimic the behaviour of a larger one.", "In one of our summer projects at NLP Town, together with our intern Simon Lepercq, we set out to investigate the effectiveness of model distillation for sentiment analysis. Like Pang, Lee and Vaithyanathan in their seminal paper, our goal was to build an NLP model that was able to distinguish between positive and negative reviews. We collected product reviews in six languages: English, Dutch, French, German, Italian and Spanish. The reviews with one or two stars we gave the label negative, and those with four or five stars we considered positive. We used 1000 examples for training, 1000 for development (early stopping) and 1000 examples for testing.", "The first step was to determine a baseline for our task. With an equal number of positive and negative examples in each of our data sets, a random baseline would obtain an accuracy of 50% on average. As a simple machine learning baseline, we trained a spaCy text classification model: a stacked ensemble of a bag-of-words model and a fairly simple convolutional neural network with mean pooling and attention. To this we added an output layer of one node and had the model predict positive when its output score was higher than 0.5 and negative otherwise. This baseline achieved an accuracy of between 79.5% (for Italian) and 83.4% (for French) on the test data \u2014 not bad, but not a great result either.", "Because of its small training set, our challenge is extremely suitable for transfer learning. Even if a test phrase such as great book is not present in the training data, BERT already knows it is similar to excellent novel, fantastic read, or another similar phrase that may very well occur in the training set. As a result, it should be able to predict the rating for an unseen review much more reliably than a simple model trained from scratch.", "To finetune BERT, we adapted the BERTForSequenceClassification class in the PyTorch-Transformers library for binary classification. For all six languages we finetuned BERT-multilingual-cased, the multilingual model Google currently recommends. The results confirm our expectations: with accuracies between 87.2% (for Dutch) and 91.9% (for Spanish), BERT outperforms our initial spaCy models by an impressive 8.4% on average. This means BERT nearly halves the number of errors on the test set.", "Unfortunately, BERT is not without its drawbacks. Each of our six finetuned models takes up almost 700MB on disk and their inference times are much longer than spaCy\u2019s. That makes them hard to deploy on a device with limited resources or for many users in parallel. To address these challenges, we turn to model distillation: we have our finetuned BERT models serve as teachers and spaCy\u2019s simpler convolutional models as students that learn to mimic the teacher\u2019s behavior. We follow the model distillation approach described by Tang et al. (2019), who show it is possible to distill BERT to a simple BiLSTM and achieve results similar to an ELMo model with 100 times more parameters.", "Before we can start training our small models, however, we need more data. In order to learn and mimic BERT\u2019s behavior, our students need to see more examples than the original training sets can offer. Tang et al. therefore apply three methods for data augmentation (the creation of synthetic training data on the basis of the original training data):", "Since the product reviews in our data set can be fairly long, we add a fourth method to the three above:", "These augmentation methods not only help us create a training set that is many times larger than the original one; by sampling and replacing various parts of the training data, they also inform the student model about what words or phrases have an impact on the output of its teacher. Moreover, in order to give it as much information as possible, we don\u2019t show the student the label its teacher predicted for an item, but its precise output values. In this way, the small model can learn how probable the best class was exactly, and how it compared to the other one(s). Tang et al. (2019) trained the small model with the logits of its teacher, but our experiments show using the probabilities can also give very good results.", "One of the great advantages of model distillation is that it is model agnostic: the teacher model can be a black box, and the student model can have any architecture we like. To keep our experiments simple, we chose as our student the same spaCy text classifier as we did for our baselines. The training procedure, too, remained the same: we used the same batch sizes, learning rate, dropout and loss function, and stopped training when the accuracy on the development data stopped going up. We used the augmentation methods above to put together a synthetic data set of around 60,000 examples for each language. We then collected the predictions of the finetuned BERT models for this data. Together with the original training data, this became the training data for our smaller spaCy models.", "Despite this simple setup, the distilled spaCy models outperformed our initial spaCy baselines by a clear margin. On average, they gave an improvement in accuracy of 7.3% (just 1% below the BERT models) and an error reduction of 39%. Their performance demonstrates that for a particular task such as sentiment analysis, we don\u2019t need all the expressive power that BERT offers. It is perfectly possible to train a model that performs almost as well as BERT, but with many fewer parameters.", "With the growing popularity of large transfer-learning models, putting NLP solutions into production is becoming more challenging. Approaches like model distillation, however, show that for many tasks you don\u2019t need hundreds of millions of parameters to achieve high accuracies. Our experiments with sentiment analysis in six languages demonstrate it is possible to train spaCy\u2019s convolutional neural network to rival much more complex model architectures such as BERT\u2019s. In the future, we hope to investigate model distillation in more detail at NLP Town. For example, we aim to find out what data augmentation methods are most effective, or how much synthetic data we need to train a smaller model.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Natural Language Processing Expert at www.nlp.town"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F277c7edc426c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----277c7edc426c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yvespeirsman?source=post_page-----277c7edc426c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yvespeirsman?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Yves Peirsman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7325492da145&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&user=Yves+Peirsman&userId=7325492da145&source=post_page-7325492da145----277c7edc426c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "anchor_text": "BERT"}, {"url": "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet"}, {"url": "http://www.nlp.town/", "anchor_text": "NLP Town"}, {"url": "https://nlp.stanford.edu/pubs/clark2019what.pdf", "anchor_text": "co-reference"}, {"url": "http://u.cs.biu.ac.il/~yogo/bert-syntax.pdf", "anchor_text": "syntax"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformers"}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "Facebook\u2019s XLM with 665M parameters"}, {"url": "https://openai.com/blog/gpt-2-6-month-follow-up/", "anchor_text": "GPT-2 with 774M"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Devlin et al. 2018"}, {"url": "https://blog.rasa.com/compressing-bert-for-faster-prediction-2/", "anchor_text": "Three possible approaches"}, {"url": "https://www.linkedin.com/in/simon-lepercq-26a058191", "anchor_text": "Simon Lepercq"}, {"url": "https://www.aclweb.org/anthology/W02-1011", "anchor_text": "Pang, Lee and Vaithyanathan"}, {"url": "https://spacy.io/usage/training#textcat", "anchor_text": "text classification model"}, {"url": "https://spacy.io/api/textcategorizer#architectures", "anchor_text": "stacked ensemble"}, {"url": "https://github.com/huggingface/pytorch-transformers", "anchor_text": "PyTorch-Transformers"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "model distillation"}, {"url": "https://arxiv.org/pdf/1903.12136.pdf", "anchor_text": "Tang et al. (2019)"}, {"url": "https://arxiv.org/pdf/1903.12136.pdf", "anchor_text": "Tang et al. (2019)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----277c7edc426c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----277c7edc426c---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----277c7edc426c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----277c7edc426c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----277c7edc426c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&user=Yves+Peirsman&userId=7325492da145&source=-----277c7edc426c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&user=Yves+Peirsman&userId=7325492da145&source=-----277c7edc426c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F277c7edc426c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----277c7edc426c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----277c7edc426c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----277c7edc426c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----277c7edc426c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----277c7edc426c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----277c7edc426c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yvespeirsman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yvespeirsman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yves Peirsman"}, {"url": "https://medium.com/@yvespeirsman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "221 Followers"}, {"url": "http://www.nlp.town", "anchor_text": "www.nlp.town"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7325492da145&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&user=Yves+Peirsman&userId=7325492da145&source=post_page-7325492da145--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7325492da145%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-models-with-spacy-277c7edc426c&user=Yves+Peirsman&userId=7325492da145&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}