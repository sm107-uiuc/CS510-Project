{"url": "https://towardsdatascience.com/making-your-own-alexa-entity-extraction-8c7f23eb65a", "time": 1683004310.305275, "path": "towardsdatascience.com/making-your-own-alexa-entity-extraction-8c7f23eb65a/", "webpage": {"metadata": {"title": "Extracting information from user commands | by Sigur\u00f0ur Sk\u00fali | Towards Data Science", "h1": "Extracting information from user commands", "description": "Virtual assistants have taken the world by storm in the past few years. From the virtual home assistants like Alexa and the Google Assistant, to more specialized assistants such as Erica from the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://developer.amazon.com/alexa", "anchor_text": "Alexa", "paragraph_index": 0}, {"url": "https://assistant.google.com/", "anchor_text": "Google Assistant", "paragraph_index": 0}, {"url": "https://promo.bankofamerica.com/erica/", "anchor_text": "Erica", "paragraph_index": 0}, {"url": "https://developer.amazon.com/blogs/alexa/post/36ca7d4c-cd98-40a9-a9c5-0cde2ab922ab/how-alexa-knows-that-peanut-butter-is-one-shopping-list-item-not-two?fbclid=IwAR0ctwBfmE7JUV7BR6Un_Fp54cMG9OdoLC2BurWk-HUxwbKYZShpqacm4lI", "anchor_text": "the domain-agnostic parser used by Amazon Alexa", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Named-entity_recognition", "anchor_text": "Named-Entity Recognition", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)", "anchor_text": "IOB scheme", "paragraph_index": 10}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/tree/master/commands", "anchor_text": "example data", "paragraph_index": 12}, {"url": "https://fasttext.cc/", "anchor_text": "FastText", "paragraph_index": 16}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece", "paragraph_index": 16}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L33", "anchor_text": "Vocabulary", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1508.01991.pdf", "anchor_text": "Bidirectional LSTM-CRF", "paragraph_index": 24}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/model.py", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/", "anchor_text": "his article on TimeDistributed Layers", "paragraph_index": 28}, {"url": "https://keras.io/utils/", "anchor_text": "Sequence", "paragraph_index": 42}, {"url": "https://keras.io/utils/", "anchor_text": "Keras documentation", "paragraph_index": 42}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/preprocessing.py", "anchor_text": "preprocessor", "paragraph_index": 43}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/trainer.py#L7", "anchor_text": "Trainer", "paragraph_index": 44}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "DataSequence", "paragraph_index": 45}, {"url": "https://keras.io/models/sequential/#fit_generator", "anchor_text": "fit_generator()", "paragraph_index": 47}], "all_paragraphs": ["Virtual assistants have taken the world by storm in the past few years. From the virtual home assistants like Alexa and the Google Assistant, to more specialized assistants such as Erica from the Bank of America. The rapid rise of these assistants has shown that the way we interact with machines is changing and evolving.", "In this 3-part series of articles we will cover how you can write your own simple virtual assistant or chatbot from the bottom-up in Keras using a smaller version of the domain-agnostic parser used by Amazon Alexa, and a command classification model.", "After reading this series you will have learned how to create your own virtual assistant that is simple enough to run on a Raspberry Pi or AWS Lambda, but powerful enough to be used as a basis for an enterprise solution.", "This series will be split into three parts:", "Each article will cover a separate subject that is integral to making a voice-guided virtual assistant. This first article will teach you how to extract entities from the commands sent by the user. The second will teach you how to make a network that classifies commands so that they can be sent to the correct intent handler, allowing your assistant to handle more than one intent. The third part will cover a couple of different ideas on how you can add support for voice commands to your assistant and various other improvements.", "Imagine that you are working in a bank and your job is to transfer money between accounts for customers. One day, a customer walks in and puts forth the following request:", "\u201cCan you transfer $50 from my savings account to Mark\u2019s account\u201d", "To be able to fulfill this request, you as a worker need to be able to extract three key values from the request. You need to extract:", "Since we have been trained from birth to extract information from language this task is easy for a person, but for a computer it can be tricky. Emulating this process and extracting this information accurately is the challenge that our model will face. For that we will use Named-Entity Recognition.", "Named-Entity Recognition (NER) is a sub-task of information extraction that seeks to locate named entities in unstructured text (or semi-structured text in our case). The goal of NER is to tag every single word in a sequence with a label representing the kind of entity the word belongs to.", "Figure 1 shows an example of what a sentence that has been parsed through a NER model may look like. Each word of the sentence is labeled using the IOB scheme (Inside-Outside-Beginning) with an additional connection label to label words used to connect different named entities. These labels are then used to extract entities from our command. From the labels in Figure 1, we can extract that the song we want the service to play is \u201cBohemian Rhapsody\u201d and the artist is \u201cQueen\u201d.", "Now that we know the problem and what method we\u2019re going to use to solve it, it\u2019s time to get coding!", "Before we write the actual code we\u2019re going to need some actual data to work on. Fortunately, we have prepared example data that we will be using for the purposes of this article.", "The excerpt above shows an example of the data schema we will be using. We have compiled all the possible labels for the command into labels and tokenized and labelled all the commands into training_data, where training_data['words'] is the model\u2019s input sequence and training_data['labels\u2019] is the expected output sequence.", "As with any project involving neural networks, we need to prepare our sequences before we can use them. Neural networks do not like being fed strings or characters. They like cold, hard numbers!", "So first, we need to build a vocabulary. A vocabulary is a container for all the words known to our network. This vocabulary will be used by our model to create an embedding layer (more on that later).", "You can use a pre-built vocabulary and tokenizer like FastText or SentencePiece to provide the vocabulary for your model, but that is unnecessary for small virtual assistants and will simply result in the size of the model getting bloated without any meaningful increases in accuracy. Only use them if your use case requires a bigger vocabulary and benefits from their inclusion!", "For example, in our experiments with FastText, the size of one of the models was around 1GB. But, with a smaller custom vocabulary the model size was around 15MB. The difference in accuracy, precision, and recall between the two versions was nearly nonexistent.", "With that said, let\u2019s create our custom vocabulary!", "In the code above we load in our data and gather all the unique words and characters from the commands in our dataset. In addition to all the words, we add the <unk> and <pad>token to the vocabulary for words and characters that we have not seen before. The <unk> token allows us to keep the size of our vocabulary at a fixed size and it allows the model to handle words it does not know. The <pad> token allows us to mask the padding in the sequences so it doesn\u2019t get mistaken as part of the command. This token is necessary because the length of our commands can vary, but the model expects them all to be the same length.", "Now that we have the vocabulary, we need to create a mapping function to map from words to integers. This allows us to transform the word sequences in our dataset into sequences of integers. Thankfully, our implementation of the Vocabulary class already does that for us when the words are added to the dictionary.", "Finally, we will create a mapping function for our labels.", "If you\u2019re wondering why we\u2019re using a character vocabulary as well as a word vocabulary, then that is because character embeddings are really useful for generalizing across contexts. This helps our model predict the label of a word when that word is not in our word vocabulary, as is the case with words that get the <unk> token. This is because all of the word\u2019s characters should be present in the character vocabulary.", "Now our data preparations are complete and we can get started on the model.", "The model we will be using to implement NER is called a Bidirectional LSTM-CRF model with character embeddings. The module that we\u2019ll be using can be found here. In the model we will be using seven different types of layers.", "Embedding layers are layers where text data (words/characters/sentences) represented by integer ids is converted into dense fixed-size vectors. This layer can then be trained so that the vectors represent the meaning behind the word, meaning words with similar meanings will get similar vectors.", "LSTM layers are recurrent layers that take a sequence as an input and can return either sequences (return_sequences=True) or flattened outputs.", "Bidirectional layers are wrapper layers that connect two hidden layers of opposite directions to the same output. They are useful when the context of the input is needed, such as labeling sequences of words. In essence, they allow a position in the sequence to have information about everything that came before it and everything that comes after it in the sequence.", "TimeDistributed layers are wrapper layers that allows us to apply a layer to every temporal slice of an input. Jason Brownlee explains this concept better than I ever could in his article on TimeDistributed Layers.", "Dropout layers are a regularization technique that consists of setting a fraction of weights to 0 at each update during the training to prevent overfitting. The fraction is determined by the hyperparameter used by the layer.", "Dense layers or fully connected layers are fully connected neural network layers where each input node is connected to each output node. They perform a simple linear transformation of the input.", "The Conditional Random Field (CRF) layer learns which sequences of labels are most likely. For example, it can learn that an O label should never come directly before a I-Song label in a sequence and that I-Song can only ever be preceded by B-Song or I-Song.", "In our model we have two inputs:", "First thing we do is feed both inputs to an Embedding layer that transforms the ids in each array into dense fixed-size vectors. An example of this process can be seen in Figure 3.", "You might notice that the LSTM layer that gets the character embedding vectors as inputs is wrapped by a TimeDistributed wrapper. This causes the LSTM layer to be applied to a temporal slice of the input and instead of the whole input tensor. Figure 4 gives a visual explanation to how the wrapper works.", "Because we have return_sequences=False our TimeDistributed LSTM layer reduces flattens its output so that the rank of the output tensor goes from four to three. Now we can concatenate it with the word embeddings.", "Next we feed the concatenated inputs to a BiDirectional LSTM layer. This layer outputs a contextual embedding of each word in the input sentence. This captures information about the word\u2019s position in the sentence.", "Second to last, we feed the contextual embedding to a fully connected layer that maps each of the contextual word embedding to a distribution over the output labels.", "Finally, we feed the results of the Dense layer to the CRF layer. This layer selects the most probable sequence of output labels from the tensor it gets from the Dense layer.", "Figure 5 shows a diagram of the model along with the input and output dimensions.", "One important thing to note about the implementation of Bidirectional in Keras is that the default configuration for combining the forward pass and backwards pass is concatenating them together. This means that the output size we choose for our LSTM layer gets doubled. But there are also other options, such as making the layer add the results together or return the average, that keep the output size the same instead of doubling it.", "Now that we\u2019ve covered both how to prepare all the data we need and the structure of our model, it is time to start training.", "First, we\u2019re going to create a Sequence object to prepare minibatches for the model. By using Sequence objects we can perform preprocessing on the each batch of data before it is sent to the network. This simplifies our code since it allows us to prepare only one batch at a time instead of worrying about the whole dataset. Our Sequence object is relatively simple, it\u2019s mostly the same as the example given by the Keras documentation.", "Next, we\u2019re going to create a preprocessor to handle the preprocessing of our batches. It takes our mapping functions as arguments so that they can be used to convert our word, character, and label tokens into integer ID tokens.", "Now let\u2019s initialize the model and create a Trainer object to take care of training and evaluating the model. The training and validation sets will be used during training to improve the quality of our NER model and the test set will be used at the end to see how the model performs on data it hasn\u2019t seen before. The training set will consist of 75% of the data, the validation set will be 20%, and our test set will be 5%. These ratios are anything but holy, so feel free to play around with different ratios and see what works best for you.", "Once we have split the data and created the Trainer object we can train the model. Our DataSequence object takes an input, an output, the size of each batch and a preprocessing function as arguments.", "Now we can start training the model!", "To calculate the loss of the model we\u2019re going to use the CRF loss function. We\u2019re going to use the Adam optimizer with default settings and the metric we\u2019re going to be keeping track of is the accuracy of the model. To train the model we\u2019re going to use the fit_generator() method in Keras. This method allows us to train the model using data generated batch-by-batch by our Sequence object. Thereby giving us more control over the training process.", "Figure 6 shows an example of training for four epochs. As you can see the accuracy of our model reaches 100% on the second epoch for our Wiki dataset. That means that our model is able to extract the correct information out of every given command.", "In our own virtual assistant that uses the same method we have shown you in this article, the lowest accuracy we have for any single intent is roughly 99.5%. The rest have about 100% accuracy. So we can see that this model is highly effective at retrieving entities from text commands.", "Congratulations for making it to the end of the information extraction part of the series. After reading this article, you should understand how Alexa and other virtual assistants and chatbots are extracting information from your requests and how you can create models to do the same. If you have any questions, feel free to contact me or leave a comment.", "In the next part we will look into how our assistant chooses which NER model should handle the incoming command.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c7f23eb65a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sigurdurssigurg?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sigurdurssigurg?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Sigur\u00f0ur Sk\u00fali"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F10517e62b9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&user=Sigur%C3%B0ur+Sk%C3%BAli&userId=10517e62b9e&source=post_page-10517e62b9e----8c7f23eb65a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/making-your-own-alexa", "anchor_text": "Making Your Own Alexa"}, {"url": "https://developer.amazon.com/alexa", "anchor_text": "Alexa"}, {"url": "https://assistant.google.com/", "anchor_text": "Google Assistant"}, {"url": "https://promo.bankofamerica.com/erica/", "anchor_text": "Erica"}, {"url": "https://developer.amazon.com/blogs/alexa/post/36ca7d4c-cd98-40a9-a9c5-0cde2ab922ab/how-alexa-knows-that-peanut-butter-is-one-shopping-list-item-not-two?fbclid=IwAR0ctwBfmE7JUV7BR6Un_Fp54cMG9OdoLC2BurWk-HUxwbKYZShpqacm4lI", "anchor_text": "the domain-agnostic parser used by Amazon Alexa"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial", "anchor_text": "Github link for the lazy"}, {"url": "https://en.wikipedia.org/wiki/Named-entity_recognition", "anchor_text": "Named-Entity Recognition"}, {"url": "https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)", "anchor_text": "IOB scheme"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/tree/master/commands", "anchor_text": "example data"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L9", "anchor_text": "Dataset"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "Vocabulary"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "Vocabulary"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L33", "anchor_text": "Vocabulary"}, {"url": "https://arxiv.org/pdf/1508.01991.pdf", "anchor_text": "Bidirectional LSTM-CRF"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/model.py", "anchor_text": "here"}, {"url": "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/", "anchor_text": "his article on TimeDistributed Layers"}, {"url": "https://keras.io/utils/", "anchor_text": "Sequence"}, {"url": "https://keras.io/utils/", "anchor_text": "Keras documentation"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/preprocessing.py", "anchor_text": "Preprocessor"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/preprocessing.py", "anchor_text": "preprocessor"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/model.py#L11", "anchor_text": "BiLSTMCRF"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/trainer.py#L7", "anchor_text": "Trainer"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/trainer.py#L7", "anchor_text": "Trainer"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "DataSequence"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "DataSequence"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/datautils.py#L83", "anchor_text": "DataSequence"}, {"url": "https://keras.io/models/sequential/#fit_generator", "anchor_text": "fit_generator()"}, {"url": "https://github.com/Skuldur/virtual-assistant-tutorial/blob/master/commands/wiki_commands.json", "anchor_text": "Wiki dataset"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8c7f23eb65a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8c7f23eb65a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c7f23eb65a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/alexa?source=post_page-----8c7f23eb65a---------------alexa-----------------", "anchor_text": "Alexa"}, {"url": "https://medium.com/tag/making-your-own-alexa?source=post_page-----8c7f23eb65a---------------making_your_own_alexa-----------------", "anchor_text": "Making Your Own Alexa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&user=Sigur%C3%B0ur+Sk%C3%BAli&userId=10517e62b9e&source=-----8c7f23eb65a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&user=Sigur%C3%B0ur+Sk%C3%BAli&userId=10517e62b9e&source=-----8c7f23eb65a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c7f23eb65a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c7f23eb65a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c7f23eb65a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sigurdurssigurg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sigurdurssigurg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sigur\u00f0ur Sk\u00fali"}, {"url": "https://medium.com/@sigurdurssigurg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "641 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F10517e62b9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&user=Sigur%C3%B0ur+Sk%C3%BAli&userId=10517e62b9e&source=post_page-10517e62b9e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F23e6439fc768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-your-own-alexa-entity-extraction-8c7f23eb65a&newsletterV3=10517e62b9e&newsletterV3Id=23e6439fc768&user=Sigur%C3%B0ur+Sk%C3%BAli&userId=10517e62b9e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}