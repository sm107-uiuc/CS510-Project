{"url": "https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9", "time": 1683002603.387531, "path": "towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9/", "webpage": {"metadata": {"title": "2019 \u2014 Year of BERT and Transformer | by Manu Suryavansh | Towards Data Science", "h1": "2019 \u2014 Year of BERT and Transformer", "description": "The BERT paper was released in late 2018, after which in 2019 we saw a lot of activity in the field of Natural Language Processing (NLP) and Natural Language Understanding (NLU) from BERT being used\u2026"}, "outgoing_paragraph_urls": [{"url": "https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6", "anchor_text": "project", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Sesame_Street", "anchor_text": "Sesame street", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "ELMO paper", "paragraph_index": 0}, {"url": "https://allenai.org/", "anchor_text": "Allen Institute of AI", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2Vec", "paragraph_index": 1}, {"url": "https://fasttext.cc/", "anchor_text": "FastText", "paragraph_index": 1}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove", "paragraph_index": 1}, {"url": "https://allennlp.org/elmo", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md", "anchor_text": "AllenNLP", "paragraph_index": 2}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "Tensorflow hub", "paragraph_index": 2}, {"url": "https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md", "anchor_text": "Flair", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "ULMFiT", "paragraph_index": 3}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "here", "paragraph_index": 5}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://github.com/google-research/bert", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://tfhub.dev/google/collections/bert/1", "anchor_text": "module", "paragraph_index": 5}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "search results", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL", "paragraph_index": 6}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1901.11117", "anchor_text": "Evolved Transformer", "paragraph_index": 7}, {"url": "https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html", "anchor_text": "Evolved Transformer", "paragraph_index": 7}, {"url": "https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html", "anchor_text": "evolution-based", "paragraph_index": 7}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2", "paragraph_index": 8}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 8}, {"url": "https://talktotransformer.com/", "anchor_text": "https://talktotransformer.com/", "paragraph_index": 8}, {"url": "https://transformer.huggingface.co/", "anchor_text": "https://transformer.huggingface.co/", "paragraph_index": 8}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1904.09223", "anchor_text": "ERNIE", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1907.12412", "anchor_text": "ERNIE 2", "paragraph_index": 9}, {"url": "https://github.com/PaddlePaddle/ERNIE", "anchor_text": "Github repo.", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 10}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "Salesforce CTRL", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT", "paragraph_index": 12}, {"url": "https://github.com/google-research/ALBERT", "anchor_text": "ALBERT repo", "paragraph_index": 12}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGlue", "paragraph_index": 14}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD", "paragraph_index": 15}, {"url": "https://leaderboard.allenai.org/orb/submissions/public", "anchor_text": "ORB", "paragraph_index": 16}, {"url": "https://leaderboard.allenai.org/open_book_qa/submissions/public", "anchor_text": "OpenBookQA", "paragraph_index": 16}, {"url": "https://leaderboard.allenai.org/", "anchor_text": "https://leaderboard.allenai.org/", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT", "paragraph_index": 17}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://github.com/dmis-lab/biobert", "anchor_text": "BioBERT", "paragraph_index": 19}, {"url": "https://dmis.korea.ac.kr/", "anchor_text": "DMIS lab", "paragraph_index": 19}, {"url": "https://camembert-model.fr/", "anchor_text": "CamemBERT", "paragraph_index": 20}, {"url": "https://www.graphcore.ai/posts/new-graphcore-ipu-benchmarks", "anchor_text": "Graphcore", "paragraph_index": 21}, {"url": "https://github.com/graphcore/examples/tree/master/applications/popart/bert", "anchor_text": "code", "paragraph_index": 21}, {"url": "https://habana.ai/habana-labs-goya-delivers-inferencing-on-bert/", "anchor_text": "Habana", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1904.01766", "anchor_text": "VideoBERT", "paragraph_index": 22}, {"url": "https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html", "anchor_text": "VideoBERT", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1908.02265", "anchor_text": "ViLBERT", "paragraph_index": 23}, {"url": "https://spacy.io/", "anchor_text": "Spacy", "paragraph_index": 25}, {"url": "https://explosion.ai/blog/spacy-transformers", "anchor_text": "Recently", "paragraph_index": 25}, {"url": "https://github.com/explosion/spacy-transformers", "anchor_text": "spacy-transformers", "paragraph_index": 25}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace Transformers", "paragraph_index": 26}, {"url": "https://github.com/huggingface/swift-coreml-transformers", "anchor_text": "https://github.com/huggingface/swift-coreml-transformers", "paragraph_index": 26}, {"url": "https://allennlp.org/", "anchor_text": "AllenNLP", "paragraph_index": 27}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair", "paragraph_index": 28}, {"url": "https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md", "anchor_text": "BERT, ELMO, XLNET", "paragraph_index": 28}, {"url": "https://gluon-nlp.mxnet.io/", "anchor_text": "GluonNLP", "paragraph_index": 29}, {"url": "https://mxnet.apache.org/", "anchor_text": "Apache MXNet", "paragraph_index": 29}, {"url": "https://gluon-nlp.mxnet.io/examples/index.html", "anchor_text": "examples/tutorials", "paragraph_index": 29}, {"url": "https://twitter.com/MSuryavansh", "anchor_text": "https://twitter.com/MSuryavansh", "paragraph_index": 30}], "all_paragraphs": ["The BERT paper was released in late 2018, after which in 2019 we saw a lot of activity in the field of Natural Language Processing (NLP) and Natural Language Understanding (NLU) from BERT being used in production for search at Google to several other architectures which showed even better results than BERT. BERT showed excellent results on multiple NLP tasks like Sentiment analysis, Question Answering, Sentence similarity, etc. I worked on a project using the BERT pre-trained weights in January 2019 so it might be because I was working in this area but everywhere I would see some reference to BERT \u2014 like medium articles, BERT on iOS, hardware acceleration for BERT, Kaggle competitions, bigger models inspired by BERT, etc. I also think the Sesame street name had something to do with the popularity of BERT, although the credit to starting the name trend goes to the ELMO paper (Deep contextualized word representations) from the Allen Institute of AI. BERT uses a bi-directional Transformer and a lot of other architectures which came after BERT also used some form of Transformer based on the paper \u201cAttention is all you need\u201d.", "Word2Vec model which was released in 2013 is still quite popular and it\u2019s often the first thing to try in any NLP problem. FastText and Glove embeddings are also very useful.", "ELMO\u2014 Elmo uses a bi-directional LSTM. The pre-trained Elmo weights can be downloaded from here. To use Elmo embeddings one can use the AllenNLP library, Tensorflow hub or the Flair library.", "ULMFiT \u2014 The ULMFiT paper by Jeremy Howard and Sebastian ruder describes techniques to fine tune a language model for specific tasks, it uses LSTM\u2019s. You can find how to use ULMFiT using fastai here.", "The above two papers came before BERT and didn\u2019t use transformer-based architectures.", "BERT \u2014 The original paper is here, there is also a very good tutorial with illustrations by Jay Alammar here. The pre-trained weight can be downloaded from official Github repo here. BERT is also available as a Tensorflow hub module. There are various other libraries which also make it easy to use the pre-trained embedding to finetune them, they are mentioned in this post later. The below timeline some of the major paper which came after all of them in 2019. Google even started using BERT in production for improving search results.", "Transformer-XL \u2014 Transformer-XL released in Jan 2019 improves on the transformer by using a architecture which allows learning beyond a fixed length context.", "Evolved Transformer \u2014 Around the same time as Transformer-XL, the Evolved Transformer was released, which is a Transformer architecture developed by conducting an evolution-based neural architecture search (NAS).", "GPT-2 \u2014 After BERT, I think the other project which got the most amount of news coverage was GPT-2 from OpenAI due to its ability to generate almost human-like sentences and also the initial decision from OpenAI to not release the largest model due to concerns regarding the model being used to create fake news, etc. They released the largest model after almost 10 months. You can play with the model at https://talktotransformer.com/ and https://transformer.huggingface.co/ by Huggingface. I think if it different name even a Sesame Street character it would have been even more popular :)", "ERNIE and ERNIE 2 \u2014 Currently ERNIE 2.0 is at #1 position on the GLUE leaderboard. Github repo.", "RoBERTa \u2014 This paper for FAIR measures the impact of various hyper-parameters of BERT and shows that the original BERT models were under-trained and with more training/tuning it can outperform the initial results. Currently, the results from RoBERTa are at#8 on the GLUE leaderboard!", "Salesforce CTRL \u2014 CTRL model has 1.6 Billion parameters and provides ways to control the generation of artificial text.", "ALBERT \u2014 This paper describes parameter reduction techniques to lower memory reduction and increase the training speed of BERT models. ALBERT repo has the pre-trained weights. ALBERT base model has 12M parameters whereas the BERT base model has 110M parameters!", "One of the ways to evaluate these language models is the Glue Benchmark which consists of evaluating the model various NLP tasks like classification, QA, etc. At the time of its release, BERT was at the top of the table, but within just 1 year it has moved to position 19 (as of 2 Jan 2020).", "There is now a SuperGlue benchmark also which consists of more difficult language understanding tasks.", "For evaluating question answering system SQuAD ( Stanford Question Answering Dataset) is commonly used, BERT and later transformer-based models are at top here also:", "There are various other leaderboards like ORB, OpenBookQA etc on https://leaderboard.allenai.org/ and in most leaderboards there is some reference to BERT!", "DistilBERT \u2014 Smaller BERT using model distillation from Huggingface, blog post here.", "Megatron-LM \u2014 Project to efficiently train very large language models from Nvidia.", "BioBERT \u2014 BERT based model trained on the biomedical domain by DMIS lab.", "CamemBERT \u2014 Language model for french language based on RoBERTa architecture from Facebook.", "BERT Hardware Acceleration \u2014 Google uses TPU to train large BERT models faster, other companies like Graphcore (code) and Habana have also shows how their custom SoC\u2019s can accelerate BERT inference and training.", "VideoBERT \u2014 VideoBERT is an example cross-modal (or multi-modal) deep learning, where the model learns from video in a self-supervised manner.", "ViLBERT \u2014 Vision-and-Language BERT model for learning joint visual and language representation.", "Below are some of the libraries which I think are must know if one is working in the area of NLP \u2014", "Spacy \u2014 Spacy is a popular and fast library for various NLP tasks like tokenization, POS (Part of Speech), etc. It also comes with pre-trained models for Named Entity Recognition (NER)etc. Recently it added support for transformer-based language models like BERT through spacy-transformers, this library uses the Huggingface transformer library.", "HuggingFace Transformers \u2014 It was one of the first libraries to provide a Pytorch implementation of BERT and originally it was called \u201c PyTorch-pretrained-bert\u201d. Later they added more models like GPT-2, XLNET, etc and now the library is called just \u201ctransformers\u201d. In less than one year it has become one of the most popular NLP libraries and makes it very easy to use BERT and other models. Using another of their libraries ( https://github.com/huggingface/swift-coreml-transformers) you can use BERT , GPT-2 etc on iOS!", "AllenNLP \u2014 This is NLP library from Allen Institute of AI, built on top of PyTorch.", "Flair \u2014 Also an NLP library which comes with models for NER, POS, etc, also supports BERT, ELMO, XLNET etc embeddings.", "GluonNLP \u2014 Its the NLP toolkit on top of Apache MXNet and was one of the first libraries to include pre-trained BERT embeddings. It has a lot of useful examples/tutorials.", "Hopefully, in 2020 there will be even more innovations and breakthroughs in the area of NLP. To learn more about NLP and AI you can follow me on twitter \u2014 https://twitter.com/MSuryavansh", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff200b53d05b9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://msuryavanshi.medium.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": ""}, {"url": "https://msuryavanshi.medium.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Manu Suryavansh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e8e94dc49d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&user=Manu+Suryavansh&userId=1e8e94dc49d2&source=post_page-1e8e94dc49d2----f200b53d05b9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6", "anchor_text": "project"}, {"url": "https://en.wikipedia.org/wiki/Sesame_Street", "anchor_text": "Sesame street"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "ELMO paper"}, {"url": "https://allenai.org/", "anchor_text": "Allen Institute of AI"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2Vec"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove"}, {"url": "https://allennlp.org/elmo", "anchor_text": "here"}, {"url": "https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md", "anchor_text": "AllenNLP"}, {"url": "https://tfhub.dev/google/elmo/3", "anchor_text": "Tensorflow hub"}, {"url": "https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md", "anchor_text": "Flair"}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "ULMFiT"}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "here"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here"}, {"url": "https://github.com/google-research/bert", "anchor_text": "here"}, {"url": "https://tfhub.dev/google/collections/bert/1", "anchor_text": "module"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "search results"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL"}, {"url": "https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1901.11117", "anchor_text": "Evolved Transformer"}, {"url": "https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html", "anchor_text": "Evolved Transformer"}, {"url": "https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html", "anchor_text": "evolution-based"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://talktotransformer.com/", "anchor_text": "https://talktotransformer.com/"}, {"url": "https://transformer.huggingface.co/", "anchor_text": "https://transformer.huggingface.co/"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "https://arxiv.org/abs/1904.09223", "anchor_text": "ERNIE"}, {"url": "https://arxiv.org/abs/1907.12412", "anchor_text": "ERNIE 2"}, {"url": "https://github.com/PaddlePaddle/ERNIE", "anchor_text": "Github repo."}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNET"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/", "anchor_text": "Salesforce CTRL"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT"}, {"url": "https://github.com/google-research/ALBERT", "anchor_text": "ALBERT repo"}, {"url": "https://arxiv.org/abs/1910.13034", "anchor_text": "Big BIRD"}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "Source"}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGlue"}, {"url": "https://super.gluebenchmark.com/leaderboard", "anchor_text": "Source"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "Source"}, {"url": "https://leaderboard.allenai.org/orb/submissions/public", "anchor_text": "ORB"}, {"url": "https://leaderboard.allenai.org/open_book_qa/submissions/public", "anchor_text": "OpenBookQA"}, {"url": "https://leaderboard.allenai.org/", "anchor_text": "https://leaderboard.allenai.org/"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT"}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "here"}, {"url": "https://github.com/dmis-lab/biobert", "anchor_text": "BioBERT"}, {"url": "https://dmis.korea.ac.kr/", "anchor_text": "DMIS lab"}, {"url": "https://deepset.ai/german-bert", "anchor_text": "German Language BERT"}, {"url": "https://camembert-model.fr/", "anchor_text": "CamemBERT"}, {"url": "https://www.graphcore.ai/posts/new-graphcore-ipu-benchmarks", "anchor_text": "Graphcore"}, {"url": "https://github.com/graphcore/examples/tree/master/applications/popart/bert", "anchor_text": "code"}, {"url": "https://habana.ai/habana-labs-goya-delivers-inferencing-on-bert/", "anchor_text": "Habana"}, {"url": "https://arxiv.org/abs/1904.01766", "anchor_text": "VideoBERT"}, {"url": "https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html", "anchor_text": "VideoBERT"}, {"url": "https://arxiv.org/abs/1908.02265", "anchor_text": "ViLBERT"}, {"url": "https://spacy.io/", "anchor_text": "Spacy"}, {"url": "https://explosion.ai/blog/spacy-transformers", "anchor_text": "Recently"}, {"url": "https://github.com/explosion/spacy-transformers", "anchor_text": "spacy-transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace Transformers"}, {"url": "https://github.com/huggingface/swift-coreml-transformers", "anchor_text": "https://github.com/huggingface/swift-coreml-transformers"}, {"url": "https://allennlp.org/", "anchor_text": "AllenNLP"}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair"}, {"url": "https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md", "anchor_text": "BERT, ELMO, XLNET"}, {"url": "https://gluon-nlp.mxnet.io/", "anchor_text": "GluonNLP"}, {"url": "https://mxnet.apache.org/", "anchor_text": "Apache MXNet"}, {"url": "https://gluon-nlp.mxnet.io/examples/index.html", "anchor_text": "examples/tutorials"}, {"url": "https://twitter.com/MSuryavansh", "anchor_text": "https://twitter.com/MSuryavansh"}, {"url": "https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6", "anchor_text": "Using Transfer Learning for NLP with Small DataAchieve high text classification accuracy with smaller datasets.blog.insightdatascience.com"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "Understanding searches better than ever beforeIf there's one thing I've learned over the 15 years working on Google Search, it's that people's curiosity is endless\u2026www.blog.google"}, {"url": "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "anchor_text": "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language ProcessingOne of the biggest challenges in natural language processing (NLP) is the shortage of training data. Because NLP is a\u2026ai.googleblog.com"}, {"url": "https://blog.rasa.com/compressing-bert-for-faster-prediction-2/", "anchor_text": "Learn how to make BERT smaller and fasterIn this blog post, we discuss ways to make huge models like BERT smaller and faster.blog.rasa.com"}, {"url": "https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html", "anchor_text": "ALBERT: A Lite BERT for Self-Supervised Learning of Language RepresentationsEver since the advent of BERT a year ago, natural language research has embraced a new paradigm, leveraging large\u2026ai.googleblog.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f200b53d05b9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f200b53d05b9---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f200b53d05b9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f200b53d05b9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&user=Manu+Suryavansh&userId=1e8e94dc49d2&source=-----f200b53d05b9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&user=Manu+Suryavansh&userId=1e8e94dc49d2&source=-----f200b53d05b9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff200b53d05b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f200b53d05b9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f200b53d05b9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f200b53d05b9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f200b53d05b9--------------------------------", "anchor_text": ""}, {"url": "https://msuryavanshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://msuryavanshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manu Suryavansh"}, {"url": "https://msuryavanshi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "390 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e8e94dc49d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&user=Manu+Suryavansh&userId=1e8e94dc49d2&source=post_page-1e8e94dc49d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F73857d7357dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2019-year-of-bert-and-transformer-f200b53d05b9&newsletterV3=1e8e94dc49d2&newsletterV3Id=73857d7357dd&user=Manu+Suryavansh&userId=1e8e94dc49d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}