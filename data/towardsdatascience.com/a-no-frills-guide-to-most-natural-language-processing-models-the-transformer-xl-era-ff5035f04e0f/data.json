{"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f", "time": 1683004681.005155, "path": "towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f/", "webpage": {"metadata": {"title": "A no-frills guide to most Natural Language Processing Models \u2014 The Transformer (XL) Era | by Ilias Miraoui | Towards Data Science", "h1": "A no-frills guide to most Natural Language Processing Models \u2014 The Transformer (XL) Era", "description": "LSTMs were immensely popular but they also had a lot of constraints. They are computation-heavy and tend to have difficulties maintaining long-term dependencies (despite their name). In 2018, Google\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention Is All You Need\u201d", "paragraph_index": 0}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/4", "anchor_text": "DANs", "paragraph_index": 3}, {"url": "https://tfhub.dev/google/universal-sentence-encoder-large/5", "anchor_text": "Transformers", "paragraph_index": 3}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub", "paragraph_index": 3}, {"url": "https://skylion007.github.io/OpenWebTextCorpus/", "anchor_text": "their versions", "paragraph_index": 8}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "readily accessible", "paragraph_index": 10}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners", "paragraph_index": 14}, {"url": "https://tfhub.dev/google/collections/bert/1", "anchor_text": "BERT", "paragraph_index": 19}, {"url": "https://tfhub.dev/google/albert_large/3", "anchor_text": "ALBERT", "paragraph_index": 19}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub", "paragraph_index": 19}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL", "paragraph_index": 25}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s Transformers library", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "paragraph_index": 32}, {"url": "https://github.com/microsoft/DeepSpeed", "anchor_text": "DeepSpeed", "paragraph_index": 34}, {"url": "https://www.microsoft.com/en-us/research/publication/zero-memory-optimization-towards-training-a-trillion-parameter-models/", "anchor_text": "ZeRO", "paragraph_index": 34}, {"url": "https://developer.msturing.org/", "anchor_text": "Microsoft\u2019s API", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://transformer.huggingface.co/", "anchor_text": "\u201cWrite With Transformer\u201d", "paragraph_index": 40}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/", "paragraph_index": 43}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/", "paragraph_index": 43}], "all_paragraphs": ["LSTMs were immensely popular but they also had a lot of constraints. They are computation-heavy and tend to have difficulties maintaining long-term dependencies (despite their name). In 2018, Google published a paper \u201cAttention Is All You Need\u201d that introduced transformers, showing that we could overcome a lot of the flaws of Recurrent Neural Networks and revolutionizing the field of language models.", "While this may still contain some jargon unexplained, information about the various concepts should be easily accessible via other posts.", "Google released the Universal Sentence Encoder and aimed to provide a sentence embedding that would be particularly adapted for transfer learning and could be used for a wide variety of tasks (hence \u201cuniversal\u201d).", "The Universal Sentence Encoder was initially released in two versions: one leveraging Deep Averaging Networks (DANs), and the other one using Transformers. Since the initial implementation, Google has released numerous versions based on both DANs or Transformers and made them very easily accessible on their Tensorflow Hub platform. All the models take words, sentences or groups of sentences and output a vector of 512 dimensions.", "To train the model, Google used a wide variety of data sources and tasks but the main one revolves around identifying sentences that are similar (what is called \u201csemantic textual similarity\u201d) on the Stanford Natural Language Inference corpus . This language model performs particularly well on this task.", "Advantages:- The model performs particularly well on sentence similarity- Performs relatively well on a wide variety of tasks \u201cout of the box\u201d- Transforming your input into embeddings is very quick (much faster than for most LSTM models)", "Disadvantages:- The model can embed words or sentences. However, it has been trained on sentences and so should be used with a bit more care when the inputs are not proper \u201csentences\u201d- Its particular training (focused mainly on sentence similarity) positions it uniquely but also makes it that the USE does not perform as well as other models on various tasks such as text generations", "Initially, OpenAI refused to release their model when they developed Open-GPT2 as they deemed it too dangerous and were worried that people might use it maliciously.", "The model has ~1.5 billion parameters and was trained on OpenAI\u2019s WebText dataset which contains more than 8 million documents and ~40 GB of text data (some people tried to replicate and open-sourced their versions).", "Open-GPT2 consists of transformer decoder blocks with multi-head attention and is trained on language modeling, predicting the next token one at a time. Because of this training, it is particularly well adapted for text generation.", "OpenAI eventually released the model to the public and it is now readily accessible.", "The model does require significant disk space, compute and memory resources.", "Advantages:- Excellent at Text Generation because of the dataset and the left-to-right language modeling training method (clearly outperforms BERT and other similar models that learned from masked language models)", "Disadvantages:- Used only for text generation: the non-profit organization sees potential use-cases for the model in grammar assistance, autocompletion, creative writing and the creation of games/chatbots but this remains limited- GPT-2 takes more than 5Gb and requires very high compute and memory requirements (Distilled models help but are still a work in progress)", "Radford, et al. Language Models are Unsupervised Multitask Learners (2019)", "Google released BERT at the end of 2018 and attracted a lot of attention. Leveraging a multi-layer bidirectional transformer architecture (i.e. with an encoder and a decoder), the model set new records on a wide range of tasks.", "BERT is mainly trained on two tasks: - Masked language model: the model attempts to predict~15% of the words of a sentence that are randomly masked- Identifying the next sentence", "BERT is very memory-heavy and is relatively slow. The base and the large version have 10 and 340 million parameters respectively.", "In 2019, another research team released ALBERT, introducing parameter-reduction techniques that allowed to decreased memory requirements and improve training speed while achieving significantly better results than BERT, being State Of The Art (SOTA) on some of the main benchmarks. ALBERT has 18x times fewer parameters than BERT-large and the training is 1.7x faster.", "Both BERT and ALBERT are readily accessible and you can easily create your embeddings through Tensorflow Hub.", "Advantages:- Excellent performance on a wide variety of tasks (Classification, Sentence Similarity, Name Entity Recognition, Question Answering)- Models available in over 100 different languages on Tensorflow Hub", "Disadvantages:- Slow: High memory and time requirements may be a hindrance for many tasks, even using ALBERT- Poor performance on text generation because of it\u2019s bi-directional training", "Devlin, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018), ACL", "Lan, et al. ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations (2019)", "A team of researchers from Google Brain and Carnegie Mellon University worked on BERT\u2019s main flaws and leveraged the new Transformer-XL architecture to release XLNet, achieving SOTA on 18 NLP tasks.", "XLNet is based on Transformer-XL. The latter improves on the Transformer architecture by being a lot faster (1800+ according to Google) and by adding segment-level recurrence as well as relative positional encoding. It is able to process much bigger sentences and to retain longer-term dependencies than the standard transformer architecture. Transformer-XL can learn dependencies ~80% and ~450% longer than RNNs and standard Transformers respectively.", "(AL)BERT displayed two big problems:1. The masked tokens appear in training but not in fine-tuning, inducing a train-test skew2. The masked tokens are predicted in parallel and so dependencies between them are often not handled properly.", "By using \u201cpermutation language modeling\u201d, XLNet is able to overcome this. Instead of predicting the next tokens or the previous tokens, XLNet predicts tokens in random order. Given that Transformer-XL allows it to keep the positional embeddings consistent and that the model separates the positioning from the remaining of the embedding and keeps it available when trying to predict the masked tokens, the model can still process the tokens with the required order.", "Thanks to the Transformer-XL architecture, previous segments are cached and frozen and make permutation language modeling possible without needing to know previous permutation orders.", "With all its improvements, XLNet effectively replaced (AL)BERT as the new reference in language models. The model can be easily used and tweaked on both Tensorflow 2.0 and PyTorch using Hugging Face\u2019s Transformers library.", "Advantages:- Better performance than (AL)BERT on almost any tasks- A lot faster to train and use than (AL)BERT- Decent in text generation thanks to its \u201cpermutation language modeling\u201d and its autoregressive approach", "Disadvantages:- The model still requires a lot of memory and compute resources- Model\u2019s availability is a work in progress", "Yang, et al. XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)", "In February 2020, Microsoft presented a generative language model with 17-billion parameters. The model has 78 Transformer layers and 28 attention heads.", "A lot of the model\u2019s achievement stems from the parallelization Microsoft achieved to train Turing-NLG across GPUs. The authors took great advantage of PyTorch and recent Microsoft libraries/publications such as DeepSpeed and ZeRO that significantly improve parallel training of deep learning models with more than a billion parameters.", "For now, the authors have remained mostly silent on a lot of the details of the models, mentioning the different layer types and that it was trained in a multi-task fashion. Nonetheless, a model of that size marks a significant breakthrough in massive deep learning language models and may be an indication of future trends.", "The model was released very recently but it is possible to generate embeddings with this model using Microsoft\u2019s API.", "Advantages:- Achieves SOTA in numerous tasks and is very efficient in text generation", "Disadvantages:- Relatively difficult to access, not possible to train your own model- Not a lot of details have been disclosed so far", "Over the last two years, transformers have completely revolutionized the world of NLP. The field has gone a long way from the static embeddings (I wrote an overview of those here) and the first contextualized representations (I wrote an(other) overview of those here). Transformer, Transformer-XL, and advances in distributed computing have allowed for the development of models with significantly more parameters that are able to retain an increasing amount of long-term dependencies. Some of the most recent models have been able to beat humans in a number of NLP tasks and the text they can generate has become harder and harder to distinguish from a human-written one. New models that become benchmarks seem to appear every 6 months and it has never been more exciting than now to be in NLP.", "If you want to have fun with generative modeling and create cool texts, I highly recommend Hugging Face\u2019s \u201cWrite With Transformer\u201d.", "PS: I am currently a Master of Engineering Student at Berkeley and I am still learning about all of this. If there is anything that stands to be corrected or that is not clear, please let me know. You can also email me here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Enthusiast | Master of Engineering Student @ UC Berkeley | https://imiraoui.github.io/ | https://www.linkedin.com/in/iliasmiraoui/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fff5035f04e0f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Ilias Miraoui"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a----ff5035f04e0f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention Is All You Need\u201d"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/4", "anchor_text": "DANs"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder-large/5", "anchor_text": "Transformers"}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub"}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "Universal Sentence Encoder"}, {"url": "https://skylion007.github.io/OpenWebTextCorpus/", "anchor_text": "their versions"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "http://jalammar.github.io/illustrated-gpt2/"}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "readily accessible"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://tfhub.dev/google/collections/bert/1", "anchor_text": "BERT"}, {"url": "https://tfhub.dev/google/albert_large/3", "anchor_text": "ALBERT"}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub"}, {"url": "https://tfhub.dev/", "anchor_text": "Tensorflow Hub"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s Transformers library"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"url": "https://github.com/microsoft/DeepSpeed", "anchor_text": "DeepSpeed"}, {"url": "https://www.microsoft.com/en-us/research/publication/zero-memory-optimization-towards-training-a-trillion-parameter-models/", "anchor_text": "ZeRO"}, {"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/"}, {"url": "https://developer.msturing.org/", "anchor_text": "Microsoft\u2019s API"}, {"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "Turing-NLG: A 17-billion-parameter language model by Microsoft"}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687", "anchor_text": "here"}, {"url": "https://transformer.huggingface.co/", "anchor_text": "\u201cWrite With Transformer\u201d"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ff5035f04e0f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ff5035f04e0f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----ff5035f04e0f---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ff5035f04e0f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ff5035f04e0f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----ff5035f04e0f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----ff5035f04e0f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fff5035f04e0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ff5035f04e0f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ff5035f04e0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ilias Miraoui"}, {"url": "https://medium.com/@ilias.miraoui/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/"}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6c78c71c0a4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f&newsletterV3=de0de51bac0a&newsletterV3Id=6c78c71c0a4d&user=Ilias+Miraoui&userId=de0de51bac0a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}