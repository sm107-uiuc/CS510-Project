{"url": "https://towardsdatascience.com/seq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c", "time": 1683011678.891675, "path": "towardsdatascience.com/seq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c/", "webpage": {"metadata": {"title": "Seq2Seq modelling for English to French Translations | by Daniel Chow | Towards Data Science", "h1": "Seq2Seq modelling for English to French Translations", "description": "During quarantine I tried to learn how to speak French to keep myself occupied. This attempt was not successful, so instead I built a neural network to try and learn it for me. To do this I used a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.isi.edu/natural-language/download/hansard/", "anchor_text": "here", "paragraph_index": 1}, {"url": "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf", "anchor_text": "difficulties", "paragraph_index": 9}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "https://nlp.stanford.edu/pubs/glove.pdf", "paragraph_index": 30}, {"url": "https://github.com/danch12/seq2seq_translation/tree/master", "anchor_text": "here", "paragraph_index": 37}], "all_paragraphs": ["During quarantine I tried to learn how to speak French to keep myself occupied. This attempt was not successful, so instead I built a neural network to try and learn it for me.", "To do this I used a seq2seq recurrent neural network with an attention layer, trained on the House and Senate Debates of the 36th parliament of Canada (found here). Attention based seq2seq models are used commonly throughout a wide range of tasks, in fact if you have ever used Siri or Google Translate chances are you have benefited from this kind of model.", "But before I get into how we can build one ourselves, I thought it would be a good idea to quickly touch on some of the theory behind how a seq2seq model works- focusing on a couple of the key elements that allow these models to perform so well.", "A seq2seq model\u2019s goal in regard to NLP is to essentially compute the probability of an output sequence of words when considering an input sequence of words. This is useful for machine translation as we may have a bunch of potential translations and we want to learn which sequence is best \u2014 eg the dog is big > the big is dog. Or we may need to choose a word that\u2019s most appropriate for a given sequence \u2014 eg running late for work > jogging late for work.", "For machine translation we will need to encode a sequence of words into a vector that is easily digestible by the model, and then have the model decode this vector into our target language.We could use one model to do both the encoding and decoding but problems arise when you consider the input length may not match the output length, for example \u201cmy mum loves France\u201d translates to \u201cma maman aime la france\u201d \u2014 needing an extra word. These output sequence lengths are not necessarily known beforehand.", "This leads us onto seq2seq models. The idea being that you get one model \u2014 the encoder \u2014 to take a sequence of words and turn them into an encoder vector which represents all the information in the sentence you want to translate. Then you get a decoder that takes this vector and outputs the translated words using a softmax function.", "First the encoder takes an input sequence and returns its own hidden state.When computing the encoders hidden states (ht) we only need to consider the current input (xt) and the previous steps hidden state (ht-1) so the equation looks something like this -", "Then the decoder takes the last hidden vector of the encoder (c=ht), the previous hidden state (ht-1) and the previous predicted output word (yt-1). It may seem strange that we use both the previous predicted output word considering we calculate it from the last hidden vector of the encoder as well but using it allows the softmax to have an impact on current prediction (yt). Additionally rather than a probability distribution yt-1 actually gives a definitive answer on what the previous word was which helps prevent the model from repeating words. So overall our formula for the decoder hidden states looks something like this -", "Now all we need to do is apply a softmax function at the end as in essence this will still be a multiclass classification problem but now our classes are each word in the target language at the next time step.", "So above we have laid out the structure for a seq2seq model. But you may be wondering what the encoding and decoding layers are made of. The encoder/ decoder layers can be made of basic RNN cells but these cells cannot look for context in long sequences. Therefore if it was trying to predict the next word based on previous ones, it may handle something like \u201cthe man sat on the chair\u201d quite well because sat and chair don\u2019t have a big gap between them. However for something like \u201cIn the middle of cooking a steak\u2026. Then I sat down and ate the steak\u201d it may not perform as well because there is a much bigger gap between the word we want to predict (steak) and the context for this word (For details on why a RNN cell can perform badly on long sequences Bengio et al have great paper detailing the difficulties)", "This is where LSTM cells come in. Rather than just combining the hidden state from the previous time step with the current inputs, there are four layers that come into play. The basic idea is that these cells can keep information inside the cell that will be useful in future decisions.", "At each time step we will have the following things to modify-", "These first three equations are all very similar. sigmoid functions so their outputs will lie between 0 and 1, which means their output vectors can define how much of another vector can pass through the first one. With xt being the current input, ht-1 being the previous time steps hidden layer and W U being the weights.", "And then we have the new memory cell which is identical to a standard RNN layer except that we control its output with the output of the input vector. We can think of this layer as the candidate hidden state.", "And then we combine them all together with the final memory cell and final hidden state.", "The input gate determines how much we care about the current word vector. In other words it decides how much of the newly computed state for the current input you want to let through. For example if we were trying to predict the sentiment of a movie review but the current vector was talking about what the person had for dinner then the weights would probably be close to zero and the current vector would not pass through the gate.", "The output gate separates out what matters for a current prediction vs what matters for the future, so it doesn\u2019t have an effect on the output of the cell at the current time step but it will change its hidden state. This is useful as sometimes the current step may not be useful to the output right now but could have some benefit later on down the line.", "The forget gate decides what we should forget from the the long term state (h(t-1)) using the current steps inputs. So going back to the previous example if we were trying to predict the sentiment of a movie review and the previous time step was just all about the plot but the current time step was something like \u201cI hated it\u201d you would probably want to forget the previous information and just focus on the current time step. Important to note as all these gates are vectors we shouldn\u2019t think of them as absolutes but instead will forget certain elements of a long hidden unit.", "All the above layers are single layer neural nets which we then put together in the final memory cell (ct) like so -", "This equation makes intuitive sense as our forget gate (ft) decides how much of the previous memory we want to forget so we apply it to the previous time step. Whilst we want to combine the input gate (it) and the standard RNN cell (c\u0303t) together for the current time step.", "Finally we use this memory cell and the output gate to determine the hidden state ht at time t -", "And that\u2019s basically it! For a more detailed view on how LSTMs work here are a couple of great resources -", "Before we get into the code I\u2019m just going to quickly discuss the attention layer as it plays a big part in the performance of this model.", "Although the LSTM cells help a lot with the context of a word being far away from a word the model is trying to predict, it does not completely alleviate the problem. The attention layer allows the decoder to focus on the parts of the sequence needed for getting the output correct at that time step therefore shortening the path between the input word to its translation, thereby alleviating some of the memory limitations that LSTMs can have. It does this by providing a way of scoring tokens in the target sequence against all the tokens on the source sequence and using this to change the input to the decoder sequence. The scores are then fed into a softmax activation to make a attention distribution.", "How does attention fit in with the rest of the model architecture? Instead of only sending the encoder\u2019s final states, we send all of its outputs to the decoder. At each time step the decoder\u2019s attention cell computes a weighted sum of all of these encoder outputs and using a softmax activation function determines what words it will focus on in each step. For a more detailed explanation of attention mechanisms see here -", "Now that we have had a quick run through of the theory we can jump into the actual coding, I won\u2019t be covering all the code in the project but will link the github repo with everything at the end. Additionally if you don\u2019t have GPU access on your computer I would recommend using something like Google Colab so you can use their free GPUs as it will speed up training times massively.", "First step is to clean the data and make it into a useable format.To do this we will need to remove all the weird stuff like white spaces, punctuation and unprintable symbols.", "Next we need to limit the length of each of the sentences to make training times quicker. Additionally for the target sequence we will need to add a start character (<s>) and an finishing character (</s>) to the beginning and end of each seqence. We do this so that when we want to translate unseen data we can feed in the start character to get the model running and also give the model a way of stopping when the translation is finished.Finally we will need to pad the english data so every sequence is the same size.", "The following step in our preparation is to create dictionaries for each of the languages, where each of words in the dictionary relates to a number that can be easily encoded for the model. Importantly we will add a <unk> symbol into the dictionaries which accounts for words that are outside the model\u2019s vocabulary.", "Lastly we have to actually encode our training and test data. At each time step we will feed the model both the input data and the target data at one step behind.", "So we could go from here straight into modelling, but there is one extra step we can take that can lead to great increases in model performance. I won\u2019t go into too much detail, but basically we can give words preset weightings within the model that reflect how similar the words are \u2014 eg king and queen are more similar than king and carrot. For a more detailed view on this concept here is the original paper \u2014 https://nlp.stanford.edu/pubs/glove.pdf", "Ok now that we have done all of the data preparation we can get onto creating the model. The encoder model is a two layer LSTM, with the decoder being a single layer LSTM and an attention layer added as well. This provided great performance with a validation accuracy of 0.96 after only a few epochs but in the future I would like to look into bidirectional LSTMs to increase performance.", "After training the initial model we then transfer the trained layers into an inference model so we can actually put it to use.", "Finally after creating our inference model we can use it to translate whatever english sentence we want! We do this by feeding in our english sentence into the inference encoder and then feeding in the encoded data plus the start token to the decoder. Following this we use a while loop to continue through the sentence until the model predicts the end token (</s>) or the max sentence length is reached.", "So how did the model do? I tested it out on a couple of sentences from the validation set below -", "Following this ,I wanted to try a sentence that was completely new to the model -", "All in all pretty great for only a couple of training epochs!", "In this article we have shown the power of seq2seq models in machine translation. However there is still more to do! In the future I would like to introduce beam search and bidirectional layers as they can increase performance even further. For the full code used in this project you can visit the github repo here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff6e4aa7aa02c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@danielchow_56180?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielchow_56180?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Daniel Chow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46256a5b2f01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&user=Daniel+Chow&userId=46256a5b2f01&source=post_page-46256a5b2f01----f6e4aa7aa02c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.isi.edu/natural-language/download/hansard/", "anchor_text": "here"}, {"url": "https://unsplash.com/@sanekovs?utm_source=medium&utm_medium=referral", "anchor_text": "Alex Ovs"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf", "anchor_text": "difficulties"}, {"url": "https://www.researchgate.net/publication/334360853_Sequence-to-Sequence_Natural_Language_to_Humanoid_Robot_Sign_Language", "anchor_text": "Link"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM NetworksPosted on August 27, 2015 Humans don't start their thinking from scratch every second. As you read this essay, you\u2026colah.github.io"}, {"url": "https://distill.pub/2016/augmented-rnns/", "anchor_text": "Attention and Augmented Recurrent Neural NetworksRecurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of\u2026distill.pub"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "https://nlp.stanford.edu/pubs/glove.pdf"}, {"url": "https://github.com/danch12/seq2seq_translation/tree/master", "anchor_text": "here"}, {"url": "https://medium.com/tag/seq2seq?source=post_page-----f6e4aa7aa02c---------------seq2seq-----------------", "anchor_text": "Seq2seq"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----f6e4aa7aa02c---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f6e4aa7aa02c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f6e4aa7aa02c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----f6e4aa7aa02c---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&user=Daniel+Chow&userId=46256a5b2f01&source=-----f6e4aa7aa02c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&user=Daniel+Chow&userId=46256a5b2f01&source=-----f6e4aa7aa02c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff6e4aa7aa02c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f6e4aa7aa02c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f6e4aa7aa02c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielchow_56180?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@danielchow_56180?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Chow"}, {"url": "https://medium.com/@danielchow_56180/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46256a5b2f01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&user=Daniel+Chow&userId=46256a5b2f01&source=post_page-46256a5b2f01--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F46256a5b2f01%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fseq2seq-modelling-for-english-to-french-translations-f6e4aa7aa02c&user=Daniel+Chow&userId=46256a5b2f01&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}