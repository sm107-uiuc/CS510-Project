{"url": "https://towardsdatascience.com/designing-a-feature-selection-pipeline-in-python-859fec4d1b12", "time": 1683003765.691669, "path": "towardsdatascience.com/designing-a-feature-selection-pipeline-in-python-859fec4d1b12/", "webpage": {"metadata": {"title": "Designing a Feature Selection Pipeline in Python | by Frank Ceballos | Towards Data Science", "h1": "Designing a Feature Selection Pipeline in Python", "description": "Materials and methods: Using Scikit-learn, we generate a Madelon-like data set for a classification task. The main components of our workflow can be summarized as follows: (1) Generate the data set\u2026"}, "outgoing_paragraph_urls": [{"url": "http://archive.ics.uci.edu/ml/datasets/madelon", "anchor_text": "Madelon", "paragraph_index": 1}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb?source=post_page---------------------------", "anchor_text": "article", "paragraph_index": 3}, {"url": "https://pandas.pydata.org/?source=post_page---------------------------", "anchor_text": "Pandas", "paragraph_index": 3}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn", "paragraph_index": 3}, {"url": "https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-", "anchor_text": "my GitHub", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0", "anchor_text": "Will Koehrsen", "paragraph_index": 3}, {"url": "https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://stackoverflow.com/questions/36200913/generate-n-random-numbers-from-a-skew-normal-distribution-using-numpy", "anchor_text": "StackOverFlow", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation matrix", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient", "paragraph_index": 41}, {"url": "http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/", "anchor_text": "BorutaPy", "paragraph_index": 44}, {"url": "https://explained.ai/rf-importance/", "anchor_text": "Terrence Parr", "paragraph_index": 44}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest Classifier", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees Classifier", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html", "anchor_text": "Gradient Boosting Classifier", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest Classifier", "paragraph_index": 61}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees Classifier", "paragraph_index": 61}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html", "anchor_text": "Gradient Boosting Classifier", "paragraph_index": 61}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html", "anchor_text": "RFECV", "paragraph_index": 61}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html", "anchor_text": "RFECV Sklearn", "paragraph_index": 62}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn", "paragraph_index": 73}], "all_paragraphs": ["Purpose: To design and develop a feature selection pipeline in Python.", "Materials and methods: Using Scikit-learn, we generate a Madelon-like data set for a classification task. The main components of our workflow can be summarized as follows: (1) Generate the data set (2) create training and test sets. (3) Feature selection algorithms are applied to reduce the number of features.", "Hardware: We train and evaluate our models on a workstation equipped with Inter(R)Core(TM) i7\u20138700 with 12 CPU @ 3.70 Ghz and NVIDIA GeForce RTX 2080.", "Note: In the case you\u2019re starting from scratch, I will advise you to follow this article and install all the necessary libraries. Finally, it will be assumed that the reader is familiar with Python, Pandas, and Scikit-learn. The whole contents of this article can be found on my GitHub. You\u2019re welcomed to fork it. I\u2019ve to mentioned that a few feature selection methods were inspired from Will Koehrsen feature selection article.", "Notation: Bold text will be use to represent Python objects such as lists, dictionaries, tuples, dataframes, or will be referring to a figure or a script. This notation will be used to represent classes and function parameters as well as Python packages.", "Feature selection is the process of identifying a representative subset of features from a larger cohort. One can either choose to manually select the features or apply one of the many automated methods. The difficulty in manually selecting features is that it requires expert knowledge about the data at hand. For example, in radiation oncology a radiation therapy plan (a 3D image where each voxel represents a feature) is reduced to a series of features that were manually crafted and are supported by decades of radiation therapy research and cooperative clinical investigations. Each of these features quantify specific characteristics of the radiation therapy plan that have been shown to correlate with patient\u2019s clinical outcomes. Thankfully, automated methods for feature selection have been developed and can be easily be used to alleviate this task; however, one must apply these automated methods with a grain of salt since they might lead to erroneous conclusions. From my experience, I would say that the best way to apply feature selection methods is by using domain knowledge along with automated methods to determine a representative subset of features.", "In the context of machine learning, the advantages that one gains from feature selection are numerous. For example, finding the most descriptive features reduces a model\u2019s complexity, makes it easier to find the best solution, and most importantly, it decreases the time it takes to train the model. Additionally, if you understand what the representative features mean, you will gain a deeper insight into the problem you're facing. In some instances, a slight performance boost can be gained. It must be noted that by removing features from your system your model\u2019s to performance might be slightly worse (since you\u2019re trying to make a prediction with less information).", "More often than not, we do not have spare time to conduct thorough studies about our data to determine what features to craft and how to use them. Faced with a data set of +1000s of features, the task of determining a representative subset of features might seem daunting but luckily redundancy is our data can be our friend. Typically, when faced with a data set with +1000s of features, you will notice that a large fraction of features are highly correlated (redundant), irrelevant to the outcome, or simply noise. Out of +1000s of features you start with, you might end up with 10 features that \u201cadequately\u201d represent the whole data set.", "In this article, we will discuss the following feature selection algorithms and their limitations:", "Before we dive into this article, let\u2019s install the necessary packages. We will assume that the reader haves Anaconda installed with the following packages: Numpy , Seaborn , Matplotlib , Pandas , and Sklearn.", "The only package that you might be missing is BorutaPy so let\u2019s take care of this. Open your terminal (if you\u2019re in a Mac) or Anaconda Prompt (if you\u2019re in a Windows machine). Activate your environment and run the following command:", "You will need to visit my GitHub here and download the tools folder. The entire contents of this article are in the file named FeatureSelectionPipe.py", "Place the tools folder in your working directory and you will be set.", "So before we begin, let\u2019s import all the packages.", "We will create various groups of synthetic features to explore the performance and the validity of different feature selection algorithms. First, we will generate a Madelon-like synthetic data set. The Madelon data set (which we won\u2019t use) is an artificial data set that contains 32 clusters placed on the vertices of a five-dimensional hyper-cube with sides of length 1. The clusters are randomly labeled 0 or 1 (2 classes). The Madelon-like data set that we will generate will contain 100 features, where 10 of them will be informative, 50 will be redundant (but informative), 25 of them will be repeated, and 15 will be useless since they will be filled with random noise. In total, we will have 1000 samples. To add a bit more noise to the data, we will randomly flip 5 percent of the labels. The second group of features will contain 3 critical important features. The critical important features by design should be challenging to select by automated feature selection algorithms. Finally, the last group of features will simply be the Madelon-like and the critical features added together into a bigger data set.", "We are now ready to create our Madelon-like data. In line 4\u20139 of Script 2a, we define the number of features we want from each type (informative, redundant, repeated, useless). After that, in line 12\u201316, we create our column labels. From line 19\u201323, we create our data as well as the labels. In line 27, we convert the numpy array into a pandas DataFrame.", "Next, we will create our critically important features. Here, we will make the assumption that after decades of theoretical, numerical, and empirical studies, these 3 features have been determined to be especially important to the classification process. To make this more clear, let me provide you with an example. In radiation oncology, the maximum radiation delivered to an organ can be correlated to the toxicity experienced by patients that undergo radiation therapy treatments. Therefore, is of critical importance that the maximum amount of radiation delivered to every organ be taken into account in order to ensure the safe treatment of patients. For the sake of illustration, let\u2019s assume that the maximum radiation that can be delivered to the bladder can be 100 before a patient starts showing symptoms of toxicity. So our data might look like this:", "Because doctors know that large amounts of radiation to the bladder can cause complications, most of the radiation therapy plans that are generated will meet the maximum radiation criteria establish. However, in Table 1, patient number 6 fails to meet the criteria of receiving less than 100 radiation to the bladder. In other words, 95% of the data meets the criteria and only 5% doesn\u2019t. Therefore, if you were to implement a method to automatically select features, is likely that such a method will fail to recognize that the maximum dose to the bladder is of critical importance if you don\u2019t take the proper precautions. Nonetheless, we know that such a feature is of significant importance and should always be taken into account for the safety of the patient. Hence, the motivation behind introducing critically important features into our data set to test the efficiency of the automated feature selection methods.", "To create the 3 critical important features, will draw from three different and Gaussian distributions each centered at a different value. The features will be generated with the following helper function I found in StackOverFlow:", "In lines 5\u201310 of Script 2c, we set the parameters for the critical features distribution. Then in line 13\u201325, we run a for loop that creates the three critical features which then get stored in a list (line 25). I will like to note that in line 15, we fix the seed so that we can both work with identical distributions.", "Before we add these critical features to our data set, we need to define the y_critical target. In line 5\u20137 of Script 2d, we define a target for each critical feature by setting a threshold that determines the True and False cases. These threshold values were chosen such that each critical feature contains 95% positive cases and 5% negative cases. Finally, in line 10 we define y_critical by multiplying the targets of the critical features.", "As a sanity check, let\u2019s visualize the distribution of the critical features.", "From Figure 1, you will notice there are three distinct Gaussian distributions. The leftmost distribution represents the first critical feature, the center-most distribution represents the second critical feature, and the rightmost distribution represents the third critical feature. The regions labeled as green denote the positive class in each feature and the regions labeled as red show the negative class.", "We are now ready to put everything together. In line 12 of Script 2f, we concatenate X_madelon and X_critical. Then in line 15\u201316, the target y_all is defined as the element-wise multiplication of y_madelon and y_critical.", "Before we move on, let\u2019s review the group of features we have created:", "X_madelon: This is a group of synthetic features that were created using the make_classification() class. In principle, the optimal number of features to select from this group should be the informative features (10 in total).", "X_critical: The critical features are 3 in total. Each has Gaussian distribution with a different mean value. Each critical feature contains 90 % positive cases and 10 % false cases. By construction, these features might be hard to select by automated method; however, to a person with domain knowledge about the data at hand, these can easily be determined to be important to the outcome.", "X_all: This is a group of features that contains both the X_madelon and X_critical features. The columns of this data set will be ordered as follows:", "Before we do anything else, let\u2019s split our data. For each group of features, we will create a training and test set using a 70/30 split.", "In this section, we will visualize the data to confirm what we already know. We will start by exploring the correlation between our features. In Script 3a, we first we calculate the Spearman\u2019s correlation matrix which values are in the range between -1 to 1 (line 5). A value of 1 signifies a strong positive correlation and value of -1 signifies a strong negative correlation. When the correlation value is close to zero, this means that no correlation exists between the features. Since negative and positive correlation are just correlations, we then take the absolute value of the correlation matrix (line 5)and the use seaborn package to create heatmap (line 14), see Figure 2.", "In Figure 2, a light color signifies a low correlation and a dark blue color a high correlation. Notice how the informative features have low correlation with each other (upper left corner). The redundant features start showing a higher correlation (the heat map turns more blue in this region) like we expect it to happen. Additionally, some of the repeated are highly correlated with other features like we expected. Finally, the useless features and the critical are not correlated with other features. We know that the useless features are filled with random noise and should be not be correlated with the outcome. In the other hand, the critical features are correlated with the outcome, although they show no correlation with other features.", "Next let\u2019s visualize the distribution of each feature using box plots. A box plot shows the quartiles of the data set and allows us to determine any anomalies and outliers in our data. In Script 3b, we first set the graph style, so that we don\u2019t have the ugly gray default seaborn background in our figure (line 5\u20138). Then, we create a box plots figure for the features X_all_train (line 14).", "In Figure 3, we can see that most features contain outliers. Moreover, all the Madelon-like features are centered around zero and most seem to have a Gaussian distribution. The critical features are centered around different mean values, like we showed before in Figure 1.", "In this section, we will be using FeatureSelector, a tool that applies the following four feature selection methods:", "This tool is designed to apply different feature selection methods in a given sequence. For example, you could first remove correlated feature and then use recursive feature elimination to further reduce the amount of selected features.", "Removing Features with a Large Fraction of Constant Values", "One can argue that a feature that exhibit little to no variation as you change the samples will not be very useful for a model.", "For example, if your data looks like the table on the right, it will be reasonable to select Feature 1, Feature 2, Feature 4 and drop Feature 3. This is because the value of Feature 3 doesn't change very much across different samples. Keeping Feature 3 will add complexity to your model and it\u2019s very likely that little to no predictive performance will be observed by keeping it. However, the last statement needs to be empirically verified. To establish the threshold that will flag features that contain a large fraction of constant values in order to remove them you will need to experiment. Just because a feature contains the same value 90% of the time, doesn\u2019t mean that it will be useless for a model.", "In Script 4a, we start by importing the feature selection tool. In line 7, we define a dict named step1 where we specify the feature selection method to apply and its parameters. For example, to remove features that have a large fraction of constant values (95 % or more), we set the key to 'Constant Features' and the value to {'frac_constant_values': 0.95}. In line 10, we save step1 in a list named steps, then we initiate an instance of FeatureSelector (line 13). The FeatureSelector haves a fit() and transform() method, much like Sklearn transformer. The fit() method learns which features to select from a training set (line 16) and the transform() method reduces a data set to only the selected features (line 19). The fit() method takes as input:", "The transform() method takes as input:", "The output of fit will be print out to the consoles the features that were removed. In this case, you can see an empty list being displayed which signifies that no features were removed in this step.", "Let\u2019s assume that if a group of features is highly correlated, we can randomly select one of them and discard the rest without losing much information. To measure the correlation between features, we will use Spearman\u2019s correlation coefficient. To remove correlated features, we will again use FeatureSelector. In Script 4b, we start by defining a dict that describes feature selection method and its parameters. For example, in this case we set the key to 'Correlated Features' and the value to {'correlation_threshold': 0.95}. This algorithm will group features that share a correlation value of 0.95 or greater. Then for each group of correlated features one feature is selected and the rest are discarded. In line 8, we save step1 in a list, then initialize an instance of FeatureSelector (line 11). Finally, we fit the FeatureSelector and transform our data.", "Notice that the output of fit() shows all the repeated features were removed. Before we move on let\u2019s end this section by inspecting the type of features in X_selected .", "In Figure 6, notice that none of the redundant or useless features was removed. In other words, if you only remove correlated features from your cohort, expect to still have redundant and useless features. In the positive side, we see that the 10 informative and 3 critical features were part of the selected features.", "We have shown that simple feature selection methods can execute fast and can remove a large fraction of features from our cohort; however, the remaining features are not necessarily relevant to the outcome or can be redundant (see Figure 6). To overcome this limitation, the BorutaPy algorithm was developed. Simply put, it uses the feature importance attribute of tree-based models to determine features that carry usable information for predictions. However, Terrence Parr and others have raised concerns about the validity of the feature importance attribute of tree-based models.", "To select relevant features, we start by defining a tree-based Sklearn estimator (a Random Forest Classifier, Extra Trees Classifier, Gradient Boosting Classifier) with a feature_importances_ attribute. In our case, we will use a Random Forest Classifier, see line 5\u20137 in Script 4c. Next, in line 10\u201315, we define a dictionary named step1 with a key 'Relevant Features' . The key value of 'Relevant Features' is a dictionary specifying the number of times to cross-validated the results (line 10) and the BorutaPy parameters. For each training fold, the relevant features are established. Then, features that are deemed relevant in every training fold are selected as the relevant features in the training set. In line 15, we fixed random_state in order to obtain deterministic results. The rest of script should be self-explanatory by this point.", "Script 4c, took about 5 minutes to execute. If you were to increase cv , n_estimators , or max_iter , expect an increase in time. Additionally, a larger data set with more samples or features will increase the time it would take to figure out the relevant features. The progress bar you see in Figure 7 will track the progress each time the relevant features have been determined in a training fold. So be patient if you don\u2019t see it filling up at first.", "We are ready to examine the contents of selected features.", "In the bar chart shown in Figure 8, notice that all the useless features were removed. This is what we expected from the Boruta algorithm. Nonetheless, we still have 20 redundant, 9 repeated, 5 informative, and 2 critical features. This means that our implementation of the Boruta algorithm couldn\u2019t get rid of all the redundant features, kept some of the repeated features, removed half of the informative features, and discarded one of the critical features. Figure 8 shows that we can\u2019t blindly trust the results of automated feature selection methods. Remember, we demanded that all three critical features needed to be included in our selected features. Losing one of them can lead to fatal predictions! Moreover, if we were to use a different estimator, for example, an Extra Trees Classifier, the selected features would be different (see Figure 9).", "When using the Boruta algorithm to select features, is important to remember that the features that are selected as relevant are only significant to the specific model under inspection. Moreover, when the model used to rank the features has the capacity to overfit, it\u2019s likely that the ranking of the features is misleading. To demonstrate the pitfalls of an over fitted Random Forest we train a model and evaluated its performance in both the training and test set, see Script 4d.", "Here we can observe that the training accuracy of the Random Forest in the training set is 100%. When evaluating the performance in the test set we see that the accuracy drops to 80 % (obviously we\u2019re over fitting). Inspecting the feature importance of the over fitted Random Forest reveals that a lot of the useless features (random noise) are ranked higher than informative and redundant features, see Figure 10. Ideally, the useless features should be ranked the lowest.", "Seeing that the ranked features of an over fitted model are unreliable, I will advise to first tuned your model, cross-validate the results, and evaluate it\u2019s performance in the train and the test set, see Script 4e. Once you minimize the over fitting, use your model to select features.", "The results show that the accuracy in the training set is 96% and in the test set is 81%. God damn it, we are still over fitting! Let\u2019s inspect how the tuned Random Forest ranks the features, see Figure 11.", "Interesting! We get much better results than what we expected. Even though the tuned Random Forest is over fitted, it does a better job at ranking features. For example, we can see that the critical features are ranked highly and most of the useless features are ranked at the bottom.", "To determine the relevant features with the tuned Random Forest, run Script 4f.", "In Figure 12, we can see that the selected features contain all three critical features which are very important to us. Additionally, note that you can trust these results more than the outcome obtained with the out of tune Random Forest.", "Now we will train a Random Forest using only the selected features, see Script 4g. We will set the parameters of the Random Forest to be those determined earlier in Script 4e. Let me mention that it's a good idea to tuned the Random Forest using the selected features. Remember that when we tuned the Random Forest in Script 4e, we used all 103 features. Now the selected features are about 35. However, we will skip this step here but you should do it. Finally, we will evaluate its performance both in the training and test set.", "Before we discuss the results, let me remind you with what we started. When training an out of tune Random Forest with all the features, we obtained an accuracy of 100 % in the training set and 80 % in the test set. Let\u2019s call this our baselines. After \u201ctuning\u201d the Random Forest (to make the feature ranking more reliable), selecting features with Boruta algorithm, and then training the Random Forest with the selected features, we obtained an accuracy of 94% in the training set and accuracy of 83% in the test set (see the output of Script 4g above). We can conclude several things from these results:", "Although we still have considerable over fitting, we have made some gains in performance, we have less over fitting, and have dropped the model\u2019s complexity. I will say this is a win but it\u2019s not good enough.", "Determining the features that maximize a model\u2019s performance", "We can also select features by applying a recursive feature elimination (RFE) algorithm. The RFE method determines the minimal subset of features you need that will maximize a model\u2019s performance. However, you run the risk of throwing away meaningful features \u2014 so keep that in mind. Therefore, if your task is to identify relevant features that are important to the outcome using RFE might not be the way to go. The algorithm basically works this way. First, train a model that can rank features using all the available features in your data set. Second, measure the model\u2019s performance. Third, rank the features and remove the lowest ranking feature. Repeat step 1 to step 3 until all the features are exhausted. By iteratively removing features while keeping track of the model\u2019s performance, you can determine the number features you need to achieve the highest performance.", "To determine the features that will maximize a model\u2019s performance, we start by defining a tree-based Sklearn estimator (a Random Forest Classifier, Extra Trees Classifier, Gradient Boosting Classifier) with a feature_importances_ or coef_attribute. Make sure that your base estimator is not over fitting your data. In our case, we will use a Random Forest Classifier with the tuned paramaters found in Script 4e, see line 5\u20138 in Script 4h. Next, in line 11\u201315, we define a dictionary named step1 with a key 'RFECV Features' . The key value of 'RFECV Features' is a dictionary specifying the number of times to cross-validated the results (line 11) and other RFECV parameters. The RFECV object is the Sklearn implementation of RFE method where the results are cross-validated. The rest of script should be self-explanatory by this point.", "Figure 13 shows the selected features determined by Sklean RFECV method. Unfortunately, we still have a lot of redundant and repeated features. The FeatureSelector class, haves a rfecv attribute where the fitted RFECV Sklearn object is saved. You can look a bit more deeper into the results of the RFE features by inspecting the contents of the fitted RFECV object.", "In Figure 14, the model\u2019s performance as a function of a number of features is shown. As you can see, the performance peak\u2019s around 40 features with an accuracy of about 0.80. Since the number of selected features are about 50 (see Figure 13), we can conclude that the RFECV Sklearn object overestimates the minimum number of features we need to maximize the model\u2019s performance. In my opinion, you be better off if you simply selected the top 13 ranked features where the model\u2019s accuracy is about 79%. However, the RFECV Skelarn object does provide you with that information. If you were eager to know what were the top 13 ranked features, you will need to write your own version of the RFE algorithm.", "In the previous section, we learn that we need to be cautious and alert to the dangers of automatic feature selection methods. By applying them and then trusting the results blindly you might end up making terrible miscalculation. Furthermore, not a single feature selection algorithm produced the results that we wanted \u2014 selected only informative and critical features.", "To improve our feature selection, we can apply feature selection algorithms as a series of steps. For example, we can first remove moderate to highly correlated features (line 11) and then apply the RFE method as shown in Script 5a.", "This time, in the selected features we have 8 informative, all 3 critical features, and 20 redundant features. These are the best results we have obtained by far but let us investigate a bit more further. Remember, that the FeatureSelector object saves the fitted RFECV object in its rfecv attribute.", "So we can already see from the performance curve that model\u2019s performance peaks around 20 features. Additionally, we can conclude that 15 features will be enough to get a high performance. We now evaluate the model\u2019s performance using the selected features in Script 5b.", "From the output of Script 5b, we can see that we increased the accuracy in the training set to 84%. Slowly but surely we have made gains. You can use Script 4e to tune the model again and see if you get any more gains in performance.", "So how the hell do we design a feature selection pipeline that will yield all the informative and critical features and no redundant or useless features? Well, the short answer is that I don\u2019t know. If you were to figure this problem out, you could take your algorithm quite literally to bank. The question you should be asking is, \u201cDo I really need to determine the most useful features or do I want to train a model with the smallest subset of features?\u201d If your answer is \u201cto determine the most useful features\u201d the methods presented here will shed some light on your search for the best features. However, if you only care about training the best possible model with a smaller subset of features, the feature selection algorithms presented here will do the job.", "If you will like to use the feature selection tool, make sure that you have it on your working directory like so:", "Before you use it to select features ensure the following conditions are met:", "Once you prepare your data you can use Script 6a to guide your feature selection process. You will need to experiment to determine what feature selection algorithms to apply. In Script 6a, we start by tuning our model (line 5\u201329), then we define four feature selection methods and apply them (line 32\u201356). We then use the selected features to train a model and evaluate its performance (line 59\u201375).", "Find me at LinkedIn. Until next time! Take care and code everyday!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F859fec4d1b12&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----859fec4d1b12---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F859fec4d1b12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=-----859fec4d1b12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F859fec4d1b12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=-----859fec4d1b12---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@oneshotespresso?utm_source=medium&utm_medium=referral", "anchor_text": "Hans Vivek"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://archive.ics.uci.edu/ml/datasets/madelon", "anchor_text": "Madelon"}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb?source=post_page---------------------------", "anchor_text": "article"}, {"url": "https://pandas.pydata.org/?source=post_page---------------------------", "anchor_text": "Pandas"}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn"}, {"url": "https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-", "anchor_text": "my GitHub"}, {"url": "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0", "anchor_text": "Will Koehrsen"}, {"url": "https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-", "anchor_text": "here"}, {"url": "https://stackoverflow.com/questions/36200913/generate-n-random-numbers-from-a-skew-normal-distribution-using-numpy", "anchor_text": "StackOverFlow"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation matrix"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient"}, {"url": "http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/", "anchor_text": "BorutaPy"}, {"url": "https://explained.ai/rf-importance/", "anchor_text": "Terrence Parr"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html", "anchor_text": "Gradient Boosting Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html", "anchor_text": "Gradient Boosting Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html", "anchor_text": "RFECV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html", "anchor_text": "RFECV Sklearn"}, {"url": "https://github.com/frank-ceballos/Designing-A-Robust-Feature-Selection-Pipeline-", "anchor_text": "my Github"}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn"}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "Frank CeballosGraphswww.frank-ceballos.com"}, {"url": "https://www.linkedin.com/in/frank-ceballos", "anchor_text": "Frank Ceballos - Postdoctoral Fellow - Medical College of Wisconsin | LinkedInI'm a postdoctoral fellow in the Medical College of Wisconsin where I analyze high-dimensional complex clinical data\u2026www.linkedin.com"}, {"url": "https://github.com/frank-ceballos", "anchor_text": "frank-ceballos - OverviewDismiss Sign up for your own profile on GitHub, the best place to host code, manage projects, and build software\u2026github.com"}, {"url": "https://medium.com/tag/python?source=post_page-----859fec4d1b12---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----859fec4d1b12---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----859fec4d1b12---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----859fec4d1b12---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----859fec4d1b12---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F859fec4d1b12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=-----859fec4d1b12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F859fec4d1b12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=-----859fec4d1b12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F859fec4d1b12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----859fec4d1b12---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3961b72dd61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&newsletterV3=176ad7e37afe&newsletterV3Id=f3961b72dd61&user=Frank+Ceballos&userId=176ad7e37afe&source=-----859fec4d1b12---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Written by Frank Ceballos"}, {"url": "https://frank-ceballos.medium.com/followers?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "856 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----859fec4d1b12---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3961b72dd61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-a-feature-selection-pipeline-in-python-859fec4d1b12&newsletterV3=176ad7e37afe&newsletterV3Id=f3961b72dd61&user=Frank+Ceballos&userId=176ad7e37afe&source=-----859fec4d1b12---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Frank Ceballos"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Scikit-Learn Decision Trees ExplainedTraining, Visualizing, and Making Predictions with Decision Trees"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "\u00b79 min read\u00b7Feb 23, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&user=Frank+Ceballos&userId=176ad7e37afe&source=-----803f3812290d----0-----------------clap_footer----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d?source=author_recirc-----859fec4d1b12----0---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=-----859fec4d1b12----0-----------------bookmark_preview----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----859fec4d1b12----1---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----859fec4d1b12----1-----------------bookmark_preview----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----859fec4d1b12----2---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----859fec4d1b12----2-----------------bookmark_preview----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-packages-from-github-with-conda-commands-ebf10de396f4?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/i-want-to-be-the-very-best?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-packages-from-github-with-conda-commands-ebf10de396f4?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "Installing Packages from GitHub with Conda CommandsA quick dirty self note"}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-packages-from-github-with-conda-commands-ebf10de396f4?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": "\u00b73 min read\u00b7Jan 1, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fi-want-to-be-the-very-best%2Febf10de396f4&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fi-want-to-be-the-very-best%2Finstalling-packages-from-github-with-conda-commands-ebf10de396f4&user=Frank+Ceballos&userId=176ad7e37afe&source=-----ebf10de396f4----3-----------------clap_footer----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-packages-from-github-with-conda-commands-ebf10de396f4?source=author_recirc-----859fec4d1b12----3---------------------eec7cb63_7ef1_4a0e_b477_3709b119807c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febf10de396f4&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fi-want-to-be-the-very-best%2Finstalling-packages-from-github-with-conda-commands-ebf10de396f4&source=-----859fec4d1b12----3-----------------bookmark_preview----eec7cb63_7ef1_4a0e_b477_3709b119807c-------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "See all from Frank Ceballos"}, {"url": "https://towardsdatascience.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://pauliusztin.medium.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://pauliusztin.medium.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Paul Iusztin"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "How to Quickly Design Advanced Sklearn PipelinesCompose all the components from Scikit-Learn Pipelines to build custom production-ready models."}, {"url": "https://towardsdatascience.com/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b710 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3cc97b59ce16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16&user=Paul+Iusztin&userId=8323de62a1a1&source=-----3cc97b59ce16----0-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3cc97b59ce16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-quickly-design-advanced-sklearn-pipelines-3cc97b59ce16&source=-----859fec4d1b12----0-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://audhiaprilliant.medium.com/?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://audhiaprilliant.medium.com/?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Audhi Aprilliant"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Part 1 \u2014 End to End Machine Learning Model Deployment Using FlaskHow to prepare the data and develop a machine learning model for loan approval prediction app"}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b713 min read\u00b7Nov 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2F1df8920da9c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fpart-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3&user=Audhi+Aprilliant&userId=140ffe7d74ee&source=-----1df8920da9c3----1-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1df8920da9c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fpart-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3&source=-----859fec4d1b12----1-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Kay Jan Wong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Feature Encoding Techniques in Machine Learning with Python Implementation6 feature encoding techniques to consider for your data science workflows"}, {"url": "https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b710 min read\u00b7Jan 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbf933e64aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa&user=Kay+Jan+Wong&userId=fee8693930fb&source=-----dbf933e64aa----0-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/feature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa?source=read_next_recirc-----859fec4d1b12----0---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbf933e64aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-encoding-techniques-in-machine-learning-with-python-implementation-dbf933e64aa&source=-----859fec4d1b12----0-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/@dimitris.effrosynidis?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/@dimitris.effrosynidis?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Dimitris Effrosynidis"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Ensemble Feature Selection for Machine LearningSelect the best features by combining individual feature selection methods"}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b75 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0df77b970f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-feature-selection-for-machine-learning-c0df77b970f9&user=Dimitris+Effrosynidis&userId=ff294d269093&source=-----c0df77b970f9----1-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----859fec4d1b12----1---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0df77b970f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-feature-selection-for-machine-learning-c0df77b970f9&source=-----859fec4d1b12----1-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://animadurkar.medium.com/?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://animadurkar.medium.com/?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Ani Madurkar"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Training XGBoost with MLflow Experiments and HyperOpt TuningA starting point on your MLOps Journey"}, {"url": "https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b710 min read\u00b7Jan 9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0d3a4994ea6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6&user=Ani+Madurkar&userId=c9b0adccc01d&source=-----c0d3a4994ea6----2-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/training-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6?source=read_next_recirc-----859fec4d1b12----2---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0d3a4994ea6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-xgboost-with-mlflow-experiments-and-hyperopt-c0d3a4994ea6&source=-----859fec4d1b12----2-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Saupin Guillaume"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "How Does XGBoost Handle Multiclass Classification?It\u2019s crucial to understand the underlying workings of classification using this kind of model, as it impacts performance."}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": "\u00b77 min read\u00b7Jan 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&user=Saupin+Guillaume&userId=891e27328f3a&source=-----6c76ba71f6f0----3-----------------clap_footer----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----859fec4d1b12----3---------------------c645cafe_63d2_4e3b_a855_faf06dac84f4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&source=-----859fec4d1b12----3-----------------bookmark_preview----c645cafe_63d2_4e3b_a855_faf06dac84f4-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----859fec4d1b12--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}