{"url": "https://towardsdatascience.com/deepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e", "time": 1682993527.507786, "path": "towardsdatascience.com/deepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e/", "webpage": {"metadata": {"title": "DeepMind\u2019s Playing Capture The Flag with Deep Reinforcement Learning | by SAGAR SHARMA | Towards Data Science", "h1": "DeepMind\u2019s Playing Capture The Flag with Deep Reinforcement Learning", "description": "DeepMind and other universities has published many End to End Reinforcement Learning papers that are used for problems that can be solved by a single agent. End to End RL algorithms learns both\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@sagarsharma4244", "anchor_text": "Medium", "paragraph_index": 42}, {"url": "https://twitter.com/SagarSharma4244", "anchor_text": "Twitter", "paragraph_index": 42}], "all_paragraphs": ["DeepMind and other universities has published many End to End Reinforcement Learning papers that are used for problems that can be solved by a single agent. End to End RL algorithms learns both feature representation and decision making in the network by taking pixels as the input and the controls as output.", "The real world contain problems that needs multiple individuals acting independently but still collaborating together to achieve a single goal. From playing games like football or basketball to landing a rocket on the moon, a team of individuals works together following a strategy to complete the faster, safer by reducing the risk of failure. This paper can be used to solve many real life tasks, so let\u2019s breakdown the paper to understand their solution.", "DeepMind has build an End-to-End population based RL algorithm that tackles the problem successfully using a two-tier optimisation process & by training individuals, acting and learning independent to each other in a team based 3D multi-agent environment (Capture the Flag) working together strategically to achieve a single goal.", "This leads to models that suffers from high complexity of the learning problem that arises from the concurrent adaptation of other learning agents in the environment.", "The game Capture the Flag has all the traits of the problem above:", "1.A 3D first person view multi-agent game.(also can implemented in robotics due to fpv similarity)", "2. Agents unaware of each other decisions playing in same environment as opponent or teammate.", "3.Strategy based game for learning higher cognitive skills.", "Also, the indoor & outdoor theme maps are randomly generated for every game. Two opposing teams consisting of multiple individual players compete to capture each other\u2019s flags by strategically navigating, tagging, and evading opponents. The team with the greatest number of flag captures after five minutes wins the game.", "To develop more generalised policies & learning agent capable of acquiring generalised skills, training fixed teams of agents on a fixed map reduces the diversity in the training data \u2014 instead the paper devise an algorithm & training procedure that enables agents to acquire policies that are robust to the variability of maps, number of players, and choice of teammates, a paradigm closely related to ad-hoc team play .", "The final win/loss is a delayed episodic signal received from the environment, making it difficult to optimise 1000\u2019s of actions performed by the agent on the basis of only one binary signal at the end of 5 minute game.", "This makes it difficult to recognise the actions that were effective in winning the game from the ones that did not help.", "We can solve the problem by increasing the number of rewards in the game. By using more frequent Internal rewards, the rewards can be given on the basis of actions performed by the agent.", "The memory and long-term temporal reasoning requirements of high level strategic CTF play is met by introducing an agent architecture that features a multi-timescale representation \u2014 reminiscent of what has been observed in primate cerebral cortex and an external working memory module \u2014 broadly inspired by human episodic memory.", "These 3 innovations are integrated with in a scalable, massively distributed & asynchronous computational framework.", "In the game, agent receives raw RBG pixels input Xt from the first person perspective at timestep t, produces control action at and receives game points \u03c1t to train agent\u2019s policy \u03c0.", "The goal of the reinforcement learning is to find a policy that maximises the the expected cumulative \u03b3-discounted reward over a CTF game with T time steps.", "\u03c0 is parameterised by a multi-time scale Recurrent Neural Network with external memory.", "The agent\u2019s architecture model constructs a temporally hierarchical temporal representation space and uses recurrent latent variable for sequential agent to promote the use of memory & temporally coherent action sequences.", "where winning operator > returns 1 if left wins, 0 for losing, and randomly break ties. Also, \u03c9 representing specific maps of the games.", "For The Win teams \u2014 Now that we are using more frequent Internal Rewards rt, we can operationalise the idea of each agent having a denser reward function by specifying rt = w(\u03c1t) based on available game point signals \u03c1t (points are registered for events such as capturing a flag) and allowing the agent to learn the transformation w such that policy optimisation on the internal rewards r t optimises the policy For The Win, giving us the FTW agent.", "Traditional methods used for training 1000s of multi-agent environment at such a high scale together is not supported making the methods unstable.", "Scalability \u2014 Population of total P different agents are trained in parallel with each other by introducing diversity amongst players to stabilise the training(54).", "Matchmaking \u2014 To improve the skills of the agents, the teammates & opponents are sampled from population P . The agents indexed by \u03b9 for a training game using a stochastic matchmaking scheme mp(\u03c0) that biases co-players to be of similar skill to player p, increasing the uncertainty.", "Agent Skill Level \u2014 Agents skill score are estimated online by calculating Elo Score (15) based on the output of training games.", "Meta-optimization \u2014 It is a method of using one optimizaiton method to train other optimizers. The paper uses population to meta-optimize the internal rewards & hyperparameters of RL process itself. This can be seen as two-tier optimization RL problem. Inner Optimisation aka J inner: The inner optimisation is soled by RL and it maximises J inner, the agents\u2019 expected future discounted internal rewards. Outer Optimisation aka J outer: It is solved with Population Based Training (PBT) and it is maximised w.r.t. internal reward schemes wp and hyperparameters \u03c6p , with the inneroptimisation providing the meta transition dynamics.", "PBT is an online evolutionary process which adapts internal rewards and hyperparameters and performs model selection by replacing under-performing agents with mutated versions of better agents.", "This joint optimization of agents\u2019 policies helps in utilising the potential of combining learning and evolution together which results in maximisation of:", "To assess the generalisation performance of agents during training a tournament is conducted on procedurally generated maps with Ad-hoc matches involving three types of agents.", "1. FTW clearly exceeded the win-rate of humans with maps which neither agent nor human had seen previously, i.e. zero-shot generalisation, with a team of two humans on average capturing 16 flags per game less than a team of two FTW agents.", "2. Human-agent v/s agent-agent \u2014 Only as part of a h-a team did we observe a human winning over an a-a team (5% win probability).", "3. Pro gamers v/s FTW \u2014 Even after twelve hours of practice the human game testers were only able to win 25% of games against the agent team.", "4.Tagging accuracy of agents FTW agents were 80% while humans were lagging at only 48% success. Agents won the match even after their tagging accuracy were artificially reduced to humans accuracy.", "Superior observations & controls resolution of humans helped them to surpass FTW agents in Successful long range tagging at 17% & agents at 0.5%.", "But FTW again surpassed humans in short range tagging reaction time with 258ms and humans at 559ms.", "To investigate how network have learned skills with such high -level rich representation.The network was asking past, present or future states of the game. For Example \u2014", "Similarly, total 200 binary questions was asked based on the features of the games to see the internal representation of the network.", "According to the authors, the agent has knowledge of a given feature if logistic regression on the internal state of the agent accurately models the feature. Interestingly, the FTW agent\u2019s representation was found to encode features related to the past particularly well: e.g. the FTW agent was able to classify the state both flags are stray (flags dropped not at base) with 91% AUCROC (area under the receiver operating characteristic curve), compared to 70% with the self-play baseline.", "I encourage you to look at the paper to see more detailed stats.", "There are many more visualisations in the paper for your help. I have selected few that need less explanation:", "In this paper, artificial agent using only pixels and game points as input can learn to play highly competitively in a rich multi-agent environment. This was achieved by combining a number of innovations in agent training- population based training of agents, internal reward optimisation, and temporally hierarchical RL \u2014 together with scalable computational architectures.", "This paper can be used to solve other problems around you that contain memory and temporally extended interfaces difficulties in their solution. So, I encourage you to read the paper to have fun & understand the methods emerged at the edge of our knowledge about Machine Learning andpush the boundaries by implementing the paper, solving real world problems to live on the edge of the human knowledge.", "Thank you for reading the article. To read more breakdown of your favroite papers and get #PaperExplained notifications. Follow me on Medium and Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance Writer. React developer. Deep learning/AI Electronics \ud83d\udce9 sagarsharma4244@gmail.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa9f71256442e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9f71256442e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sagarsharma4244?source=post_page-----a9f71256442e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagarsharma4244?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "SAGAR SHARMA"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F165370addbb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&user=SAGAR+SHARMA&userId=165370addbb5&source=post_page-165370addbb5----a9f71256442e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1807.01281", "anchor_text": "Research Paper"}, {"url": "https://www.google.com/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwicj_D8ppncAhUGu48KHT4hAlEQjRx6BAgBEAU&url=https%3A%2F%2Fdanieltakeshi.github.io%2F2016%2F11%2F25%2Fframe-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games%2F&psig=AOvVaw21UK-nNUVQJ4JpWaWKWSZw&ust=1531475887884647", "anchor_text": "Source"}, {"url": "https://medium.com/@sagarsharma4244", "anchor_text": "Medium"}, {"url": "https://twitter.com/SagarSharma4244", "anchor_text": "Twitter"}, {"url": "https://twitter.com/SagarSharma4244", "anchor_text": ""}, {"url": "https://medium.com/@sagarsharma4244", "anchor_text": ""}, {"url": "https://hackernoon.com/deepminds-amazing-mix-match-rl-techique-a6f8ce6ac0b4", "anchor_text": "DeepMind\u2019s Amazing Mix & Match RL Technique#1 Research Paper Explainedhackernoon.com"}, {"url": "https://towardsdatascience.com/playing-atari-with-6-neurons-open-source-code-b94c764452ac", "anchor_text": "Playing ATARI with 6 Neurons | Open Source Code#2 Research Paper Explainedtowardsdatascience.com"}, {"url": "https://hackernoon.com/google-xs-deep-reinforcement-learning-in-robotics-using-vision-7a78e87ab171", "anchor_text": "Google X\u2019s Deep Reinforcement Learning in Robotics using Vision#3 Research Paper Explainedhackernoon.com"}, {"url": "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6", "anchor_text": "Activation Functions: Neural NetworksSigmoid, tanh, Softmax, ReLU, Leaky ReLU EXPLAINED !!!towardsdatascience.com"}, {"url": "https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9", "anchor_text": "Epoch vs Batch Size vs IterationsKnow your code\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a9f71256442e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a9f71256442e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----a9f71256442e---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/tech?source=post_page-----a9f71256442e---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/news?source=post_page-----a9f71256442e---------------news-----------------", "anchor_text": "News"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&user=SAGAR+SHARMA&userId=165370addbb5&source=-----a9f71256442e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&user=SAGAR+SHARMA&userId=165370addbb5&source=-----a9f71256442e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa9f71256442e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a9f71256442e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a9f71256442e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a9f71256442e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a9f71256442e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a9f71256442e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a9f71256442e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagarsharma4244?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sagarsharma4244?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "SAGAR SHARMA"}, {"url": "https://medium.com/@sagarsharma4244/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F165370addbb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&user=SAGAR+SHARMA&userId=165370addbb5&source=post_page-165370addbb5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8dc3f98660e7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e&newsletterV3=165370addbb5&newsletterV3Id=8dc3f98660e7&user=SAGAR+SHARMA&userId=165370addbb5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}