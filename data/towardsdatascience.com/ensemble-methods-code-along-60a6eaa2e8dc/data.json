{"url": "https://towardsdatascience.com/ensemble-methods-code-along-60a6eaa2e8dc", "time": 1683010517.076566, "path": "towardsdatascience.com/ensemble-methods-code-along-60a6eaa2e8dc/", "webpage": {"metadata": {"title": "Ensemble Methods Code-Along. A code-along of Random Forest\u2026 | by Leana Critchell | Towards Data Science", "h1": "Ensemble Methods Code-Along", "description": "As the subtitle suggests, this code-along post is for beginners interested in making their first, more advanced, supervised machine learning model. Perhaps you\u2019re wanting to know how to improve your\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Titanic score on Kaggle", "paragraph_index": 0}, {"url": "https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/", "anchor_text": "this article", "paragraph_index": 13}, {"url": "https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600", "anchor_text": "this blog", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052", "anchor_text": "Decision Tree", "paragraph_index": 16}, {"url": "https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/", "anchor_text": "bagging", "paragraph_index": 17}, {"url": "https://ieeexplore.ieee.org/abstract/document/709601", "anchor_text": "subspace sampling method", "paragraph_index": 17}, {"url": "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf", "anchor_text": "paper", "paragraph_index": 18}, {"url": "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm", "anchor_text": "website", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/understanding-random-forest-58381e0602d2", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/", "anchor_text": "this", "paragraph_index": 23}, {"url": "https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/", "anchor_text": "this", "paragraph_index": 23}, {"url": "https://www.kaggle.com/becksddf/churn-in-telecoms-dataset", "anchor_text": "here", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a", "anchor_text": "this article", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/why-you-should-do-feature-engineering-first-hyperparameter-tuning-second-as-a-data-scientist-334be5eb276c", "anchor_text": "Here\u2019s an article about hyperparameter tuning and feature engineering.", "paragraph_index": 70}, {"url": "http://Data-Birds.com", "anchor_text": "Data-Birds.com", "paragraph_index": 77}], "all_paragraphs": ["As the subtitle suggests, this code-along post is for beginners interested in making their first, more advanced, supervised machine learning model. Perhaps you\u2019re wanting to know how to improve your Titanic score on Kaggle \u2014 well this code along will show you a way that could boost your score significantly straight away.", "This is for people who learn best by doing.", "My aim is to demystify the application of machine learning. Yes, the theory behind machine learning can be quite complex and I strongly encourage you to dive deeper and expose yourself to the underlying \u2018math\u2019 of the things you do and use. However, we all need to start somewhere and sometimes getting a feel for how things work and seeing results can motivate your deeper learning.", "I don\u2019t like to repeat content that is already saturated on platforms like Medium etc., so for this reason, I am not deep-diving on how these algorithms work.", "I will provide very brief \u201cin a nutshell, this is what\u2019s going on\u201d explanations of each method and will point you in the direction of some relevant articles, blogs and papers (and I really do encourage you to dive deeper).", "This code-along aims to help you jump right in and get your hands dirty with building a machine learning model using 3 different ensemble methods: random forest, AdaBoosting and gradient boosting.", "I assume you have a general understanding of supervised vs. unsupervised learning and some knowledge of basic decision tree models.", "Don\u2019t over think this one \u2014", "Ensemble models are an ensemble of models!", "The idea behind ensemble methods is the idea of \u201cwisdom of the crowd\u201d. If you ask me", "\u201cDoes my bum look good in this skirt?\u201d", "But perhaps you want to make sure I\u2019m not just being polite, so you ask 5 other people and they give you the same response. After asking 20 more people, you\u2019re feeling pretty confident about yourself (as you should!).", "This is the idea behind ensemble methods.", "Ensemble models give better predictions by combining the predictions of lots of single models. This might be done through aggregation of prediction results or by improving upon model predictions. For this reason, ensemble methods tend to win competitions. For more on this, please see this article and this blog for starters.", "Random forests, AdaBoosting and Gradient boosting are just 3 ensemble methods that I\u2019ve chosen to look at today, but there are many others!", "Random Forest is a supervised learning algorithm for both classification and regression problems.", "In a nutshell, the Random Forest algorithm is an ensemble of Decision Tree models.", "The decision tree algorithm chooses its splits based on maximising information gain at every stage, so creating multiple decision trees on the same dataset will result in the same tree. For our ensemble method to be effective, we need variability in our individual models. The random forest algorithm takes advantage of bagging and the subspace sampling method to create this variability.", "Please see Leo Breiman\u2019s paper and website for the nitty-gritty details of random forest. Or if you\u2019re more of a blog/article person, see here.", "AdaBoosting a.k.a. Adaptive Boosting, was the first boosting algorithm to be invented and so I touch on it here out of nostalgia. There have since been many boosting algorithms that have improved upon AdaBoosting but it\u2019s still a good place to start to learn about boosting algorithms.", "In a nutshell, the AdaBoost model is trained on a subsample of a dataset and assigns weights to each point in the dataset and changes those weights upon each model iteration. If the learner (current model) correctly classifies a point, the point\u2019s weight is reduced and if the learner incorrectly classifies a point, the point\u2019s weight is increased.", "Gradient Boosting is a more advanced boosting algorithm and takes advantage of gradient decent, which you might remember from linear regression.", "In a nutshell, Gradient Boosting improves upon each weak learner in a similar way as the AdaBoosting algorithm, except gradient boosting calculates the residuals at each point and combines it with a loss function. So the algorithm uses gradient descent to minimise the overall loss and uses the gradients and loss as predictors to train the next learner.", "For more, have a read of this and this.", "OK OK! Let\u2019s get to the fun part!", "Let\u2019s do some super quick exploratory data analysis. I chose this dataset deliberately because it\u2019s already pretty clean when you download it from Kaggle. But, as new data scientists, it\u2019s important for us to continue to hone our EDA skills.", "As you probably guessed from the title of the dataset, this model aims to predict churn \u2014 a very common problem businesses face.", "Thinking about which metrics we want to use to evaluate our model, let\u2019s think about what we want our model to predict and what is worse: a false negative prediction or a false positive prediction.", "Our model should predict whether a customer in our dataset will stay with the company(False) or if they will leave (True).", "False negative: the model predicts that a customer stays with the company (False), when in fact that customer churns (True).", "False positive: the model predicts that a customer will churn (True), when in fact they will stay(False).", "Given this, we would probably argue that false negatives are more costly to the company as it would be a missed opportunity to market towards keeping those customers. For this reason, we will use accuracy and recall scores to evaluate our model performance.*", "First, download the data to your directory here.", "From here, we can see that a row represents a Telecom customer.", "We can identify pretty quickly what our target variable is going to be: churn.", "I don\u2019t like those spaces in the column headings so we\u2019ll change that, check our data types and inspect any missing data (null values).", "As I mentioned, I picked this dataset because it\u2019s already pretty clean. Our datatypes make sense and you can see we have no null values. Of course, seeing 3333 non-null does not necessarily mean we don\u2019t have null values \u2014 sometimes we have null values in disguise in our datasets. I have inspected the unique values for each column and can confirm that the data looks complete at this point (of course, please do let me know if you do find something suspicious that I\u2019ve missed!).", "You may have noticed from our df.info() earlier that we have 4 columns of object type. 3 of these columns are useful categories: state, international_plan and voice_mail_plan.", "The other object column is phone_number which, you guessed it, is a customer\u2019s phone number. I argue that a person\u2019s phone number shouldn\u2019t have any great bearing on whether they decide to stick with a phone company, so for this reason, I choose to simply drop this column from our feature set. **", "So, let\u2019s now dummy out the remaining 3 categorical columns. This will add a lot of columns to our feature set, which in turn will add some complexity to our model but for our example sake, we won\u2019t worry too much about this right now.", "To learn more about dummy variables, one-hot-encoding methods and why I specified drop_first = True (the dummy trap), have a read of this article.", "This is a good opportunity to do another X.head() to look at how your data-frame looks now.", "First, we split our X and y data into a training set used for training the model, and a testing set used for (you guessed it) testing the model. I\u2019ve chosen to do a 0.25 split here.", "Throughout our modelling here, I\u2019m mostly going to use the default parameters for the model objects upon instantiating them. I will talk about hyperparameter tuning at the end but in short, all the fun is in tuning the parameters so I want to leave that up to you to explore. I want to focus on showing you how the models change just by using their defaults.", "For brevity sake, I am simply going to fit the models and spit out some metrics to compare each model.", "As we mentioned above, we\u2019ll use accuracy_score and recall_score as our metrics to compare.", "To see if our ensemble methods are any good, we first need to know how a single, base model performs on our data. Since this is a classification problem, we\u2019re going to use a Decision Tree as our FSM. Since Decision Trees have a habit of overfitting, I\u2019m going to set a max_depth of 5.", "This is suspiciously high and highlights some issues we have with accuracy score\u2026", "But, just going by accuracy here, this model isn\u2019t doing too badly and there isn\u2019t a huge discrepancy between training and test scores.", "We can see here there is a slightly larger difference between the train and test recall scores. Still, these scores are quite good for a first model.", "Remember, for this problem, we care more about recall score since we want to catch false negatives. Recall is what we want to try to maximise from this model.", "Next, we create a random forest model with max_depth of 5. Some versions of sklearn raise a warning if we leave n_estimators blank so I\u2019ve set it to 100 here.", "Our accuracy score has actually gone down from our first model. Let\u2019s check recall:", "Wow, our recall score has gone way down for our random forrest model! This might seem strange at first, but there are many reasons why this might happen. One conceptual reason to keep in mind is that the single decision tree model is one model, where as, by nature, the random forest model is making its final predictions based on the votes from all the trees in the forest. For this reason, we get a more accurate predictions than the \u2018opinion\u2019 of one tree.", "We also haven\u2019t taken into consideration class imbalances or hyperparameters, so this example is a little contrived.", "Let\u2019s continue in the same way and see how to implement AdaBoosting and Gradient Boosting models and compare their performance:", "Surprisingly, our accuracy score for the test data stayed the same but we had some improvement in the accuracy score for the test data.", "Our recall score has improved somewhat significantly from the random forest model.", "Let\u2019s see how Gradient Boosting performs.", "Our highest accuracy scores so far. They\u2019re not too far away from our first decision tree model. There\u2019s also no significant evidence of overfitting on preliminary inspection.", "Once again, our highest recall score so far and this does outperform our first model significantly.", "Given these 4 models, we would choose the gradient boosting model as our best.", "As you can see, it\u2019s not difficult to employ these models \u2014 we\u2019re simply creating model objects and comparing results \u2014 we\u2019re not thinking too deeply about what\u2019s happening under the hood. It\u2019s easy to create and experiment with these different methods to see which ones out perform each other.", "It should be noted, however, that this is a very contrived example meant only to show you how you can play around with these models and that ensemble methods generally do better than single models. The metrics presented in these models do not necessary have meaningful impact as we used all of the default parameters, so here are some next steps to start thinking about and acquainting yourself with\u2026", "Hyperparameters are parameter values that are set before the learning process. This is different from model parameters that we \u2018discover\u2019 after we have trained our model.", "If you are familiar with linear regression, the slope and intercept parameters are the model parameters that we are trying to optimise by training our model.", "An example of a hypterparameter that we tuned in this example was when we set max_depth = 5, we set this before we fitted the model.", "I understand you may not have come across a lot of these concepts and so this will become more clear as you do. But I encourage you to experiment with different hyperparameters to see how the models change with different parameter tunings.", "Hint: In Jupyter and other IDEs, shift + tab inside the parentheses of any method or class allows you to quickly inspect the parameters the object takes. For the model classes, this will show most of the hyperparameters you can play with.", "Don\u2019t underestimate the value of \u201ctinkering\u201d", "Here\u2019s an article about hyperparameter tuning and feature engineering.", "There\u2019s one line of code that I can use to explain what class imbalance is:", "As you can see here, nearly 86% of our data is labeled False. What this means is that our models can become unfairly biased towards False predictions simply because of the ratio of that label in our data. This is what we call a \u2018class imbalance\u2019 problem.", "There are lots of ways to deal with class imbalances, including class weights and SMOTE. Taking this into consideration with our dataset here will improve our models significantly.", "I hope you\u2019ve seen the value in how these ensemble methods can improve upon your models and I encourage you to try them out on different datasets to get a feel for them. As you learn more about the theory behind these algorithms, their hyperparameters, applying regularisation and class balancing methods, you\u2019ll have a good head start on how these features play out and what effect they have on your models.", "Thank you to the authors of the blogs and papers I have pointed readers to, to supplement this blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Pianist turned Data Scientist | Active real estate investor | Co-Founder of Data-Birds.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F60a6eaa2e8dc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://leana-m-critchell.medium.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": ""}, {"url": "https://leana-m-critchell.medium.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Leana Critchell"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd758941b6236&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&user=Leana+Critchell&userId=d758941b6236&source=post_page-d758941b6236----60a6eaa2e8dc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Titanic score on Kaggle"}, {"url": "https://www.kaggle.com/becksddf/churn-in-telecoms-dataset/data#", "anchor_text": "The Churn in Telecom dataset from Kaggle"}, {"url": "https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/", "anchor_text": "this article"}, {"url": "https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600", "anchor_text": "this blog"}, {"url": "https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052", "anchor_text": "Decision Tree"}, {"url": "https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/", "anchor_text": "bagging"}, {"url": "https://ieeexplore.ieee.org/abstract/document/709601", "anchor_text": "subspace sampling method"}, {"url": "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf", "anchor_text": "paper"}, {"url": "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm", "anchor_text": "website"}, {"url": "https://towardsdatascience.com/understanding-random-forest-58381e0602d2", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c", "anchor_text": "here"}, {"url": "https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/", "anchor_text": "this"}, {"url": "https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/", "anchor_text": "this"}, {"url": "https://www.kaggle.com/becksddf/churn-in-telecoms-dataset", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/why-you-should-do-feature-engineering-first-hyperparameter-tuning-second-as-a-data-scientist-334be5eb276c", "anchor_text": "Here\u2019s an article about hyperparameter tuning and feature engineering."}, {"url": "https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124", "anchor_text": "Here\u2019s an article on some of the different metrics you can investigate."}, {"url": "https://medium.com/tag/data-science?source=post_page-----60a6eaa2e8dc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ensemble-learning?source=post_page-----60a6eaa2e8dc---------------ensemble_learning-----------------", "anchor_text": "Ensemble Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----60a6eaa2e8dc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----60a6eaa2e8dc---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/technology?source=post_page-----60a6eaa2e8dc---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&user=Leana+Critchell&userId=d758941b6236&source=-----60a6eaa2e8dc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&user=Leana+Critchell&userId=d758941b6236&source=-----60a6eaa2e8dc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F60a6eaa2e8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----60a6eaa2e8dc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----60a6eaa2e8dc--------------------------------", "anchor_text": ""}, {"url": "https://leana-m-critchell.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://leana-m-critchell.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Leana Critchell"}, {"url": "https://leana-m-critchell.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "http://Data-Birds.com", "anchor_text": "Data-Birds.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd758941b6236&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&user=Leana+Critchell&userId=d758941b6236&source=post_page-d758941b6236--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd758941b6236%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-methods-code-along-60a6eaa2e8dc&user=Leana+Critchell&userId=d758941b6236&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}