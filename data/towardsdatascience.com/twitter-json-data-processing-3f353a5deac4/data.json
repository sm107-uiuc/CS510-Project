{"url": "https://towardsdatascience.com/twitter-json-data-processing-3f353a5deac4", "time": 1683010307.866393, "path": "towardsdatascience.com/twitter-json-data-processing-3f353a5deac4/", "webpage": {"metadata": {"title": "Twitter JSON data processing. Cleaning and polishing a dataframe of\u2026 | by H\u00e9ctor Ram\u00edrez, Ph.D. | Towards Data Science", "h1": "Twitter JSON data processing", "description": "Twitter allows collecting tweets using tweepy, a Python library for accesing the Twitter API. Here I don\u2019t intend to give a tutorial on how to collect tweets, as there are already several good\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tweepy.org", "anchor_text": "tweepy", "paragraph_index": 0}, {"url": "https://pandas.pydata.org", "anchor_text": "pandas", "paragraph_index": 2}, {"url": "https://pypi.org/project/country-converter/", "anchor_text": "country converter", "paragraph_index": 2}, {"url": "https://geopy.readthedocs.io/en/stable/#", "anchor_text": "GeoPy", "paragraph_index": 2}, {"url": "https://spacy.io", "anchor_text": "spaCy", "paragraph_index": 2}, {"url": "https://pypi.org/project/googletrans/", "anchor_text": "googletrans", "paragraph_index": 2}, {"url": "https://www.nltk.org", "anchor_text": "NLTK", "paragraph_index": 2}, {"url": "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object", "anchor_text": "tweet object", "paragraph_index": 3}, {"url": "http://www.slaw.ca/wp-content/uploads/2011/11/map-of-a-tweet-copy.pdf", "anchor_text": "Twitter Status Object map", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Language_localisation", "anchor_text": "language localization", "paragraph_index": 7}, {"url": "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object", "anchor_text": "Twitter JSON", "paragraph_index": 10}, {"url": "https://github.com/annexare/Countries/tree/master/data", "anchor_text": "this repository", "paragraph_index": 19}, {"url": "https://github.com/konstantinstadler/country_converter", "anchor_text": "country converter", "paragraph_index": 21}, {"url": "https://geopy.readthedocs.io/en/latest/#", "anchor_text": "GeoPy", "paragraph_index": 22}, {"url": "https://tqdm.github.io", "anchor_text": "tqdm", "paragraph_index": 24}, {"url": "https://spacy.io/", "anchor_text": "spaCy", "paragraph_index": 28}, {"url": "https://pypi.org/project/googletrans/", "anchor_text": "googletrans", "paragraph_index": 29}, {"url": "https://stackoverflow.com/questions/49497391/googletrans-api-error-expecting-value-line-1-column-1-char-0", "anchor_text": "https://stackoverflow.com/questions/49497391/googletrans-api-error-expecting-value-line-1-column-1-char-0", "paragraph_index": 30}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK", "paragraph_index": 33}, {"url": "https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f", "anchor_text": "[Ref.]", "paragraph_index": 34}, {"url": "https://github.com/hectoramirez/Language-localization_FIFA/blob/master/Countries/countries_lang_full.csv", "anchor_text": "this repository", "paragraph_index": 40}, {"url": "http://linkedin.com/in/harr/", "anchor_text": "linkedin.com/in/harr/", "paragraph_index": 47}, {"url": "http://xing.com/profile/Hector_Ramirez23", "anchor_text": "xing.com/profile/Hector_Ramirez23", "paragraph_index": 47}], "all_paragraphs": ["Twitter allows collecting tweets using tweepy, a Python library for accesing the Twitter API. Here I don\u2019t intend to give a tutorial on how to collect tweets, as there are already several good reviews (take a look at the references below), but rather to give you a full example of how to process the tweet objects in order to build a clean dataframe on which we can perform social media analyses.", "TL;TR: Along the way, we will flatten the Twitter JSON, select the text objects among the several options (main tweet, re-tweet, quote, etc.), clean them (remove non-alphabetic characters), translate non-English tweets, compute the sentiment of the text, and associate a location given a user-defined location or the automatic geolocalization.", "Libraries to use: pandas, country converter, GeoPy, spaCy, googletrans, NLTK.", "Each tweet object comes in JSON format, a mix of \u2018root-level\u2019 attributes, and child objects (which are represented with the {} notation). The Twitter developer page gives the following example:", "This is of course a small sample out of the huge dictionary composing each tweet. Another popular example is this Twitter Status Object map.", "For most kinds of analyses, we will surely need attributes as the tweet text, the user screen-name or the tweet place. Unfortunately, as you can see, these attributes don\u2019t come in a clean format, instead they are spread across the JSON levels \u2014 e.g., the tweet location coordinates are located in", "It is due to this fact that the collected tweets need a large process of cleaning and transforming, which is the purpose of this post.", "I recently carried out a language localization project where I needed to do a social media analysis on Twitter. For this, I collected 52830 tweets over the course of several days containing the following keywords: \u2018#FIFA20\u2019, \u2018#FIFA21\u2019, \u2018FIFA20\u2019, \u2018FIFA21\u2019, \u2018FIFA 20\u2019, \u2018FIFA 21\u2019 and \u2018#EASPORTSFIFA\u2019. Then, in order to do a proper analysis on them, I had to previously clean each tweet object so I could draw meaningful conclusion.", "Due to the nature of that project, I was mainly interested in data regarding the location of the tweet (country and coordinates), the sentiment of the English version of the text, and the language the text was tweeted in. And it was the goal of the processing steps to polish and find this attributes. You can find the details of the project in the following repository:", "Let\u2019s use this dataset to exemplify the steps of tweets processing!", "As we saw, there are multiple fields in the Twitter JSON which contains textual data. In a typical tweet, there\u2019s the tweet text, the user description, and the user location. In a tweet longer than 140 characters, there\u2019s also the extended tweet child JSON. And in a quoted tweet, there\u2019s the original tweet text and the commentary with the quoted tweet.", "To analyze tweets at scale, we will want to flatten the tweet JSON into a single level. This will allow us to store the tweets in a DataFrame format. To do this, we will define the function flatten_tweets() which will take several fields regarding text and location (this one stored in place). Take a look:", "Now, you may want to study all the text fields (main, re-tweet or quote), however, here I will keep just one text field for simplicity. For this, we now define a function select_text(tweets) that selects the main text whether the tweet is a principal tweet or a re-tweet, and we decide to drop the quoted text as it usually is repetitive and may not be informative.", "We now build the data frame. Notice that we choose the main columns (fields) relevant for a social media analysis. This includes the tweet language, lang, and the user-location, which is set manually by the user. We also keep the country, country_code and coordinates fields from place. These fields appear when the tweet is geo-tagged and it is usually contained in less than the 10% of the total of tweets. The following code block builds the dataframe:", "The head of the dataframe would look like:", "Notice how only almost half of the tweets include a manually-set user-location field and not even the 1% are geo-tagged, i.e., they don\u2019t provide a place field. This highlights the importance of collecting as many tweets as you can!", "In the following, we are interested in cleaning and polishing each of the dataframe columns.", "In this part of this process we will replace the languages codes in lang by the language standard name. As mentioned in the documentation:", "When present, [lang] indicates a BCP 47 language identifier corresponding to the machine-detected language of the Tweet text, or und if no language could be detected.", "We perform this step using the auxiliary languages.json file in this repository. This file maps the language code to the language standard names. The following code will do the trick:", "Now we move to process the locations. We will first treat theplace fields and then theuser-location field.", "The data in the place object is \u2013\u2013obiously\u2013\u2013 more reliable than the user-location. Therefore, although it constitutes the 0.91% of our tweets, we will take care of it. First, the country code in place-country_code comes in ISO 2 form, for which we will translate it to ISO 3 form with country converter. Then, we will perform the same to change place-country names to the standard, short names. This is advantageous because, for instance, Plotly maps use ISO 3 codes to localize countries.", "Here we take the manually-set user-locations and translate them to country names and codes \u2013\u2013 this involves some trusting on the user. We do this using the GeoPy library to identify a location (which could be an address) and assign a country to it. Also, again, we use country_converter to find the country codes in ISO 3 form.", "A word of caution: GeoPy connects to an API and, unfortunately, it takes almost a second for each call. This makes the process of computing ~ 50 K tweets rather slow.", "Note: tqdm is a python library, with an excellent implementation to pandas, that outputs a progress bar while the code is running. It\u2019ll make your life easier!", "Finally, we reduce the place-country and user-country columns to one by keeping the former when it exists, otherwise we keep the latter. We do the same for the codes columns:", "At this point, our dataset looks like this:", "It is now time to process the tweets\u2019 text. This will involve removing non-alphabetic characters and translate non-English tweets. We will however retain both options and actually use the texts with emojis and other characters for analysis given that our sentiment analyzer can handle them very well.", "To remove non-alphabetic characters, we use spaCy as it is quite straightforward and we do not need to specify the regular expression. Keep in mind that the following block removes emojis and words with apostrophes like \u201cI\u2019m\u201d, \u201cy\u2019all\u201d, \u201cdon\u2019t\u201d, etc.", "To translate the non-English tweets, we use googletrans which, as GeoPy, connects to its API, however it is much faster.", "Another word of caution: It exists a poorly documented error discussed, e.g., here: https://stackoverflow.com/questions/49497391/googletrans-api-error-expecting-value-line-1-column-1-char-0, which makes disconnects you and blocks your IP. To bypass this error, I use np.array_split() to divide the dataframe into several chunks and process each of them at a time in a loop. By doing this, the error doesn\u2019t occur, but I still save each chunks' translations to a csv so if in any iteration something went wrong, I can recompute just one chunk. I also instantiate Translator() each time.", "And we finally append the original, unprocessed English texts to text_english:", "At this point, the dataframe looks like:", "We finally compute the sentiment of each tweet. For this, we use NLTK\u2019s SentimentIntensityAnalyzer object from the nltk.sentiment.vader library.", "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. [Ref.]", "This library is super simple to use, as you can see:", "Notice that polarity_score outputs the probability that the text is negative, neutral or positive and a compound score. We then extract the latter and append the scores to the dataframe.", "For the sake of presentation, we re-order the columns.", "The final dataset should look like this:", "To exemplify what could be done with this dataset, let\u2019s build a visualization of the mean tweet sentiment score by country:", "Notice that we used a country/languages dataframe which can be found in this repository. The code above outputs the following Plotly map:", "This world map doesn\u2019t look very positive \ud83d\ude15", "The full code used in this post can be found in my repository:", "I recently got my PhD in Physics, and I\u2019m currently making my way into the data science world. Any comment and/or suggestion about this post is highly appreciated. Also, take a look at my other stories:", "Finally, feel free to connect with me in LinkedIn:", "Datacamp \u2014 Analyzing Social Media Data in Python:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Big Data Analyst @ XING \u2014 part of NEW WORK SE | PhD in Theoretical Physicist | linkedin.com/in/harr/ | xing.com/profile/Hector_Ramirez23"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3f353a5deac4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hectoramirez?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hectoramirez?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "H\u00e9ctor Ram\u00edrez, Ph.D."}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2e35a42940fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&user=H%C3%A9ctor+Ram%C3%ADrez%2C+Ph.D.&userId=2e35a42940fd&source=post_page-2e35a42940fd----3f353a5deac4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ravinepz?utm_source=medium&utm_medium=referral", "anchor_text": "Ravi Sharma"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.tweepy.org", "anchor_text": "tweepy"}, {"url": "https://pandas.pydata.org", "anchor_text": "pandas"}, {"url": "https://pypi.org/project/country-converter/", "anchor_text": "country converter"}, {"url": "https://geopy.readthedocs.io/en/stable/#", "anchor_text": "GeoPy"}, {"url": "https://spacy.io", "anchor_text": "spaCy"}, {"url": "https://pypi.org/project/googletrans/", "anchor_text": "googletrans"}, {"url": "https://www.nltk.org", "anchor_text": "NLTK"}, {"url": "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object", "anchor_text": "tweet object"}, {"url": "http://www.slaw.ca/wp-content/uploads/2011/11/map-of-a-tweet-copy.pdf", "anchor_text": "Twitter Status Object map"}, {"url": "https://en.wikipedia.org/wiki/Language_localisation", "anchor_text": "language localization"}, {"url": "https://github.com/hectoramirez/Language-localization_FIFA", "anchor_text": "hectoramirez/Language-localization_FIFAThis is an end-to-end project where we aim to perform a language localization for the FIFA videogame with only public\u2026github.com"}, {"url": "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object", "anchor_text": "Twitter JSON"}, {"url": "https://github.com/annexare/Countries/tree/master/data", "anchor_text": "this repository"}, {"url": "https://github.com/konstantinstadler/country_converter", "anchor_text": "country converter"}, {"url": "https://geopy.readthedocs.io/en/latest/#", "anchor_text": "GeoPy"}, {"url": "https://tqdm.github.io", "anchor_text": "tqdm"}, {"url": "https://spacy.io/", "anchor_text": "spaCy"}, {"url": "https://pypi.org/project/googletrans/", "anchor_text": "googletrans"}, {"url": "https://stackoverflow.com/questions/49497391/googletrans-api-error-expecting-value-line-1-column-1-char-0", "anchor_text": "https://stackoverflow.com/questions/49497391/googletrans-api-error-expecting-value-line-1-column-1-char-0"}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK"}, {"url": "https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f", "anchor_text": "[Ref.]"}, {"url": "https://github.com/hectoramirez/Language-localization_FIFA/blob/master/Countries/countries_lang_full.csv", "anchor_text": "this repository"}, {"url": "https://github.com/hectoramirez/Language-localization_FIFA/blob/master/Tweets%20processing%20and%20sentiment.py", "anchor_text": "hectoramirez/Language-localization_FIFAEnd-to-end study of EA Sports' FIFA localization. Contribute to hectoramirez/Language-localization_FIFA development by\u2026github.com"}, {"url": "https://towardsdatascience.com/your-live-covid-19-tracker-with-airflow-and-github-pages-658c3e048304", "anchor_text": "Your live Covid-19 tracker with Airflow and GitHub PagesLoad the data, make great visualizations with Bokeh, host them in your GitHub Pages website and let Airflow automate\u2026towardsdatascience.com"}, {"url": "https://www.linkedin.com/in/harr/", "anchor_text": "H\u00e9ctor Ram\u00edrez \u2014 Valencia Area, Spain | Professional Profile | LinkedInI recently earned my PhD in Physics where I specialized in experimental data analysis and mathematical modeling. I led\u2026www.linkedin.com"}, {"url": "https://learn.datacamp.com/courses/analyzing-social-media-data-in-python", "anchor_text": "Sign inSign in to DataCamp accountlearn.datacamp.com"}, {"url": "https://towardsdatascience.com/my-first-twitter-app-1115a327349e", "anchor_text": "My First Twitter AppHow to use Python and Tweepy to create your own datasettowardsdatascience.com"}, {"url": "https://towardsdatascience.com/tweepy-for-beginners-24baf21f2c25", "anchor_text": "Tweepy for beginnersUsing Twitter\u2019s API to build your own data settowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-to-access-twitters-api-using-tweepy-5a13a206683b", "anchor_text": "How to Access Twitter\u2019s API using TweepyA step-by-step guide (with code and tips) on obtaining a large Twitter dataset using the easy-to-use Python librarytowardsdatascience.com"}, {"url": "https://medium.com/@leowgriffin/scraping-tweets-with-tweepy-python-59413046e788", "anchor_text": "Scraping Tweets with Tweepy PythonThis is a step by step guide to scrape Twitter tweets using a Python library called Tweepy.medium.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3f353a5deac4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/social-media-analysis?source=post_page-----3f353a5deac4---------------social_media_analysis-----------------", "anchor_text": "Social Media Analysis"}, {"url": "https://medium.com/tag/twitter?source=post_page-----3f353a5deac4---------------twitter-----------------", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3f353a5deac4---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/sentiment-analysis?source=post_page-----3f353a5deac4---------------sentiment_analysis-----------------", "anchor_text": "Sentiment Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&user=H%C3%A9ctor+Ram%C3%ADrez%2C+Ph.D.&userId=2e35a42940fd&source=-----3f353a5deac4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&user=H%C3%A9ctor+Ram%C3%ADrez%2C+Ph.D.&userId=2e35a42940fd&source=-----3f353a5deac4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3f353a5deac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3f353a5deac4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3f353a5deac4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3f353a5deac4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3f353a5deac4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hectoramirez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hectoramirez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "H\u00e9ctor Ram\u00edrez, Ph.D."}, {"url": "https://medium.com/@hectoramirez/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "32 Followers"}, {"url": "http://linkedin.com/in/harr/", "anchor_text": "linkedin.com/in/harr/"}, {"url": "http://xing.com/profile/Hector_Ramirez23", "anchor_text": "xing.com/profile/Hector_Ramirez23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2e35a42940fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&user=H%C3%A9ctor+Ram%C3%ADrez%2C+Ph.D.&userId=2e35a42940fd&source=post_page-2e35a42940fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa50ec09a3388&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-json-data-processing-3f353a5deac4&newsletterV3=2e35a42940fd&newsletterV3Id=a50ec09a3388&user=H%C3%A9ctor+Ram%C3%ADrez%2C+Ph.D.&userId=2e35a42940fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}