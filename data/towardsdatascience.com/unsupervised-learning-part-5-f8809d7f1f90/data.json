{"url": "https://towardsdatascience.com/unsupervised-learning-part-5-f8809d7f1f90", "time": 1683012204.378808, "path": "towardsdatascience.com/unsupervised-learning-part-5-f8809d7f1f90/", "webpage": {"metadata": {"title": "Unsupervised Learning \u2014 Part 5. Advanced GAN Methods | by Andreas Maier | Towards Data Science", "h1": "Unsupervised Learning \u2014 Part 5", "description": "In this tutorial we look into advanced GAN methods, e.g. to prevent mode collapse using unrolled GANs and multi-resolution methods such as StackGAN and LAPGAN."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/unsupervised-learning-part-4-eeb4d3ab601", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/4Ot22wkEdfU", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/segmentation-and-object-detection-part-1-b8ef6f101547", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "http://dpkingma.com/wordpress/wp-content/%20uploads/2015/12/talk_nips_workshop_2015.pdf", "anchor_text": "nice talk about variational auto-encoders that we are linking here", "paragraph_index": 26}, {"url": "https://www.youtube.com/watch?v=AJVyzd0rqdc", "anchor_text": "here", "paragraph_index": 26}, {"url": "https://github.com/soumith/ganhacks", "anchor_text": "here in GAN hacks. If you ever wanted how to name your GAN, then you could have a look at this reference. Of course, we also have plenty of references and I really recommend looking at all of them. So, thank you very much for", "paragraph_index": 26}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 27}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 27}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 27}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 27}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 27}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 27}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 27}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 27}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog", "paragraph_index": 27}, {"url": "http://dpkingma.com/wordpress/wp-content/%20uploads/2015/12/talk_nips_workshop_2015.pdf", "anchor_text": "Link", "paragraph_index": 28}, {"url": "https://www.youtube.com/watch?v=AJVyzd0rqdc", "anchor_text": "Link", "paragraph_index": 28}, {"url": "https://github.com/soumith/ganhacks", "anchor_text": "Link", "paragraph_index": 28}, {"url": "https://github.com/hindupuravinash/the-gan-zoo", "anchor_text": "Link", "paragraph_index": 28}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. Try it yourself! If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning to the last video where we discussed the different algorithms regarding generative adversarial networks. Today, we want to look into the fifth part of our lecture on unsupervised training. These are essentially tricks of the trade concerning GANs.", "One trick that can help you quite a bit is one-sided label smoothing. So, what you may want to do is replace your targets of the real samples with a smoothed version. So. instead of using a probability of one, you use a 0.9 probability. You do not use the same for the fake samples. You don\u2019t change their label because otherwise, this will reinforce incorrect behavior. So, your generator would produce samples that resemble the data or samples it already makes. Benefits are that you can prevent the discriminator from giving very large gradients to your generator and you also prevent extrapolating to encourage extreme samples.", "Is balancing between the generator and the discriminator necessary? No, it\u2019s not. The GANs work by estimating the ratio of data and model density. So, the ratio is estimated correctly only when the discriminator is optimal. So, it\u2019s fine if your discriminator overpowers the generator. When the discriminator gets too good, your gradients, of course, may vanish. Then, you can use tricks like the non-saturating loss the Wasserstein GANs as we talked about earlier. You may also run into the problem that your generator gradients may get too large. In this case, you can use the trick of label smoothing.", "Of course, you can also work with deep convolutional GANs. So, this is DCGAN where you implement a deep learning approach into the generator. So, you can replace pooling layers with strided convolutions and transpose convolutions. You can fully remove the connected hidden layers for deeper architectures. The generator then typically uses ReLU activations, except for the output layer which uses a hyperbolic tangent. The discriminator for example here uses a leaky ReLU activation for all the layers and they use batch normalization.", "If you do that, then you may end up in the following problem: You can see here some generation results. Within the batches, there may be a very strong intra-batch correlation. So within the batch, all of the generated images look very similar.", "This brings us to the concept of virtual batch normalization. You don\u2019t want to use one batch normalization instance for both mini-batches. You could use two separate batch normalizations or even better, you use the virtual batch normalization. In case this is too expensive, you choose instance normalization for each sample, subtract the mean, and divide by the standard deviation. In case, you choose virtual batch normalization, you create a reference batch R of random samples and fix them once at the start of the training. Then, for each x subscript i of the current mini-batch, you create a new virtual batch that is the reference batch union the x subscript i. Then, you compute the mean and standard deviation of this virtual batch. You always need to propagate then R forward in addition to the current batch. This then allows you to normalize x subscript i with these statistics. So, this may be kind of expensive, but we have seen that this is very useful for stabilizing the training and removing the intra-batch correlations.", "There\u2019s also the idea of historical averaging. So there, you add a penalty term that punishes weights which are rather far away from the historical average. This historical average of the parameters can then be updated in an online fashion. Similar tricks from reinforcement learning can also work for generative adversarial networks like experience replay. You keep a replay buffer of past generations and occasionally show them. You keep checkpoints from the past generators and discriminators and occasionally swap them out for a few iterations.", "So, if you do so, then you can do things like the DCGAN. Here are bedrooms after just one epoch. You can see that you are able to generate quite a few different bedrooms. So it is very interesting what kind of diversity in terms of generalization you can actually achieve.", "Another interesting observation is that you can do vector arithmetic on the generated images. So, you can generate for example the mean of three instances of condition \u201cman with glasses\u201d. With this mean, you can subtract for example the mean of \u201cman without glasses\u201d and then you compute the mean of \u201cwoman without glasses\u201d and add it on top. What you get is \u201cwoman with glasses\u201d.", "So, you can really use the constrained generation with this trick in order to generate something where you potentially don\u2019t have a conditioning variable for. So the GANs learn a distribution representation that disentangles the concept of gender from the concept of wearing glasses. If you\u2019re interested in these experiments I recommend looking into [1].", "So let\u2019s look at a couple of more advanced GAN methods. A typical problem that may occur is mode collapse. So if you try to have this kind of target distribution with several cluster centers that are distributed in a ring-like fashion in 2-D space, then you can see that it may occur that your generator rotates through the modes of the data distribution. So, you do 5,000, steps 10,000 steps, up to the 25,000 steps, and you start jumping between the modes. The problem here is of course that you never converge to a fixed distribution. A possible reason may be that the minimization of G over the maximization of D is not equal to the maximization of D over the minimization of G. So, the discriminator in the inner loop converges to the correct distribution, the generator in the inner loop places all mass on the most likely point. In practice, if you do a simultaneous stochastic gradient descent of both networks, both effects actually can appear. Solutions are to use mini discrimination or unrolled GANs.", "So, mini-batch discrimination follows the intuition to allow D to look at multiple samples in combination to help the generator avoid collapsing. So, you extract features from an intermediate layer and then you add a mini-batch layer that computes for each feature the similarity to all other samples of the mini-batch. You concatenate the similarity vector to each feature. Then, you compute these mini-batch features separately for samples from the generator and from the training data. Your discriminator still outputs 0 and 1 but uses the similarity of all samples in the mini-batch as side information. So, this is mini-batch discrimination.", "You can also follow the concept of the unrolled GAN. So, here you would like to have the ideal generator. The problem here is that essentially we ignore the maximum operation when computing the generator\u2019s gradient. Here, the idea is to regard our V as a cost for the generator and you need to backpropagate through the maximization operation.", "This essentially leads to a very similar concept as we\u2019ve already seen in recurrent neural networks. So, you can unroll over several of those iterations using stochastic gradient descent on the different parameter sets. So, you get this kind of unrolled gradients over the forward pass over several iterations. This then allows us to build a computational graph describing several steps of learning in the discriminator. You backpropagate through all of the steps when you compute the generator\u2019s gradient. Fully maximizing the discriminator\u2019s value function is intractable, of course. So, you have to stop at a low number of k but you can see that already with a low number like 10, this substantially reduces the mode collapse.", "So, here is an example of the unrolled GAN. You have the target function of the standard GAN. You have this alternating mode and with the unrolled GAN, you can then see that in step zero you still have the same distribution. But already in step five thousand, you can see that the distribution is spread over a much larger area. In step number ten thousand, you can see that the entire domain is filled. After fifteen thousand steps, you form a ring. After twenty thousand steps, you can see that our certain maxima are appearing and after twenty-five thousand steps, we manage to mimic the original target distribution.", "You can also use GANs for semi-supervised learning. So here, the idea is to use the GAN by turning a K class problem into a K+1 class problem. There, you have the true classes which are the target classes for supervised learning.", "Then you have some additional class which models the fake inputs that have been generated by our generator G. The probability of being real is essentially the sum of all the real classes and the discriminator is then used as a classifier within the GAN game.", "Again, we can also use other ideas from our deep learning class. For example, the Laplacian pyramid. You can also do a Laplacian pyramid of GANs. So, we have observed so far that GANs are pretty good at generating low-resolution images. The high-resolution images are much more difficult. Here, you can then start by generating low-resolution images. So here, we go from right to left. They would start with generating from noise, a small resolution image. Then, you have a generator that takes the generated image as a condition and additional noise to generate an update for the high frequency to upscale the image. Lastly, you can do this again in the next scale where you try to predict again the high frequencies of these images, you add the high frequencies and use this to upscale and again you can get an obscured image that is missing high frequencies. Again, you use this as a conditioning vector on generating the high-frequency image to put back the high frequencies. So step-by-step, we use conditional GANs to generate the missing high frequencies in this context.", "You train the generators by training a discriminator on each level. The inputs for the discriminators are the different scaled images. So you downsize the image step-by-step and from the downscaled image you can then compute the difference image. The difference image between the two will give you the correct high frequencies. You train the discriminator in a way that it can differentiate the correctly generated high frequencies from the artificially generated high frequencies. So, this is the LAPGAN training, but we still only have 64 by 64 pixels.", "Another idea is StackGAN. This is now used for generating photorealistic images from a text. So, the task is you have some text and generate a fitting image. You decompose the problem into sketch refinement using a two-stage conditional GAN. The analogon, of course, here is that, for example, in human painting, you would first sketch and then draw the fine details. So, you have two stages. One GAN that draws a low-resolution image. It\u2019s conditioned with the text descriptions and it plans the rough shape, basic colors, and the background from the given text. The Stage-II GAN is then generating the high-resolution images conditioned on Stage-I result and the text descriptions. It corrects for defects and adds details.", "So, here are some examples where you have the text description. We are generating birds here from descriptions. You can see that the Stage-I generation still is missing many details. With the Stage-II generation, a lot of the problems that have been caused in Stage-I are fixed and you can see that the new images have a much much higher and better resolution. You can see the whole paper in [20].", "Let\u2019s summarize: GANs are generative models that use supervised learning to approximate an intractable cost function. You can simulate many different cost functions. It\u2019s actually hard to find an equilibrium between the discriminator and the generator. It cannot generate discrete data, by the way. You can still use it for semi-supervised classification, transfer learning, multimodal outputs, and domain transfer. There are a lot of papers appearing right now. These can also create high-resolution output such as BigGAN, for example, that does really highly resolved images.", "So next time on deep learning, we have a couple of interesting things coming up. We will go into how to adapt neural networks to localize objects. So, we go into object detection and localization. We will then detect object classes. We try to detect instances of classes. Then, we will go even further and try to segment the outlines of objects.", "So, we really want to go into image segmentation and not just find objects and images, but really try to identify their outlines. We will also look into one technique very soon that has been cited very, very often. So, you can check the citation count every day and you will see that it has increased.", "We also have some comprehensive questions here: What\u2019s the basic idea of contrastive divergence? What is defining the characteristic of an autoencoder? How to denoising autoencoders work? Also, don\u2019t forget about the variational auto-encoders. I think this is a very cool concept and the reparametrization trick tells us how can you backpropagate through a sampling process which is pretty cool. Of course, you should look into GANs. What is an optimal discriminator? What\u2019s the problem of mode collapse? I particularly encourage you to have a look at the Cycle GANs. So Cycle GANs are very, very popular right now. They are really nice because you can tackle things like unpaired domain translation. This is being used in many, many different applications. So, you should know about those methods if you really want to put in your CV that you have experience with deep learning. Then, you should know about such methods. Well, there\u2019s some further reading. There\u2019s a very nice talk about variational auto-encoders that we are linking here. There\u2019s a great tutorial about GANs by Goodfellow \u2014 the GAN father. So, I recommend watching this video here. Then, we have some more tricks of the trade that you can find here in GAN hacks. If you ever wanted how to name your GAN, then you could have a look at this reference. Of course, we also have plenty of references and I really recommend looking at all of them. So, thank you very much for watching this video. I hope you liked our small summary of generative adversarial networks and looking forward to meeting you in the next one. Thank you very much and bye-bye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced. If you are interested in generating transcripts from video lectures try AutoBlog.", "Link \u2014 Variational Autoencoders: \u000fLink \u2014 NIPS 2016 GAN Tutorial of GoodfellowLink \u2014 How to train a GAN? Tips and tricks to make GANs work (careful, noteverything is true anymore!) Link -\u000f Ever wondered about how to name your GAN?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff8809d7f1f90&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akmaier.medium.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----f8809d7f1f90---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!"}, {"url": "https://towardsdatascience.com/unsupervised-learning-part-4-eeb4d3ab601", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/4Ot22wkEdfU", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/segmentation-and-object-detection-part-1-b8ef6f101547", "anchor_text": "Next Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/NBxIigNCryk", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/2a3U4oHuvlE", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/OouuNmFud78", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/cbdIUBSn4PU", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "http://dpkingma.com/wordpress/wp-content/%20uploads/2015/12/talk_nips_workshop_2015.pdf", "anchor_text": "nice talk about variational auto-encoders that we are linking here"}, {"url": "https://www.youtube.com/watch?v=AJVyzd0rqdc", "anchor_text": "here"}, {"url": "https://github.com/soumith/ganhacks", "anchor_text": "here in GAN hacks. If you ever wanted how to name your GAN, then you could have a look at this reference. Of course, we also have plenty of references and I really recommend looking at all of them. So, thank you very much for"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog"}, {"url": "http://dpkingma.com/wordpress/wp-content/%20uploads/2015/12/talk_nips_workshop_2015.pdf", "anchor_text": "Link"}, {"url": "https://www.youtube.com/watch?v=AJVyzd0rqdc", "anchor_text": "Link"}, {"url": "https://github.com/soumith/ganhacks", "anchor_text": "Link"}, {"url": "https://github.com/hindupuravinash/the-gan-zoo", "anchor_text": "Link"}, {"url": "http://www.foldl.me/2015/conditional-gans-face-generation/", "anchor_text": "http://www.foldl.me/2015/conditional-gans-face-generation/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f8809d7f1f90---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f8809d7f1f90---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f8809d7f1f90---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f8809d7f1f90---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----f8809d7f1f90---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&user=Andreas+Maier&userId=b1444918afee&source=-----f8809d7f1f90---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&user=Andreas+Maier&userId=b1444918afee&source=-----f8809d7f1f90---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff8809d7f1f90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f8809d7f1f90---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f8809d7f1f90--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-part-5-f8809d7f1f90&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}