{"url": "https://towardsdatascience.com/deep-dive-into-convolutional-networks-48db75969fdf", "time": 1682995460.2936618, "path": "towardsdatascience.com/deep-dive-into-convolutional-networks-48db75969fdf/", "webpage": {"metadata": {"title": "Deep-dive into Convolutional Networks | by Antonino Ingargiola | Towards Data Science", "h1": "Deep-dive into Convolutional Networks", "description": "From the building-blocks (pooling, batch normalization, 1x1 filters) to the most advanced architectures (Inception, ResNet), interpretability, and bias."}, "outgoing_paragraph_urls": [{"url": "https://www.deeplearningbook.org/contents/convnets.html#pf21", "anchor_text": "loosely inspired by the visual cortex", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet", "paragraph_index": 0}, {"url": "https://medium.com/p/48db75969fdf#53d2", "anchor_text": "next section", "paragraph_index": 4}, {"url": "https://medium.com/p/48db75969fdf#db48", "anchor_text": "Pooling section", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning", "paragraph_index": 6}, {"url": "http://setosa.io/ev/image-kernels/", "anchor_text": "great interactive demonstration", "paragraph_index": 10}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "An Overview of Normalization Methods in Deep Learning", "paragraph_index": 25}, {"url": "https://doi.org/10.1109/CVPR.2012.6248076", "anchor_text": "Jia et al 2012", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "original paper", "paragraph_index": 34}, {"url": "https://medium.com/p/48db75969fdf#5055", "anchor_text": "next section", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning", "paragraph_index": 40}, {"url": "https://www.media.mit.edu/projects/gender-shades", "anchor_text": "researcher Joy Buolamwini found", "paragraph_index": 48}, {"url": "http://terencebroad.com/convnetvis/vis.html", "anchor_text": "Topological Visualisation of a Convolutional Neural Network", "paragraph_index": 54}], "all_paragraphs": ["Convolutional Networks (ConvNets) are a class of efficient neural networks that achieve impressive performances in perceptual tasks such as object recognition. Their architecture is loosely inspired by the visual cortex. In 2012 AlexNet, a type of ConvNet, won by a large margin the ILSVRC 2012 competition, starting the huge wave of interest in deep learning that continues today. In 2019, the state of the art architecture for object detection is ResNet, which is a type of ConvNet.", "In this article, I assume some familiarity with standard fully-connected neural networks (or multi-layer perceptron, MLP). After a high-level view of ConvNets activations, I will deep-dive into the concept of convolution and other building blocks (pooling, batch normalization, 1x1 filters, etc). Next, I will briefly illustrate some advanced architectures that achieve state-of-the-art results (Inception, ResNet). In the final part, I will touch the topics of interpretability and bias. Each section contains a list of references and links for further study. Some concepts are generally applicable to deep neural networks, but I will illustrate them in the context of ConvNets.", "If you are new to ConvNets, it will take some time to digest the material, take your time and read many sources. You can use this post as a quick reference of ideas surrounding ConvNets. If you find any error or if there are other topics you think I missed let me know in the comments section.", "Convolutional Neural Nets (ConvNets) are a class of neural networks specialized for image processing. As other neural networks, they transform input to output through many layers. In ConvNets, layers have a convolution step, a pooling step (optional) and a non-linear activation. Each layer in a neural net transforms the input tensor into the output tensor through linear an non-linear operations. All these intermediate tensors (including the network input and output) are called activations and they are all different representations of the input.", "I like to start illustrating ConvNets by visualizing the shape of activations as we go from input to output (see Figure 1). Each layer transforms the activation through both linear an non-linear operations (we will see the details in the next section). As we go through the layers, the spatial dimension of the activations shrinks while the depth increases. The last part of the ConvNet transforms the 3D activation to 1D, typically by average pooling (see Pooling section). Finally, 1 or 2 fully-connected layers project the activation into the output space for the final classification. In this post, I am using classification as an example of the final task. Some architectures avoid the final dense layers by directly generating a 1D activation with length matching the number of categories.", "The flow of activations shows how the input is represented in an increasingly \u201crich\u201d feature space (increased depth) while trading off spatial information (decreased height/width). The last fully-connected layers forego any spatial information in order to achieve the final classification task. As we go through layers the features not only increase in number (depth size) but also in complexity, being a combination of features in the previous layer. In other words, the network builds a hierarchical representation of the input: the first layers represents the input in term of elementary features such as edges, the second layer in term of more complex features such as corners, etc. A deeper layer can recognize abstract features such as an eye or even a face. The striking part is that the ConvNet will learn this hierarchy of feature autonomously during training.", "During training, the network will learn a representation that is good for solving the assigned task. With a large and diverse dataset such as ImageNet (millions of images classified in 1000 categories), the learned representations will be general enough to be useful for many other visual perception tasks, even on different or very specific domains. This is the foundation of transfer learning: training a model on a big dataset once, then fine-tuning the model on a new domain-specific (and potentially small) data-set. This allows to quickly adapt a pre-trained network to solve new problems quickly and with high accuracy.", "Let\u2019s zoom-in now into one convolutional layer. Keep in mind that what we call convolution in neural nets is a bit different than the classical 2D convolution in signal processing. While the broad idea is similar, it is not mathematically the same.", "Both classical and deep-learning convolution compute the output by applying kernel to an input array. Each output pixel is the sum of the element-by-element product between input and kernel (dot product). By shifting the kernel over the input, we obtain the different output pixels. The number of pixels we shift at each step (1 or more) is called the stride.", "One fundamental difference is in the shape of the input and output tensors: in neural nets we have additional dimensions.", "If you are not familiar with 2D convolutions, have a look this great interactive demonstration to gain some an intuition.", "In 2D ConvNets, the convolution has the following properties:", "The receptive field is the 3D region of the input contributing to the output pixel (see the yellow cuboid in Figure 2). Note that one output pixel has many \u201cvalues\u201d, one for each kernel (64 in Figure 2). By arranging all the output vectors corresponding to different receptive fields we obtain the full 3D activation.", "Typically, the receptive fields of two adjacent output locations will partially overlap. There is no overlapping only if the stride equals the kernel size.", "All the kernels in a layer (64 in Figure 2) can be arranged in single 4-D tensor of shape", "(# kernels, kernel size, kernel size, input_depth)", "The parameters include all the weights in the kernels plus the 1D bias vector.", "The bias introduces one additional parameter per kernel. Like kernels, the bias is the same for each spatial position, so there are as many bias parameters as the number of kernels (or output depth).", "Putting bias and weights together, the total parameters in a layer sum up to:", "(# kernels x kernel size x kernel size x input_depth)", "In practice, the activations of Figure 1 are not computed for a single image but for a mini-batch. In this case, all the activations will have an additional dimension of size batch_size. The batch size must be taken into account because it directly influences the RAM needed to train and evaluate the model. Typically we use the largest batch size that can fit in the GPU RAM.", "Batch normalization (BatchNorm) is one of the most important advances in deep learning in recent years. BatchNorm speeds up and stabilizes training on virtually any neural net architecture, including ConvNets.", "Curiously, the original BatchNorm authors attributed the improved performance to a reduction in the \u201cinternal covariance shift\u201d. But it was recently discovered that BatchNorm is instead smoothing the optimization landscape, allowing larger learning rates to quickly converge to more accurate solutions. Just a reminder that theory, even if compelling or \u201cintuitive\u201d, must always be empirically validated.", "BatchNorm is unquestionably the most popular normalization method in deep learning, but it is not the only one. Research is very active in this area and we may see new advances in the near future. The problem is two-fold. On one hand, BachNorm is difficult to apply to recurrent networks due to its reliance on the mini-batch mean and standard deviation. On the other hand, the effect of BatchNorm was quite fortuitous, and more research in understanding how BatchNorm helps optimization can lead to even better normalization approaches.", "For brevity, I will only mention one alternative normalization scheme: weight normalization. In this scheme, instead of normalizing the activations, we normalize the weights. In particular, we normalize each kernel (all the weights contributing to a single activation) to have unit norm. Then, to preserve the model expressiveness, we also add a scale parameter for each activation. In principle, this should help training in a similar way as BatchNorm, by providing a single direct \u201cknob\u201d to change each activation, thus providing an \u201ceasier\u201d (i.e. smoother) path toward the minimum.", "Many other normalization methods have been proposed each with its own pro and contra. For an excellent overview please see \u201cAn Overview of Normalization Methods in Deep Learning\u201d by Keita Kurita.", "Convolutional blocks are oftentimes followed by a pooling block to reduce the activation spatial dimensions. Pooling helps in reducing memory consumption in deeper layers. It is also an important step to convert the spatial information into features. According to the Deep Learning Book by Ian Goddfellow et al.", "pooling helps to make the representation approximately invariant to small translations of the input.", "There are different pooling strategies. The most common are max-polling and average pooling. In all cases, pooling reduces an input \u201cblock\u201d (receptive field) into a 1x1 output block, while keeping the depth unchanged. The reduction is done by selecting the max input activation (max-pooling) or by taking an average (average-pooling). Similar to convolution, a pooling block maps a receptive field to a single \u201cpixel\u201d in the output. For this reason, we can define a polling spatial size (2x2, 3x3, etc.) and stride. Usually, the stride is chosen to have non-overlapping receptive fields to achieve a reduction in spatial size. Oftentimes, the last pooling layer is an average over the whole spatial activations (global average pooling or GAP) resulting in a 1x1 output activation (the 1D activation of size 512 in Figure 1). Unlike convolution, pooling does not have any parameters and the number of output features (depth) is always the same as the input.", "Pooling layers with a \u201clearnable structure\u201d have been proposed but have enjoyed limited popularity so far (Jia et al 2012).", "Some architectures use a 1x1 filter. In this case, the filter maps input of shape", "Note that only the number of features changes, while height and width remain the same. In this case, each output pixel is a vector of num_filters_o features that depends only on one input pixel (a vector of size num_filters_i). Each output feature is a (different) linear combination of the input features for the same pixel which is a receptive field of size 1x1.", "The 1x1 filter is used to reduce the number of output features thus reducing the computational cost while keeping the spatial dimension unchanged. For example, the inception network uses 1x1 filters to reduce the features and create \u201cbottlenecks\u201d which make the architecture more computationally affordable. However, if the bottleneck is too tight it may end up hurting the network performances.", "When the size of the convolution kernel is larger than 1 x 1,each output feature is still a linear combination of all the input featuresin the receptive field, which in this case is >1 pixel wide.", "The 1x1 convolution was called Network in Network in the original paper by Lin et al. The original paper described it as a \u201cmini\u201d fully connected layer between the 1x1 input and output features. Note that the same fully connected layer is applied to at each spatial position using the same weights.", "The ILSVRC 2014 winner was the GoogLeNet architecture by Szgedy et al. which introduces the inception module shown below.", "In ConvNets, an important choice is the spatial size of the convolution kernel. The size is typically 7x7 for the first layer and 3x3 for all the following layers. Instead of choosing one size for the convolution, the inception module performs many convolutions in parallel. Figure 5 shows the inception block as proposed in the inception v1 paper. Convolutions with size 1x1, 3x3 and 5x5 (blue blocks) as well as a max-pooling (red block) are performed on the same input. Additional 1x1 convolutions (yellow blocks) reduce the depth size in order to heavily reduce the memory requirements. These parallel paths produce output tensors (with the same spatial size) which are concatenated along the depth to form the layer output.", "Since the first paper, many updates to the inception architecture have been proposed including inception v2, v3, v4, and inception-resnet. The latter combines the inception idea of multiple convolutions with skip-connections (from ResNets, see next section).", "One known problem in neural networks with many layers is the vanishing gradient. In essence, during back-propagation, the derivative gets multiplied by the derivative in the previous layer. So, by the time we reach the first layers the gradient can become vanishingly small or can explode (overflow). This effect makes it hard to train deep neural nets, including ConvNets. To address this problem, Kaiming He et al. introduced the \u201cskip connection\u201d which forms the building block of the ResNet architecture.", "In ResNet, the output of a layer is fed not only to the next layer but also to the input of two layers ahead. The input is added to the layer output and then fed to the next layer.", "Since winning the ILSVRC 2015 competition, ResNet is still the state-of-the-art ConvNet architecture. Pretrained ResNet34 or ResNet50 is the de facto standard in transfer learning to implement specialized applications spanning from medical imaging to teddy bear detectors.", "In order to build trust in intelligent systems and move towards their meaningful integration into our everyday lives, it is clear that we must build \u201ctransparent\u201d models that explaining why they predict what they predict.", "Understanding how a neural network reaches a decision is notoriously a difficult task. Interpreting neural network results is not only important as a scientific endeavor but also required by many applications. Neural net interpretability is an active research topic involving several network visualization techniques.", "Broadly speaking there are two broad techniques on interpretability. One is called attribution aiming to find the region in the input image used to reach the decision. The second is called feature visualization aiming to visualize which features in an input image activate a specific neuron or group of neurons.", "On some architectures, attribution can be done by overlaying spatial activations in the hidden layers with the input image and plotting a so-called saliency map (Figure 8, right panel). Saliency maps have the same spatial resolution as the last 3D activation, which is low but oftentimes sufficient. An extension of this approach applicable to any architecture is Grad-CAM, where the saliency map is a weighted mean of the last spatial activations (last activation with a 3D shape). The weights in this weighted mean are computed from the gradient of the loss function with respect to each activation. Grad-CAM can be applied to any network, even to non-classification tasks.", "For feature visualization, we can plot kernel weights in each layer. Each kernel shows the pattern detected in layer input. Interpretation, in this case, is easier in the first layer but becomes more difficult for deeper layers. Another simple approach is plotting the activations for a given input.", "A more nuanced approach in generating (via optimization) an input image to maximally activate a neuron, a channel, a layer or a class (Figure 9). This allows building atlases of \u201cfeatures\u201d that visually represents what the network responds to in each layer. This approach suffers from a lack of diversity of generated images, which may not represent the full set of spatial features the network responds to. For further information, papers published by Distill.pub are both insightful and graphically stunning.", "A discussion of ConvNets cannot be complete without mentioning the issue of bias. Bias in machine learning comes from bias in datasets and/or algorithms, which in turn reflect biases of the people creating the system. While bias is a serious problem in machine learning, ConvNets applications offer some glaring examples of how it can affect people lives.", "Remember that a network will \u201clearn\u201d representations useful to solve a task. For example, if our goal is recognizing faces, the ideal dataset should be as diverse as possible in order to represent all the ethnic, age, and gender groups in a balanced way. In practice, most popular datasets over-represent white males. As researcher Joy Buolamwini found, this leads to heavy biases in the current state of the art commercial face-recognition systems. In these systems, faces of women of color are recognized with orders of magnitude lower accuracy than while males (Figure 4). These systems have been or will be deployed to identify suspects of a crime, for example. Unfortunately, if you are a dark-skinned woman, you will be misidentified for a criminal at a hundredfold higher rate than a white man!", "As machine learning practitioners, we cannot forgo our moral responsibilities. We know that the systems we create can disrupt people lives at an unprecedented scale. We must thus take steps to overcome this bias. Rachel Thomas, one of \u201c20 Incredible Women in AI\u201d according to Forbes, has written about the bias issue at length and her posts are an excellent source of information.", "The topics covered here are far from complete. Here a few topics I have not covered:", "In this post, I touched several fundamental aspects of ConvNets. Even the most advanced architectures are based on the basic building block of the convolutional layer.", "ConvNets may have \u201csolved\u201d the image identification problem but many problems stills exist. Despite recent progress, interpreting results is still a challenge, an issue impeding applications in some fields. Better generalization with smaller datasets would also greatly expand the class of treatable problems. But, most importantly, we need to acknowledge and try to overcome social bias. Given the dramatic implications on individuals and communities, it is paramount to strive for more fairness in these system.", "Here you find general references on ConvNets. References for specific topics are at the end of each section.", "Header image from Topological Visualisation of a Convolutional Neural Network.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Biophysicist, Electrical Engineer, Ph.D. I write about machine learning and data science."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F48db75969fdf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----48db75969fdf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tritemio?source=post_page-----48db75969fdf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tritemio?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Antonino Ingargiola"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d706469449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&user=Antonino+Ingargiola&userId=8d706469449&source=post_page-8d706469449----48db75969fdf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://terencebroad.com/convnetvis/vis.html", "anchor_text": "http://terencebroad.com/convnetvis/vis.html"}, {"url": "https://blog.floydhub.com/building-your-first-convnet/", "anchor_text": "FloydHub"}, {"url": "https://www.deeplearningbook.org/contents/convnets.html#pf21", "anchor_text": "loosely inspired by the visual cortex"}, {"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet"}, {"url": "https://medium.com/p/48db75969fdf#28b3", "anchor_text": "An overview of a ConvNets"}, {"url": "https://medium.com/p/48db75969fdf#53d2", "anchor_text": "Convolution step"}, {"url": "https://medium.com/p/48db75969fdf#90eb", "anchor_text": "Receptive Field"}, {"url": "https://medium.com/p/48db75969fdf#5738", "anchor_text": "Number of parameters"}, {"url": "https://medium.com/p/48db75969fdf#a105", "anchor_text": "Mini-batches"}, {"url": "https://medium.com/p/48db75969fdf#a07c", "anchor_text": "Batch normalization"}, {"url": "https://medium.com/p/48db75969fdf#fbde", "anchor_text": "Other normalizations"}, {"url": "https://medium.com/p/48db75969fdf#db48", "anchor_text": "Pooling"}, {"url": "https://medium.com/p/48db75969fdf#d8f9", "anchor_text": "1x1 convolutions"}, {"url": "https://medium.com/p/48db75969fdf#9916", "anchor_text": "Inception"}, {"url": "https://medium.com/p/48db75969fdf#5055", "anchor_text": "ResNet"}, {"url": "https://medium.com/p/48db75969fdf#d7d7", "anchor_text": "Interpretability"}, {"url": "https://medium.com/p/48db75969fdf#006f", "anchor_text": "Bias"}, {"url": "https://medium.com/dataseries/deep-dive-into-convolutional-networks-48db75969fdf#11b2", "anchor_text": "References"}, {"url": "https://medium.com/p/48db75969fdf#53d2", "anchor_text": "next section"}, {"url": "https://medium.com/p/48db75969fdf#db48", "anchor_text": "Pooling section"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning"}, {"url": "https://github.com/vdumoulin/conv_arithmetic", "anchor_text": "source"}, {"url": "http://setosa.io/ev/image-kernels/", "anchor_text": "great interactive demonstration"}, {"url": "http://setosa.io/ev/image-kernels/", "anchor_text": "http://setosa.io/ev/image-kernels/"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "arXiv:1502.03167"}, {"url": "https://www.deeplearningbook.org/contents/optimization.html#pf2b", "anchor_text": "\u201cBatch Normalization"}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "arXiv:1805.11604"}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "source"}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "An Overview of Normalization Methods in Deep Learning"}, {"url": "https://arxiv.org/abs/1602.07868", "anchor_text": "arXiv:1602.07868"}, {"url": "https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/", "anchor_text": "An Overview of Normalization Methods in Deep Learning,"}, {"url": "https://doi.org/10.1109/CVPR.2012.6248076", "anchor_text": "Jia et al 2012"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "arXiv:1312.4400"}, {"url": "https://www.deeplearningbook.org/contents/convnets.html#pfa", "anchor_text": "Ch 9.3: Pooling"}, {"url": "https://www.deeplearningbook.org/contents/convnets.html#pfa", "anchor_text": "Ch 9.3"}, {"url": "https://doi.org/10.1109/CVPR.2012.6248076", "anchor_text": "10.1109/CVPR.2012.6248076"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "original paper"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "arXiv:1312.4400"}, {"url": "https://medium.com/p/48db75969fdf#5055", "anchor_text": "next section"}, {"url": "https://arxiv.org/abs/1409.4842", "anchor_text": "arXiv:1409.4842"}, {"url": "https://arxiv.org/abs/1602.07261", "anchor_text": "arXiv:1602.07261"}, {"url": "https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202", "anchor_text": "A Simple Guide to the Versions of the Inception Network"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Transfer_learning", "anchor_text": "transfer learning"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "arXiv:1512.03385"}, {"url": "https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035", "anchor_text": "An Overview of ResNet and its Variants"}, {"url": "https://arxiv.org/abs/1610.02391", "anchor_text": "arXiv:1610.02391"}, {"url": "https://doi.org/10.23915/distill.00007", "anchor_text": "Olah et al. 2017."}, {"url": "https://doi.org/10.23915/distill.00007", "anchor_text": "Olah et al. 2017."}, {"url": "https://arxiv.org/abs/1610.02391", "anchor_text": "arXiv:1610.02391"}, {"url": "http://cs231n.github.io/understanding-cnn/", "anchor_text": "\u201cUnderstanding CNN"}, {"url": "http://doi.org/10.23915/distill.00007", "anchor_text": "doi:10.23915/distill.00007"}, {"url": "https://doi.org/10.23915/distill.00015", "anchor_text": "doi:10.23915/distill.00015"}, {"url": "https://www.fast.ai/2019/01/29/five-scary-things/", "anchor_text": "(source)"}, {"url": "https://www.media.mit.edu/projects/gender-shades", "anchor_text": "researcher Joy Buolamwini found"}, {"url": "https://www.fast.ai/2019/01/29/five-scary-things/", "anchor_text": "Five Things That Scare Me About AI"}, {"url": "https://www.media.mit.edu/projects/gender-shades/", "anchor_text": "Gender Shades"}, {"url": "https://distill.pub/2019/safety-needs-social-scientists/", "anchor_text": "AI Safety Needs Social Scientists"}, {"url": "https://doi.org/10.23915/distill.00014", "anchor_text": "10.23915/distill.00014"}, {"url": "http://ruder.io/optimizing-gradient-descent/", "anchor_text": "numerous optimization methods"}, {"url": "https://arxiv.org/abs/1603.07285", "anchor_text": "convolution arithmetic"}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "minimal adversarial perturbations"}, {"url": "https://arxiv.org/abs/1805.09190", "anchor_text": "is underway"}, {"url": "https://www.deeplearningbook.org/contents/convnets.html", "anchor_text": "Ch. 9. Convolutional Networks"}, {"url": "https://course.fast.ai/videos/?lesson=6", "anchor_text": "\u201cLesson 6: Regularization; Convolutions; Data ethics"}, {"url": "http://neuralnetworksanddeeplearning.com/chap6.html", "anchor_text": "\u201cCh. 6: Deep Learning"}, {"url": "https://neuralnetworksanddeeplearning.com/", "anchor_text": "https://neuralnetworksanddeeplearning.com/"}, {"url": "http://cs231n.github.io/convolutional-networks/", "anchor_text": "CS231n Convolutional Neural Networks for Visual Recognition"}, {"url": "https://arxiv.org/abs/1603.07285", "anchor_text": "arXiv:1603.07285"}, {"url": "https://github.com/vdumoulin/conv_arithmetic#convolution-arithmetic", "anchor_text": "their animations"}, {"url": "https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/", "anchor_text": "A Beginner\u2019s Guide To Understanding Convolutional Neural Networks"}, {"url": "https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1", "anchor_text": "Intuitively Understanding Convolutions for Deep Learning"}, {"url": "http://terencebroad.com/convnetvis/vis.html", "anchor_text": "Topological Visualisation of a Convolutional Neural Network"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----48db75969fdf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----48db75969fdf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----48db75969fdf---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/tag/bias?source=post_page-----48db75969fdf---------------bias-----------------", "anchor_text": "Bias"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----48db75969fdf---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://creativecommons.org/licenses/by-sa/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&user=Antonino+Ingargiola&userId=8d706469449&source=-----48db75969fdf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&user=Antonino+Ingargiola&userId=8d706469449&source=-----48db75969fdf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F48db75969fdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----48db75969fdf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----48db75969fdf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----48db75969fdf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----48db75969fdf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----48db75969fdf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----48db75969fdf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tritemio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tritemio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Antonino Ingargiola"}, {"url": "https://medium.com/@tritemio/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d706469449&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&user=Antonino+Ingargiola&userId=8d706469449&source=post_page-8d706469449--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8d706469449%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-dive-into-convolutional-networks-48db75969fdf&user=Antonino+Ingargiola&userId=8d706469449&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}