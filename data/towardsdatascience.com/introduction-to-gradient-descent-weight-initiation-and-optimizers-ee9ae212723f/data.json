{"url": "https://towardsdatascience.com/introduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f", "time": 1683011025.262748, "path": "towardsdatascience.com/introduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f/", "webpage": {"metadata": {"title": "Introduction to Gradient Descent: Weight Initiation and Optimizers | by Abhijit Roy | Towards Data Science", "h1": "Introduction to Gradient Descent: Weight Initiation and Optimizers", "description": "Gradient Descent is one of the main driving algorithms behind all machine learning and deep learning methods. This mechanism has undergone several modifications over time in several ways to make it\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2", "anchor_text": "my article on it", "paragraph_index": 0}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 60}], "all_paragraphs": ["Gradient Descent is one of the main driving algorithms behind all machine learning and deep learning methods. This mechanism has undergone several modifications over time in several ways to make it more robust. In this article, we will be talking about two of them. For this, you will need a very clear intuition about what gradient descent is and how it operates. If you have any doubt or confusion about the topic, feel free to visit my article on it.", "Let\u2019s learn about two types of curves.", "This is a convex curve. It has a single global minimum. So, where ever we initialize any point we can use the gradients to reach the global minimum. If we consider the points as our weights and the curve as the loss function, we can easily converge to the minimum of the loss function using the derivative of the loss function with respect to weights.", "This is a non-convex curve, now, here if we use gradient descent from any point (x,y) on the curve, we are not sure to reach the global minimum. We may get stuck at the local minima. For example, (x1,y1) may get stuck at the local minimum following it, but (x2,y2) will reach the global easily. So, here the position of the point matters.", "A non-convex curve has three points, which will be of interest. Saddle points, which are the plateau-like region, the local, and the global minima. All these points will have", "Now, if we consider x as the weights and y=f(x) as the loss function, dL/dW=0", "Our equation of gradient descent until now, we have used,", "So, if dL/dW is 0, the weights remain the same and we get stuck in a sub-optimal state. These kinds of non-convex curves are very common in Neural networks due to complex functions.", "Now let\u2019s talk about some other issues, with the gradient descent algorithms.", "In Neural Networks, if we are using very deep networks, we have a problem called Vanishing and Exploding gradients. We have seen, in neural networks, we obtain an equation like", "and send this through several activation functions f(x) and g(x). On performing such operations multiple times, we obtain complex equations. For example,", "We have seen this in my previous article. We can see, we multiply the weights several times. More the number of layers, more number of multiplications.", "Now, if the weights are initialized with very high values, the gradient value will explode, this is called exploding Gradient Problem. Similarly, if the values of the weights are low, the gradient value almost vanishes due to multiplication with very small values over and over. This is called Vanishing Gradient Problem.", "3. The behavior of different types of gradient descent techniques.", "In my previous article, we talked about three types of gradient descent techniques.", "So, it takes the error of all the samples in the batch and calculates the sum of all the errors. Now, due to the fact it takes in the full set together, in one epoch it updates the weights only once. Also, as we use the same batch over each time, the distribution of the set does not change and so the weight distribution also remains similar on each update. If we visualize the descent,", "it will look something like this.", "For a higher dimension of weights, we represent as contours. The first graph shows the gradient movement in a 2d plane. The second graph shows the gradient movement for a complex function shown using contours and the third graph shows the convergence of the loss function.", "So, we can see that gradient descent is pretty smooth and loss function converges quickly. Here in the case of Batch Descent, we have to send the whole batch, if it has 1 million entries, it will be computationally very inefficient. So, we need to move to something different.", "2. Stochastic Gradient Descent: Here one sample of the training set is passed through the model at a time. So, the error is calculated as", "It is very fast. It updates the weights after each error calculation. If there are 100 samples in the set, in an epoch, SGD updates the weights 100 times once after each sample is predicted by the model. Now, each time, it tries a new sample which in most cases will be different from the previous one, correspondingly the distribution of the weights will also vary accordingly. Say there may be an outlier, for which the feature value distribution is different, correspondingly the weight distribution will vary for the sample, and it will be different from the previous sample. So, the gradient descent will get a bit noisy. Its visualization will look like,", "So, we can see, here also it converges, but the descent gets a bit noisy. This noise makes the gradient descent slow for Stochastic Gradient Descent.", "I shows what the descent should have been II shows descent for SGD. We can see that apart from the horizontal component a vertical component is developed. This vertical component is responsible for slowing things down. It is called Noise.", "3. Mini Batch Gradient Descent: Here we divide the whole training data set into smaller mini-batches and train using those. So, if there are 100 data in the training set and we take 25 as our sampling size, we obtain 4 mini-batches. The error is calculated as:", "where k is the number of samples in a batch. So, in each epoch, the error is calculated for 4 batches and updated 4 times. Here also, there is a noise problem but it is less compared to SGD. If we visualize this we will get,", "we will get something like this. We can see, there is noise but it is much less than the SGD. But to speed up the gradient descent we need to remove the noise as much as possible and dampen the vertical component of the movement as much as possible.", "So, we have listed the three important challenges of the gradient descent mechanisms.", "The method of weight initiation is used to solve our 2nd problem which is the exploding and vanishing gradient problem.", "We know that our neurons work on the equation,", "where n is the number of features and correspondingly the number of weights. We try to initialize the weights in such a manner such that,", "So, for the weight corresponding to each feature, we initialize the weight as", "where \u2018a\u2019 is a constant and \u2018a\u2019 varies from 1 to n-1. This prevents the weights from being initialized too big or too small.", "To solve the 1st and the 3rd problem, we use algorithms called optimizers. There are several optimizers which have been developed with two primary goals.", "Before we move to the optimizers let\u2019s see a concept called exponentially weighted average.", "In the diagram, the deep blue points are the data points. Now, we can see the data points are very randomly distributed and hence is very noisy. To, decrease the noise and to extract some actual trend of the distribution, we use a technique called moving average consideration.", "Say, to decide the variation of the value of a stock in stock marketing, we often look at the values of how it has performed in the last few days. If we consider the last 10 days value it is called 10 moving average and so on. This gives us a kind of approximation and helps to decrease the randomness.", "Similarly, in this case also to get the value at the nth time unit, we consider the weighted average of the last few time units along with the current value. Now, why we call it weighted? We call it so because it has a constant involved and as we move to a further time unit, the impact of the value of that time unit on our current time unit value starts decreasing.", "This is the equation we will use. Here V is the weighted average at time t and X is the data value at time t. Now, the parameter Beta is said to be the approximation of how many days average we are taking.", "Normally, we take the value of beta as 0.9. 0.9 value corresponds to the number of days moving average we are calculating.", "Now, if we have to calculate the weighted average of 50 days with beta 0.9", "This is the calculation, thus we are providing different weights to the impact of different time units. The (0.9)\u00b9\u2070 becomes very less and is not considered to effect after that. This is how the value is calculated.", "When we take the value 0.9 we obtain the red line in the image. It gives quite a good approximation of our data points. The green line represents beta = 0.5. It is a 2-time unit average, so it adapts to the change in the data point very quickly, and still has a high degree of noise, while the blue corresponds to 0.98 beta value. It is 50 days average so, needs a lot of time to adapt.", "But, here there is a problem at t=1, the estimation comes out very poor. If X1=10", "as V0=0. So, we use a modification called Bias Correction.", "This is done to get a correct approximate for the lower t values. For higher t values, the (beta)^t gets very less and insignificant and", "So, the value remains the same.", "This technique uses the weighted-average method to stabilize the vertical movements and also the problem of the suboptimal state.", "If we see the image we will see that, it shows the noisy movements introduced in the descent. Now if we decompose each movement we will get two components, the right movement, and the vertical movement. Now as we take the weighted average the vertical movements which are actually in a direction opposite to each other for example 1 and 2, 3 and 4 are all opposite, this dampens the magnitude of movements in the vertical direction but the horizontal movement is same for all. So the horizontal movement speeds up as the movements add up positively to solve our problem.", "Another thing to notice is, W(old) is no longer equal to W(new) when dL/dW=0. It has another component that keeps the descent from entering a suboptimal state.", "Adaptive Gradient optimizer uses a technique of modifying the learning rate during training. It starts with a high learning rate and the rate decreases as it converges. It is seen that this method helps to converge better and as when we approach the minima it takes very small steps and moves very carefully, so there is very little chance of overstepping the optimum point. It also helps to avoid saddle points and the local minima.", "So, it uses a modified learning rate parameter that adapts to the changes over time.", "So, it changes the learning rate alpha to alpha\u00b9. We will define alpha\u00b9 as follows.", "So, if we see the second equation it takes summation of squares of all the gradients until that unit. In the first equation, we can see, it starts with a fixed learning rate alpha. The epsilon value is 10^-8 it is value to prevent the denominator from being 0.", "The alpha\u00b9 value gets smaller as the (nu) value increases as time units pass. So, more the value of t more is the nu value and less is the alpha\u00b9, less is learning rate, and the steps taken towards the minima shortens.", "In Adagrad, a problem remains i,e the nu value becomes very large and also the dL/dW \u2019s sudden changes affects the learning parameter. So, in RMSProp the weighted average was introduced to calculate the nu value also. Though, here the beta value of the weighted average is taken as 0.999.", "So, RMSProp regulates the learning rate parameter using the weighted average method.", "Adam or Adaptive Moment optimizer is based on the use of both RMSprop and SGD with Moment along with bias correction.", "Adam optimizer is the most robust optimizer and most used. It optimizes the learning rate as well as introduce moments to solve the challenges in gradient descent.", "In this article, we have talked about the challenges to gradient descent and the solutions used. We have also talked about several optimizers in detail.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fee9ae212723f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----ee9ae212723f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2", "anchor_text": "my article on it"}, {"url": "https://medium.com/tag/optimizer?source=post_page-----ee9ae212723f---------------optimizer-----------------", "anchor_text": "Optimizer"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----ee9ae212723f---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ee9ae212723f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/adam?source=post_page-----ee9ae212723f---------------adam-----------------", "anchor_text": "Adam"}, {"url": "https://medium.com/tag/sgd?source=post_page-----ee9ae212723f---------------sgd-----------------", "anchor_text": "Sgd"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----ee9ae212723f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----ee9ae212723f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fee9ae212723f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ee9ae212723f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ee9ae212723f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ee9ae212723f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ee9ae212723f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "458 Followers"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}