{"url": "https://towardsdatascience.com/something-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13", "time": 1683015568.1685388, "path": "towardsdatascience.com/something-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13/", "webpage": {"metadata": {"title": "Use NLP and ML to Structure Extracted Web Data | Towards Data Science", "h1": "Something From Nothing: Use NLP and ML to Extract and Structure Web Data", "description": "Use Python, Natural Language Processing, and Machine Learning to Extract and Structure Extracted Web Data from the Institute for the Study of War."}, "outgoing_paragraph_urls": [{"url": "http://www.understandingwar.org/", "anchor_text": "Institute for the Study of War", "paragraph_index": 0}, {"url": "https://colab.research.google.com/drive/1pTrOXW3k5VQo1lEaahCo79AHpyp5ZdfQ?usp=sharing", "anchor_text": "follow this link", "paragraph_index": 1}, {"url": "https://www.kaggle.com/connerbrew2/isw-web-scrape-and-nlp-enrichment", "anchor_text": "follow this link", "paragraph_index": 1}, {"url": "https://spacy.io/api/doc", "anchor_text": "Spacy", "paragraph_index": 2}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "BeautifulSoup", "paragraph_index": 2}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK", "paragraph_index": 2}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "documentation", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Lists_of_countries_and_territories", "anchor_text": "Wikipedia", "paragraph_index": 10}, {"url": "https://medium.com/@ODSC/an-introduction-to-natural-language-processing-nlp-8e476d9f5f59", "anchor_text": "this article on Medium", "paragraph_index": 13}, {"url": "https://opencagedata.com/api", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://medium.com/datadriveninvestor/tf-idf-in-natural-language-processing-8db8ef4a7736", "anchor_text": "this article on Medium", "paragraph_index": 21}, {"url": "https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175", "anchor_text": "k-means clustering", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/investigate-anomalies-in-temporal-data-with-machine-learning-b3da86793142", "anchor_text": "my time-series analysis article", "paragraph_index": 25}, {"url": "https://medium.com/free-code-camp/learn-mongodb-a4ce205e7739", "anchor_text": "this article on Medium", "paragraph_index": 31}, {"url": "https://www.linkedin.com/in/cmbrew/", "anchor_text": "reach out", "paragraph_index": 32}, {"url": "https://www.linkedin.com/in/cmbrew/", "anchor_text": "https://www.linkedin.com/in/cmbrew/", "paragraph_index": 34}], "all_paragraphs": ["In this article, we will be creating a structured document database based on the Institute for the Study of War (ISW) production library. ISW creates informational products for diplomatic and intelligence professionals to gain a deeper understanding of conflicts occurring around the world.", "To see the original code and notebook associated with this article, follow this link. To access the final structured dataset, hosted on Kaggle, follow this link.", "This article will be an exercise in web extraction, natural language processing (NLP), and named entity recognition (NER). For the NLP, we will primarily be using the open-source Python libraries NLTK and Spacy. This article is intended to be demonstration of a use-case for web extraction and NLP, not a comprehensive beginner tutorial to the usage of either technique. If you are new to NLP or web extraction, I would urge you to follow a different guide or look through the Spacy, BeautifulSoup, and NLTK documentation pages.", "First, we will initialize the data fields we want in our final structured data. For each document, I want to extract the title, date of publication, names of people, names of places, and various other information. We\u2019ll also enhance the information that already exists in the document\u2014 for example, we\u2019ll use the place names in the document to get relevant coordinates, which could be useful for visualizing data later on.", "We will be extracting our documents from ISW\u2019s production library. First, we will scrape the \u2018browse\u2019 page to get individual href links for each product. Then we store those links in a list for our extraction functions to visit later.", "The first few functions we\u2019ll write are fairly straightforward text extraction. This tutorial is not intended to be a tutorial on the usage of BeautifulSoup \u2014 for an introduction to web scraping in Python, check out the documentation here.", "For our first function, we will be extracting the publication date. It scans through the html document extracted from the product\u2019s webpage, and finds a field with the class of \u2018submitted\u2019. This contains our production date.", "Next, we want the product title. Again, this field is conveniently labeled with a class of \u2018title\u2019.", "Finally, we will extract the full text of the document. When I extract text, I often follow an \u2018extract-first, filter-later\u2019 style of web extraction. That means that, in my initial text extraction, I perform minimal filtering and processing of the text. I prefer to conduct that processing later on in my analysis as it becomes necessary. However, if you are more advanced, you may want to conduct more pre-processing of the extracted text than the below function demonstrates. Again, I recommend you follow the documentation for reference.", "For my get_contents function, I stuck to the bare bones \u2014 I listed a few html parents in a blacklist, for text that I don\u2019t want to be extracted. Then I extract all the text from the page and append it into a temporary string, which in turn is appended into the list content_text.", "Next, we will figure out what countries are referenced in the product. There are many APIs that could be used in checking text content for countries, but here we will use a simple method: a list of all the countries in the world. This list is derived from Wikipedia.", "After the function identified all_mentioned_countries in the document, it uses basic statistical analysis to identify which countries are featured most prominently \u2014 these countries are most likely to be the point of focus for the document\u2019s narrative. To do this, the function counts the number of times a country is mentioned throughout the document and then finds countries mentioned more times than the average. These countries are then appended to a key_countries list.", "Next, we want to enrich our data. Ultimately, the goal of structuring data is typically to perform some kind of analysis or visualization \u2014 in the case of this international conflict information, it would be valuable to plot the information geographically. To do this, we need coordinates corresponding to the documents.", "First, we will use natural language processing (NLP) and named entity recognition (NER) to extract place-names from the text. NLP is a form of machine learning, in which computer algorithms use grammar and syntax rules to learn relationships between words in text. Using that learning, NER is able to understand the role that certain words play within a sentence or paragraph. This tutorial is not intended to be a comprehensive introduction to NLP \u2014 for such a resource, try this article on Medium.", "To then find coordinates for the place names, we will use the Open Cage API to query for coordinates; you can make a free account and receive an API key here. There are many other popular geo-coding APIs to choose from, but through trial and error I found Open Cage to have the best performance given obscure place names in the Middle East.", "First, we iterate through each place name retrieved from the document and query it in Open Cage. Once this is done, we will cross-reference the Open Cage results with the mentioned_countries list created earlier. This will ensure that the query results we retrieve are located in the correct place.", "Next, we will extract the names of people mentioned in the document. To do this, we will again use the NER algorithms from the NER-D python library.", "In the final structured data, I only want full names. Wouldn\u2019t it be confusing to find a data entry with a \u2018mentioned person\u2019 of \u201cJack\u201d or \u201cJohn\u201d? To accomplish this, we will once again employ some rudimentary statistics. The function will track full names when they are mentioned, usually in the beginning of the text.", "When a partial name is mentioned later, it will reference the list of full names to identify who the partial name is referencing. For example, if a news article read as follows: \u2018Joe Biden is running for President. Joe is best known as the Vice President for former President Barrack Obama.\u2019 We know that Joe is referencing Joe Biden, because his full name was given earlier in the text. This function will operate in that same way.", "In the case of duplicates, the function will use the same statistics used earlier for the country function. It will measure a count of how many times a name was mentioned, and use that as the most likely identifier. Example: \u2018Joe Biden and his son, Hunter Biden, are popular US politicians. Joe Biden is the former VP. Biden is now making a run for president against incumbent Donald Trump\u2019 We know that \u2018Biden\u2019 is referencing \u2018Joe Biden\u2019 from context. The passage is clearly about Joe Biden, not Hunter Biden, based on the statistical focus of the text.", "Once the function has figured out all the full names mentioned, it will add them to a list. It will then query each name in Wikipedia, to verify that it is the name of an influential person worthy of being included in the structured data.", "Our next task is to extract keywords from the text. The most common method of doing this is by using a method called Term Frequency-Inverse Document Frequency (TF-IDF). Basically, TF-IDF models measure how often a term or word was used within a single document, then compares that to its average usage throughout the entire corpus of documents. If a term is used frequently in a single document, and infrequently across the entire corpus of documents, then it is likely that term represents a keyword unique to that specific document. This article is not meant to be a comprehensive overview of TF-IDF models. For more information, check out this article on Medium.", "First, our function will create what is commonly known as a \u2018bag-of-words\u2019. This will track every word used in every document. Then, it will count every usage of every word in each document \u2014 the term frequency. Then, it takes the common logarithm of every sentence in every document containing the term \u2014 the inverse document frequency. Those values are then written to coordinates in a matrix, which is then sorted to help us find the words most likely to represent unique keywords for our document.", "One of the most common tasks in NLP is known as topic modeling. This is a form of clustering that attempts to automatically sort documents into categories based on their text content. In this specific instance, I would like to know at-a-glance what topics ISW is covering. By sorting documents into categories based on text content, I can easily gain an at-a-glance understanding of the document\u2019s main ideas.", "For this example, I will be using a k-means clustering algorithm to conduct topic modeling. First, I will use a TF-IDF algorithm again to vectorize each document. Vectorization is a machine-learning term that refers to the transformation of non-numeric data into numeric spatial data that the computer can use to conduct machine learning tasks.", "Once documents are vectorized, helper functions check to see what the optimal number of clusters are. (The k in k-means). In this case, the optimal number was 50. Once I found the optimal number, in this example I commented out that line of code and manually adjusted the parameters to equal 50. That is because the dataset I am analyzing does not change often, so I can expect the number of optimal clusters to stay the same over time. For data that changes more frequently, you should return the optimal number of clusters as a variable \u2014 this will help your clustering algorithm to automatically set its optimal parameters. I demonstrate an example of this in my time-series analysis article.", "Once each cluster is complete, I save the number of each cluster (1\u201350) to a list of cluster_numbers and the keywords making up each cluster to a list of cluster_keywords. These cluster keywords will be used later to add a title to each topic cluster.", "Finally, we will extract our data. Using the list of hrefs we got earlier, it is time to apply all of our extraction functions to the web content.", "Our next problem is this: Our clusters gave us a list of words that are associated with each cluster, but the clusters are titled simply with numbers. This gives us the opportunity to plot a word cloud or other interesting visualization that can help us understand each cluster, but it not as useful for at-a-glance understanding in a structured dataset. Additionally, I believe that some documents may fall within multiple topic categories. Multiple clustering is not supported by k-means, so I will have to identify these documents manually. First, I\u2019ll print the first few rows of keywords to get an idea of the data I\u2019m dealing with.", "After significant experimentation with a variety of techniques, I decided on a very simple approach. I scanned each list of keywords pertaining to each cluster, and noted significant keywords in each that related to a specific topic. At this stage, domain knowledge was key. I know, for example, that Aleppo in an ISW document is almost certainly mentioned in reference to the Syrian Civil War. For your data, if you lack the appropriate domain knowledge, you may need to do further research, consult someone else on your team, or define a more advanced programmatic method for titling your clusters.", "For this example, however, the simple approach works well. After making note of several significant keywords present in the cluster lists, I made a few lists of my own that contained keywords associated to the final topic categories I wanted in the structured data. The function simply compares each cluster\u2019s list of keywords with the lists I created, then assigned a topic name based on matches in the lists. It then appends those final topics to a list of topic_categories.", "The very last step is to bring together all our extracted data. For this data, I prefer the JSON format. This is because I wanted to structure certain types of data differently \u2014 for example, the locations field will include a list of dictionaries of place names, latitudes, and longitudes. In my opinion, JSON format is the most effective way to store such formatted data to a local disk. I also backed up a copy of this database in a document database, MongoDB, but that is not the focus of this article. If you are interested in saving your structured data to a document database, try this article on Medium.", "Now we\u2019re done! We extracted links from a web page, then used those links to extract even more content from the web site. We used that content to then extract and enhance that information using external APIs, ML clustering algorithms, and NLP. NLP today is one of the foremost buzz-words in the business intelligence community, and now you can confidently execute intermediate-level operations in NLP for document analysis. You can conduct TF-IDF vectorization, keyword extraction, and topic modeling. These are the cornerstones of NLP. Please reach out if you have more questions or need information, and good luck in your future NLP endeavors!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Marine Corps Veteran and Data Scientist Specializing in National Security and Counter-Terrorism. Find me on Linkedin at: https://www.linkedin.com/in/cmbrew/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3f49b2f72b13&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@conner.brew?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conner.brew?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Conner Brew"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5368ea679397&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&user=Conner+Brew&userId=5368ea679397&source=post_page-5368ea679397----3f49b2f72b13---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "http://www.understandingwar.org/", "anchor_text": "Institute for the Study of War"}, {"url": "https://colab.research.google.com/drive/1pTrOXW3k5VQo1lEaahCo79AHpyp5ZdfQ?usp=sharing", "anchor_text": "follow this link"}, {"url": "https://www.kaggle.com/connerbrew2/isw-web-scrape-and-nlp-enrichment", "anchor_text": "follow this link"}, {"url": "https://spacy.io/api/doc", "anchor_text": "Spacy"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "BeautifulSoup"}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "here"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "documentation"}, {"url": "https://en.wikipedia.org/wiki/Lists_of_countries_and_territories", "anchor_text": "Wikipedia"}, {"url": "https://medium.com/@ODSC/an-introduction-to-natural-language-processing-nlp-8e476d9f5f59", "anchor_text": "this article on Medium"}, {"url": "https://opencagedata.com/api", "anchor_text": "here"}, {"url": "https://medium.com/datadriveninvestor/tf-idf-in-natural-language-processing-8db8ef4a7736", "anchor_text": "this article on Medium"}, {"url": "https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175", "anchor_text": "k-means clustering"}, {"url": "https://towardsdatascience.com/investigate-anomalies-in-temporal-data-with-machine-learning-b3da86793142", "anchor_text": "my time-series analysis article"}, {"url": "https://medium.com/free-code-camp/learn-mongodb-a4ce205e7739", "anchor_text": "this article on Medium"}, {"url": "https://www.linkedin.com/in/cmbrew/", "anchor_text": "reach out"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3f49b2f72b13---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3f49b2f72b13---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3f49b2f72b13---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data?source=post_page-----3f49b2f72b13---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----3f49b2f72b13---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&user=Conner+Brew&userId=5368ea679397&source=-----3f49b2f72b13---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&user=Conner+Brew&userId=5368ea679397&source=-----3f49b2f72b13---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3f49b2f72b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3f49b2f72b13---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3f49b2f72b13--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conner.brew?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conner.brew?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Conner Brew"}, {"url": "https://medium.com/@conner.brew/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "54 Followers"}, {"url": "https://www.linkedin.com/in/cmbrew/", "anchor_text": "https://www.linkedin.com/in/cmbrew/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5368ea679397&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&user=Conner+Brew&userId=5368ea679397&source=post_page-5368ea679397--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1ebfe96413&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsomething-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13&newsletterV3=5368ea679397&newsletterV3Id=b1ebfe96413&user=Conner+Brew&userId=5368ea679397&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}