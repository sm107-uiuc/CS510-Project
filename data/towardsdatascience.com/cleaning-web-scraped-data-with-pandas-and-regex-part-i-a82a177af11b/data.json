{"url": "https://towardsdatascience.com/cleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b", "time": 1683001130.888594, "path": "towardsdatascience.com/cleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b/", "webpage": {"metadata": {"title": "Cleaning Web-Scraped Data With Pandas and Regex! (Part I) | by Rohan Gupta | Towards Data Science", "h1": "Cleaning Web-Scraped Data With Pandas and Regex! (Part I)", "description": "Now that I\u2019ve started programming on a daily basis, I decided to invest in a laptop that would allow me to multi-task smoothly and run all my desired applications the way I\u2019m used to. I own a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn/related", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://www.scrapehero.com/scrape-product-data-from-amazon/", "anchor_text": "this guide", "paragraph_index": 7}, {"url": "https://www.scrapehero.com/scrape-product-data-from-amazon/", "anchor_text": "Learn How to Scrape the Data", "paragraph_index": 7}], "all_paragraphs": ["Now that I\u2019ve started programming on a daily basis, I decided to invest in a laptop that would allow me to multi-task smoothly and run all my desired applications the way I\u2019m used to.", "I own a Unix-based Macbook, but its M3 processor just doesn\u2019t cut it when I\u2019m working on projects that require processing a lot of images or videos. I also have a Windows notebook from work with a decent i5, but after using it, I\u2019ve realized that my needs are much higher.", "However, I\u2019m not about to just drop $2000 any machine with good specs. I actually want to understand how the components/specs of a laptop (Size, Storage, RAM, Processor, etc.) contribute to form an overall representation of the Lapton in the market at a certain price. For example, I want to derive:", "There are a number of insights I could derive from scraping product data, and I will eventually develop an algorithm to find the right Laptop given a number of inputs.", "Thanks to a friend, I found out about the \u201cWeb Scraper\u201d extension on Google Chrome. You can find it here. The extension helped me collect the required data for Laptops on Amazon.", "This web-scraper does what most others do: it collects information we want from the page source. Websites don\u2019t always make it easy for you to extract data from their pages, therefore you would need to clean the extracted data before you can use it for any kind of analysis.", "So what do I mean by \u201ccleaning\u201d the data?Often, the data will have some impurities, such as NaN (empty) values, unnecessary columns, unreadable characters or spaces ( \u044f or \\s). In the following examples of my laptop dataset, you\u2019ll see how all our data gets stored in a single column and we would need to split each set of words into separate columns.", "For the Web-Scraping, you can follow this guide on how to extract product data on Amazon using the Web Scraper extension on Google Chrome. I will specifically be working with product data on Amazon, although I believe you can scrape from a lot of websites. The important thing is to find the right JSON code for your extractions (as you can see in the guide). Before you move on to the next part, Learn How to Scrape the Data. If you already have the scraped data as a CSV file, then feel free to move on.", "Now that you have your scraped data as a CSV, let\u2019s load up a Jupyter notebook and import the following libraries:", "Then upload data and read it with df = pd.read_csv('amazon.csv') . The table should look like the output below.", "So here\u2019s what I thought of the data: Not completely useless because it does contain all the required information, especially in the product_specification column. However, when we look at one instance of this column, it looks like:", "Looks like it\u2019s totally possible to separate these components into different columns for each item in my dataset. To start off, first I\u2019ll separate my data and copy only the required columns into a new DataFrame.", "Now that we have our columns in data, let\u2019s start the cleaning process.", "Think like a Data Scientist, but also.. think like Marie Kondo", "First, we\u2019re going to have to drop all the rows that contain Null/NaN values. This is because we can\u2019t run regex functions on columns that don\u2019t have any value, we would end up with unmatching index values and it would be hard to merge our cleaned columns to the dataset later.", "Now that we have dropped our null values, we can start to clean the dataset. The first step would be to remove the \\s, \\n, \\t characters. As you see in the picture above, our information is clouded within \\n characters.", "Note: I used \\s\\s\\s because after some experimentation, I noticed a pattern of three-spaced tabs. However, you can also just use \\s if you don\u2019t have such a pattern.", "This next step is also extremely important. We have to drop the specific rows that don\u2019t contain information on the following. The only problem is that it will reduce the size of your dataset significantly. However, in my case, I would say that is better than having empty values. Empty values would have to be filled out by mean, median or mode, neither of which would make sense here. Thus, I will drop the rows which don\u2019t have my required data.", "In the lines above, when I use the ~ operator, the function will do the opposite. So when I say ~data.product_specification.str.contains, I\u2019m actually doing a \u201cdoesn\u2019t-contain\u201d function and will not drop strings that contain the value inside the bracket. Inside the bracket, I specify the regex string I\u2019m looking for. For example, if I want 0 or more words after the word \u201cCard Description\u201d, I can say Card Description (\\w+){0,}. I will explain it further in the section after this.", "Now that I know the length of my data will not be compromised, I can convert the product_specification column into a list for the regex functionality.", "This is where I\u2019ll explain the regex syntax and how I extracted the textual data from each column. In the lines below, I\u2019m extracting the screen sizes of each laptop in my list and storing them in a separate list called \u201cscreen_size\u201d.", "The image below shows us what screen_size looks like after the regex loop.", "As you can see, for each laptop I have the word \u201cScreen Size\u201d along with the size (15.6, 17.3, etc) stored in the list.", "So if you closely look at the regex syntax, you can see I asked for a float number after the word \u201cScreen Size \u201d", "The reason we can\u2019t use \\d is that these aren\u2019t integers, but float types. Therefore I have to specify that there will be a \u201c.\u201d between 2 integers with \\d*\\.?\\d+.", "Let\u2019s see another example with extraction of the processor speed for each laptop.", "We repeat the steps, writing similar lines while changing the regex statement.", "The image below shows us what processor looks like after the regex loop.", "Now, if you look at the difference between this one and the last one, you can see that I added \\s\\w+ after the \\d*\\.?\\d+ because now we have another word \u201cGHz\u201d after the float number.", "Thus, it\u2019s important to look at the data and know what type of information you\u2019ll be collecting for each component.", "We would need to perform the same kind of loop for each of the components. If you look at my gist at the end, you will find all the examples already done in there.", "Note: This is only the first part! In the next post, which will come out shortly, I will store all these lists into my DataFrame df and perform more sanitary operations such as cleaning names, replacing infrequent categories, removing $ signs and units such as \u201cGHz\u201d or \u201cGB\u201d.", "Below, you can find my notebook with all the code to clean this data.", "Stay tuned for more \u2014 Part 2 Coming Soon.", "Follow Rohan Gupta for other Data Science content and tutorials!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist/Analyst/Writer \u2014 I love spreading knowledge."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa82a177af11b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a82a177af11b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rohanguptaa33?source=post_page-----a82a177af11b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rohanguptaa33?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Rohan Gupta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff463f1bf80e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&user=Rohan+Gupta&userId=f463f1bf80e2&source=post_page-f463f1bf80e2----a82a177af11b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral", "anchor_text": "JESHOOTS.COM"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn/related", "anchor_text": "here"}, {"url": "https://unsplash.com/@creativeexchange?utm_source=medium&utm_medium=referral", "anchor_text": "The Creative Exchange"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.scrapehero.com/scrape-product-data-from-amazon/", "anchor_text": "this guide"}, {"url": "https://www.scrapehero.com/scrape-product-data-from-amazon/", "anchor_text": "Learn How to Scrape the Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a82a177af11b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/pandas?source=post_page-----a82a177af11b---------------pandas-----------------", "anchor_text": "Pandas"}, {"url": "https://medium.com/tag/regex?source=post_page-----a82a177af11b---------------regex-----------------", "anchor_text": "Regex"}, {"url": "https://medium.com/tag/python?source=post_page-----a82a177af11b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----a82a177af11b---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&user=Rohan+Gupta&userId=f463f1bf80e2&source=-----a82a177af11b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&user=Rohan+Gupta&userId=f463f1bf80e2&source=-----a82a177af11b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa82a177af11b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a82a177af11b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a82a177af11b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a82a177af11b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a82a177af11b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a82a177af11b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a82a177af11b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rohanguptaa33?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rohanguptaa33?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Gupta"}, {"url": "https://medium.com/@rohanguptaa33/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "567 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff463f1bf80e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&user=Rohan+Gupta&userId=f463f1bf80e2&source=post_page-f463f1bf80e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffa9b9e13a272&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleaning-web-scraped-data-with-pandas-and-regex-part-i-a82a177af11b&newsletterV3=f463f1bf80e2&newsletterV3Id=fa9b9e13a272&user=Rohan+Gupta&userId=f463f1bf80e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}