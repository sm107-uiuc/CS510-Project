{"url": "https://towardsdatascience.com/knowledge-distillation-a-survey-through-time-187de05a278a", "time": 1683013930.9954402, "path": "towardsdatascience.com/knowledge-distillation-a-survey-through-time-187de05a278a/", "webpage": {"metadata": {"title": "Knowledge Distillation \u2014 A Survey Through Time | by Nishant Nikhil | Towards Data Science", "h1": "Knowledge Distillation \u2014 A Survey Through Time", "description": "In 2012, AlexNet outperformed all the existing models on the ImageNet data. Neural networks were about to see major adoption. By 2015, many state of the arts were broken. The trend was to use neural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "knowledge distillation", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "https://arxiv.org/abs/1503.02531", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1412.6550", "anchor_text": "FitNets: Hints for Thin Deep Nets", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1612.03928", "anchor_text": "Paying more attention to attention: Improving the performance of convolutional neural networks via Attention Transfer", "paragraph_index": 9}, {"url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf", "anchor_text": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning", "paragraph_index": 11}, {"url": "https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer", "anchor_text": "Paraphrasing Complex Network: Network Compression via Factor Transfer", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1904.01866", "anchor_text": "A Comprehensive Overhaul of Feature Distillation", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1910.10699", "anchor_text": "Contrastive Representation Distillation", "paragraph_index": 23}], "all_paragraphs": ["In 2012, AlexNet outperformed all the existing models on the ImageNet data. Neural networks were about to see major adoption. By 2015, many state of the arts were broken. The trend was to use neural networks on any use case you could find. The success of VGG Net further affirmed the use of deeper-model or ensemble of models to get a performance boost.", "(Ensemble of models is only a fancy term. It means averaging of outputs from multiple models. Like if there are three models and two models predict \u2018A\u2019 while one model predicts \u2018B\u2019, then take the final prediction as \u2018A\u2019 (two votes versus one vote))", "But these deeper models and these ensemble of models are too costly to run during inference. (An ensemble of 3 models uses 3x the amount of computations of a single model).", "Geoffrey Hinton, Oriol Vinyals and Jeff Dean came up with a strategy to train shallow models guided by these pre-trained ensembles. They called this knowledge distillation because you distill knowledge from a pre-trained model to a new model. As this seems like a teacher guiding a student, so this is also called teacher-student learning. https://arxiv.org/abs/1503.02531", "In Knowledge Distillation they used the output probability of the pre-trained model as the labels for the new shallow model. Through this blog you would go through the improvements of this technique.", "In 2015 came FitNets: Hints for Thin Deep Nets (published at ICLR\u201915)", "FitNets add an additional term along with the KD loss. They take representation from the middle point of both the networks, and add a mean square loss between the feature representations at these points.", "The trained-network is providing a learnt-intermediate-representation which the new-network is mimicking. These representations help the student to learn efficiently, and were called hints.", "Looking back, this choice of using a single point for giving hints is sub-optimal. A lot of subsequent papers try to improve these hints.", "Paying more attention to attention: Improving the performance of convolutional neural networks via Attention Transfer was published at ICLR 2017", "They have similar motivation as FitNets, but rather than the representations from a point in the network, they use the attention maps as the hints. (MSE over attention maps of student and teacher). They also use multiple points in the network for giving hints, rather than the one point hint in FitNets", "In the same year, A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning was published at CVPR 2017.", "This is also similar to FitNets and the attention transfer paper. But instead of the representation and the attention maps, they give hints using the Gram matrices.", "They have an analogy for this in the paper:", "\u201cIn the case of people, the teacher explains the solution process for a problem, and the student learns the flow of the solution procedure. The student DNN does not necessarily have to learn the intermediate output when the specific question is input but can learn the solution method when a specific type of question is encountered. In this manner, we believe that demonstrating the solution process for the problem provides better generalization than teaching the intermediate result.\u201d", "To measure this \u201cflow of solution procedure\u201d, they use a gram matrix between the feature maps of two layers. So instead of the intermediate feature representation as the hints in FitNets, this uses Gram matrix between feature representations as the hints.", "Then in 2018 came Paraphrasing Complex Network: Network Compression via Factor Transfer published at NeurIPS 2018", "They add another module to the model which they call paraphraser. It is basically an auto-encoder which doesn\u2019t reduce dimensions. From the last layer they fork out another layer which trains on the reconstruction loss.", "The student has another module named translator. It embeds the outputs of the student\u2019s last layer to the teacher-paraphraser\u2019s dimensions. And they use this latent paraphrased representation from the teacher as hints.", "tl;dr The student should be able to construct an auto-encoded representation of the input from the teacher network.", "In 2019, A Comprehensive Overhaul of Feature Distillation was published at ICCV 2019.", "They claim that the position from which we take the hints isn\u2019t optimal. The outputs are refined through ReLU and some information is lost during that transformation. They propose a marginReLU activation (a shifted ReLU). \u201cIn our margin ReLU, the positive (beneficial) information is used without any transformation while the negative (adverse) information is suppressed. As a result, the proposed method can perform distillation without missing the beneficial information\u201d", "They employ a partial L2 distance function which is designed to skip the distillation of information on a negative region. (No loss if both the feature vector from student and from the teacher at that location is negative)", "Contrastive Representation Distillation was published at ICLR 2020. Here also the student learns from the teacher\u2019s intermediate representations, but instead of MSE loss they use a contrastive loss over them.", "In total, these different models have employed different methods to", "Another interesting way to look at these ideas is that new ideas are vector sum of old ideas.", "What could be other vector sums?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learner | Applied Scientist @amazonIN | prev @IITKGP @GSoC @eccvconf EMNLP\u201918"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F187de05a278a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----187de05a278a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nishantnikhil?source=post_page-----187de05a278a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nishantnikhil?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Nishant Nikhil"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd21b74dc6d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&user=Nishant+Nikhil&userId=d21b74dc6d70&source=post_page-d21b74dc6d70----187de05a278a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "knowledge distillation"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "https://arxiv.org/abs/1503.02531"}, {"url": "https://nervanasystems.github.io/distiller/knowledge_distillation.html)", "anchor_text": "https://nervanasystems.github.io/distiller/knowledge_distillation.html)"}, {"url": "https://arxiv.org/abs/1412.6550", "anchor_text": "FitNets: Hints for Thin Deep Nets"}, {"url": "https://arxiv.org/abs/1612.03928", "anchor_text": "Paying more attention to attention: Improving the performance of convolutional neural networks via Attention Transfer"}, {"url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf", "anchor_text": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning"}, {"url": "https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer", "anchor_text": "Paraphrasing Complex Network: Network Compression via Factor Transfer"}, {"url": "https://arxiv.org/abs/1904.01866", "anchor_text": "A Comprehensive Overhaul of Feature Distillation"}, {"url": "https://arxiv.org/abs/1910.10699", "anchor_text": "Contrastive Representation Distillation"}, {"url": "https://arxiv.org/abs/1911.04252", "anchor_text": "Self-Training with Noisy Student Improves ImageNet classification"}, {"url": "https://twitter.com/nishantiam/status/1295076936469762048", "anchor_text": "https://twitter.com/nishantiam/status/1295076936469762048"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----187de05a278a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----187de05a278a---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----187de05a278a---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----187de05a278a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/knowledge-distillation?source=post_page-----187de05a278a---------------knowledge_distillation-----------------", "anchor_text": "Knowledge Distillation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&user=Nishant+Nikhil&userId=d21b74dc6d70&source=-----187de05a278a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&user=Nishant+Nikhil&userId=d21b74dc6d70&source=-----187de05a278a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----187de05a278a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F187de05a278a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----187de05a278a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----187de05a278a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----187de05a278a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----187de05a278a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----187de05a278a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----187de05a278a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nishantnikhil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nishantnikhil?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nishant Nikhil"}, {"url": "https://medium.com/@nishantnikhil/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd21b74dc6d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&user=Nishant+Nikhil&userId=d21b74dc6d70&source=post_page-d21b74dc6d70--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F45514760512&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-distillation-a-survey-through-time-187de05a278a&newsletterV3=d21b74dc6d70&newsletterV3Id=45514760512&user=Nishant+Nikhil&userId=d21b74dc6d70&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}