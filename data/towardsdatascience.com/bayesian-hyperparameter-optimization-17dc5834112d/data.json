{"url": "https://towardsdatascience.com/bayesian-hyperparameter-optimization-17dc5834112d", "time": 1683001520.024375, "path": "towardsdatascience.com/bayesian-hyperparameter-optimization-17dc5834112d/", "webpage": {"metadata": {"title": "Bayesian Hyperparameter Optimization | by Matti Karppanen | Towards Data Science", "h1": "Bayesian Hyperparameter Optimization", "description": "Hyperparameters are the knobs, levers and screws used to tune how a machine learning algorithm learns. You can get more performance out of a properly tuned machine learning system. The most common\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search", "anchor_text": "Grid Search", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search", "anchor_text": "Random Search", "paragraph_index": 1}], "all_paragraphs": ["Hyperparameters are the knobs, levers and screws used to tune how a machine learning algorithm learns. You can get more performance out of a properly tuned machine learning system.", "The most common way hyperparameters are used in the field of machine learning is either blindly using defaults or using some combination of hyperparameters that seemed to work well on another problem, or that you saw someone \u201crespected\u201d using on the internet. Other practitioners use either Grid Search or Random Search. These have the advantage of being massively distributable. But they are brute force, and computationally extremely expensive. They waste a lot of trials exploring pointless combinations of hyperparameters in silly portions of the hyperparameter space. Nor is there any guarantee of randomly stumbling to the global maximum. Furthermore, Grid Search suffers deeply from the curse of dimensionality both in terms of accuracy and the number points to check.", "Let\u2019s consider one-dimensional Bayesian Optimization for the sake of simplicity. The idea is the same for higher-dimensional hyperparameter spaces. Assume the black curve is our underlying function and the dots are observations. The observations can be, and in practice are, noisy, meaning that they do not hit the underlying \u201cground truth\u201d function perfectly. The X-axis represents values of a single hyperparameter, like the learning rate of your machine learning algorithm. The Y-axis is model performance using a metric where larger numbers mean better performance.", "We use a random process to create a large bunch of candidate functions so that the red dot observations could have been the output of that function. We assume that these functions are continuous and smooth\u00b9. The confidence bounds in the gif below represent the possible values and routes of these functions. In other words, we take a random sample from the set of all the smooth continuous functions which go through (or close by) all the red dots.", "Next, we choose a candidate to explore next by taking the combination of hyperparameters which maximizes expected improvement\u00b2. The 1D iterative process looks like this:", "For those who wish to follow along with Python code, I created notebook on Google Colab in which we optimize XGBoost hyperparameters with Bayesian optimization on the Scania Truck Air Pressure System dataset. Then we compare the results to random search. You can try for yourself by clicking the \u201cOpen in Colab\u201d button below.", "[1] Smooth means that we use a covariance function to ensure that nearby values of X correspond to nearby values of y.", "[2] There are other ways, or more properly acquisition functions, to choose the next candidate point. Expected Improvement (EI) is the most common due to being very performant and cheap to calculate, as well as intuitive to understand.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F17dc5834112d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----17dc5834112d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@matti.karppanen?source=post_page-----17dc5834112d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matti.karppanen?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Matti Karppanen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb692df26c76c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&user=Matti+Karppanen&userId=b692df26c76c&source=post_page-b692df26c76c----17dc5834112d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search", "anchor_text": "Grid Search"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search", "anchor_text": "Random Search"}, {"url": "https://unsplash.com/@johnmoeses?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "John Moeses Bauan"}, {"url": "https://unsplash.com/s/photos/math?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----17dc5834112d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&user=Matti+Karppanen&userId=b692df26c76c&source=-----17dc5834112d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&user=Matti+Karppanen&userId=b692df26c76c&source=-----17dc5834112d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F17dc5834112d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----17dc5834112d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----17dc5834112d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----17dc5834112d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----17dc5834112d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----17dc5834112d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----17dc5834112d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matti.karppanen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matti.karppanen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matti Karppanen"}, {"url": "https://medium.com/@matti.karppanen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb692df26c76c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&user=Matti+Karppanen&userId=b692df26c76c&source=post_page-b692df26c76c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb3d09fae0a4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-hyperparameter-optimization-17dc5834112d&newsletterV3=b692df26c76c&newsletterV3Id=b3d09fae0a4f&user=Matti+Karppanen&userId=b692df26c76c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}