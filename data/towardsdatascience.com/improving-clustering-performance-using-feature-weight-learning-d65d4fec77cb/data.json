{"url": "https://towardsdatascience.com/improving-clustering-performance-using-feature-weight-learning-d65d4fec77cb", "time": 1682994259.957247, "path": "towardsdatascience.com/improving-clustering-performance-using-feature-weight-learning-d65d4fec77cb/", "webpage": {"metadata": {"title": "Improving Clustering Performance Using Feature Weight Learning | by Colin Sinclair | Towards Data Science", "h1": "Improving Clustering Performance Using Feature Weight Learning", "description": "Clustering is an unsupervised machine learning methodology that aims to partition data into distinct groups, or clusters. There are a few different forms including hierarchical, density, and\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Clustering is an unsupervised machine learning methodology that aims to partition data into distinct groups, or clusters. There are a few different forms including hierarchical, density, and similarity based. Each have a few different algorithms associated with it as well. One of the hardest parts of any machine learning algorithm is feature engineering, which can especially be difficult with clustering as there is no easy way to figure out what best segments your data into separate but similar groups.", "The guiding principle of similarity based clustering is that similar objects are within the same cluster and dissimilar objects are in different clusters. This is not different than the goal of most conventional clustering algorithms. With similarity based clustering, a measure must be given to determine how similar two objects are. This similarity measure is based off distance, and different distance metrics can be employed, but the similarity measure usually results in a value in [0,1] with 0 having no similarity and 1 being identical. To measure feature weight importance, we will have to use a weighted euclidean distance function. The similarity measure is defined in the following:", "\u03b2 here is a value that we will actually have to solve for, (w) represents the distance weight matrix, and d represents the pairwise distances between all objects. To solve for \u03b2, we have to use the assumption that if using the standard weights(all 1's), our similarity matrix would uniformly distributed between [0,1] resulting in a mean of .5. So to find \u03b2, we solve the equation:", "If using a weighted euclidean distance, it is possible to use this similarity matrix to identify what features introduce more noise and which ones are important to clustering. The ultimate goal is to minimize the \u201cfuzziness\u201d of the similarity matrix, trying to move everything in the middle (ie .5) to either 1 or 0. For this purpose we use the loss metric:", "Here (1) represents the base weights (all 1's), and \u03c1 represents the resulting fuzzy partition matrix that is a product of the weights used in the euclidean distance function between points p and q.", "We can then attempt to use Gradient Descent on this loss function to try and minimize it with respect to the similarity matrix. Gradient Descent is one of the most common optimization algorithms in machine learning that is used to find best parameters of a given function by using the function gradient, a combination of the partial derivatives. By taking steps proportional to the negative of the gradient, we can try to find the local minimum of the function. We will continually update the weights until either our maximum number of iterations has been met, or the function converges. So the gradient descent will be of our loss function with a partial derivative in respect to the weights. We will update the weights every iteration with respect to the gradient and learning rate.", "Where n is the learning rate defined. n is a very important parameter, as something too small will require too much computation, while too big and the function may never converge.", "If you can think of it in terms of a 3D graph, it would be like stretching or shrinking each axis, in a way that would put our points into tighter groups, that are further away from each other. We are not actually changing the locations of the data, we are solely transforming how we measure the distances that drive our similarity metrics.", "Here is a created example where I introduce 3 clusters with separate centroids on the first two variables, but introduce a third noise variable that would make the clustering more difficult. These are colored by the actual cluster labels given when the data is created. When eliminating the third noise variable, we can see it would be much easier to identify clusters.", "Although it mostly is difficult to see the differences because of the 3D perspective, you can see how much more defined the clusters are with the learned feature weights. By stretching out the main feature that can easily separate them, it was able to better identify clusters.", "A good representation of its effectiveness is fuzzy c-means, a relative of the commonly used k-means algorithm. It works in a very similar fashion to k-means, but rather results in something called the fuzzy partition matrix instead of just a cluster label.", "The fuzzy partition matrix is a set of weights that measure how similar a single point is to a given cluster center, close to how our similarity matrix is used previously. It can also be calculated using a weighted distance metric which we can feed our new found optimal weights. This will also then go back into updating the cluster centers. Like K-means, this results in the cluster centers shifting with each iteration, until the maximum number of iterations or a certain improvement threshold has been met.", "In fuzzy c-means, you would have a very similar goal as to our original loss function. You would like less \u201cfuzzyness\u201d from points, and you want them all to be as close as possible to their cluster centers, and further away from others. A good measure of the fuzzy clustering algorithm is Dunn\u2019s partition coefficient, a sum of all components of the fuzzy partition matrix.", "Let\u2019s try using fuzzy c-means on the Iris data set with and without our learned feature weights. Here the output of fuzzy c-means comparing all variables, assuming 3 clusters(since we know that from the data set).", "Notice how the boundaries between some are less defined, and because we have multiple features equally weighted, it can be blurred. Now, when applying the feature weighted learning approach, we get normalized distance weights of:", "There are still fuzzy boundaries, mostly on features where we deemed them 0 value in the distance weights, but the algorithm put a major focus on petal length. We resulted in similar clusters, stronger boundaries (on some features), and overall our fuzzy partition coefficient increased by ~23%!", "We also now know that if we wanted to generate rules about classifying them, we could just focus on 2 features instead of 4!", "Just because it is easiest to see the results using fuzzy c-means does not mean this improvement measure can only be used for that algorithm. You can use it in many ways, scaling or just better understanding your data before clustering. I recently used this feature reduction and importance technique in an application that used the OPTICS algorithm, and saw improved results by scaling my features in accordance with the feature weight learning algorithm.", "I have built this feature weight learning into a stand alone repository if you would like to check it out yourself or use it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Operations Research, Data Science, Causal Inference"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd65d4fec77cb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@csinclair0?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@csinclair0?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Colin Sinclair"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F500f7dca1ad3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&user=Colin+Sinclair&userId=500f7dca1ad3&source=post_page-500f7dca1ad3----d65d4fec77cb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/Csinclair0/feature_learning.git", "anchor_text": "Csinclair0/feature_learningfeature weight learning based off similarity based clustering - Csinclair0/feature_learninggithub.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d65d4fec77cb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/clustering?source=post_page-----d65d4fec77cb---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d65d4fec77cb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----d65d4fec77cb---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&user=Colin+Sinclair&userId=500f7dca1ad3&source=-----d65d4fec77cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&user=Colin+Sinclair&userId=500f7dca1ad3&source=-----d65d4fec77cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd65d4fec77cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d65d4fec77cb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d65d4fec77cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@csinclair0?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@csinclair0?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Colin Sinclair"}, {"url": "https://medium.com/@csinclair0/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "101 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F500f7dca1ad3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&user=Colin+Sinclair&userId=500f7dca1ad3&source=post_page-500f7dca1ad3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa3dcb3a834f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-clustering-performance-using-feature-weight-learning-d65d4fec77cb&newsletterV3=500f7dca1ad3&newsletterV3Id=a3dcb3a834f8&user=Colin+Sinclair&userId=500f7dca1ad3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}