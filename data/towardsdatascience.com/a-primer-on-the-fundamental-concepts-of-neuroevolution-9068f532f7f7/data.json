{"url": "https://towardsdatascience.com/a-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7", "time": 1683003059.9429939, "path": "towardsdatascience.com/a-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7/", "webpage": {"metadata": {"title": "A Primer on the Fundamental Concepts of Neuroevolution | by Paul Pauls | Towards Data Science", "h1": "A Primer on the Fundamental Concepts of Neuroevolution", "description": "Neuroevolution is a machine learning technique that improves the rough abstractions of brains that are artificial neural networks (abbr. ANNs) by generating increasingly better topologies, weights\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/PaulPauls/Tensorflow-Neuroevolution", "anchor_text": "Tensorflow-Neuroevolution framework", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/rezsa", "anchor_text": "Rezsa Farahani", "paragraph_index": 0}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "new state-of-the-art 83.9% top-1 / 96.6% top-5 ImageNet accuracy", "paragraph_index": 31}, {"url": "https://github.com/PaulPauls/Tensorflow-Neuroevolution", "anchor_text": "Tensorflow-Neuroevolution framework", "paragraph_index": 34}, {"url": "https://www.linkedin.com/in/rezsa", "anchor_text": "Rezsa Farahani", "paragraph_index": 35}, {"url": "https://www.linkedin.com/in/dynamicwebpaige", "anchor_text": "Paige Bailey", "paragraph_index": 35}, {"url": "https://github.com/PaulPauls/Primer_on_Neuroevolution_Blog_Post", "anchor_text": "my github repository associated with this series of blog posts", "paragraph_index": 36}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf", "anchor_text": "Evolving Neural Networks through Augmenting Topologies", "paragraph_index": 37}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf", "anchor_text": "Efficient Evolution of Neural Network Topologies", "paragraph_index": 38}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf", "anchor_text": "Efficient Evolution of Neural Networks through Complexification", "paragraph_index": 39}, {"url": "https://ieeexplore.ieee.org/document/6792316", "anchor_text": "A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks", "paragraph_index": 40}, {"url": "https://doi.org/10.1177%2F1059712310379923", "anchor_text": "Evolving Plastic Neural Networks with Novelty Search", "paragraph_index": 41}, {"url": "https://eplex.cs.ucf.edu/papers/risi_gecco10.pdf", "anchor_text": "Evolving the Placement and Density of Neurons in the HyperNEAT Substrate", "paragraph_index": 42}, {"url": "http://eplex.cs.ucf.edu/papers/risi_gecco11.pdf", "anchor_text": "Enhancing ES-HyperNEAT to Evolve More Complex Regular Neural Networks", "paragraph_index": 43}, {"url": "https://ieeexplore.ieee.org/document/6756960", "anchor_text": "A Neuroevolution Approach to General Atari Game Playing", "paragraph_index": 44}, {"url": "https://ieeexplore.ieee.org/document/8080444", "anchor_text": "DLNE: A hybridization of deep learning and neuroevolution for visual control", "paragraph_index": 45}, {"url": "https://arxiv.org/abs/1703.01041", "anchor_text": "Large-Scale Evolution of Image Classifiers", "paragraph_index": 46}, {"url": "http://nn.cs.utexas.edu/?miikkulainen:chapter18", "anchor_text": "Evolving Deep Neural Networks", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1802.01548", "anchor_text": "Regularized Evolution for Image Classifier Architecture Search", "paragraph_index": 48}, {"url": "https://doi.org/10.1038/s42256-018-0006-z", "anchor_text": "Designing Neural Networks Through Neuroevolution", "paragraph_index": 49}, {"url": "http://nn.cs.utexas.edu/?liang:gecco19", "anchor_text": "Evolutionary Neural AutoML for Deep Learning", "paragraph_index": 50}, {"url": "http://nn.cs.utexas.edu/?neuroevolution", "anchor_text": "Neuroevolution Research Group", "paragraph_index": 51}], "all_paragraphs": ["Neuroevolution is a machine learning technique that improves the rough abstractions of brains that are artificial neural networks (abbr. ANNs) by generating increasingly better topologies, weights and hyperparameters by the means of evolutionary algorithms. Just as nature improves the faculty of the brain by mutating, crossing over or removing some of the underlying genetic code \u2014 does neuroevolution mutate the topology and weights of ANNs, combine features of high performing ANNs or eliminate low performing ANNs in order to find increasingly better solutions. This blog post is the first in a series of three and will give an extensive introduction into the fundamentals of neuroevolution, point out what aspects to consider when designing a neuroevolution algorithm yourself, as well as highlight important and recent contributions to this field of research. The next two parts will present a deep dive into the open source landscape surrounding neuroevolution, paying special attention to the design and usage of the Tensorflow-Neuroevolution framework [1] I created together with Rezsa Farahani during the Google Summer of Code 2019.", "Neuroevolution is a machine learning technique that generates increasingly better neural networks for the problem its applied to by using a population-based optimization method that continuously increases the quality of each neural network in that population. Each individual in that population is not stored as complex neural network, but as a genome, which is a simplified genetic representation that can be mapped to a neural network. The neuroevolution process itself first initializes a set of these genomes and then applies them to the problem environment after which each genome is assigned a fitness score based on how well the neural network solves the applied problem. For example, this fitness score could be a function of the accuracy achieved in an image recognition task, how close a robotic arm moved to the intended trajectory or how well an agent performed in a virtual environment such as a video game.", "Once the initial population has been created the optimization loop starts and the population is continuously mutated, recombined, evaluated and naturally selected. If these steps are performed iteratively with the whole population being in only one step at a time are we talking about generational neuroevolution. If the design of the neuroevolution algorithm allows for asynchronicity and the optimization loop is performed on a per genome basis is this usually termed competitive coevolution. In both cases does this endless loop of introducing innovation, evaluating it and then categorizing it constitute an optimization process that will eventually yield a neural network that solves the applied problem very well but might have been impossible to come up with by hand. Figure 1 below illustrates this typical optimization process of a generational neuroevolution algorithm \u2014 visualizing all steps of the aforementioned training loop.", "Neuroevolution is an agnostic process that, while requiring parameters for its own evolutionary process, does not stipulate any specific topology or hyperparameters for the generated neural networks. Instead ANNs are developed through feedback on what effectively works well on the problem environment. This extent of choices (topologies, weights, hyperparameters, etc) that may be used for the formation and complete specification of the generated neural networks is generally referred to as the search space. While the agnostic nature of the neuroevolution process allows for a very broad search space can it be sensible to limit granularity of the search space in order to traverse it faster. The faculty that maps genomes to their according neural networks is responsible for the granularity of the search space by limiting the complexity of the genome\u2019s encoding. This faculty is therefore called the genetic encoding.", "In order to allow for a broad search space with an appropriate granularity is it crucial to design the genetic encoding and the neuroevolution algorithm that operates on it, according to the requirements of the problem environment. Hence in order to design a good neuroevolution scheme do we first need to establish the fundamental understanding of a genetic encoding, as will be done in the next section. Following that we\u2019ll take a closer look at each step of the aforementioned evolutionary optimization loop that is displayed in figure 1 in the order in which they become relevant.", "In order to efficiently mutate and recombine ANNs is an efficient representation of the ANN necessary. Efficient representation of neural networks allows the frequently employed genetic operations to not have to analyze highly complex data structures but for them to analyze compact genetic codes that can be processed fast. As a consequence can genetic operations quickly determine sensible mutations or if two genomes are eligible for recombination. Neuroevolution algorithms therefore operate exclusively on the genetic encoding instead of the complex data structures popular popular machine learning frameworks operate on. Though of course does the genetic encoding allow a mapping between the two representations.", "These efficient genetic representations of a genome are termed genotypes, while the accordingly mapped neural networks are termed phenotypes. The terminology was adopted from genetic evolution, since neuroevolution is strictly speaking genetic evolution with the constraint that all phenotypes are neural networks.", "Genetic encodings can generally be divided into two subcategories: direct encodings and indirect encodings. Though there is also the third category of developmental encodings are we omitting this encoding as it is of little relevance in recent years. Direct encodings represent each aspect of the neural network they encode explicitly in its genetic representation. Figure 2 above is an example of a direct encoding as it is employed by the popular NEAT algorithm [3][4][5]. It encodes each connection and its according weight directly in the genotype but limits the search space by excluding the possibility of bias and activation functions in the resulting ANN. Such direct encodings can represent arbitrary feed-forward and recurrent topologies, allowing for possibly perfectly suited and at the same time minimal topologies. Allowing such a high degree of flexibility in the topology however results in an enormous search space with fine granularity, requiring a well designed neuroevolution algorithm in order to traverse that search space quickly.", "Indirect encodings on the other hand stipulate a custom encoding that is often not intuitively translatable into an ANN, but requires a separate translation faculty that is also stipulated by the indirect encoding in order to map a genotype to a neural network. Figure 3 above showcases an example of an indirect layer encoding and the faculty that maps the stipulated indirect encoding into an ANN. If the indirect encoding is well designed does this allow for a meaningful and fast traversal through the search space by even simple mutations (e.g. a bit flip in the example above) as well as fast recombinations of otherwise complex neural networks.", "However, whereas ANNs can be quickly created from direct encodings, is the translation faculty of indirect encodings slowing the processing speed and potentially too coarse-grained. Benefits and drawbacks of both encodings and especially how well the concrete implementation of either encoding fits the problem domain at hand have to be considered before deciding upon either one.", "Yet both genetic encodings demonstrate well how the genetic encoding determines the size of the search space, e.g. by not allowing activation functions or certain layer types, as well as determine its granularity, e.g. by only allowing to adjust hidden layer sizes by a multiple of 32.", "Closely related to the genetic encoding employed by the neuroevolution scheme is the method through which the defined search-space is actually traversed. This method is the process of reproduction and usually breaks down to creating new genomes through either mutating or recombining genomes that are deemed fit to be parents for the next generation. Mutation lets offspring genomes explore the viability of novel ANN architectures, weight distributions and hyperparameters. Recombining genomes, which is essentially the process of merging two promising genomes and their distinct features, spreads beneficial features throughout the rest of the population.", "Mutation is closely related to the genetic encoding in the sense that it can only mutate the parameters of a neural network to the extent in that they are represented in the genetic encoding. Defining mutation for a neuroevolution algorithm is therefore defining three aspects within the possible range that the genetic encoding allows for. First, what part of the genetic encoding will be mutated? It could be the topology, weights, hyperparameters or in above\u2019s example of an indirect encoding a random selection of bits. Furthermore, to what extent will the chosen part of the genome be mutated? A neuroevolution algorithm could for example employ extensive mutations for low-fitness genomes and minuscule mutations for high-performing genomes \u2014 effectively expressing the approach to drastically change the design of the ANN if it\u2019s not working or working out the details of an ANN if it is working well. The last thing to be defined for mutation is if it is somehow directional or random. The mutation of weights is often performed randomly in environments that lack clear ground truth, though is kept at bay by, for example, obtaining the new random value from a normal distribution centered around the current value. Directed mutation however is also possible if the neuroevolution algorithm is able to map a lack of performance in the problem environment to a deficit in the design or parameters of the neural network.", "Recombination on the other hand does not mutate genomes, but still creates novelty by combining two parent-genomes and their distinctive features into an innovative offspring-genome. If the recombination method is well designed and can merge the beneficial features of both parent-genomes in a lossless way does this allow for the aforementioned spread of beneficial features throughout the whole population \u2014 raising the fitness of all present genomes. The core issue of a well designed recombination method is therefore its property of being lossless, which is the property of merging (also called crossing over) the genetic encoding of two genomes without losing any of the encoded ANN features. Prior to the neuroevolution algorithm NEAT did neuroevolution schemes that were utilizing direct encoding face the problem of lossy crossovers that came up during mutation and a later recombination of two independent genomes. Figure 4 above illustrates this problem that was termed the competing-conventions problem. The NEAT algorithm proposed a method called historical markings, which provided each mutation with a unique identifier that finally enabled a lossless recombination of genomes, making it still a benchmark neuroevolution algorithm to this day.", "Though the step of evaluating genomes on the problem environment seems to be the simplest one in the overarching optimization loop is it still important to shortly address it \u2014 if only to point out potential caveats and possibilities. Fundamentally is the method of evaluation simply the process of mapping a genome to a neural network as stipulated by its genetic encoding, applying it to the problem environment and then calculating a fitness value depending on how well the ANN performed. Some neuroevolution algorithms also include the additional step of weight training a neural network before it is applied in the actual fitness scoring run. While this is method is very sensible is it only possible if the problem environment exhibits a clear ground truth.", "The only reasonably modifiable aspect in this whole process of evaluation is the way in which fitness is determined, though this is completely dependent on the nature of the applied problem and has to be decided on a case to case basis. Common methods for determining the fitness value are setting it equal to its accuracy in image recognition environments or equal to the points achieved in a video game environment.", "Novelty Search is also an important concept to consider when determining the fitness calculation, as this concept rewards genomes with novel approaches to the problem with higher fitness values. An agent in a video game environment would for example receive a fitness boost if he entered an unknown area though has overall achieved less points. This facilitates innovation in the gene pool and therefore an overall broader and more promising evolutionary process. Novelty Search was introduced in 2010 in the paper \u2018Evolving Plastic Neural Networks with Novelty Search\u2019 [7].", "Creating a well designed method of evaluation is other than that mostly an issue of its concrete implementation. Logging possible ground truths or adequately parallelizing the evaluation environments \u2014 since the nature of having multiple neural networks that are completely independent renders it a trivially parallelizable problem. Though this is not a research issue, is it an equally important aspect to consider when implementing the neuroevolution scheme.", "While the method of reproduction defines the way in which the search space is explored, is the choice of which genomes will serve as the parents for offspring and which genomes will be removed from the gene-pool another very important aspect of a neuroevolution scheme. The method through which genomes are chosen to be parents for offspring is also heavily dependent on the kind of neuroevolution algorithm that is used.", "In generational neuroevolution choosing the genomes suitable as parents for the next generation usually comes down to choosing a stipulated fraction of the best performing genomes in the current generation. This fraction is often supplied as a one of the many parameters that can still configure a predefined neuroevolution algorithm. The fraction of genomes chosen to be parents does not have to be complementary with the fraction of genomes chosen to be removed from the gene pool, as genomes can be promising enough to warrant potential for innovative mutation, though may be removed themselves in the following generation. An exemplary configuration for a neuroevolution algorithm would be the choice to utilize the top 20% of genomes as parents, though remove the 90% of the worst performing genomes for the following generation. Depending on if the generational neuroevolution algorithm employs speciation, does the choice of genomes that are selected to be parents or selected to be removed change significantly, usually enabling a more guided evolutionary process.", "Speciation is a tool of generational neuroevolution that clusters the individuals of the population according to apparent features like topological similarities in the phenotype or more covert features like reaching certain states in the applied problem environment. The resulting clusters are then considered species and ideally \u2014 depending on how well the method of speciation is chosen \u2014 represent genuinely distinct approaches to the problem at hand. This allows the reproductive process to guide the evolution by allocating more offspring to species with a higher average fitness and therefore favor the development of distinct and promising offspring. Well designed speciation however also protects innovation in the gene pool by still allocating some offspring to species with novel distinct features that have not yet been given the time to optimize (e.g. a distinctive topological addition in the ANN, whose weights are not yet optimized). Designing an appropriate method of speciation that segregates individuals of the population in distinct species based on genuinely different approaches to the problem can therefore be a very powerful tool in the larger neuroevolutionary method that both guides the evolution into a beneficial direction as well as protects innovative new paths.", "On the other hand, in a competitive coevolution scheme are genomes usually randomly paired and compared, with the fitter or more innovative one being chosen as parent and the other one being removed from the gene-pool. Due to the asynchronous nature of competitive coevolution is it impossible to categorize and classify all genomes in the population at once, significantly restricting the possibility to guide the evolution with methods like speciation or such. Though while the methods of natural and parental selection are simpler in competitive coevolution, is this made up by the effectively perfect parallelizability and scalability.", "Neuroevolution is a very broad approach to optimizing artificial neural networks to better perform on the problem environment. It is a versatile tool that is not limited to one aspect of machine learning but can be applied to improve methods of Computer Vision, Natural Language Processing or agent-based environments \u2014 as its versatility and search space are only bound by the expressiveness of its genetic encoding. The aspects to consider and combine when creating a neuroevolution algorithm are plentiful, though foremost need to be determined through an exact analysis of the problem and its specific characteristics, such that they may be exploit the problem environment most effectively.", "An extensive introduction to neuroevolution wouldn\u2019t be complete without referencing important research in the field, especially in recent years. While the Neuroevolution Research Group at the University of Texas at Austin probably keeps the most complete collection of publications [17] in the field, does the following list of landmark research and their innovative ideas provide a good start. Though great research has been produced prior to the first entry in this list (see Yao\u2019s Evolving Artificial Neural Networks from 1999 [2]), do we focus on research that is most relevant to this day.", "Evolving Neural Networks through Augmenting Topologies, 2002 [3][4][5] Introduced the Neuroevolution of Augmenting Topologies (NEAT) algorithm, surpassing most established neuroevolution algorithms at the time. NEAT evolves limited direct encodings in a strictly additive fashion through mutation and lossless recombination, while protecting innovation through speciation. NEAT can be considered as the archetypical neuroevolution algorithm due to its simplicity and simultaneous power and is therefore considered a key benchmark algorithm to this day.", "A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks, 2009 [6] Introduced HyperNEAT, an extension of NEAT that uses the same principles but employs an indirect encoding that is termed Compositional Pattern Producing Networks (CPPNs). These CPPNs allow neural networks to reproduce symmetric and repeating motifs that not only more accurately reflect the composition of human brains but also exploit the geometry and structure inherent in applied problems by mapping it onto the network topology.", "Evolving the Placement and Density of Neurons in the HyperNEAT Substrate, 2010 [8] Introduced ES-HyperNEAT, which further expands HyperNEAT by allowing for a denser substrate in the CPPN-encoded pattern in areas of higher information. While HyperNEAT leaves the decision of where to place hidden neurons in a potentially infinitely dense geometry up to the user, does ES-HyperNEAT employ a quadtree-like structure to decide on density and placement of those hidden nodes. In its iterated form [9] is it able to outperform classical HyperNEAT in key benchmarks.", "A Neuroevolution Approach to General Atari Game Playing, 2014 [10] This paper demonstrates neuroevolution\u2019s potential for General Video Game Playing (GVGP) as it employs the algorithms CNE, CMA-ES, NEAT and HyperNEAT to solve 61 Atari 2600 video games. While neuroevolution has been deployed prior to this study in GVGP or in videograme AI, does this constitute the largest scale demonstration as far as we are aware. The results are as expected in that they indicate that direct encoding methods work best on compact state representations while indirect-encodings allow scaling to higher dimensional representations.", "DLNE: A hybridization of deep learning and neuroevolution for visual control, 2017 [11] Introduces in my opinion one of the most interesting and promising usage of neuroevolution by combining it with CNN\u2019s for image recognition. This paper investigates the potential of neuroevolution in a complex first person shooter videogame environment by performing image recognition via CNN\u2019s, converting the recognized images into feature representations and then feeding those into neuroevolution optimized neural networks. Though the validity of this approach largely depends on the quality of the feature representation does even the employed simple approach indicate a promising research direction.", "Large-Scale Evolution of Image Classifiers, 2017 [12] Showcased the high potential of neuroevolution by evolving image classifier networks at an unprecedented scale on the CIFAR-10/100 datasets. The researchers utilized a simple competitive coevolutionary scheme without speciation or crossover, only utilizing basic operations to mutate the Tensorflow models, subsequently train them and then evaluate them against each other. All genomes were hence independent and trivially parallelizable, enabling the enormous scale, which was further accelerated by utilizing fast GPU implementations of Tensorflow models.", "Evolving Deep Neural Networks, 2018 [13] Introduces CoDeepNEAT an extension to the well established NEAT algorithm for the rise of deep learning in recent years. This paper first defines DeepNEAT, a layer indirect encoding (complete with hyperparameters, etc) to represent whole deep neural networks in a compact form. Those DeepNEAT genomes are then considered modules and coevolved with blueprints, which are made up of multiple repeating modules, therefore exploiting the fact that most successful networks (such as GoogleNet/ResNet) are composed of repeating motifs. The network is benchmarked on the CIFAR-10 dataset and yields promising results.", "Regularized Evolution for Image Classifier Architecture Search, 2018 [14] This paper represents an extension to Real et al\u2019s earlier paper Large-Scale Evolution of Image Classifiers, introducing the evolved image classifier AmoebaNet-A, which surpassed any preceding design \u2014 including hand-designs \u2014 and setting a new state-of-the-art 83.9% top-1 / 96.6% top-5 ImageNet accuracy. They achieved this by modifying the earlier defined simple competitive coevolutionary scheme with an age property favoring younger genotypes. They did this while doing a controlled comparison against a well known reinforcement learning algorithm, giving evidence that evolution can obtain results faster with the same hardware.", "Designing Neural Networks Through Neuroevolution, 2019 [15] This paper is an excellent review of modern neuroevolution, put together by the four most prominent researchers in the field. It summarizes and illustrates every important aspect of neuroevolution and even though it does not introduce novel concepts is the intention of this review to renew interest in the field, much like this blog post. We highly recommend anyone interested in the field to read this paper at some point.", "Evolutionary Neural AutoML for Deep Learning, 2019 [16] Introduces the powerful evolutionary AutoML framework called LEAF, utilizing the the existing state-of-the-art evolutionary algorithm CoDeepNEAT from the same authors. Though the authors did not disclose their source code do they provide an extensive discussion of its design and provide valuable input. The performance o the evolutionary AutoML framework is evaluated on medical image classification and natural language analysis tasks and is every promising, outperforming existing state-of-the-art AutoML systems and the best hand designed solutions.", "The second and third part of this blog series will focus on the open source landscape surrounding neuroevolution, with a special focus on the Tensorflow-Neuroevolution framework [1]. Part 2 will focus on the design and performance of the Tensorflow-Neuroevolution framework, introducing its modular approach, giving a concrete example of designing a custom neuroevolution algorithm yourself as well as providing performance benchmarks to other libraries. Part 3 will extensively introduce the advanced neuroevolution methods of AmoebaNET and CoDeepNEAT while also closely referencing the code base and giving a concrete example of their usage.", "I would like to thank Rezsa Farahani, with whom I closely worked on neuroevolution and the TFNE framework since he was my mentor for the Google Summer of Code 2019. I would also like to thank the greater Google Summer of Code and Tensorflow Team, especially Paige Bailey for their continuous support and feedback and a great program.", "All illustrations that have not otherwise been quoted with a source are created by me and are uploaded on my github repository associated with this series of blog posts. I also intend to publish any code or material created in future blog posts of this series in this same repository.", "[3] O.Stanley, Miikkulainen; Evolving Neural Networks through Augmenting Topologies; MIT Press; 2002", "[4] O.Stanley, Miikkulainen; Efficient Evolution of Neural Network Topologies; University of Texas at Austin; 2002", "[5] O.Stanley; Efficient Evolution of Neural Networks through Complexification; University of Texas at Austin; 2004", "[6] O.Stanley, D\u2019Ambrosio, Gauci; A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks; MITP; 2009", "[7] Risi, Hughes, O.Stanley; Evolving Plastic Neural Networks with Novelty Search; Adaptive Behavior \u2014 Animals, Animats, Software Agents, Robots, Adaptive Systems; 2010", "[8] Risi, Lehman, O.Stanley; Evolving the Placement and Density of Neurons in the HyperNEAT Substrate; GECCO; 2011", "[9] Risi, O.Stanley; Enhancing ES-HyperNEAT to Evolve More Complex Regular Neural Networks; GECCO; 2011", "[10] Hausknecht, Lehman, Miikkulainen, A Neuroevolution Approach to General Atari Game Playing; IEEE; 2014", "[11] Precht Poulsen, Thorhauge, Hvilshj Funch, Risi; DLNE: A hybridization of deep learning and neuroevolution for visual control; IEEE; 2017", "[12] Real, Moore, Selle, Saxena, Leon Suematsu, Tan, Le, Kurakin; Large-Scale Evolution of Image Classifiers; CoRR; 2017", "[13] Miikkulainen, Liang, Meyerson, Rawal, Fink, Francon, Raju, Shahrzad, Navruzyan, Duffy, Hodjat; Evolving Deep Neural Networks; Artificial Intelligence in the Age of Neural Networks and Brain Computing; 2018", "[14] Real, Aggarwal, Huang, Le; Regularized Evolution for Image Classifier Architecture Search; CoRR; 2018", "[15] O.Stanley, Clune, Lehman, Miikkulainen; Designing Neural Networks Through Neuroevolution; Nature Machine Intelligence; 2019", "[16] Liang, Meyerson, Hodjat, Fink, Mutch, Miikkulainen; Evolutionary Neural AutoML for Deep Learning; GECCO; 2019", "[17] University of Texas at Austin; Neuroevolution Research Group", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning GDE and Open-Source enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9068f532f7f7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@paulpauls?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulpauls?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Paul Pauls"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba8edca3b219&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&user=Paul+Pauls&userId=ba8edca3b219&source=post_page-ba8edca3b219----9068f532f7f7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-dive-into-ne", "anchor_text": "A Deep Dive into Neuroevolution"}, {"url": "https://github.com/PaulPauls/Tensorflow-Neuroevolution", "anchor_text": "Tensorflow-Neuroevolution framework"}, {"url": "https://www.linkedin.com/in/rezsa", "anchor_text": "Rezsa Farahani"}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "new state-of-the-art 83.9% top-1 / 96.6% top-5 ImageNet accuracy"}, {"url": "https://github.com/PaulPauls/Tensorflow-Neuroevolution", "anchor_text": "Tensorflow-Neuroevolution framework"}, {"url": "https://www.linkedin.com/in/rezsa", "anchor_text": "Rezsa Farahani"}, {"url": "https://www.linkedin.com/in/dynamicwebpaige", "anchor_text": "Paige Bailey"}, {"url": "https://github.com/PaulPauls/Primer_on_Neuroevolution_Blog_Post", "anchor_text": "my github repository associated with this series of blog posts"}, {"url": "https://github.com/PaulPauls/Tensorflow-Neuroevolution", "anchor_text": "Tensorflow-Neuroevolution Framework"}, {"url": "https://ieeexplore.ieee.org/abstract/document/784219", "anchor_text": "Evolving Artificial Neural Networks"}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf", "anchor_text": "Evolving Neural Networks through Augmenting Topologies"}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf", "anchor_text": "Efficient Evolution of Neural Network Topologies"}, {"url": "http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf", "anchor_text": "Efficient Evolution of Neural Networks through Complexification"}, {"url": "https://ieeexplore.ieee.org/document/6792316", "anchor_text": "A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks"}, {"url": "https://doi.org/10.1177%2F1059712310379923", "anchor_text": "Evolving Plastic Neural Networks with Novelty Search"}, {"url": "https://eplex.cs.ucf.edu/papers/risi_gecco10.pdf", "anchor_text": "Evolving the Placement and Density of Neurons in the HyperNEAT Substrate"}, {"url": "http://eplex.cs.ucf.edu/papers/risi_gecco11.pdf", "anchor_text": "Enhancing ES-HyperNEAT to Evolve More Complex Regular Neural Networks"}, {"url": "https://ieeexplore.ieee.org/document/6756960", "anchor_text": "A Neuroevolution Approach to General Atari Game Playing"}, {"url": "https://ieeexplore.ieee.org/document/8080444", "anchor_text": "DLNE: A hybridization of deep learning and neuroevolution for visual control"}, {"url": "https://arxiv.org/abs/1703.01041", "anchor_text": "Large-Scale Evolution of Image Classifiers"}, {"url": "http://nn.cs.utexas.edu/?miikkulainen:chapter18", "anchor_text": "Evolving Deep Neural Networks"}, {"url": "https://arxiv.org/abs/1802.01548", "anchor_text": "Regularized Evolution for Image Classifier Architecture Search"}, {"url": "https://doi.org/10.1038/s42256-018-0006-z", "anchor_text": "Designing Neural Networks Through Neuroevolution"}, {"url": "http://nn.cs.utexas.edu/?liang:gecco19", "anchor_text": "Evolutionary Neural AutoML for Deep Learning"}, {"url": "http://nn.cs.utexas.edu/?neuroevolution", "anchor_text": "Neuroevolution Research Group"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9068f532f7f7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neuroevolution?source=post_page-----9068f532f7f7---------------neuroevolution-----------------", "anchor_text": "Neuroevolution"}, {"url": "https://medium.com/tag/evolutionary-algorithms?source=post_page-----9068f532f7f7---------------evolutionary_algorithms-----------------", "anchor_text": "Evolutionary Algorithms"}, {"url": "https://medium.com/tag/deep-dive-into-ne?source=post_page-----9068f532f7f7---------------deep_dive_into_ne-----------------", "anchor_text": "Deep Dive Into Ne"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9068f532f7f7---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&user=Paul+Pauls&userId=ba8edca3b219&source=-----9068f532f7f7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&user=Paul+Pauls&userId=ba8edca3b219&source=-----9068f532f7f7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9068f532f7f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9068f532f7f7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9068f532f7f7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9068f532f7f7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9068f532f7f7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulpauls?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@paulpauls?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Pauls"}, {"url": "https://medium.com/@paulpauls/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "40 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba8edca3b219&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&user=Paul+Pauls&userId=ba8edca3b219&source=post_page-ba8edca3b219--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F57e308bb0092&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-primer-on-the-fundamental-concepts-of-neuroevolution-9068f532f7f7&newsletterV3=ba8edca3b219&newsletterV3Id=57e308bb0092&user=Paul+Pauls&userId=ba8edca3b219&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}