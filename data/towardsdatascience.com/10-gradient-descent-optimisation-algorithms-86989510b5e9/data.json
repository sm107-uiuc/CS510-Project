{"url": "https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9", "time": 1682993975.930567, "path": "towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9/", "webpage": {"metadata": {"title": "10 Stochastic Gradient Descent Optimisation Algorithms + Cheatsheet | by Raimi Karim | Towards Data Science", "h1": "10 Stochastic Gradient Descent Optimisation Algorithms + Cheatsheet", "description": "Stochastic gradient descent optimisation algorithms you should know for deep learning"}, "outgoing_paragraph_urls": [{"url": "https://remykarem.github.io/blog/gradient-descent-optimisers.html", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "stochastic gradient descent", "paragraph_index": 4}, {"url": "https://remykarem.github.io/backpropagation-demo/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods", "anchor_text": "Polyak, 1964", "paragraph_index": 12}, {"url": "https://www.semanticscholar.org/paper/On-the-momentum-term-in-gradient-descent-learning-Qian/735d4220d5579cc6afe956d9f6ea501a96ae99e2", "anchor_text": "Ning Qian, 1999", "paragraph_index": 13}, {"url": "http://proceedings.mlr.press/v28/sutskever13.pdf", "anchor_text": "Sutskever et al.", "paragraph_index": 13}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "James", "paragraph_index": 13}, {"url": "http://jmlr.org/papers/v12/duchi11a.html", "anchor_text": "Duchi et al., 2011", "paragraph_index": 14}, {"url": "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf", "anchor_text": "Hinton et al., 2012", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1212.5701", "anchor_text": "Zeiler, 2012", "paragraph_index": 17}, {"url": "http://proceedings.mlr.press/v28/sutskever13.html", "anchor_text": "Sutskever et al., 2013", "paragraph_index": 20}, {"url": "http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf", "anchor_text": "Nesterov, 1983", "paragraph_index": 24}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "comment", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Kingma & Ba, 2014", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Kingma & Ba, 2015", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)", "anchor_text": "infinity norms", "paragraph_index": 28}, {"url": "http://cs229.stanford.edu/proj2015/054_report.pdf", "anchor_text": "Dozat, 2015", "paragraph_index": 32}, {"url": "https://openreview.net/pdf?id=ryQu7f-RZ", "anchor_text": "Reddi et al., 2018", "paragraph_index": 36}, {"url": "https://gist.github.com/remykarem/d5eff32a67ceeec8e653d1016525a649", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://quicklatex.com/", "anchor_text": "QuickLaTeX", "paragraph_index": 39}, {"url": "http://ruder.io/optimizing-gradient-descent", "anchor_text": "An overview of gradient descent optimization algorithms", "paragraph_index": 54}, {"url": "https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281", "anchor_text": "Line-by-Line Word2Vec Implementation", "paragraph_index": 55}, {"url": "https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843", "anchor_text": "Step-by-Step Tutorial on Linear Regression with Stochastic Gradient Descent", "paragraph_index": 56}, {"url": "https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889", "anchor_text": "Counting No. of Parameters in Deep Learning Models", "paragraph_index": 57}, {"url": "https://medium.com/@renjietan", "anchor_text": "Ren Jie", "paragraph_index": 58}, {"url": "https://medium.com/@derekchia", "anchor_text": "Derek", "paragraph_index": 58}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "James", "paragraph_index": 58}, {"url": "https://www.twitter.com/remykarem", "anchor_text": "Twitter", "paragraph_index": 59}, {"url": "http://www.linkedin.com/in/raimibkarim", "anchor_text": "LinkedIn", "paragraph_index": 59}, {"url": "https://remykarem.github.io/", "anchor_text": "remykarem.github.io", "paragraph_index": 59}, {"url": "http://remykarem.medium.com/membership", "anchor_text": "remykarem.medium.com/membership", "paragraph_index": 61}], "all_paragraphs": ["(I maintain a cheat sheet of these optimisers including RAdam in my blog here.)", "Changelog:5 Jan 2022 \u2014 Fix typos4 May 2020 \u2014 Fix typo in Nadam formula in Appendix 221 Mar 2020 \u2014 Replace V and S with m and v respectively, update dead links, review the idea of learning rate and gradient components, update intuitions6 Oct 2019 \u2014 Improve on the idea of EMA of gradients22 Sep 2019 \u2014 Rearrange content order and remove \u2018evolutionary map\u2019", "Gradient descent is an optimisation method for finding the minimum of a function. It is commonly used in deep learning models to update the weights of a neural network through backpropagation.", "In this post, I will summarise the common gradient descent optimisation algorithms used in popular deep learning frameworks (e.g. TensorFlow, Keras, PyTorch). The purpose of this post is to make it easy to read and digest the formulae using consistent nomenclature since there aren\u2019t many such summaries out there. At the end of this post is a cheat sheet for your reference.", "This post assumes that the reader has some knowledge about gradient descent / stochastic gradient descent.", "(For a demo on a linear regression problem using gradient descent optimisers like SGD, momentum and Adam, click here.)", "Recall that the vanilla stochastic gradient descent (SGD) updates weights by subtracting the current weight by a factor (i.e. \u03b1, the learning rate) of its gradient.", "Variations in this equation are commonly known as stochastic gradient descent optimisers. There are 3 main ways how they differ:", "As you will see later, these optimisers try to improve the amount of information used to update the weights, mainly through using previous (and future) gradients, instead of only the present available gradient.", "Below is a table that summarises which \u201ccomponents\u201d are being adapted:", "Appendix 3: Learning rate schedulers vs. stochastic gradient descent optimisers", "As we have seen earlier, the vanilla SGD updates the current weight using the current gradient \u2202L/\u2202w multiplied by some factor called the learning rate, \u03b1.", "Instead of depending only on the current gradient to update the weight, gradient descent with momentum (Polyak, 1964) replaces the current gradient with m (\u201cmomentum\u201d), which is an aggregate of gradients. This aggregate is the exponential moving average of current and past gradients (i.e. up to time t). Later in this post, you will see that this momentum update becomes the standard update for the gradient component for most optimisers.", "On the origins of momentumNote that many articles reference the momentum method to the publication by Ning Qian, 1999. However, the paper titled Sutskever et al. attributed the classical momentum to a much earlier publication by Polyak in 1964, as cited above. (Thank you to James for pointing this out.)", "Adaptive gradient, or AdaGrad (Duchi et al., 2011), acts on the learning rate component by dividing the learning rate by the square root of v, which is the cumulative sum of current and past squared gradients (i.e. up to time t). Note that the gradient component remains unchanged like in SGD.", "Notice that \u03b5 is added to the denominator. Keras calls this the fuzz factor, a small floating-point value to ensure that we will never have to come across division by zero.", "Root mean square prop or RMSprop (Hinton et al., 2012) is another adaptive learning rate that tries to improve AdaGrad. Instead of taking the cumulative sum of squared gradients like in AdaGrad, we take the exponential moving average (again!) of these gradients. Similar to momentum, we will slowly see that this update becomes the standard update for the learning rate component for most optimisers.", "Like RMSprop, Adadelta (Zeiler, 2012) is also another improvement from AdaGrad, focusing on the learning rate component. Adadelta is probably short for \u2018adaptive delta\u2019, where delta here refers to the difference between the current weight and the newly updated weight.", "The difference between Adadelta and RMSprop is that Adadelta removes the use of the learning rate parameter completely by replacing it with D, the exponential moving average of squared deltas.", "with D and v initialised to 0, and", "After Polyak had gained his momentum (pun intended \ud83d\ude2c), a similar update was implemented using Nesterov Accelerated Gradient (Sutskever et al., 2013). This update utilises m, the exponential moving average of what I would call projected gradients.", "The last term in the second equation is a projected gradient. This value can be obtained by going \u2018one step ahead\u2019 using the previous velocity (Eqn. 4). This means that for this time step t, we have to carry out another forward propagation before we can finally execute the backpropagation. Here\u2019s how it goes:", "2. Carry out forward propagation, but using this projected weight.", "4. Compute V and w accordingly.", "On the origins of NAGNote that the original Nesterov Accelerated Gradient paper (Nesterov, 1983) was not about stochastic gradient descent and did not explicitly use the gradient descent equation. Hence, a more appropriate reference is the above-mentioned publication by Sutskever et al. in 2013, which described NAG\u2019s application in stochastic gradient descent. (Again, I\u2019d like to thank James\u2019s comment on HackerNews for pointing this out.)", "Adaptive moment estimation, or Adam (Kingma & Ba, 2014), is simply a combination of momentum and RMSprop. It acts upon", "with m and v initialised to 0.", "Proposed default values by the authors:", "AdaMax (Kingma & Ba, 2015) is an adaptation of the Adam optimiser by the same authors using infinity norms (hence \u2018max\u2019). m is the exponential moving average of gradients, and v is the exponential moving average of past p-norm of gradients, approximated to the max function as seen below. Refer to the paper for their proof of convergence.", "is the bias correction for m and", "with m and v initialised to 0.", "Proposed default values by the authors:", "Nadam (Dozat, 2015) is an acronym for Nesterov and Adam optimiser. The Nesterov component, however, is a more efficient modification than its original implementation.", "First, I\u2019d like to show that the Adam optimiser can also be written as:", "Nadam uses Nesterov to update the gradient one step ahead by replacing the previous m_hat in the above equation to the current m_hat:", "with m and v initialised to 0.", "Another variant of Adam is the AMSGrad (Reddi et al., 2018). This variant revisits the adaptive learning rate component in Adam and changes it to ensure that the current v is always larger than the v from the previous time step.", "with m and v initialised to 0.", "Please reach out to me if something is amiss, or if something in this post can be improved! \u270c\ud83c\udffc", "Gist for the above can be found here. Image generated with QuickLaTeX. (Thank you Ravi for pointing out the typo in Nadam\u2019s update.)", "Here I\u2019d like to share with you some intuition why gradient descent optimisers use exponential moving average for the gradient component and root mean square for the learning rate component.", "Why take exponential moving average of gradients?", "Recall that we need to update the weight, and to do so we need to make use of some value. The only value we have is the current gradient, so let\u2019s just use this information to update the weight.", "But taking only the current gradient value is not enough. We want our updates to be \u201cbetter guided.\u201d And this is achieved through using the previous information about gradients. So let\u2019s include previous gradients too, by aggregating the current gradient and past gradients.", "One way to aggregate these gradients is to take a simple average of all the past and current gradients. But wait, this means each of these gradients are equally weighted. Would that be fair? Maybe. Maybe not.", "What we could do is to take the exponential moving average, where past gradient values are given higher weights (importance) than the current one. Intuitively, discounting the importance given to the current gradient would ensure that the weight update will not be sensitive to the current gradient.", "Why divide learning rate by root of exponential average of squared gradients?", "The goal of adapting the learning rate is to make the optimiser \u2018smarter\u2019 by dividing the learning rate by the root mean square of multiple gradients. So let\u2019s ask ourselves these questions:", "The first question has been answered from the previous section \u2014 on top of the value of the current gradient, we also want to utilise the information from the past gradients.", "To answer the second question, firstly, consider a simple case where the average magnitude of the gradients for the past few iterations has been 0.01. Since this value is close to 0, it means that we have been on an approximately flat surface (imagine a section of the 3D loss landscape that is flat). Where we are now, the earth is pretty flat, so we\u2019re pretty confident moving about this area. In fact, we want to get out of this area as fast as possible and look for a downward slope that could possibly lead us to a global minimum. (You might find some articles mentioning that this has the effect of \u201cacceleration.\u201d) Therefore, we want to increase the learning rate component (learn faster) when the magnitude of the gradients is small. To establish this inverse proportion relationship, we take the fixed learning rate \u03b1 and divide it by the average magnitude of the gradient. This adapted learning rate (which is now a large value) is then multiplied by the gradient component, giving us a large weight update in magnitude (no matter positive or negative).", "The intuition holds true too for the converse. Suppose the average of our gradients has been very high, about 2.7. This means we have been on steep slopes. We want to move with caution so we take smaller steps, and this can be achieved by performing the same division.", "In the last question, the reason why we take exponential moving average has been apparent to us from the previous section. The reason why we take the square of the gradient is simply that when dealing with the learning rate component, we are concerned with its magnitude. A natural choice to \u2018offset\u2019 this is to take its root. There might be math behind this, but let\u2019s just use this intuition to convince ourselves for now.", "Some of you might ask \u2014 what\u2019s the difference between learning rate schedulers and stochastic gradient descent optimisers? The main difference between these two is that stochastic gradient descent optimisers adapt the learning rate component by multiplying the learning rate by a factor which is a function of the gradients, whereas learning rate schedulers multiply the learning rate by a factor which is a function of the time step (or even a constant).", "The respective papers mentioned above for every optimiser", "An overview of gradient descent optimization algorithms (ruder.io)", "Line-by-Line Word2Vec Implementation (on word embeddings)", "Step-by-Step Tutorial on Linear Regression with Stochastic Gradient Descent", "Counting No. of Parameters in Deep Learning Models", "Thanks to Ren Jie, Derek, William Tjhi, Chan Kai, Serene, and James for ideas, suggestions, and corrections to this article.", "Follow me on Twitter @remykarem or LinkedIn. You may also reach out to me via raimi.bkarim@gmail.com. Feel free to visit my website at remykarem.github.io.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83c\uddf8\ud83c\uddec Software Engineer at GovTech \u2022 Master of Computing AI at NUS \u2022 Subscribe at remykarem.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F86989510b5e9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://remykarem.medium.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Raimi Karim"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2958659896a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&user=Raimi+Karim&userId=c2958659896a&source=post_page-c2958659896a----86989510b5e9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/y8iR4t4MTF8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Julian Zett"}, {"url": "https://unsplash.com/search/photos/mountain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://remykarem.github.io/blog/gradient-descent-optimisers.html", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "stochastic gradient descent"}, {"url": "https://remykarem.github.io/backpropagation-demo/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average", "anchor_text": "exponential moving average"}, {"url": "https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods", "anchor_text": "Polyak, 1964"}, {"url": "https://www.semanticscholar.org/paper/On-the-momentum-term-in-gradient-descent-learning-Qian/735d4220d5579cc6afe956d9f6ea501a96ae99e2", "anchor_text": "Ning Qian, 1999"}, {"url": "http://proceedings.mlr.press/v28/sutskever13.pdf", "anchor_text": "Sutskever et al."}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "James"}, {"url": "http://jmlr.org/papers/v12/duchi11a.html", "anchor_text": "Duchi et al., 2011"}, {"url": "https://keras.io/optimizers/#adagrad", "anchor_text": "Keras"}, {"url": "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf", "anchor_text": "Hinton et al., 2012"}, {"url": "https://keras.io/optimizers/#rmsprop", "anchor_text": "Keras"}, {"url": "https://arxiv.org/abs/1212.5701", "anchor_text": "Zeiler, 2012"}, {"url": "https://keras.io/optimizers/#adadelta", "anchor_text": "Keras"}, {"url": "http://proceedings.mlr.press/v28/sutskever13.html", "anchor_text": "Sutskever et al., 2013"}, {"url": "http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf", "anchor_text": "Nesterov, 1983"}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "comment"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Kingma & Ba, 2014"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Kingma & Ba, 2015"}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)", "anchor_text": "infinity norms"}, {"url": "http://cs229.stanford.edu/proj2015/054_report.pdf", "anchor_text": "Dozat, 2015"}, {"url": "https://keras.io/optimizers/#nadam", "anchor_text": "Keras"}, {"url": "https://openreview.net/pdf?id=ryQu7f-RZ", "anchor_text": "Reddi et al., 2018"}, {"url": "https://keras.io/optimizers/#adam", "anchor_text": "Keras"}, {"url": "https://gist.github.com/remykarem/d5eff32a67ceeec8e653d1016525a649", "anchor_text": "here"}, {"url": "https://quicklatex.com/", "anchor_text": "QuickLaTeX"}, {"url": "http://ruder.io/optimizing-gradient-descent", "anchor_text": "An overview of gradient descent optimization algorithms"}, {"url": "https://distill.pub/2017/momentum/", "anchor_text": "Why Momentum Really Works"}, {"url": "https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45", "anchor_text": "Animated RNN, LSTM and GRU"}, {"url": "https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281", "anchor_text": "Line-by-Line Word2Vec Implementation"}, {"url": "https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843", "anchor_text": "Step-by-Step Tutorial on Linear Regression with Stochastic Gradient Descent"}, {"url": "https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889", "anchor_text": "Counting No. of Parameters in Deep Learning Models"}, {"url": "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3", "anchor_text": "Attn: Illustrated Attention"}, {"url": "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a", "anchor_text": "Illustrated: Self-Attention"}, {"url": "https://medium.com/@renjietan", "anchor_text": "Ren Jie"}, {"url": "https://medium.com/@derekchia", "anchor_text": "Derek"}, {"url": "https://news.ycombinator.com/item?id=18525494#18528682", "anchor_text": "James"}, {"url": "https://www.twitter.com/remykarem", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/in/raimibkarim", "anchor_text": "LinkedIn"}, {"url": "https://remykarem.github.io/", "anchor_text": "remykarem.github.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----86989510b5e9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----86989510b5e9---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----86989510b5e9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----86989510b5e9---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----86989510b5e9---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&user=Raimi+Karim&userId=c2958659896a&source=-----86989510b5e9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&user=Raimi+Karim&userId=c2958659896a&source=-----86989510b5e9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F86989510b5e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----86989510b5e9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----86989510b5e9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----86989510b5e9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----86989510b5e9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----86989510b5e9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----86989510b5e9--------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raimi Karim"}, {"url": "https://remykarem.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "http://remykarem.medium.com/membership", "anchor_text": "remykarem.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2958659896a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&user=Raimi+Karim&userId=c2958659896a&source=post_page-c2958659896a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F307a18475417&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-gradient-descent-optimisation-algorithms-86989510b5e9&newsletterV3=c2958659896a&newsletterV3Id=307a18475417&user=Raimi+Karim&userId=c2958659896a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}