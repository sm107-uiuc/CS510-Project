{"url": "https://towardsdatascience.com/spectral-clustering-aba2640c0d5b", "time": 1682995107.0061738, "path": "towardsdatascience.com/spectral-clustering-aba2640c0d5b/", "webpage": {"metadata": {"title": "Spectral Clustering. Foundation and Application | by William Fleshman | Towards Data Science", "h1": "Spectral Clustering", "description": "In this post, we\u2019ll cover the ends and outs of spectral clustering for graphs and other data. Clustering is one of the main tasks in unsupervised machine learning."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this post, we\u2019ll cover the ins and outs of spectral clustering for graphs and other data. Clustering is one of the main tasks in unsupervised machine learning. The goal is to assign unlabeled data to groups, where similar data points hopefully get assigned to the same group.", "Spectral clustering is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.", "Spectral clustering uses information from the eigenvalues (spectrum) of special matrices built from the graph or the data set. We\u2019ll learn how to construct these matrices, interpret their spectrum, and use the eigenvectors to assign our data to clusters.", "Critical to this discussion is the concept of eigenvalues and eigenvectors. For a matrix A, if there exists a vector x which isn\u2019t all 0\u2019s and a scalar \u03bb such that Ax = \u03bbx, then x is said to be an eigenvector of A with corresponding eigenvalue \u03bb.", "We can think of the matrix A as a function which maps vectors to new vectors. Most vectors will end up somewhere completely different when A is applied to them, but eigenvectors only change in magnitude. If you drew a line through the origin and the eigenvector, then after the mapping, the eigenvector would still land on the line. The amount which the vector is scaled along the line depends on \u03bb.", "We can easily find the eigenvalues and eigenvectors of a matrix using numpy in Python:", "Eigenvectors are an important part of linear algebra, because they help describe the dynamics of systems represented by matrices. There are numerous applications which utilize eigenvectors, and we\u2019ll use them directly here to perform spectral clustering.", "Graphs are a natural way to represent many types of data. A graph is a set of nodes with a corresponding set of edges which connect the nodes. The edges may be directed or undirected and can even have weights associated with them.", "A network of routers on the internet can easily be represented as a graph. The routers are the nodes, and the edges are the connections between pairs of routers. Some routers might only allow traffic in one direction, so the edges could be directed to represent which direction traffic can flow. The weights on the edges could represent the bandwidth available along that edge. With this setup, we could then query the graph to find efficient paths for transmitting data from one router to another across the network.", "Let\u2019s use the following undirected graph as a running example:", "This graph has 10 nodes and 12 edges. It also has two connected components {0,1,2,8,9} and {3,4,5,6,7}. A connected component is a maximal subgraph of nodes which all have paths to the rest of the nodes in the subgraph.", "Connected components seem important, if our task is to assign these nodes to communities or clusters. A simple idea would be to make each connected component its own cluster. This seems reasonable for our example graph, but it\u2019s possible that the entire graph might be connected, or that the connected components are very large. There could also be smaller structures within a connected component which are good candidates for communities. We\u2019ll shortly see the importance of this connected component idea for spectral clustering.", "We can represent our example graph as an adjacency matrix, where the row and column indices represent the nodes, and the entries represent the absence or presence of an edge between the nodes. The adjacency matrix for our example graph looks like this:", "In the matrix, we see that row 0, column 1 has a value of 1. That means that there is an edge connecting node 0 with node 1. If the edges were weighted, the weights of the edges would go in this matrix instead of just 1s and 0s. Since our graph is undirected, the entries for row i, col j will be equal to the entry at row j, col i. The last thing to notice is that the diagonal of this matrix is all 0, since none of our nodes have edges to themselves.", "The degree of a node is how many edges connect to it. In a directed graph we could talk about in-degree and out-degree, but in this example we just have degree since the edges go both ways. Looking at our graph, we see that node 0 has degree 4, since it has 4 edges. We could also get the degree by taking the sum of the node\u2019s row in the adjacency matrix.", "The degree matrix is a diagonal matrix where the value at entry (i, i) is the degree of node i. Let\u2019s find the degree matrix for our example:", "First, we took the sum across axis 1 (the rows) of our adjacency matrix, and then we put those values into a diagonal matrix. From the degree matrix, we can easily see that nodes 0 and 5 have 4 edges, while the rest of the nodes have only 2.", "Now we\u2019re going to calculate the Graph Laplacian. The Laplacian is just another matrix representation of a graph. It has several beautiful properties, which we will take advantage of for spectral clustering. To calculate the normal Laplacian (there are several variants), we just subtract the adjacency matrix from our degree matrix:", "The Laplacian\u2019s diagonal is the degree of our nodes, and the off diagonal is the negative edge weights. This is the representation we are after for performing spectral clustering.", "As mentioned, the Laplacian has some beautiful properties. To get a sense for this, let\u2019s examine the eigenvalues associated with the Laplacian as I add edges to our graph:", "We see that when the graph is completely disconnected, all ten of our eigenvalues are 0. As we add edges, some of our eigenvalues increase. In fact, the number of 0 eigenvalues corresponds to the number of connected components in our graph!", "Look closely as that final edge is added, connecting the two components into one. When this happens, all of the eigenvalues but one have been lifted:", "The first eigenvalue is 0 because we only have one connected component (the whole graph is connected). The corresponding eigenvector will always have constant values (in this example all the values are close to 0.32).", "The first nonzero eigenvalue is called the spectral gap. The spectral gap gives us some notion of the density of the graph. If this graph was densely connected (all pairs of the 10 nodes had an edge), then the spectral gap would be 10.", "The second eigenvalue is called the Fiedler value, and the corresponding vector is the Fiedler vector. The Fiedler value approximates the minimum graph cut needed to separate the graph into two connected components. Recall, that if our graph was already two connected components, then the Fiedler value would be 0. Each value in the Fiedler vector gives us information about which side of the cut that node belongs. Let\u2019s color the nodes based on whether their entry in the Fielder vector is positive or not:", "This simple trick has segmented our graph into two clusters! Why does this work? Remember that zero eigenvalues represent connected components. Eigenvalues near zero are telling us that there is almost a separation of two components. Here we have a single edge, that if it didn\u2019t exist, we\u2019d have two separate components. So the second eigenvalue is small.", "To summarize what we know so far: the first eigenvalue is 0 because we have one connected component. The second eigenvalue is near 0 because we\u2019re one edge away from having two connected components. We also saw that the vector associated with that value tells us how to separate the nodes into those approximately connected components.", "You may have noticed that the next two eigenvalues are also pretty small. That tells us that we are \u201cclose\u201d to having four separate connected components. In general, we often look for the first large gap between eigenvalues in order to find the number of clusters expressed in our data. See the gap between eigenvalues four and five?", "Having four eigenvalues before the gap indicates that there is likely four clusters. The vectors associated with the first three positive eigenvalues should give us information about which three cuts need to be made in the graph to assign each node to one of the four approximated components. Let\u2019s build a matrix from these three vectors and perform K-Means clustering to determine the assignments:", "The graph has been segmented into the four quadrants, with nodes 0 and 5 arbitrarily assigned to one of their connected quadrants. That is really cool, and that is spectral clustering!", "To summarize, we first took our graph and built an adjacency matrix. We then created the Graph Laplacian by subtracting the adjacency matrix from the degree matrix. The eigenvalues of the Laplacian indicated that there were four clusters. The vectors associated with those eigenvalues contain information on how to segment the nodes. Finally, we performed K-Means on those vectors in order to get the labels for the nodes. Next, we\u2019ll see how to do this for arbitrary data.", "Look at the data below. The points are drawn from two concentric circles with some noise added. We\u2019d like an algorithm to be able to cluster these points into the two circles that generated them.", "This data isn\u2019t in the form of a graph. So first, let\u2019s just try a cookie cutter algorithm like K-Means. K-Means will find two centroids and label the points based on which centroid they are closest too. Here are the results:", "Obviously, K-Means wasn\u2019t going to work. It operates on euclidean distance, and it assumes that the clusters are roughly spherical. This data (and often real world data) breaks these assumptions. Let\u2019s try to tackle this with spectral clustering.", "There are a couple of ways to treat our data as a graph. The easiest way is to construct a k-nearest neighbors graph. A k-nearest neighbors graph treats every data point as a node in a graph. An edge is then drawn from each node to its k nearest neighbors in the original space. Generally, the algorithm isn\u2019t too sensitive of the choice of k. Smaller numbers like 5 or 10 usually work pretty well.", "Look at the picture of the data again, and imagine that each point is connected to its 5 closest neighbors. Any point in the outer ring should be able to follow a path along the ring, but there won\u2019t be any paths into the inner circle. It\u2019s pretty easy to see that this graph will have two connected components: the outer ring and the inner circle.", "Since we\u2019re only separating this data into two components, we should be able to use our Fiedler vector trick from before. Here\u2019s the code I used to perform spectral clustering on this data:", "The nearest neighbor graph is a nice approach, but it relies on the fact that \u201cclose\u201d points should belong in the same cluster. Depending on your data, that might not be true. A more general approach is to construct an affinity matrix. An affinity matrix is just like an adjacency matrix, except the value for a pair of points expresses how similar those points are to each other. If pairs of points are very dissimilar then the affinity should be 0. If the points are identical, then the affinity might be 1. In this way, the affinity acts like the weights for the edges on our graph.", "How to decide on what it means for two data points to be similar is one of the most important questions in machine learning. Often domain knowledge is the best way to construct a similarity measure. If you have access to domain experts, ask them this question.", "There are also entire fields dedicated to learning how to construct similarity metrics directly from data. For example, if you have some labeled data, you can train a classifier to predict whether two inputs are similar or not based on if they have the same label. This classifier can then be used to assign affinity to pairs of unlabeled points.", "We\u2019ve covered the theory and application of spectral clustering for both graphs and arbitrary data. Spectral clustering is a flexible approach for finding clusters when your data doesn\u2019t meet the requirements of other common algorithms.", "First, we formed a graph between our data points. The edges of the graph capture the similarities between the points. The eigenvalues of the Graph Laplacian can then be used to find the best number of clusters, and the eigenvectors can be used to find the actual cluster labels.", "I hope you enjoyed this post and find spectral clustering useful in your work or exploration.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faba2640c0d5b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@william.fleshman?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&user=William+Fleshman&userId=797753643492&source=post_page-797753643492----aba2640c0d5b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&user=William+Fleshman&userId=797753643492&source=-----aba2640c0d5b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&user=William+Fleshman&userId=797753643492&source=-----aba2640c0d5b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faba2640c0d5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aba2640c0d5b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aba2640c0d5b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/@william.fleshman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "865 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&user=William+Fleshman&userId=797753643492&source=post_page-797753643492--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf0fad40b9b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-clustering-aba2640c0d5b&newsletterV3=797753643492&newsletterV3Id=bf0fad40b9b5&user=William+Fleshman&userId=797753643492&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}