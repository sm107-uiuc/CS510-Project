{"url": "https://towardsdatascience.com/mc-control-methods-50c018271553", "time": 1683011566.356562, "path": "towardsdatascience.com/mc-control-methods-50c018271553/", "webpage": {"metadata": {"title": "MC Control Methods. Constant-\u03b1 MC Control | Towards Data Science", "h1": "MC Control and Temporal-Difference Methods", "description": "In this new post we will improve the Monte Carlo Control Methods to estimate the optimal policy presented in the previous post."}, "outgoing_paragraph_urls": [{"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post", "paragraph_index": 11}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 21}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 21}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "GitHub", "paragraph_index": 34}, {"url": "http://www.incompleteideas.net/papers/sutton-88-with-erratum.pdf", "anchor_text": "research paper", "paragraph_index": 48}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "Greedy in the Limit with Infinite Exploration", "paragraph_index": 68}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction", "paragraph_index": 71}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "In the next post", "paragraph_index": 75}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 76}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 76}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 77}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 78}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 80}], "all_paragraphs": ["In this new post of the \u201cDeep Reinforcement Learning Explained\u201d series, we will improve the Monte Carlo Control Methods to estimate the optimal policy presented in the previous post. In the previous algorithm for Monte Carlo control, we collect a large number of episodes to build the Q-table. Then, after the values in the Q-table have converged, we use the table to come up with an improved policy.", "However, Monte Carlo prediction methods can be implemented incrementally, on an episode-by-episode basis and this is what we will do in this post. Even though the policy is updated before the values in the Q-table accurately approximate the action-value function, this lower-quality estimate nevertheless still has enough information to help propose successively better policies.", "Furthermore, the Q-table can be updated at every time step instead of waiting until the end of the episode using Temporal-Difference Methods. We will review them also in this post.", "In the previous post we have introduced how the Monte Carlo control algorithm collects a large number of episodes to build the Q-table ( policy evaluation step). Then, once the Q-table closely approximates the action-value function q\u03c0\u200b, the algorithm uses the table to come up with an improved policy \u03c0\u2032 that is \u03f5-greedy with respect to the Q-table (indicated as \u03f5-greedy(Q) ), which will yield a policy that is better than the original policy \u03c0 (policy improvement step).", "Maybe would it be more efficient to update the Q-table after every episode? Yes, we could amend the policy evaluation step to update the Q-table after every episode of interaction. Then, the updated Q-table could be used to improve the policy. That new policy could then be used to generate the next episode, and so on:", "The most popular variation of the MC control algorithm that updates the policy after every episode (instead of waiting to update the policy until after the values of the Q-table have fully converged from many episodes) is the Constant-alpha MC Control.", "In this variation of MC control, during the policy evaluation step, the Agent collects an episode", "using the most recent policy \u03c0. After the episode finishes in time-step T, for each time-step t, the corresponding state-action pair (St, At) is modified using the following update equation:", "where Gt is the return at time-step t, and Q(St,At) is the entry in the Q-table corresponding to state St\u200b and action At\u200b.", "Generally speaking, the basic idea behind this update equation is that the Q(St\u200b,At\u200b) element of Q-table contains the Agent\u2019s estimate for the expected return if the Environment is in state St\u200b and the Agent selects action At\u200b. Then, If the return Gt\u200b is not equal to the expected return contained in Q(St\u200b,At\u200b), we \u201cpush\u201d the value of Q(St\u200b,At\u200b) to make it agree slightly more with the return Gt. The magnitude of the change that we make to Q(St\u200b,At\u200b) is controlled by the hyperparameter \u03b1 that acts as a step-size for the update step.", "We always should set the value for \u03b1 to a number greater than zero and less than (or equal to) one. In the outermost cases:", "In the previous post we advanced that random behavior is better at the beginning of the training when our Q-table approximation is bad, as it gives us more uniformly distributed information about the Environment states. However, as our training progresses, random behavior becomes inefficient, and we want to use our Q-table approximation to decide how to act. We introduced Epsilon-Greedy policies in the previous post for this purpose, a method that performs such a mix of two extreme behaviors which just is switching between random and Q policy using the probability hyperparameter \u03f5. By varying \u03f5 , we can select the ratio of random actions.", "We will define that a policy is \u03f5-greedy with respect to an action-value function estimate Q if for every state,", "So the larger \u03f5 is, the more likely you are to pick one of the non-greedy actions.", "To construct a policy \u03c0 that is \u03f5-greedy with respect to the current action-value function estimate Q, mathematically we will set the policy as", "if action a maximizes Q(s,a). Else", "In this equation, it is included an extra term \u03f5/\u2223A(s)\u2223 for the optimal action (\u2223A(s)\u2223 is the number of possible actions) because the sum of all the probabilities needs to be 1. Note that if we sum over the probabilities of performing all non-optimal actions, we will get (\u2223A(s)\u2223\u22121)\u00d7\u03f5/\u2223A(s)\u2223, and adding this to 1\u2212\u03f5+\u03f5/\u2223A(s)\u2223 , the probability of the optimal action, the sum gives one.", "Remember that in order to guarantee that MC control converges to the optimal policy \u03c0\u2217\u200b, we need to ensure the conditions Greedy in the Limit with Infinite Exploration (presented in the previous post) that ensure the Agent continues to explore for all time steps, and the Agent gradually exploits more and explores less. We presented that one way to satisfy these conditions is to modify the value of \u03f5 , making it gradually decay, when specifying an \u03f5-greedy policy.", "The usual practice is to start with \u03f5 = 1.0 (100% random actions) and slowly decrease it to some small value \u03f5 > 0 (in our example we will use \u03f5 = 0.05) . In general, this can be obtained by introducing a factor \u03f5-decay with a value near 1 that multiply the \u03f5 in each iteration.", "We can summarize all the previous explanations with this pseudocode for the constant-\u03b1 MC Control algorithm that will guide our implementation of the algorithm:", "In this section, we will write an implementation of constant-\ud835\udefc MC control that can help an Agent recover the optimal policy the Blackjack Environment following the pseudocode introduced in the previous post.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "In the previous post, we implemented a policy where the player almost always sticks if the sum of her cards exceeds 18 for the BlackJack Environment. In this case, the function generate_episode sampled an episode using this defined policy by the programmer.", "Here, instead of being the policy hardcoded by the programmer, the MC Control algorithm will estimate and return an optimal policy, together with the Q-table:", "Specifically, policy is a dictionary whose key corresponds to a states (a 3-tuple indicating the player\u2019s current sum, the dealer\u2019s face-up card, and whether or not the player has a usable ace) and the value of the corresponding entry indicates the action that the Agent chooses after observing this state following this policy.", "Remember that the other dictionary returned by the function, the Q-tableQ, is a dictionary where the key of a given entry in the dictionary corresponds to a states and the value of the corresponding entry contains an array of dimension equal to the number of actions (2 dimensions in our case) where each element contains the estimated action-value for each action.", "As input this MC Control algorithm has the following arguments:", "Before starting to program, a code based on the previously presented pseudocode takes a moment to see how we modify the value of \u03f5 , making it gradually decay when specifying an \u03f5-greedy policy. Remember that this is important to guarantee that MC control converges to the optimal policy \u03c0\u2217\u200b.", "With the following code that sets the value for \u03f5 in each episode and monitor its evolutions with a print we can check that selecting an eps_decay=0.9999965 we can obtain the gradual decay of \u03f5:", "Before entering the loop over episodes, we initialize the value of epsilon to one. Then, for each episode, we slightly decay the value of Epsilon by multiplying it by the value eps_decay. We don\u2019t want Epsilon to get too small because we want to constantly ensure at leans some small amount of exploration throughout the process.", "Let\u2019s start to program a code based on the previously presented pseudocode. The first thing, following the pseudocode, is to initialize all the values in the Q-table to zero. So Qis initialized to an empty dictionary of arrays with the total number of actions that are in the Environment:", "After that, we loop num_episodesover episodes, and then with each episode we compute the corresponding \u03f5, construct the corresponding \u03f5-greedy policy with respect to the most recent estimate of the Q-table, and then generate an episode using that \u03f5-greedy policy. Finally, we update the Q-table using the update equation presented before:", "After finishing the loop of episodes, the policy corresponding to the final Q-table is calculated with the following code:", "That is, the policy indicates for each state which action to take, which just corresponds to the action that has the maximum action-value in the Q-table.", "See the GitHub for the complete code the main algorithm of our Constant-\u03b1 MC Control method.", "The construction of the corresponding \u03f5-greedy policy and the generation of an episode using this \u03f5-greedy policy are wrapped up in the generate_episode_from_Q function instantiated in the previous code.", "This function takes as input the Environment, the most recent estimate of the Q-table, the value of current Epsilon and the number of actions. As an output, it returns an episode.", "The Agent will use the Epsilon-greedy policy to select actions. We have implemented that using the random.choicemethod from Numpy, which takes as input the set of possible actions and the probabilities corresponding to the Epsilon greedy policy. The obtention of the action probabilities corresponding to \u03f5-greedy policy will be done using this code:", "If you take a look at get_probsfunction code, it implements the epsilon-greedy policy detailed in the previous section.", "Obviously, if the state is not already in Q-table, we randomly choose one action using the action_space.sample(). The complete code for this function that generates an episode following the epsilon-greedy policy is coded as follows:", "Once we have the episode we just look at each state-action and we apply the update equation:", "The code that programs this equation is", "As in the example of the previous post, we can obtain the corresponding estimated optimal state-value function and plot it:", "Remember there are two plots corresponding to whether we do or don\u2019t have a usable ace.", "With a simple visual analysis of the graphs of this post and those of the previous post, we can see that the policy obtained with the MC Control presented here is better since the state-value values are much higher.", "One of the main drawbacks of Monte Carlo (MC) method is the fact that the Agent has to wait until the end of an episode to obtain the actual return (experienced return) before it can update and make any improvements to the value function estimate. This implies that Monte Carlo has pretty solid convergence properties because it updates the value function estimate towards the actual return G, which is an unbiased estimate of the true value function.", "However, while the actual returns G are pretty accurate estimates, it is also a high variance estimate of the true value function. This is because actual returns accumulate many random events in the same trajectory; all actions, all next states, all rewards are random events. That means that the actual return collects and compounds all of that randomness for multiple time steps. This is why we say that in Monte Carlo, the actual return is unbiased, but high variance.", "Also, due to the high variance of the actual returns, Monte Carlo can be very sample inefficient. All of that randomness becomes noise that can only be alleviated with lots of trajectories and actual returns samples. One way to diminish the high variance issue, instead of using the actual return, is to estimate a return and learn from uncompleted episodes.", "This approach, named Temporal Differences (TD), was presented by Richard Sutton in 1988 in a research paper. This article showed that while methods such as MC calculate errors using the differences between predicted and actual returns, TD was able to use the difference between temporally successive predictions (thus the name Temporal-Difference learning).", "This finding had a great impact on the advancement of Reinforcement Learning methods, and TD learning is the precursor of techniques such as SARSA, DQN, and more, that we will present later in this series.", "It should be noted that the estimated return used in these methods is a biased estimate because we use an estimate to calculate an estimate. This way of updating an estimate with an estimate is referred to as bootstrapping (which we will talk about later in this series), and has a certain similarity with what the Dynamic Programming methods do.", "But despite being a biased estimate method, it also has a much lower variance than the Monte Carlo method. This is because the TD target depends only on a single action, a single transition, and a single reward, so there is much less randomness being accumulated. As a consequence, TD methods usually learn much faster than MC methods.", "As we already introduced, the methods called Temporal-Difference (TD) learning are a combination of Monte Carlo (MC) ideas and dynamic programming (DP) ideas. Like MC methods, TD methods can learn directly from raw experience without a model of the Environment\u2019s dynamics. However, like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.", "Remember that the update equation for Monte Carlo Control after a complete episode is:", "In this update equation , we look up the current estimate Q(St\u200b,At\u200b) from the Q-table and compare it to the return Gt\u200b that we actually experienced after visiting the state-action pair. We use that new return Gt to make our Q-table a little more accurate. The only real difference between TD methods and MC methods is that TD methods update the Q-table at every time step instead of waiting until the end of the episode.", "There are three main variations called, Sarsa, Sarsamax, and Expected Sarsa, that implement three versions of the update equation. With the exception of this new update step, all the rest of the code is identical to what we saw in the Monte Carlo Control Method. Let\u2019s see each one of them.", "With this method, we begin by initializing all action-values to zero in constructing the corresponding Epsilon Greedy policy. Then, the Agent begins interacting with the environment and receives the first state S0. Next, it uses the policy to choose the action A0. Immediately after it, it receives a reward R1 and next state S1. Then, the agent again uses the same policy to pick the next action A1. After the sequence", "the method updates the action-value Q-table corresponding to the previous state-action pair. However, instead of using the return as an alternative estimate for updating the Q-table, we use the sum of the immediate reward R1 and the discounted value of the next state action pair Q(S1\u200b, A1) multiplied by gamma factor:", "The update equation expressed more generally is:", "Remember that with the exception of this new update step, the rest of the code is identical to what we did in the Monte Carlo Control case. In particular, we will use the \u03f5-greedy(Q) policy to select actions at every time step.", "Following, the reader can find a global scheme that summarizes what has been explained:", "Another TD control method is Sarsamax, also known as Q-Learning, that works slightly differently from de Sarsa.", "This method begins with the same initial values for the action-values and the policy of Sarsa. The agent receives the initial state S0, the first A0 action is still chosen from the initial policy. But then, after receiving the reward R1 and next state S1, instead of using the same policy to pick the next action A1 (the action that was selected using the Epsilon Greedy policy), we\u2019ll update the policy before choosing the next action.", "In particular, for the estimation, we will consider the action using a Greedy policy, instead of the Epsilon Greedy policy. That means that the will use the action that maximizes the Q(s,a) value for a given action. So the update equation for Sarsamax will be:", "where we rely on the fact that the greedy action corresponding to a state is just the one that maximizes the action values for that state.", "And so what happens is after we update the action value for time step zero using the greedy action, we then select A1 using the Epsilon greedy policy corresponding to the action values we just updated. And this continues when we received a reward and next state. Then, we do the same thing we did before where we update the value corresponding to S1 and A1 using the greedy action, then we select A2 using the corresponding Epsilon greedy policy, and so on. Following the same schematization made with sarsa, it can be visually summarized as:", "A final version of the update equation is Expected Sarsa. While Sarsamax takes the maximum over all actions of all possible next state-action pairs, Expected Sarsa uses the expected value of the next state-action pair, where the expectation takes into account the probability that the Agent selects each possible action from the next state:", "Below there is a summary of the three TD update equations (and MC):", "All three TD methods converge to the optimal action-value function q\u2217\u200b (and so yield the optimal policy \u03c0\u2217\u200b) if the value of \u03f5 decays in accordance with the Greedy in the Limit with Infinite Exploration (GLIE) conditions, and the step-size parameter \u03b1 is sufficiently small.", "Note that TD Methods update their estimates based in part on other estimates. They learn a guess from a guess, they bootstrap. In TD methods the return starting from a state-action pair is estimated while in MC we use the exact return Gt.", "Bootstrapping in RL can be read as \u201cusing estimated values in the update step for the same kind of estimated value\u201d.", "Neither MC methods nor one-step TD methods are always the best. The n-step Bootstrapping Methods unify the MC Methods and TD Methods, that generalize both methods so that one can shift from one to the other smoothly as needed to meet the demands of a particular task. These methods are out of the scope of this series, however, if the reader is interested in further details, he or she can start with Chapter 5 of the textbook Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.", "Let\u2019s use these presented methods to delve into one of the widely used concepts for classifying methods in Reinforcement Learning: On-policy and off-policy methods.", "We say that Sarsa and Expected Sarsa are both on-policy TD control algorithms. In this case, the same (\u03f5-greedy) policy that is evaluated and improved is also used to select actions.", "On the other hand, Sarsamax is an off-policy method, where the (greedy) policy that is evaluated and improved is different from the (\u03f5\u03f5-greedy) policy that is used to select actions.", "We have reached the end of this post!. So far, we have presented solution methods that represent the action values in a small table, the Q-table. In the next post, we will introduce you to the idea of using neural networks to expand the size and complexity of the problems that we can solve with reinforcement learning. See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F50c018271553&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 14"}, {"url": "https://torres-ai.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----50c018271553---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c018271553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----50c018271553---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50c018271553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=-----50c018271553---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/6-obtenci%C3%B3n-de-pol%C3%ADticas-%C3%B3ptimas-87267b32df00", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/6-obtenci%C3%B3n-de-pol%C3%ADticas-%C3%B3ptimas-87267b32df00", "anchor_text": "6. Obtenci\u00f3n de pol\u00edticas \u00f3ptimasAcceso abierto al cap\u00edtulo 6 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "previous post"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_14_Monte_Carlo.ipynb", "anchor_text": "GitHub"}, {"url": "http://www.incompleteideas.net/papers/sutton-88-with-erratum.pdf", "anchor_text": "research paper"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "Greedy in the Limit with Infinite Exploration"}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "In the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----50c018271553---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----50c018271553---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----50c018271553---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----50c018271553---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----50c018271553---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c018271553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----50c018271553---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c018271553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----50c018271553---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50c018271553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----50c018271553---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----50c018271553---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Written by Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=post_page-----50c018271553--------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----50c018271553---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmc-control-methods-50c018271553&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----50c018271553---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "The Bellman EquationV-function and Q-function Explained"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "\u00b712 min read\u00b7Jun 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7----0-----------------clap_footer----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----50c018271553----0---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=-----50c018271553----0-----------------bookmark_preview----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c018271553----1---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----50c018271553----1-----------------bookmark_preview----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c018271553----2---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----50c018271553----2-----------------bookmark_preview----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "Deep Q-Network (DQN)-IIExperience Replay and Target Networks"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": "\u00b714 min read\u00b7Aug 15, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----b6bf911b6b2c----3-----------------clap_footer----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----50c018271553----3---------------------9fc0d719_3678_4586_bb12_c45e245a590b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&source=-----50c018271553----3-----------------bookmark_preview----9fc0d719_3678_4586_bb12_c45e245a590b-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "See all from Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----50c018271553----0-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----50c018271553----1-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c018271553----0---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----50c018271553----0-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----50c018271553----1---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----50c018271553----1-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c018271553----2---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----50c018271553----2-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----3-----------------clap_footer----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----50c018271553----3---------------------0881ce8b_d173_4d85_b9aa_51d2e7454865-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----50c018271553----3-----------------bookmark_preview----0881ce8b_d173_4d85_b9aa_51d2e7454865-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----50c018271553--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----50c018271553--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----50c018271553--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----50c018271553--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----50c018271553--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----50c018271553--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----50c018271553--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}