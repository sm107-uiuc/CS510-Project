{"url": "https://towardsdatascience.com/the-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a", "time": 1683000806.812581, "path": "towardsdatascience.com/the-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a/", "webpage": {"metadata": {"title": "The Deep Music Visualizer: Using sound to explore the latent space of BigGAN | by Matt Siegelman | Towards Data Science", "h1": "The Deep Music Visualizer: Using sound to explore the latent space of BigGAN", "description": "Want to make a deep music video? Wrap your mind around BigGAN. Developed at Google by Brock et al. (2018)\u00b9, BigGAN is a recent chapter in a brief history of generative adversarial networks (GANs)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/", "anchor_text": "BigGAN", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1809.11096", "anchor_text": "Brock et al. (2018)", "paragraph_index": 0}, {"url": "https://blog.floydhub.com/gans-story-so-far/", "anchor_text": "brief history", "paragraph_index": 0}, {"url": "https://twitter.com/quasimondo/status/1065610256917692416?ref_src=twsrc%5Etfw", "anchor_text": "estimated $60,000", "paragraph_index": 1}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "ImageNet classes", "paragraph_index": 2}, {"url": "https://artbreeder.com/", "anchor_text": "artbreeder", "paragraph_index": 7}, {"url": "https://medium.com/@genekogan/artist-in-the-cloud-8384824a75c7", "anchor_text": "autonomous artificial artists", "paragraph_index": 7}, {"url": "https://twitter.com/hashtag/FindingMona?src=hashtag_click", "anchor_text": "searching for the Mona Lisa", "paragraph_index": 7}, {"url": "https://twitter.com/memotv/status/1068376207127920641?ref_src=twsrc%5Etfw", "anchor_text": "BigGAN to music", "paragraph_index": 8}, {"url": "https://github.com/msieg/deep-music-visualizer/blob/master/README.md", "anchor_text": "README file", "paragraph_index": 12}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "ImageNet classes", "paragraph_index": 14}, {"url": "https://medium.com/@jamsawamsa/running-a-google-cloud-gpu-for-fast-ai-for-free-5f89c707bae6", "anchor_text": "virtual GPU on google cloud", "paragraph_index": 16}, {"url": "https://twitter.com/MattSiegelman", "anchor_text": "tweet me", "paragraph_index": 27}, {"url": "https://www.instagram.com/deep_music_visualizer/", "anchor_text": "here", "paragraph_index": 28}], "all_paragraphs": ["Want to make a deep music video? Wrap your mind around BigGAN. Developed at Google by Brock et al. (2018)\u00b9, BigGAN is a recent chapter in a brief history of generative adversarial networks (GANs). GANs are AI models trained by two competing neural networks: a generator creates new images based on statistical patterns learned from a set of example images, and a discriminator tries to classify the images as real or fake. By training the generator to fool the discriminator, GANs learn to create realistic images.", "BigGAN is considered Big because it contains over 300 million parameters trained on hundreds of google TPUs at the cost of an estimated $60,000. The result is an AI model that generates images from 1128 input parameters:", "i) a 1000-unit class vector of weights {0 \u2264 1} that correspond to 1000 ImageNet classes, or object categories.", "ii) a 128-unit noise vector of values {-2 \u2264 2} that control the visual features of objects in the output image, like color, size, position and orientation.", "A class vector of zeros except a one in the vase class outputs a vase:", "Interpolating between classes without changing the noise vector reveals shared features in the latent space, like faces:", "Interpolating between random vectors reveals deeper sorts of structure:", "If you\u2019re intrigued, join the expedition of artists, computer scientists and cryptozoologists on this strange frontier. Apps like artbreeder have provided simple interfaces for creating AI artwork, and autonomous artificial artists loom while some users occupy themselves searching for the Mona Lisa.", "Others have set BigGAN to music.", "These \u201cdeep music videos\u201d have garnered mixed reactions, varying between beautiful, trippy, and horrifying. To be fair, one is wise to fear what lurks in latent space\u2026", "What other unlikely chimeras, mythical creatures, priceless artworks and familiar dreams reside within BigGAN? To find out, we need to cover more ground. That\u2019s why I built the deep music visualizer, an open source, easy-to-use tool for navigating the latent space with sound.", "Take it for a spin and create some cool music videos along the way. Just make sure to share what you discover.", "Clone the GitHub repository and follow the installation instructions in the README file.", "Run this command in your terminal:", "What\u2019s going on here? The deep music visualizer syncs pitch with the class vector and volume and tempo with the noise vector, so that pitch controls the objects, shapes, and textures in each frame, while volume and tempo control movement between frames. At each time point in the song, a chromagram of the twelve chromatic notes determines the weights {0 \u2264 1} of up to twelve ImageNet classes in the class vector. Independently, the rate of change of the volume \u2014 mainly percussion \u2014 controls the rate of change of the noise vector.", "BigGAN is big, and therefore slow. If you ran the first example on your laptop, it will take ~7 hours to render. With a resolution of 128x128, it would only take 25 minutes (per minute of video).", "However, I recommend you generate high resolution videos by launching a virtual GPU on google cloud to significantly speed up runtime from ~7 hours to a few minutes. While it isn\u2019t free, google awards new users $300 in credit, and a GPU costs $1/hour.", "It can be useful to generate shorter videos to limit runtime while testing out some other input parameters.", "The pitch sensitivity is the sensitivity of the class vector to changes in pitch. At higher pitch sensitivity, the shapes, textures and objects in the video change more rapidly and adhere more precisely to the notes in the music.", "The tempo sensitivity is the sensitivity of the noise vector to changes in volume and tempo. Higher tempo sensitivity yields more movement.", "In this example, the classes cohere strongly to the pitch because pitch sensitivity is high, but there is little overall movement because tempo sensitivity is low.", "In this example, the class mixture hardly changes because pitch sensitivity is low, but there is more overall movement because tempo sensitivity is high.", "Lower the number of classes to mix fewer objects.", "You can choose which classes you want to include in the video. The classes sync with pitches in chromatic order (A, A#, B\u2026).", "Alternatively, set sort_classes_by_power to 1 if you prefer to enter classes in a prioritized order.", "In this example, the video includes daisy (#985) and jellyfish (#107), but with more daisy than jellyfish:", "The frame length is the number of audio samples per video frame. The default frame length of 512 yields a video frame rate of ~43 fps. Decreasing the frame length increases the frame rate so the image updates more frequently (but the video will take longer to render). This is most useful for visualizing rapid music.", "I hope you found this tutorial interesting and informative. If you want to express thanks, tweet me a deep music video you create with this code!", "You can find more of my videos here.", "[1] A. Brock, J. Donahu and K. Simonyan, Large Scale GAN Training for High Fidelity Natural Image Synthesis (2018), Eighth International Conference on Learning Representations.", "[2] N. Kriegeskorte, Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing (2015), Annual Review of Vision Science.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a PhD student in the Dynamic Perception and Memory Lab in the Psychology department at Columbia University, where I use AI to study the brain."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F198cd37dac9a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mes2338?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mes2338?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Matt Siegelman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8023eb434921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&user=Matt+Siegelman&userId=8023eb434921&source=post_page-8023eb434921----198cd37dac9a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/", "anchor_text": "BigGAN"}, {"url": "https://arxiv.org/abs/1809.11096", "anchor_text": "Brock et al. (2018)"}, {"url": "https://blog.floydhub.com/gans-story-so-far/", "anchor_text": "brief history"}, {"url": "https://twitter.com/quasimondo/status/1065610256917692416?ref_src=twsrc%5Etfw", "anchor_text": "estimated $60,000"}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "ImageNet classes"}, {"url": "https://artbreeder.com/", "anchor_text": "artbreeder"}, {"url": "https://medium.com/@genekogan/artist-in-the-cloud-8384824a75c7", "anchor_text": "autonomous artificial artists"}, {"url": "https://twitter.com/hashtag/FindingMona?src=hashtag_click", "anchor_text": "searching for the Mona Lisa"}, {"url": "https://twitter.com/memotv/status/1068376207127920641?ref_src=twsrc%5Etfw", "anchor_text": "BigGAN to music"}, {"url": "https://github.com/msieg/deep-music-visualizer/blob/master/README.md", "anchor_text": "README file"}, {"url": "https://github.com/msieg/deep-music-visualizer", "anchor_text": "msieg/deep-music-visualizerThe Deep Music Visualizer uses BigGAN (Brock et al., 2018), a generative neural network, to visualize music. Like this\u2026github.com"}, {"url": "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a", "anchor_text": "ImageNet classes"}, {"url": "https://medium.com/@jamsawamsa/running-a-google-cloud-gpu-for-fast-ai-for-free-5f89c707bae6", "anchor_text": "virtual GPU on google cloud"}, {"url": "https://twitter.com/MattSiegelman", "anchor_text": "tweet me"}, {"url": "https://www.instagram.com/deep_music_visualizer/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Synesthesia", "anchor_text": "synesthesia"}, {"url": "https://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035447", "anchor_text": "representational similarity between deep neural networks and the human visual cortex"}, {"url": "https://twitter.com/MichaelFriese10/status/1169756130391650306?ref_src=twsrc%5Etfw", "anchor_text": "Neither did BigGAN"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----198cd37dac9a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----198cd37dac9a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/music?source=post_page-----198cd37dac9a---------------music-----------------", "anchor_text": "Music"}, {"url": "https://medium.com/tag/art?source=post_page-----198cd37dac9a---------------art-----------------", "anchor_text": "Art"}, {"url": "https://medium.com/tag/vid\u00e9o?source=post_page-----198cd37dac9a---------------vid%C3%A9o-----------------", "anchor_text": "Vid\u00e9o"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&user=Matt+Siegelman&userId=8023eb434921&source=-----198cd37dac9a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&user=Matt+Siegelman&userId=8023eb434921&source=-----198cd37dac9a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F198cd37dac9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----198cd37dac9a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----198cd37dac9a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----198cd37dac9a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----198cd37dac9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mes2338?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mes2338?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matt Siegelman"}, {"url": "https://medium.com/@mes2338/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "55 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8023eb434921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&user=Matt+Siegelman&userId=8023eb434921&source=post_page-8023eb434921--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2abcd18fa15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a&newsletterV3=8023eb434921&newsletterV3Id=2abcd18fa15a&user=Matt+Siegelman&userId=8023eb434921&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}