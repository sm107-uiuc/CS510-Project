{"url": "https://towardsdatascience.com/automate-your-job-search-with-python-and-github-actions-1dc818844c0", "time": 1683016943.1860852, "path": "towardsdatascience.com/automate-your-job-search-with-python-and-github-actions-1dc818844c0/", "webpage": {"metadata": {"title": "Automate your job search with Python and Github Actions | by Ioannis Foukarakis | Towards Data Science", "h1": "Automate your job search with Python and Github Actions", "description": "Job hunting is a time-consuming task. A lot of different sites for job searches exist, but there is not a \u201cone size fits all\u201d. Job openings are available in job aggregators, LinkedIn, career pages of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scrapy.org/", "anchor_text": "Scrapy", "paragraph_index": 3}, {"url": "https://requests.readthedocs.io/en/master/", "anchor_text": "requests", "paragraph_index": 3}, {"url": "https://github.com/mitsuhiko/pipsi", "anchor_text": "pipsi", "paragraph_index": 5}, {"url": "https://docs.scrapy.org/en/latest/topics/items.html", "anchor_text": "Items", "paragraph_index": 10}, {"url": "https://docs.scrapy.org/en/latest/topics/spiders.html", "anchor_text": "spiders", "paragraph_index": 11}, {"url": "https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy-spider", "anchor_text": "scrapy.Spider", "paragraph_index": 12}, {"url": "https://www.workable.com/", "anchor_text": "Workable", "paragraph_index": 14}, {"url": "https://recruitee.com", "anchor_text": "Recruitee", "paragraph_index": 14}, {"url": "https://docs.scrapy.org/en/latest/topics/selectors.html", "anchor_text": "selectors", "paragraph_index": 19}, {"url": "https://developers.google.com/web/tools/chrome-devtools/open#last", "anchor_text": "Developer tools (in Chrome/Brave)", "paragraph_index": 25}, {"url": "https://developer.mozilla.org/en-US/docs/Tools", "anchor_text": "Web developer tools (Firefox),", "paragraph_index": 25}, {"url": "https://docs.scrapy.org/en/latest/topics/item-pipeline.html", "anchor_text": "item pipelines", "paragraph_index": 35}, {"url": "https://docs.scrapy.org/en/latest/topics/exporters.html", "anchor_text": "item exporters", "paragraph_index": 41}, {"url": "https://simonwillison.net/2020/Oct/9/git-scraping/", "anchor_text": "Git scrapping", "paragraph_index": 46}, {"url": "https://github.com/ifoukarakis/openings/", "anchor_text": "https://github.com/ifoukarakis/jobscrapper/", "paragraph_index": 48}, {"url": "https://github.com/ifoukarakis/openings/", "anchor_text": "https://github.com/ifoukarakis/jobscrapper/", "paragraph_index": 51}], "all_paragraphs": ["Job hunting is a time-consuming task. A lot of different sites for job searches exist, but there is not a \u201cone size fits all\u201d. Job openings are available in job aggregators, LinkedIn, career pages of individual companies, even as tweets or in Git repos. Following all the changes is definitely challenging.", "But what if you could build your personal job hunting tool? This post is exactly about this. With the help of scrapping tools we\u2019ll build a small proof-of-concept that helps you keep track of jobs posted to company web sites. The data will be extracted in JSON format. This way you can build your own personalized newsletter or career page. Let\u2019s get started!.", "After having a look at a few career pages, a few things are pretty obvious:", "There are a lot of tools available for scraping data from the web. Scrapy is one of the best options out there. You can definitely use a simpler solution (i.e. requests or BeautifulSoup), but scrapy offers a lot of things out of the box. Some highlights include:", "The first thing needed is to install scrapy command-line interface. You can install it globally on your python environment :", "or if you have pipsi installed:", "Creating a new project is really easy. Running the command:", "Scrapy\u2019s CLI created a directory named jobscrapper. A python package (also named jobscrapper) is also created, containing basic project configuration.", "Depending on the method you used for installing scrapy, you might want to add a requirements.txt with scrapy dependency, as well as create a virtual environment and install dependencies. This will also help if you want to automate deployment in a remote server.", "Job information may come from different web sites. Since data might be used for generating reports or newsletter, having a structured or semi-structured format will be really helpful. If you check a couple of job sites, you\u2019ll notice that the following information is commonly shared:", "Scrapy introduces Items as the abstraction for defining the structure of the output. Spiders create items by processing data from the web. Scrapy offers the Item class for defining the format of an item. The following snippet represents the structure for the extracted job opening items:", "The next step is to perform the actual information extraction from the web pages. Scrapy offers the tools for implementing spiders \u2014 components that are able to parse specific pages and extract information from them.", "A spider is a Python class that subclasses one of Scrapy\u2019s Spider classes. The attributes of the class hold information regarding the URLs to parse, the spider\u2019s name, crawling configuration etc. The parent class also offers some methods that can be overridden in order to add any custom parsing logic. All spiders in this post will use the simplest spider superclass scrapy.Spider.", "Some of the most common attributes and methods of a Scrapy Spider are:", "Spiders can be built either for specific URLs or for pages following the same structure. Luckily a lot of companies use two great services: Workable and Recruitee. Both generate career pages using customizable templates. Creating crawlers for those two services will enable us to crawl a large number of jobs.", "Let\u2019s start with Recruitee-backed career pages. If you visit any of these pages, you\u2019ll notice that they have a parent page that contains the list of jobs, as well as links to pages with specific job details.", "Back to building the spider, it seems we\u2019ll need to do the following in our code:", "The page however doesn\u2019t include the whole job information. Some fields are available on the web page with the job\u2019s details. The spider will need to:", "If you visit any Recruitee-backed job list page and right-click -> Inspect on a job opening\u2019s title, you \u2018ll see something similar to the following image:", "Scrapy is using selectors for referring to specific parts of the page. Selectors are strings representing rules that refer to specific parts of the page. They can be either XPath expressions or CSS selectors. For example div.job will refer to all div containers with class job. Checking the HTML source of the page we can see there\u2019s one such container for each job. Looking more carefully, we can see that information available includes job title, department, location and link to the job\u2019s page.", "Getting the jobs is straight forward. We just need to iterate over all divs with class job. In order to trigger a new request for each job\u2019s detail page, parse will yield one scrapy.Request for each job.", "Requests can be configured. The most important argument is url. The URLs can be constructed by getting the URL of parent page and joining the path defined in the <a href=\"...\"> field inside h5 with class job-title. However, when a request contains only the url parameter, the default method called for processing the response is parse. Job detail pages have different structure than parent pages. Thus a new method for parsing job details needs to be added.", "Let\u2019s call the method for parsing job detail pages parse_job. Its responsibility will be to load job detail pages, merge any information already available from job list page, and return an JobOpeningItem. Using the same logic as in parse, job details can be extracted using selectors.", "The tricky part is how to pass information between requests. Luckily scrapy.Request has a meta argument that can be a dictionary of values. The dictionary will be copied to the response object passed to parse_job. This meta dictionary is ideal for sharing information between the two requests.", "Let\u2019s have a look at the final code:", "Job pages powered by Workable seem to use a different approach. Instead of embedding the job information in the HTML page upon load, they use a JSON API to load job information after the page has been loaded. This is a really common pattern in web pages. You can see this happening by opening Developer tools (in Chrome/Brave) or Web developer tools (Firefox), then navigating to Network\ud83e\udc06XHR and visiting a career page. You will see something similar to this:", "The jobs request is the actual request made to the server by the Javascript code of the page in order to load the list of jobs. Headers tab contains the details of the request (it\u2019s a POST request) as well as the payload of the request. Preview tab offers a formatted version of the JSON returned by the endpoint. You might notice that there\u2019s two requests to the same jobs URLs. That\u2019s because the list of results is too big in size in order to be returned in a single response. Instead, the first request returns the first 10 results, as well as a pointer to the next page. The second request returns the rest of the results.", "If you click on a specific job on the web page, you\u2019ll notice that more XHR requests are added. One of them uses the shortcode from the previous request in order to create the URL for getting the job\u2019s details.", "As discussed earlier, the list of jobs is not part of the page, but is loaded using a new request. The spider will have to imitate this logic. The simplest solution would be to override start_requests() in order to perform a new JSON request for each job list page.", "Since the responses are in JSON format, there\u2019s no need to use selectors for extracting information. Instead standard python dictionary and array manipulation can be used. The logic is really similar as in the previous spider. Here\u2019s the first version:", "Job details are loaded as JSON objects. Creating the OpeningItem is pretty straight-forward.", "The code as is will work, but it will load jobs only from the first call on the API. Checking back on the developer tools the two XHR requests, we can see that the first response contains a field named nextPage. This value can be added to the payload of the request in order to indicate which page to use. The second request to jobs API endpoint helps understand which field should be set to the value of nextPage (it\u2019s token).", "Pagination can now be implemented by modifying parse. It will now do the following:", "You can think of the pagination logic as a recursive call to parse, using the output of the previous result.", "Data loaded from HTML pages (or even from the API) may contain additional spaces, possibly existing because of formatted HTML code. It might not make sense to keep them. A first solution would be to modify each spider to clear white spaces. But since this is a concern applying to all spiders we can do something better.", "Scrapy allows creating item pipelines. After an item has been created, it is sent to the pipelines defined in the configuration. The first step for adding an item pipeline is adding it in jobscrapper/pipelines.py . The item pipeline is a class that needs to define method process_item(self, item, spider) . This method performs any actions to all items passed to it. Here\u2019s how a pipeline that removes multiple spaces would look like:", "In order for scrapy to actually call the pipeline, the pipeline needs to be activated in settings.py. Open jobscrapper/settings.py, locate lines with ITEM_PIPELINE and edit it to look like the", "Running the spiders can be done using Scrapy\u2019s CLI. Simply run one of the following:", "The -o jobs.json option instructs the CLI tool to append results to file jobs.json. If it\u2019s not specified, results will just be printed on the terminal.", "The drawback of this approach is that if you run the two spiders, the file will contain multiple JSON documents (rather than a single one), making it difficult to parse.", "IMPORTANT NOTE: The URLs used in the code above are fictional. Make sure you replace them with real world examples! Otherwise the scraping will fail.", "The final step in python code is to store jobs as a JSON document in a separate file per company. Scrapy offers a lot of item exporters for storing parsed items. Although none of them does exactly what we want, there\u2019s a good example that can act as our guide. The idea is to create a pipeline that creates a separate exporter per company name. Then use one the existing JSON item exporter for storing the data. Here\u2019s how such an exporter would look like:", "Here\u2019s the explanation of how it works:", "The final step is to activate the pipeline in settings.py. Update ITEM_PIPELINES to look like the following snippet:", "I also defined JOBS_PATH so that all files are stored under data/ directory.", "Spiders can now be executed by running:", "The most difficult part is done. But it would be extremely nice if the process could be executed automatically, possibly sending an update. There\u2019s a really useful technique called Git scrapping that can help. The idea is that the code will run periodically. All output will be stored inside the git repo. The benefit of this approach is that changes in job postings can be tracked using Git.", "The following Github action does exactly this. Runs spiders once per week and creates a commit if something changed.", "The flow is finally completed! You can check it running at https://github.com/ifoukarakis/jobscrapper/", "The code is definitely not ready for production, neither it\u2019s something that automates job search. There\u2019s a lot of things missing.", "This post is intended as a tutorial on how to scrap data from different types of sources available on the Web. However each site might have its own terms & conditions. Make sure you read them and understand them before scrapping the data!", "If you \u2018re looking for the source code of this post, you can find it at https://github.com/ifoukarakis/jobscrapper/", "Do you have questions or suggestions? Feel free to add a comment!", "Hope you enjoyed reading this post! Happy job hunting!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software engineer, interested in machine learning projects and process, back-end development and coding in general. Geek since before it was cool to be one."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1dc818844c0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ifoukarakis.medium.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": ""}, {"url": "https://ifoukarakis.medium.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Ioannis Foukarakis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35853f381ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&user=Ioannis+Foukarakis&userId=35853f381ef7&source=post_page-35853f381ef7----1dc818844c0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@laughayette?utm_source=medium&utm_medium=referral", "anchor_text": "Marten Newhall"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://scrapy.org/", "anchor_text": "Scrapy"}, {"url": "https://requests.readthedocs.io/en/master/", "anchor_text": "requests"}, {"url": "https://github.com/mitsuhiko/pipsi", "anchor_text": "pipsi"}, {"url": "https://docs.scrapy.org/en/latest/topics/items.html", "anchor_text": "Items"}, {"url": "https://docs.scrapy.org/en/latest/topics/spiders.html", "anchor_text": "spiders"}, {"url": "https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy-spider", "anchor_text": "scrapy.Spider"}, {"url": "https://www.workable.com/", "anchor_text": "Workable"}, {"url": "https://recruitee.com", "anchor_text": "Recruitee"}, {"url": "https://docs.scrapy.org/en/latest/topics/selectors.html", "anchor_text": "selectors"}, {"url": "https://developers.google.com/web/tools/chrome-devtools/open#last", "anchor_text": "Developer tools (in Chrome/Brave)"}, {"url": "https://developer.mozilla.org/en-US/docs/Tools", "anchor_text": "Web developer tools (Firefox),"}, {"url": "https://docs.scrapy.org/en/latest/topics/item-pipeline.html", "anchor_text": "item pipelines"}, {"url": "https://docs.scrapy.org/en/latest/topics/exporters.html", "anchor_text": "item exporters"}, {"url": "https://simonwillison.net/2020/Oct/9/git-scraping/", "anchor_text": "Git scrapping"}, {"url": "https://github.com/ifoukarakis/openings/", "anchor_text": "https://github.com/ifoukarakis/jobscrapper/"}, {"url": "https://github.com/ifoukarakis/openings/", "anchor_text": "https://github.com/ifoukarakis/jobscrapper/"}, {"url": "https://medium.com/tag/scrapy?source=post_page-----1dc818844c0---------------scrapy-----------------", "anchor_text": "Scrapy"}, {"url": "https://medium.com/tag/job-hunting?source=post_page-----1dc818844c0---------------job_hunting-----------------", "anchor_text": "Job Hunting"}, {"url": "https://medium.com/tag/git-scrapping?source=post_page-----1dc818844c0---------------git_scrapping-----------------", "anchor_text": "Git Scrapping"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----1dc818844c0---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/tag/python?source=post_page-----1dc818844c0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&user=Ioannis+Foukarakis&userId=35853f381ef7&source=-----1dc818844c0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&user=Ioannis+Foukarakis&userId=35853f381ef7&source=-----1dc818844c0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1dc818844c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1dc818844c0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1dc818844c0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1dc818844c0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1dc818844c0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1dc818844c0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1dc818844c0--------------------------------", "anchor_text": ""}, {"url": "https://ifoukarakis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ifoukarakis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ioannis Foukarakis"}, {"url": "https://ifoukarakis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35853f381ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&user=Ioannis+Foukarakis&userId=35853f381ef7&source=post_page-35853f381ef7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F35853f381ef7%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomate-your-job-search-with-python-and-github-actions-1dc818844c0&user=Ioannis+Foukarakis&userId=35853f381ef7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}