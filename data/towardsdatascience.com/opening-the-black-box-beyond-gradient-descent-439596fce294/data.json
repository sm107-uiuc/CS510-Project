{"url": "https://towardsdatascience.com/opening-the-black-box-beyond-gradient-descent-439596fce294", "time": 1683002537.539097, "path": "towardsdatascience.com/opening-the-black-box-beyond-gradient-descent-439596fce294/", "webpage": {"metadata": {"title": "Opening the black-box \u2014 beyond gradient descent | by Alex Shtoff | Towards Data Science", "h1": "Opening the black-box \u2014 beyond gradient descent", "description": "When training a model, we attempt to minimize the average of the model\u2019s loss functions over the training samples: For each i, f_i in the sum above is the loss incurred by the i\u1d57\u02b0 training sample as\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Proximal_gradient_method", "anchor_text": "proximal gradient", "paragraph_index": 20}], "all_paragraphs": ["When training a model, we attempt to minimize the average of the model\u2019s loss functions over the training samples:", "For each i, f_i in the sum above is the loss incurred by the i\u1d57\u02b0 training sample as a function of the model\u2019s parameter vector w. For the loss minimization task, we employ variants of the Stochastic Gradient method: at step k, we randomly choose a function f from {f_1, \u2026, f_n}, and compute:", "Many variants of the method exist, such as mini-batching, Ada-Grad and Adam, but one thing is common to all of them: each loss f is given as a \u2018black box\u2019 \u2014 nothing is assumed, except for the ability to compute its gradient.", "The black-box approach has its advantages, for example, we can easily change the loss function without changing the\u00a0training\u00a0algorithm. But this approach suffers from one major drawback\u200a\u2014\u200awe do not exploit any knowledge about f which, if exploited, might lead to a more efficient algorithm and substantially reduce the compute resources required to train our model.", "The gradient step is usually taught as \u2018take a small step in the negative-gradient direction\u2019. But there is an alternative view, the proximal view:", "Let\u2019s simplify the \u2018hairy\u2019 formula above. The blue term is the tangent, or the first order approximation at w_k, while the red term is a proximity measure to w_k. Consequently, according to the proximal view,", "at each iteration, we minimize a function which balances between descending along the tangent, and staying close to w_k.", "The balance between the two conflicting forces is determined by 1/\u03b7 \u2014 the weight of the proximity term. The sum of the tangent and the proximity term results in a \u2018tangent parabola\u2019 at w_k whose minimum is the next iterate.", "Thus, intuitively, the proximal view interprets the gradient step, as the minimizer of the tangent parabola, which locally approximates our loss function, as illustrated below. The larger \u03b7, the more \u2018flat\u2019 is the parabola, and thus we move farther away from our current iterate w_k.", "Formally, to convince ourselves that the above formula is indeed the gradient step in disguise, let\u2019s solve the minimization problem. Taking the gradient w.r.t w of the term inside the argmin and equating it with zero, results in:", "Multiplying both sides by \u03b7 and extracting w, recovers the gradient step.", "Remark: the proximal view of the gradient method is well-known for a long time in the optimization community, and can be found in Boris Polyak\u2019s \u2018Introduction to Optimization\u2019 book from 1987.", "A tangent is, of course, a reasonable approximation of functions we know nothing about. But what if we do know something? Could we replace the blue part (the tangent) with a better approximation?", "Let\u2019s look at a concrete example. Suppose that each loss is regularized, namely,", "where l\u1d62 is the (possibly weighted) \u2018raw\u2019 loss, to which we add a regularization term. In this case, we could approximate l\u1d62 with its tangent, while leaving the regularization term as is. Why should we? Because of the following intuitive rule of thumb:", "The less we approximate, the more information about the loss function is\u00a0retained\u00a0and\u00a0exploited, and thus a better algorithm is obtained.", "The original gradient step is, in our example, replaced by", "Let\u2019s see if we can obtain an explicit formula of w_{k+1}. Taking the gradient w.r.t w, and equating with zero, results in:", "We obtained an entirely new algorithm, which substantially differs from the stochastic gradient method applied to the losses directly. According to our new algorithm, at each iteration we apply a stochastic gradient step to the \u201craw\u201d loss l, and then divide the result by (2\u03b7+1).", "What happens if the regularization term is not the squared Euclidean norm, but some other function g instead? Explicitly written, the losses are", "We obtained the well known proximal gradient step. To be implementable in practice, we need an explicit formula for next iterate w_{k+1}.", "In the previous example, g was the squared Euclidean norm, and in that case we could easily obtain an explicit formula. In general, deriving such a formula is not always possible, and when it is, it depends on the function g. Each g results in a different custom update step, and thus the method is no longer \u2018black box\u2019 \u2014 we customize it for each g.", "Consequently, g should be \u2018simple\u2019 enough, so that a simple explicit formula, may be obtained, and therefore this scheme is applicable only for such \u2018simple\u2019 functions g.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D, research scientist at Yahoo Research"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F439596fce294&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----439596fce294--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----439596fce294--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alex.shtf?source=post_page-----439596fce294--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alex.shtf?source=post_page-----439596fce294--------------------------------", "anchor_text": "Alex Shtoff"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68a07c9fe175&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&user=Alex+Shtoff&userId=68a07c9fe175&source=post_page-68a07c9fe175----439596fce294---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Proximal_gradient_method", "anchor_text": "proximal gradient"}, {"url": "https://arxiv.org/abs/1810.05633", "anchor_text": "https://arxiv.org/abs/1810.05633"}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/S0167637702002316", "anchor_text": "https://www.sciencedirect.com/science/article/abs/pii/S0167637702002316"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----439596fce294---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----439596fce294---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/opening-black-box?source=post_page-----439596fce294---------------opening_black_box-----------------", "anchor_text": "Opening Black Box"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----439596fce294---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&user=Alex+Shtoff&userId=68a07c9fe175&source=-----439596fce294---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&user=Alex+Shtoff&userId=68a07c9fe175&source=-----439596fce294---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----439596fce294--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F439596fce294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----439596fce294---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----439596fce294--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----439596fce294--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----439596fce294--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----439596fce294--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----439596fce294--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----439596fce294--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----439596fce294--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----439596fce294--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alex.shtf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alex.shtf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alex Shtoff"}, {"url": "https://medium.com/@alex.shtf/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68a07c9fe175&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&user=Alex+Shtoff&userId=68a07c9fe175&source=post_page-68a07c9fe175--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8669855b735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopening-the-black-box-beyond-gradient-descent-439596fce294&newsletterV3=68a07c9fe175&newsletterV3Id=8669855b735e&user=Alex+Shtoff&userId=68a07c9fe175&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}