{"url": "https://towardsdatascience.com/website-data-cleaning-in-python-for-nlp-dda282a7a871", "time": 1683013345.466348, "path": "towardsdatascience.com/website-data-cleaning-in-python-for-nlp-dda282a7a871/", "webpage": {"metadata": {"title": "HTML Data Cleaning in Python for NLP | by Brandon Ko | Towards Data Science", "h1": "HTML Data Cleaning in Python for NLP", "description": "The most important step of any data-driven project is obtaining quality data. We discuss the following techniques for text preprocessing: HTML parsing, n-gram cleaning, stop words, tokenization."}, "outgoing_paragraph_urls": [{"url": "https://docs.python.org/3/library/urllib.html", "anchor_text": "urllib", "paragraph_index": 1}, {"url": "https://scrapyd.readthedocs.io/en/stable/index.html", "anchor_text": "scrapyd", "paragraph_index": 1}, {"url": "https://pypi.org/project/boilerpy3/", "anchor_text": "boilerpy3", "paragraph_index": 3}, {"url": "https://beautiful-soup-4.readthedocs.io/en/latest/", "anchor_text": "beautifulsoup", "paragraph_index": 6}, {"url": "https://pypi.org/project/gensim/", "anchor_text": "gensim", "paragraph_index": 14}], "all_paragraphs": ["The most important step of any data-driven project is obtaining quality data. Without these preprocessing steps, the results of a project can easily be biased or completely misunderstood. Here, we will focus on cleaning data that is composed of scraped web pages.", "There are many tools to scrape the web. If you are looking for something quick and simple, the URL handling module in Python called urllib might do the trick for you. Otherwise, I recommend scrapyd because of the possible customizations and robustness.", "It is important to ensure that the pages you are scraping contain rich text data that is suitable for your use case.", "Once we have obtained our scraped web pages, we begin by extracting the text out of each web page. Websites have lots of tags that don\u2019t contain useful information when it comes to NLP, such as <script> and <button>. Thankfully, there is a Python module called boilerpy3 that makes text extraction easy.", "We use the ArticleExtractor to extract the text. This extractor has been tuned for news articles that works well for most HTMLs. You can try out other extractors listed in the documentation for boilerpy3 and see what works best for your dataset.", "Next, we condense all newline characters (\\n and \\r) into one \\n character. This is done so that when we split the text up into sentences by \\n and periods, we don\u2019t get sentences with no words.", "If the extractors from boilerpy3 are not working for your web pages, you can use beautifulsoup to build your own custom text extractor. Below is an example replacement of the parse_html method.", "Once the text has been extracted, we want to continue with the cleaning process. It is common for web pages to contain repeated information, especially if you scrape multiple articles from the same domain. Elements such as website titles, company slogans, and page footers can be present in your parsed text. To detect and remove these phrases, we analyze our corpus by looking at the frequency of large n-grams.", "N-grams is a concept from NLP where the \u201cgram\u201d is a contiguous sequence of words from a body of text, and \u201cN\u201d is the size of these sequences. This is frequently used to build language models which can assist in tasks ranging from text summarization to word prediction. Below is an example for trigrams (3-grams):", "When we read articles, there are many single words (unigrams) that are repeated, such as \u201cthe\u201d and \u201ca\u201d. However, as we increase our n-gram size, the probability of the n-gram repeating decreases. Trigrams start to become more rare, and it is almost impossible for the articles to contain the same sequence of 20 words. By searching for large n-grams that occur frequently, we are able to detect the repeated elements across websites in our corpus, and manually filter them out.", "We begin this process by breaking up our dataset up into sentences by splitting the text chunks up by the newline characters and periods. Next, we tokenize our sentences (break up the sentence into single word strings). With these tokenized sentences, we are able to generate n-grams of a specific size (we want to start large, around 15). We want to sort the n-grams by frequency using the FreqDist function provided by nltk. Once we have our frequency dictionary, we print the top 10 n-grams. If the frequency is higher than 1 or 2, the sentence might be something you would consider removing from the corpus. To remove the sentence, copy the entire sentence and add it as a single string in the filter_strs array. Copying the entire sentence can be accomplished by increasing the n-gram size until the entire sentence is captured in one n-gram and printed on the console, or simply printing the parsed_texts and searching for the sentence. If there is multiple unwanted sentences with slightly different words, you can copy the common substring into filter_strs, and the regular expression will filter out all sentences containing the substring.", "If you run the code above on your dataset without adding any filters to filter_strs, you might get a graph similar to the one below. In my dataset, you can see that there are several 15-grams that are repeated 6, 3, and 2 times.", "Once we go through the process of populating filter_strs with unwanted sentences, our plot of 15-grams flattens out.", "Keep in mind there is no optimal threshold for n-gram size and frequency that determines whether or not a sentence should be removed, so play around with these two parameters. Sometimes you will need to lower the n-gram size to 3 or 4 to pick up a repeated title, but be careful not to remove valuable data. This block of code is designed to be an iterative process, where you slowly build the filter_strs array after many different experiments.", "After we clean the corpus, the next step is to process the words of our corpus. We want to remove punctuation, lowercase all words, and break each sentence up into arrays of individual words (tokenization). To do this, I like to use the simple_preprocess library method from gensim. This function accomplishes all three of these tasks in one go and has a few parameters that allow some customization. By setting deacc=True, accents will be removed. When punctuation is removed, the punctuation itself is treated as a space, and the two substrings on each side of the punctuation is treated as two separate words. In most cases, words will be split up with one substring having a length of one. For example, \u201cdon\u2019t\u201d will end up as \u201cdon\u201d and \u201ct\u201d. As a result, the default min_len value is 2, so words with 1 letter are not kept. If this is not suitable for your use case, you can also create a text processor from scratch. Python\u2019s string class contains a punctuation attribute that lists all commonly used punctuation. Using this set of punctuation marks, you can use str.maketrans to remove all punctuation from a string, but keeping those words that did have punctuation as one single word (\u201cdon\u2019t\u201d becomes \u201cdont\u201d). Keep in mind this does not capture punctuation as well as gensim\u2019s simple_preprocess. For example, there are three types of dashes (\u2018 \u2014 \u2019 em dash, \u2013\u2019 en dash, \u2018-\u2019 hyphen), and while simple_preprocess removes them all, string.punctuation does not contain the em dash, and therefore does not remove it.", "Once we have our corpus nicely tokenized, we will remove all stop words from the corpus. Stop words are words that don\u2019t provide much additional meaning to a sentence. Words in the English vocabulary include \u201cthe\u201d, \u201ca\u201d, and \u201cin\u201d. nltk contains a list of English stopwords, so we use that to filter our lists of tokens.", "Lemmatization is the process of grouping together different forms of the same word and replacing these instances with the word\u2019s lemma (dictionary form). For example, \u201cfunctions\u201d is reduced to \u201cfunction\u201d. Stemming is the process of reducing a word to its root word (without any suffixes or prefixes). For example, \u201crunning\u201d is reduced to \u201crun\u201d. These two steps decreases the vocabulary size, making it easier for the machine to understand our corpus.", "Now that you know how to extract and preprocess your text data, you can begin the data analysis. Best of luck with your NLP adventures!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MS CSE @UW. Software engineer that loves data."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdda282a7a871&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dda282a7a871--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@brandon.ko?source=post_page-----dda282a7a871--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brandon.ko?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Brandon Ko"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4e77e2090272&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&user=Brandon+Ko&userId=4e77e2090272&source=post_page-4e77e2090272----dda282a7a871---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/95YRwf6CNw8", "anchor_text": "Photo"}, {"url": "https://unsplash.com/@clemhlrdt", "anchor_text": "Cl\u00e9ment H"}, {"url": "https://docs.python.org/3/library/urllib.html", "anchor_text": "urllib"}, {"url": "https://scrapyd.readthedocs.io/en/stable/index.html", "anchor_text": "scrapyd"}, {"url": "https://pypi.org/project/boilerpy3/", "anchor_text": "boilerpy3"}, {"url": "https://beautiful-soup-4.readthedocs.io/en/latest/", "anchor_text": "beautifulsoup"}, {"url": "https://pypi.org/project/gensim/", "anchor_text": "gensim"}, {"url": "https://github.com/brandonko/HTML-Data-Cleaning-Python-NLP", "anchor_text": "here"}, {"url": "https://medium.com/tag/nlp?source=post_page-----dda282a7a871---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/python?source=post_page-----dda282a7a871---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dda282a7a871---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/website?source=post_page-----dda282a7a871---------------website-----------------", "anchor_text": "Website"}, {"url": "https://medium.com/tag/programming?source=post_page-----dda282a7a871---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&user=Brandon+Ko&userId=4e77e2090272&source=-----dda282a7a871---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&user=Brandon+Ko&userId=4e77e2090272&source=-----dda282a7a871---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdda282a7a871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dda282a7a871---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dda282a7a871--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dda282a7a871--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dda282a7a871--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dda282a7a871--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dda282a7a871--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brandon.ko?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@brandon.ko?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Brandon Ko"}, {"url": "https://medium.com/@brandon.ko/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4e77e2090272&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&user=Brandon+Ko&userId=4e77e2090272&source=post_page-4e77e2090272--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4e77e2090272%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwebsite-data-cleaning-in-python-for-nlp-dda282a7a871&user=Brandon+Ko&userId=4e77e2090272&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}