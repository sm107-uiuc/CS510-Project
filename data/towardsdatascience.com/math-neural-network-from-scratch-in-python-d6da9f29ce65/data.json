{"url": "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65", "time": 1682993942.5241878, "path": "towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65/", "webpage": {"metadata": {"title": "Neural Network from scratch in Python | by Omar Aflak | Towards Data Science", "h1": "Neural Network from scratch in Python", "description": "Create your own machine learning library from scratch in Python !"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this post we will go through the mathematics of machine learning and code from scratch, in Python, a small library to build neural networks with a variety of layers (Fully Connected, Convolutional, etc.). Eventually, we will be able to create networks in a modular fashion:", "I\u2019m assuming you already have some knowledge about neural networks. The purpose here is not to explain why we make these models, but to show how to make a proper implementation.", "We need to keep in mind the big picture here :", "The most important step is the 4th. We want to be able to have as many layers as we want, and of any type. But if we modify/add/remove one layer from the network, the output of the network is going to change, which is going to change the error, which is going to change the derivative of the error with respect to the parameters. We need to be able to compute the derivatives regardless of the network architecture, regardless of the activation functions, regardless of the loss we use.", "In order to achieve that, we must implement each layer separately.", "Every layer that we might create (fully connected, convolutional, maxpooling, dropout, etc.) have at least 2 things in common: input and output data.", "We can already emphasize one important point which is: the output of one layer is the input of the next one.", "This is called forward propagation. Essentially, we give the input data to the first layer, then the output of every layer becomes the input of the next layer until we reach the end of the network. By comparing the result of the network (Y) with the desired output (let\u2019s say Y*), we can calculate en error E. The goal is to minimize that error by changing the parameters in the network. That is backward propagation (backpropagation).", "This is a quick reminder, if you need to learn more about gradient descent there are tons of resources on the internet.", "Basically, we want to change some parameter in the network (call it w) so that the total error E decreases. There is a clever way to do it (not randomly) which is the following :", "Where \u03b1 is a parameter in the range [0,1] that we set and that is called the learning rate. Anyway, the important thing here is \u2202E/\u2202w (the derivative of E with respect to w). We need to be able to find the value of that expression for any parameter of the network regardless of its architecture.", "Suppose that we give a layer the derivative of the error with respect to its output (\u2202E/\u2202Y), then it must be able to provide the derivative of the error with respect to its input (\u2202E/\u2202X).", "Remember that E is a scalar (a number) and X and Y are matrices.", "Let\u2019s forget about \u2202E/\u2202X for now. The trick here, is that if we have access to \u2202E/\u2202Y we can very easily calculate \u2202E/\u2202W (if the layer has any trainable parameters) without knowing anything about the network architecture ! We simply use the chain rule :", "The unknown is \u2202y_j/\u2202w which totally depends on how the layer is computing its output. So if every layer have access to \u2202E/\u2202Y, where Y is its own output, then we can update our parameters !", "Don\u2019t forget, the output of one layer is the input of the next layer. Which means \u2202E/\u2202X for one layer is \u2202E/\u2202Y for the previous layer ! That\u2019s it ! It\u2019s just a clever way to propagate the error ! Again, we can use the chain rule :", "This is very important, it\u2019s the key to understand backpropagation ! After that, we\u2019ll be able to code a Deep Convolutional Neural Network from scratch in no time !", "This is what I described earlier. Layer 3 is going to update its parameters using \u2202E/\u2202Y, and is then going to pass \u2202E/\u2202H2 to the previous layer, which is its own \u201c\u2202E/\u2202Y\u201d. Layer 2 is then going to do the same, and so on and so forth.", "This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. Speaking of abstract, now is a good time to write our first python class.", "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods.", "As you can see there is an extra parameter in backward_propagation that I didn\u2019t mention, it is the learning_rate. This parameter should be something like an update policy, or an optimizer as they call it in Keras, but for the sake of simplicity we\u2019re simply going to pass a learning rate and update our parameters using gradient descent.", "Now let's define and implement the first type of layer: fully connected layer or FC layer. FC layers are the most basic layers as every input neurons are connected to every output neurons.", "The value of each output neuron can be calculated as the following :", "With matrices, we can compute this formula for every output neuron in one shot using a dot product :", "We\u2019re done with the forward pass. Now let\u2019s do the backward pass of the FC layer.", "Note that I\u2019m not using any activation function yet, that\u2019s because we will implement it in a separate layer!", "As we said, suppose we have a matrix containing the derivative of the error with respect to that layer\u2019s output (\u2202E/\u2202Y). We need :", "Let's calculate \u2202E/\u2202W. This matrix should be the same size as W itself : ixj where i is the number of input neurons and j the number of output neurons. We need one gradient for every weight :", "Using the chain rule stated earlier, we can write :", "That\u2019s it we have the first formula to update the weights! Now let's calculate \u2202E/\u2202B.", "Again \u2202E/\u2202B needs to be of the same size as B itself, one gradient per bias. We can use the chain rule again :", "Now that we have \u2202E/\u2202W and \u2202E/\u2202B, we are left with \u2202E/\u2202X which is very important as it will \u201cact\u201d as \u2202E/\u2202Y for the layer before that one.", "Finally, we can write the whole matrix :", "That\u2019s it! We have the three formulas we needed for the FC layer!", "We can now write some python code to bring this math to life!", "All the calculation we did until now were completely linear. It's hopeless to learn anything with that kind of model. We need to add non-linearity to the model by applying non-linear functions to the output of some layers.", "Now we need to redo the whole process for this new type of layer!", "No worries, it\u2019s going to be way faster as there are no learnable parameters. We just need to calculate \u2202E/\u2202X.", "We will call f and f' the activation function and its derivative respectively.", "As you will see, it is quite straightforward. For a given input X , the output is simply the activation function applied to every element of X . Which means input and output have the same dimensions.", "Be careful, here we are using an element-wise multiplication between the two matrices (whereas in the formulas above, it was a dot product).", "The code for the activation layer is as straightforward.", "You can also write some activation functions and their derivatives in a separate file. These will be used later to create an ActivationLayer.", "Until now, for a given layer, we supposed that \u2202E/\u2202Y was given (by the next layer). But what happens to the last layer? How does it get \u2202E/\u2202Y? We simply give it manually, and it depends on how we define the error.", "The error of the network, which measures how good or bad the network did for a given input data, is defined by you. There are many ways to define the error, and one of the most known is called MSE \u2014 Mean Squared Error.", "Where y* and y denotes desired output and actual output respectively. You can think of the loss as a last layer which takes all the output neurons and squashes them into one single neuron. What we need now, as for every other layer, is to define \u2202E/\u2202Y. Except now, we finally reached E !", "These are simply two python functions that you can put in a separate file. They will be used when creating the network.", "Almost done ! We are going to make a Network class to create neural networks very easily akin the first picture !", "I commented almost every part of the code, it shouldn\u2019t be too complicated to understand if you grasped the previous steps. Nevertheless, leave a comment if you have any question, I will gladly answer !", "Finally ! We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple XOR and a MNIST solver.", "Starting with XOR is always important as it\u2019s a simple way to tell if the network is learning anything at all.", "I don\u2019t think I need to emphasize many things. Just be careful with the training data, you should always have the sample dimension first. For example here, the input shape is (4,1,2).", "Clearly this is working, great ! We can now solve something more interesting, let\u2019s solve MNIST !", "We didn\u2019t implemented the Convolutional Layer but this is not a problem. All we need to do is to reshape our data so that it can fit into a Fully Connected Layer.", "MNIST Dataset consists of images of digits from 0 to 9, of shape 28x28x1. The goal is to predict what digit is drawn on a picture.", "This is working perfectly ! Amazing :)", "You can find the whole working code used for this post in the following GitHub repository, and Google Colab file. It also contains the code for other layers like Convolutional or Flatten.", "I\u2019ve recently put the content of that article into a beautifully animated video. You can check it out on YouTube.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd6da9f29ce65&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://omaraflak.medium.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Omar Aflak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc215fdc67eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&user=Omar+Aflak&userId=c215fdc67eb&source=post_page-c215fdc67eb----d6da9f29ce65---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@cadop?utm_source=medium&utm_medium=referral", "anchor_text": "Mathew Schwartz"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://gph.is/21pKLjE", "anchor_text": "https://gph.is/21pKLjE"}, {"url": "https://gph.is/2jzemp3", "anchor_text": "https://gph.is/2jzemp3"}, {"url": "https://github.com/OmarAflak/Medium-Python-Neural-Network", "anchor_text": "OmarAflak/Medium-Python-Neural-NetworkContribute to OmarAflak/Medium-Python-Neural-Network development by creating an account on GitHub.github.com"}, {"url": "https://colab.research.google.com/drive/10y6glU28-sa-OtkeL8BtAtRlOITGMnMw", "anchor_text": "Neural Networks from Scratch in PythonFeel free to contact mecolab.research.google.com"}, {"url": "https://medium.com/tag/python?source=post_page-----d6da9f29ce65---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d6da9f29ce65---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----d6da9f29ce65---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d6da9f29ce65---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d6da9f29ce65---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&user=Omar+Aflak&userId=c215fdc67eb&source=-----d6da9f29ce65---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&user=Omar+Aflak&userId=c215fdc67eb&source=-----d6da9f29ce65---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd6da9f29ce65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d6da9f29ce65---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d6da9f29ce65--------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Omar Aflak"}, {"url": "https://omaraflak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "497 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc215fdc67eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&user=Omar+Aflak&userId=c215fdc67eb&source=post_page-c215fdc67eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb98d8ad5cb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmath-neural-network-from-scratch-in-python-d6da9f29ce65&newsletterV3=c215fdc67eb&newsletterV3Id=b98d8ad5cb06&user=Omar+Aflak&userId=c215fdc67eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}