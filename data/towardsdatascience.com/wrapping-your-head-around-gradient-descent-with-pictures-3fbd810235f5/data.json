{"url": "https://towardsdatascience.com/wrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5", "time": 1683003321.768003, "path": "towardsdatascience.com/wrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5/", "webpage": {"metadata": {"title": "Wrapping Your Head Around Gradient Descent (with comics!) | by Johnny Burns | Towards Data Science", "h1": "Wrapping Your Head Around Gradient Descent (with comics!)", "description": "The tl;dr \u2014 There is no tl;dr. We\u2019re predicting the future here. Describing the details without making you rage quit takes a few words. The only prerequisite to understanding this post is to know\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-the-ols-method-for-simple-linear-regression-e0a4e8f692cc", "anchor_text": "ordinary least squares", "paragraph_index": 14}], "all_paragraphs": ["Because math is hard, and some of us need a slower and more visual approach.", "The tl;dr \u2014 There is no tl;dr. We\u2019re predicting the future here. Describing the details without making you rage quit takes a few words.", "The only prerequisite to understanding this post is to know what a linear function is. We see linear functions stated in many different ways, but today I\u2019m going to stick with \u201c y = mx + b \u201d since it\u2019s probably the most widely used.", "If I asked \u201cWhat is the variable of the function y = mx + b?\u201d, you\u2019d probably say \u201cx\u201d.", "Normally, you\u2019d be right, but first \u201cgotcha\u201d about gradient descent is that it\u2019s backwards.", "Instead of \u201cgiven this linear function, find these data points\u201d, we\u2019re saying \u201cgiven these data points, find the linear function\u201d", "Let that sink in, because it\u2019s super weird.", "Think of x and y more like a bunch of constants. We need to find values for \u201cm\u201d and \u201cb\u201d that fit those points. Then, we can use the line to predict new points.", "If palm readers have taught us anything, predicting the future is not an exact science. Typically, no line fits perfectly through the points, so we want to figure out the line which comes closest to hitting the points.", "This might seem like a stupid question. It seems like you should be able to measure how \u201cfar off the line\u201d each point is, and divide by the number of points to figure out (on average) how close the line is to each point.", "However, consider the following two lines, and ask yourself which one better represents the two data points:", "The first line skews toward the bottom. The second line splits directly between the data points. The second line feels like a more accurate representation of the data, but the average error is the same:", "For this reason and others, we usually measure the \u201ccloseness\u201d of a datapoint by taking the distance from the line squared. This also ensures that there is only one \u201coptimal\u201d placement.", "By using the average squared error, we can see that putting the line in the middle is better than putting the line at the top.", "There are several methods for this. The simplest method is called ordinary least squares, but it doesn\u2019t perform well on complex problems. To solve complex problems, we often use a method called \u201cgradient descent\u201d. Much of machine learning builds on this concept.", "To start, let\u2019s assume there is no \u201cmx\u201d in the equation, and just try to find good value for \u201cb\u201d. It\u2019s easier because you only have one variable to solve for.", "This might sound absurd. This means \u201cm\u201d locked at zero (a horizontal line). No matter what input you get, you always have to guess the same output.", "Bear with me a second. It\u2019s gonna get crazy real fast.", "For simplicity, let\u2019s say we have just two data points. These data points represent recent home sales. We know the square footage of each house, and how much it sold for.", "I\u2019m going to call these data points \u201cMr. East\u2019s House\u201d and \u201cMr. West\u2019s House\u201d (because one closer to the east side of the graph, and the other is closer to the West).", "We\u2019re trying to find a line y = b, that minimizes the average squared error of Mr. East\u2019s House and Mr. West\u2019s House.", "I know what you\u2019re thinking. \u201cOh\u2026 just take the average sales price\u201d.", "You\u2019re right. That method works, and it minimizes the squared error. However, this is not gradient descent. This cute little \u201caverage\u201d trick isn\u2019t going to hold up to more complex problems, so lets instead use gradient descent to find the value.", "In gradient descent, we start off by placing the line in a random spot. I\u2019m going to place it here:", "We want the line to be as close as possible to all houses. So we take a survey of the neighborhood. We knock on each of the doors and ask \u201cwhich direction should we move the line?\u201d. I\u2019m going to call this the Move Survey:", "After the move survey, the votes are unanimous, so we move the line up. Let\u2019s say we move it to 7.1", "Now we take another movement survey:", "Now it\u2019s getting interesting. Mr. West wants to move the line DOWN, and Mr. East wants to move the line UP. Imagine these neighbors in a tug-of-war (each wants the line closer to their property).", "So how do we decide where to move the line?", "Put yourself in the shoes of a homeowner and think about this:", "As shown here, each move away from your house more expensive than the last.", "Errors are very cheap at close distance, and they get very expensive far away.", "When you think about the tradeoff, It\u2019s worth adding some cheap errors to Mr. West, so we can subtract expensive errors from Mr. East.", "Mr. East is very sensitive to changes. By that, I mean that moving the line away from Mr. East costs us a lot, and moving the line closer to Mr. East saves us a lot.", "In contrast, Mr. West is not as sensitive to changes. We don\u2019t get as much benefit by moving the line toward Mr. West. It also costs less to move the line away from Mr West.", "Our survey did not take \u201csensitivity\u201d into account. We should modify the survey. We need to ask not only \u201cwhat direction should we move the line?\u201d But \u201cHow sensitive are you to line movement?\u201d", "We write the results down on our Move Survey:", "Once we\u2019ve surveyed all residents, we divide the total sensitivity by the number of residents to figure out an \u201caverage sensitivity\u201d:", "The positive average sensitivity here means that if we move the line UP, the benefits outweigh the drawbacks. The average sensitivity (0.4) tells us how strong the UP vs DOWN tradeoff is.", "We take a small percentage of that number (say, 25%). We call this percentage the \u201clearning rate\u201d.", "Then we move the line by that much.", "Going back to the tug-of-war analogy, you can imagine that Mr. East and Mr. West are pulling in opposite directions, but Mr. East is far stronger than Mr. West (because he\u2019s more sensitive).", "With the line now at 7.2, we do another survey, again asking:", "We can see that Mr. West is more sensitive than before, and Mr. East is less sensitive than before.", "Mr. East\u2019s sensitivity still outweighs Mr. West, but not by as much as before.", "Let\u2019s take 25% of this number and update the line:", "The line is now at 7.275. The move was even smaller than before.", "If we continue to do more rounds of the survey, you will see that Mr. West gets more and more sensitive, while Mr. East gets less and less sensitive. The moves also get smaller and smaller.", "As more rounds come, the tug-of-war becomes more evenly matched. Mr. East wants the line UP almost as much as Mr. West wants the line DOWN. At this point, the \u201caverage sensitivity\u201d is very small (the two sensitivities are effectively cancelling each other out).", "As the average sensitivity approaches zero (equilibrium), the line stops moving, since the line update is based on the average sensitivity.", "I\u2019m going to show you 3 different animations that all display the gradient descent as the line moves from our first survey (7.1) to equilibrium (7.5). These 3 views should help you to wrap your mind around the process.", "The following animation shows line move after each survey. You can see the moves get smaller and smaller as the line approaches the 7.5 (the minimum error point).", "This animation shows the errors of Mr. East and Mr. West. You can see here that we continue to trade expensive errors for cheaper errors until neither side is cheaper.", "When you think about it, this makes sense. The only time you want to move the line is if you gain more than you give up. When the average sensitivity is zero, you get no benefit from moving the line in either direction (since both sides are equally sensitive).", "You\u2019ll see this last type of graph see referenced a lot when talking about gradient descent. This one can be difficult to wrap your head around.", "The graph has the same shape as the previous graph, but don\u2019t be deceived, it is very different. In the previous graph, the X axis was the error (each error was a separate point), and the Y axis was squared error.", "We see here that the ideal \u2018b\u2019 value is 7.5, which produces a 0.25 average squared error.", "Our task in gradient descent is finding that point at the bottom of this curve (minimum squared error).", "When we started (at 7.1), and continued to do move surveys, we approached the bottom after several rounds. Watch the animation here:", "Before we began our gradient descent, we knew the curve would be shaped this way (it always is), but we didn\u2019t know where the minimum value was.", "After the first Move Survey, we discovered that a guess of 7.1 results in an average squared error of 0.41. At first, it might seem like this is all we know:", "However, we actually know more. We know the average sensitivity is 0.4, which tells us two things:", "I\u2019m going to mark a blue line showing what we know about where we are.", "Then, we do a second move survey, and we discover that a guess of 7.2 results in an average squared error of 0.34.", "Because of the average sensitivity (.3), we know that we\u2019re still on the LEFT side of the graph, but we\u2019re getting closer to the center. I\u2019m going to draw another line to represent this, but the line will not be as steep because we\u2019re closer to the bottom.", "As we continue to do Move Surveys, we make smaller steps, getting closer and closer to the bottom of the curve.", "The \u201csteepness\u201d of blue line represents the average sensitivity. When the \u2018b\u2019 value far away from the minimum, the slope is very steep.", "As the \u2018b\u2019 value approaches the minimum, the average sensitivity (aka, the slope), approaches zero.", "\u201cGradient\u201d is just another term for \u201cslope\u201d. As we move toward the bottom of the graph, the gradient gets smaller and smaller. This is why we call this method \u201cgradient descent\u201d.", "Whether you have two data points (as shown here), or 1000 data points. The logic for gradient decent is the same:", "In actuality, you\u2019ll never quite hit equilibrium, since the update gets smaller and smaller as it approaches the bottom, but after enough rounds, the difference will be insignificantly small.", "Now that we\u2019ve discovered how to find the minimum \u201cb\u201d value, let\u2019s ignore the \u201cb\u201d for a minute and think about how to find the minimum \u201cm\u201d value.", "This means that \u201cb\u201d is effectively stuck at a constant of zero. We can change the slope of the function using the \u201cm\u201d value, but the line will always pass through (0,0)", "Let\u2019s take another look at Mr. East and Mr. West, and take a guess at where the line might be:", "At m=1, we see that Mr. West is 2 units above the line, and Mr. East is 2 units below the line. Average squared error is 4.", "Intuitively, it seems like we\u2019re in equilibrium and cannot improve on this. However, that is not the case.", "The following animation shows what happens if we change the \u201cm\u201d value from 1 to 0.9", "Notice how we got 1 full space closer to Mr. East, but we only got 1/2 space further from Mr. West. The total squared error is reduced to 3.625", "This makes sense when you think the equation y = mx", "The \u201cm\u201d value is multiplied by \u201cx\u201d. Because Mr. East (x=10) is twice as far as Mr. West (x=5), Mr. East is twice as affected by changes in \u201cm\u201d.", "In general, the further East you are, the more sensitive you are to slope changes.", "Since we get twice the benefit moving toward Mr. East, why not just move the line all the way to Mr. East? We can move a full 2 units closer to Mr. East, and only Move 1 unit further from Mr. West.", "Since we\u2019re 3 spaces away from Mr. West, the new average squared error is (4.5). This is worse than when we started\u2026 what gives?", "Remember: As we move the line away from Mr. West, each error is more expensive than the last (due to squared error). At some point, Mr. West is so sensitive to movement that it\u2019s not worth making the tradeoff, even though you can get twice as close to Mr. East by moving the line.", "While the line above had the smallest RAW error, it does NOT have the smallest SQUARED error.", "So Mr. East and Mr. West are both sensitive, but for different reasons. It turns out that a point\u2019s sensitivity to slope changes is determined by two factors:", "We need to take both of these into account when we calculate the sensitivity.", "sensitivity = {distance from line} * {x value}", "For the first time in this post, you probably can\u2019t tell which direction to move the line by simply looking at it.", "Let\u2019s take a move survey, calculate the sensitivity, and find out.", "We can see here that even though Mr. West has half the \u201cx\u201d value, he is more sensitive due to his distance from the line.", "We know the slope needs to move UP, but adding 1.25 is much too far and would place the line above both points. That\u2019s why we only move it by a percentage of that amount (the \u201clearning rate\u201d I described previously). I\u2019ll use a learning rate of 1% here.", "The new line is at .9125. After another move survey, we can see that the squared error has decreased.", "Multiply by the learning rate to get the update value:", "After the update, the line is at .917. If we continue rounds of the move survey, the line will get closer and closer to its equilibrium (.92).", "Here is what the move survey looks like at .92", "The gradient decent process looks very similar to what we saw with the gradient descent for \u201cm\u201d.", "Let\u2019s look at the gradient decent view of slope changes:", "We see here that the squared error is minimized with an \u201cm\u201d value of 0.9.", "One thing to note about the \u201cm\u201d curve is that it\u2019s much steeper than the curve we saw for \u201cb\u201d. This is because the \u201cm\u201d value is multiplied by each of our \u201cx\u201d values, so even a small update to \u201cm\u201d can result in a large change (for better or worse). This is why it\u2019s important to keep the learning rate small.", "First, we figured out how to solve for \u201cb\u201d when \u201cm\u201d is held constant. Then, we figured out how to solve for \u201cm\u201d when \u201cb\u201d is held constant. How can we move both \u201cm\u201d and \u201cb\u201d to find the best overall fit?", "When both \u201cm\u201d and \u201cb\u201d are moveable:", "Let\u2019s run through what happens when we start with a horizontal line sitting at (y = 0x + 7.5\u201d), and do full gradient descent.", "As we continue move \u201cm\u201d and \u201cb\u201d toward equilibrium, we approach the only place where both \u201cm\u201d and \u201cb\u201d are zero (the minimum).", "Let\u2019s look at two views of this to wrap things up.", "The following animation shows repeated rounds of gradient descent, updating both \u201cb\u201d and \u201cm\u201d. Watch how the \u201cb\u201d value drops and the \u201cm\u201d value increases.", "Let\u2019s look at the \u201cgradient descent\u201d view of what\u2019s happening. Until now, we\u2019ve shown two \u201cgradient descent\u201d graphs.", "To show both \u201cm\u201d AND \u201cb\u201d vs. average squared error, we need a third axis on the graph.", "The two horizontal axes will represent values for \u201cm\u201d and \u201cb\u201d. The vertical axis represents the squared error at those points.", "3-axis gradient descent graphs typically look something like this. This graph is not based on our data, but helps you wrap your head around the concept.", "This graph helps show that there is only one place where both \u201cm\u201d and \u201cb\u201d are in equilibrium (the bottom).", "Here is the gradient descent graph based on our actual data. It is a little bit harder to look at, but it has the same \u201cdownhill-to-minimum\u201d property:", "If you inspect this graph closely, you can see that the minimum point has an \u201cm\u201d value of 6, and a \u201cb\u201d value of 0.2.", "The line \u201c y = 0.2x + 6 \u201c is in fact the ideal function and results in zero error (it crosses through both points).", "This graph looks different than the previous graph for a couple reasons:", "I\u2019m Johnny Burns, founder of FlyteHub.org, a repository of free open-source workflows to perform Machine Learning with no coding. I believe that collaborating on AI will lead to better products.", "If you\u2019re interested in seeing how the math backs up our \u201csensitivity\u201d formulas, I will do a follow up post to explain.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3fbd810235f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@johnnythehutt?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johnnythehutt?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Johnny Burns"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c4790ea801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&user=Johnny+Burns&userId=4c4790ea801&source=post_page-4c4790ea801----3fbd810235f5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/understanding-the-ols-method-for-simple-linear-regression-e0a4e8f692cc", "anchor_text": "ordinary least squares"}, {"url": "https://pxhere.com/en/photographer/616233", "anchor_text": "Oleg Magni"}, {"url": "https://pxhere.com/en/photo/1554717", "anchor_text": "pxhere"}, {"url": "https://www.pexels.com/@brett-sayles", "anchor_text": "Brett Sayles"}, {"url": "https://www.pexels.com/photo/man-wearing-cowboy-hat-2250519/", "anchor_text": "pexels"}, {"url": "https://pixnio.com/people/male-men/portrait-photo-model-fashion-sunglasses-person-man-handsome", "anchor_text": "pixnio"}, {"url": "https://pxhere.com/en/photo/498377", "anchor_text": "pxhere"}, {"url": "https://pixabay.com/users/falco-81448/", "anchor_text": "falco"}, {"url": "https://pixabay.com/photos/tug-of-war-monument-wismar-515183/", "anchor_text": "pixabay"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3fbd810235f5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----3fbd810235f5---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3fbd810235f5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3fbd810235f5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----3fbd810235f5---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&user=Johnny+Burns&userId=4c4790ea801&source=-----3fbd810235f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&user=Johnny+Burns&userId=4c4790ea801&source=-----3fbd810235f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3fbd810235f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3fbd810235f5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3fbd810235f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3fbd810235f5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3fbd810235f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johnnythehutt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johnnythehutt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Johnny Burns"}, {"url": "https://medium.com/@johnnythehutt/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "54 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c4790ea801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&user=Johnny+Burns&userId=4c4790ea801&source=post_page-4c4790ea801--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4c4790ea801%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5&user=Johnny+Burns&userId=4c4790ea801&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}