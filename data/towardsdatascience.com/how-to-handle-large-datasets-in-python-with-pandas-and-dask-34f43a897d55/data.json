{"url": "https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55", "time": 1682996257.445699, "path": "towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55/", "webpage": {"metadata": {"title": "How to handle large datasets in Python with Pandas and Dask | by Filip Ciesielski | Towards Data Science", "h1": "How to handle large datasets in Python with Pandas and Dask", "description": "Python data scientists often use Pandas for working with tables. While Pandas is perfect for small to medium-sized datasets, larger ones are problematic. In this article, I show how to deal with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://sunscrapers.com/blog/python-best-practices-how-to-contribute-to-python-open-source-projects/?utm_source=medium&utm_medium=article", "anchor_text": "open-source Python library", "paragraph_index": 9}, {"url": "http://docs.dask.org/en/latest/", "anchor_text": "Dask", "paragraph_index": 10}, {"url": "https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream", "anchor_text": "Wikipedia", "paragraph_index": 12}, {"url": "https://dumps.wikimedia.org/other/clickstream/", "anchor_text": "here", "paragraph_index": 12}], "all_paragraphs": ["Python data scientists often use Pandas for working with tables. While Pandas is perfect for small to medium-sized datasets, larger ones are problematic. In this article, I show how to deal with large datasets using Pandas together with Dask for parallel computing \u2014 and when to offset even larger problems to SQL if all else fails.", "Pandas is a wonderful library for working with data tables. Its dataframe construct provides a very powerful workflow for data analysis similar to the R ecosystem. It\u2019s fairly quick, rich in features and well-documented. In fact, it has earned its place as a fundamental tool used by data scientists who follow the Python way.", "However, in the life of a data-scientist-who-uses-Python-instead-of-R there always comes a time where the laptop throws a tantrum, refuses to do any more work, and freezes spectacularly.", "As great as it is, Pandas achieves its speed by holding the dataset in RAM when performing calculations. That\u2019s why it comes with a certain limitation (which also depends on the machine specs, of course).", "The issue often originates in an unforeseen expansion of a dataframe during an overly-complex transformation or a blind import of a table from a file. That can be very frustrating.", "One solution would be to limit the data to a smaller subset \u2014 for example, by probing every-nth value in a source. But often that\u2019s not enough.", "But what if limiting data isn\u2019t an option?", "Luckily, there\u2019s a way to address this issue.", "The most common fix is using Pandas alongside another solution \u2014 like a relational SQL database, MongoDB, ElasticSearch, or something similar. At Sunscrapers, we definitely agree with that approach.", "But you can sometimes deal with larger-than-memory datasets in Python using Pandas and another handy open-source Python library, Dask.", "Dask is a robust Python library for performing distributed and parallel computations. It also provides tooling for dynamic scheduling of Python-defined tasks (something like Apache Airflow). It\u2019s tightly integrated with NumPy and provides Pandas with dataframe-equivalent structures \u2014 the dask.dataframes \u2014 that are based on lazy loading and can be used to perform dataframe operations in chunks and in parallel. It also automatically supports grouping by performing data shuffling under the hood.", "This article outlines a few handy tips and tricks to help developers mitigate some of the showstoppers when working with large datasets in Python.", "To demonstrate the power of Pandas/Dask, I chose chose an open-source dataset from Wikipedia about the source of the site\u2019s visitors. You can get the \u2018clickstream\u2019 tables (in .tsv) here.", "The clickstream data contains 4 main columns:", "Next, I came up with a few questions to play around with my dataset and check whether the combination of Pandas and Dask does its job:", "The dataset size is 1.4 Gb, so it carries significant risk of memory overload. That\u2019s why I split the study into two parts.", "First, I implemented the analysis on a limited data subset using just the Pandas library. Then I attempted to do exactly the same on the full set using Dask.", "Ok, let\u2019s move on to the analysis.", "Let\u2019s grab our data for the analysis:", "Now let\u2019s have a look at what kind of data we have here and import it into the dataframe.", "The very first memory optimization step we can perform already at this point (assuming we know our table structure by now) is specifying the columns data types during the import (via the dtype= input parameter).", "That way, we can force Pandas to convert some values into types with a significantly lower memory footprint.", "That may not make much sense if you\u2019re dealing with a few thousand rows, but will make a noticeable difference in a few millions!", "For example, if you know that a column should only have positive integers, use unsigned integer type (uint32) instead of the regular int type (or worse \u2014 float, which may sometimes happen automatically).", "Finally, let\u2019s limit the data frame size to the first 100k rows for the sake of speed.", "Note that this is usually a bad idea; when sampling a subset, it\u2019s far more appropriate to sample every nth row to get as much uniform sampling as possible. But since we\u2019re only using it to demonstrate the analysis process, we\u2019re not going to bother:", "To answer this question, we need to create a table where we can see the aggregated sum of visitors per article and per source of origin (coming_from column).", "So, let\u2019s aggregate the table over the article, the coming_from columns, sum up the \u2019n\u2019 values, and then order the rows according to the \u2019n\u2019 sums. Here\u2019s how we approach it in Pandas:", "Now let\u2019s recreate this data using the Dask library.", "Note that the read_csv function is pretty similar to the Pandas one, except here we specify the byte-size per chunks. We perform the aggregation logic which is also almost identical to Pandas:", "That won\u2019t do any calculations yet, the top_links_grouped_dask will be a Dask delayed dataframe object. We can then launch it to be computed via the .compute() method.", "But we don\u2019t want to clog our memory, so let\u2019s save it directly to hard drive. We will use the hdf5 file format to do that. Let\u2019s declare the hdf5 store then:", "And compute the data frame into it.", "Note that ordering column values with Dask isn\u2019t that easy (after all, the data is read one chunk at a time), so we cannot use the sort_values() method like we did in the Pandas example.", "Instead, we need to use the nlargest() Dask method and specify the number of top values we\u2019d like to determine:", "It too returns a delayed Dask object, so to finally compute it (and save it to the store) we run the following:", "In this case, the result is different from the values in the Pandas example since here we work on the entire dataset, not just the first 100k rows:", "That one is easy. All we need to do is to filter out the rows that contain the \u2018external\u2019 referrer_type and \u2018other-search\u2019 coming_from values:", "We then only have to sort the values according to the number of visitors:", "How about doing the same, but on the full dataset with Dask this time?", "Since we only need to store the top 40 results, we can simply store them directly in Pandas dataframe:", "which returns this (showing only the first 5 rows here):", "This is a great question to be answered graphically, so lets plot the first 40 top values:", "The framing of this question suggests that we need to be able to calculate the fraction for a specific article title. So let\u2019s create a function that will take a dataframe and the desired article title, and then return the percentage value.", "The function will have to filter the rows for a given article, sum up all the visitors count, and then find a cumulative sum of n for a subset of visits with the \u2018link\u2019 value in the the referrer_type column:", "And let\u2019s test in on one of the articles, say one with the title \u201dJehangir_Wadia\u201d:", "Which suggests that ~81% of the \u201dJehangir_Wadia\u201d article visitors arrive there by clicking on an external link.", "How can we extend that to the entire dataset using Dask? Quite easily. All we have to do is using the dask-dataframe instead of the Pandas ones and adding the .compute() methods to two of the inner statements in the function, like that:", "Running the function will return the same result:", "To answer this question, we require two columns: one for the destination article and the origin title, as well as the sum of the number of visits. Furthermore, we have to filter out the rows with the highest number of visitors per article.", "First, let\u2019s get rid of all the unnecessary extra columns by aggregating and summing up all the \u2019n\u2019 counts over the referrer_type for every coming_from/article combination:", "Next, let\u2019s find the referrers (coming_from) that generated the highest number of visitors for each article page.", "One way to do that is using a filter table with the desired rows indices via the df.iloc[] method. So let\u2019s find those for the summed_articles table which correspond to the highest \u2019n\u2019 per article.", "We\u2019ll use a nifty Pandas method called idxmax which returns the indices of the grouped column with max values. Aggregating the summed_articles again, this time over the coming_from column, we can run it like this:", "Now we can filter out the summed_articles rows with this table:", "Finally, we need to sort the values by the highest number of visitors:", "Now, let\u2019s try recreating this moderately-complex task in Dask on the full data set. The first step is easy. We can create a table with summed_articles like this without any issues:", "But it\u2019s best not to store it in memory \u2014 we\u2019ll have to perform the aggregation later on and that will be memory-demanding. So let\u2019s write it down (as its being calculated) directly to hard drive instead, for example hdf5 or a parquet file:", "Step two is creating the filter table. That\u2019s where the problems begin: at the time of writing this article, Dask dataframes have no idxmax() implementation available. We\u2019d have to improvise somehow.", "For example, we could copy the summed_articles index into a new column and output it via a custom apply function. However, there\u2019s another problem \u2014 Dask partitioning of the data means that we can\u2019t use iloc to filter specific rows (it requires the \u201c:\u201d value for all rows).", "We could try using a loc method and select rows by checking if their indices are present in the list of previously-determined filter table, but that would be a huge computational overhead. What a bummer.", "Here\u2019s another approach: we could write a custom function for processing aggregated data and use it with the groupby-apply combination. That way, we can overcome all the above issues quite easily.", "But then\u2026 the apply method works by concatenating all of the data output from the individually processed subsets of rows into one final table, which means it will have to be transiently stored in one piece in memory, unfortunately\u2026", "Depending on our luck with the data, it can be small enough or not. I tried that a couple of times and found it clogs my (16BG RAM laptop) memory, forcing the notebook kernel to restart eventually.", "Not giving up, I resorted to the dark side of solutions by attempting to iterate over individual groups, find the right row, and append it to an hdf5/parquet storage on disk.", "First problem: DaskGroupBy object has no implementation of iteritem method (at the time of writing), so we can\u2019t use the for-in logic.", "Finally, we can find all the article/coming_from unique combinations and iterate over these values to group the summed_articles rows ourselves with the get_group() method:", "That should work, but the process would be incredibly slow. That\u2019s why I gave up on using Dask for this problem.", "The point I\u2019m trying to make here is that not all data-oriented problems can be solved (easily) with Pandas. Sure, one can invest in massive amounts of RAM, but most of the time, that\u2019s just not the way to go \u2014 certainly not for a regular data-guy with a laptop.", "That type of problems are still best tackled with the good old SQL and a relational database where even a simple SQLite could perform better and in a very reasonable time.", "We can solve this problem in several ways. Here\u2019s one of them.", "My solution is based on storing data in a PostgreSQL database and performing a composite query with the help of PARTITION BY and ROW_NUMBER functions. I use the PostgreSQL database here, but it could just as well be the latest SQLite3 too (version 3.25 or later) as it now supports the partition by functionality \u2014 and that\u2019s what we need for our solution.", "To enable saving results I created a new PostgreSQL database \u2018clickstream\u2019 running locally in a Docker container and connected to it from the Jupyter Notebook via an SQLAlchemy interface engine:", "We then perform the summation of the Dask dataframe on the group by article and coming_from columns, and clean up the string data from tabs and return characters, that would interfere with the PostgreSQL upload:", "Again, at this point we still haven\u2019t performed any editing and summed_articles is still a delayed Dask object.", "One last thing to do before uploading the dataframe to the database is creating an empty table in an existing database, so sending an empty table with the right column names will do the trick quite well:", "And finally, let\u2019s upload data into it. Note, that at the time of writing the Dask dataframe offers no to_sql method, so we can use another trick to do it quickly chunk by chunk:", "Next, we create a SELECT statement that partitions the rows by article, orders them locally by the number of visits column \u2019n\u2019 and indexes the ordered groups incrementally with integers (starting with 1 for every partitioned subset):", "Then we aggregate the rows again by the article column and return only those with the index equal to 1, essentially filtering out the rows with the maximum \u2019n\u2019 values for a given article. Here is the full SQL query:", "The above SQL query was then executed against the database via the:", "And voila, our table is ready.", "Also, apparently the difference between a hyphen and minus kept a lot of people awake at night in 2018:", "I hope this guide helps you deal with larger datasets in Python using the Pandas + Dask combo. It\u2019s clear that some complex analytical tasks are still best handled with other technologies like the good old relational database and SQL.", "Note 1: While using Dask, every dask-dataframe chunk, as well as the final output (converted into a Pandas dataframe), MUST be small enough to fit into the memory.", "Note 2: Here are some useful tools that help to keep an eye on data-size related issues:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Biophysicist turned software engineer @ Sunscrapers. Solving Python & data science problems."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F34f43a897d55&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----34f43a897d55--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@phil.thyharry?source=post_page-----34f43a897d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phil.thyharry?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Filip Ciesielski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99269edb373a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&user=Filip+Ciesielski&userId=99269edb373a&source=post_page-99269edb373a----34f43a897d55---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@djmle29n?utm_source=medium&utm_medium=referral", "anchor_text": "Debbie Molle"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://sunscrapers.com/blog/python-best-practices-how-to-contribute-to-python-open-source-projects/?utm_source=medium&utm_medium=article", "anchor_text": "open-source Python library"}, {"url": "http://docs.dask.org/en/latest/", "anchor_text": "Dask"}, {"url": "https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream", "anchor_text": "Wikipedia"}, {"url": "https://dumps.wikimedia.org/other/clickstream/", "anchor_text": "here"}, {"url": "https://dumps.wikimedia.org/other/clickstream/2018-12/clickstream-enwiki-2018-12.tsv.gz", "anchor_text": "https://dumps.wikimedia.org/other/clickstream/2018-12/clickstream-enwiki-2018-12.tsv.gz"}, {"url": "https://ipython.readthedocs.io/en/stable/interactive/magics.html", "anchor_text": "%timei"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.memory_usage.html", "anchor_text": "df.memory_usage()"}, {"url": "http://docs.dask.org/en/latest/diagnostics-local.html", "anchor_text": "ResourceProfiler"}, {"url": "http://docs.dask.org/en/latest/diagnostics-local.html", "anchor_text": "ProgressBar"}, {"url": "https://docs.python.org/3/library/sys.html#sys.getsizeof", "anchor_text": "sys.getsizeof"}, {"url": "https://docs.python.org/2/library/gc.html", "anchor_text": "gc.collect()"}, {"url": "https://medium.com/tag/data-science?source=post_page-----34f43a897d55---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----34f43a897d55---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/pandas?source=post_page-----34f43a897d55---------------pandas-----------------", "anchor_text": "Pandas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&user=Filip+Ciesielski&userId=99269edb373a&source=-----34f43a897d55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&user=Filip+Ciesielski&userId=99269edb373a&source=-----34f43a897d55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F34f43a897d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----34f43a897d55---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----34f43a897d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----34f43a897d55--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----34f43a897d55--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----34f43a897d55--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----34f43a897d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phil.thyharry?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phil.thyharry?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Filip Ciesielski"}, {"url": "https://medium.com/@phil.thyharry/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "267 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99269edb373a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&user=Filip+Ciesielski&userId=99269edb373a&source=post_page-99269edb373a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F10f50dee8de7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55&newsletterV3=99269edb373a&newsletterV3Id=10f50dee8de7&user=Filip+Ciesielski&userId=99269edb373a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}