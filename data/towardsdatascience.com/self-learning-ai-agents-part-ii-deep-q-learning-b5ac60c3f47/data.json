{"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "time": 1682993865.6681678, "path": "towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47/", "webpage": {"metadata": {"title": "Deep Q-Learning Demystified | Towards Data Science", "h1": "Self Learning AI-Agents Part II: Deep Q-Learning", "description": "Deep Q-Learning is an essential part of Deep Reinforcement Learning. This effective approach enables AI agents to learn to behave in environments with discrete action spaces."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "To understand the following topics completely I would advice you to recap the first article.", "paragraph_index": 0}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 45}], "all_paragraphs": ["In the first article of this series I presented you the concept of Markov Decision Processes which is the basis of deep reinforcement learning. To understand the following topics completely I would advice you to recap the first article.", "With Deep-Q Learning we can program AI agents that can operate in environments with discrete actions spaces. A discrete action space refers to actions that are well-defined, e.g. moving left or right, up or down.", "Atari\u2019s Breakthrough is a typical example of an environment with a discrete action space. The AI agent can move either left or right. The movement in each direction is happening with a certain velocity.", "If the agent could determine the velocity, then we would have a continues action space with an infinite amount of possible actions (movement with a different velocity). This case will be considered in the future.", "In the last article, I introduced the concept of the action-value function Q(s,a), given by Eq. 1. As a reminder the action-value function is defined as the expected return the AI agent would get by starting in state s, taking action a and then following a policy \u03c0.", "Remember: Intuitively speaking the policy \u03c0 can be described as a strategy of the agent to select certain actions depending on the current state s.", "Q(s,a) tells the agent the value (or quality) of a possible action a in a particular state s. Given a state s the action-value function calculates the quality/value for each possible action a_i in this state as a scalar value (Fig. 1). Higher quality means a better action with regards to the given objective.", "If we execute the expectation operator E in Eq. 1 we obtain a new form of the action-value function where we are dealing with probabilities. Pss\u2019 is the transition probability from state s to the next state s\u2019, determined by the environment. \u03c0(a\u2019|s\u2019) is the policy or mathematically speaking the distribution over all actions given a state s.", "Our goal in Deep Q-Learning is to solve the action-value function Q(s,a). Why do we want this? If the AI agent knows Q(s,a) then the given objective (like winning a chess game versus a human player or playing Atari\u2019s Breakthrough) can be considered as solved. The reason for this is the fact that the knowledge of Q(s,a) would enable the agent to determine the quality of any possible action in any given state. Thus the agent could behave accordingly.", "Eq. 2 gives us also a recursive solution that can be used to calculate Q(s,a). But since we are considering recursion and furthermore dealing with probabilities using this equation is not practical. Rather we must use the so-called Temporal Difference (TD) learning algorithm to solve Q(s,a) iteratively.", "In TD learning we update Q(s,a) for each action a in a state s towards the estimated return R(t+1)+\u03b3Q(s(t+1), a(t+1)) (Eq. 3). The estimated return is also called the TD-Target. Performing this update rule iteratively for each state s and action a many times yield correct action-values Q(s,a) for any state \u2014 action pair in the environment.", "The TD-Learning algorithm can be summarized in the following steps:", "Let\u2019s discuss the concept of the TD algorithm in greater detail. In TD- learning we consider the \u201cTemporal Difference\u201d of Q(s,a) \u2014 the difference between two \u201cversions\u201d of Q(s, a) separated by time once before we take an action a in state s and once after that.", "Take a look at Fig. 2. Assume the AI agent is in state s (blue arrow). In state s, he can take two different actions a_1 and a_2. Based on the calculations from some previous time steps the agent knows the action-values Q(s, a_1) and Q(s, a_2) for the two possible actions in this state.", "Based on this knowledge the agent decides to take the action a_1. After taking this action the agent is in the next state s\u2019. For taking the action a_1 he receives the immediate reward R. Being in state s\u2019 the agent can again take two possible actions a\u2019_1 and a\u2019_2 for which he knows the action-values again from some previous calculations.", "If you look on the definition of Q(s,a) in equation Eq. 1 you will recognize that in state s\u2019 we now have new information that we can use to calculate a new value for Q(s, a_1). This information is the immediate reward R received for the last action in the last state and Q(s\u2019,a\u2019) for the action a\u2019 the agent will take in this new state. The new value of Q(s, a_1) can be calculated according to the equation in Fig. 3. The right side of the equation is also what we call the TD-Target. The difference between the TD-target and the old value or \u2018temporal version\u2019 of Q(s,a_1) is called temporal difference.", "Remember: During TD-learning we calculate temporal differences for any possible action-value Q(s,a) and use them to update Q(s,a) at the same time, until Q(s,a) has converged to it\u2019s true values.", "The TD-Learning algorithm applied to Q(s,a) is commonly known as the SARSA algorithm (State\u2013Action\u2013Reward\u2013State\u2013Action). SARSA is a good example for the special kind of learning algorithms which are called on-policy algorithms.", "Earlier I introduced the policy \u03c0(a|s) as a mapping from state s to action a. One thing that you should remember at this point is that on-policy algorithms use the same policy to obtain the actions for Q(s_t, a_t) as well as the actions for Q(s(t+1), a_(t+1)) in the TD-Target. This means we are following and improving the same policy at the same time.", "We finally arrive at the heart of the article where we will discuss the concept of Q-Learning. But before we must take a look at the second special type of algorithms that are called off-policy algorithms. As you may think already Q-Learning belongs to this kind of algorithms, which is a distinction to on-policy algorithms such as SARSA.", "To understand off-policy algorithms we must introduce another policy \u00b5(a|s) and call it the behavior policy. Behavior policy determines the actions a_t~\u00b5(a|s) for Q(s_t,a_t) for all t. In the case of SARSA, the behavior policy would be the policy that we follow and try to optimize at the same time.", "In off-policy algorithms we have two different policies \u00b5(a|s) and \u03c0(a|s), \u00b5(a|s) being the behavior and \u03c0(a|s) being so-called target policy. While the behavior policy is used for calculation of Q(s_t, a_t), target policy is used for the calculation of Q(s_t, a_t) only in the TD-Target. (This concept will be more comprehensive in the next section, where actual calculations are made)", "Remember: Behavior policy picks actions for all Q(s,a). In contrast the target policy determines the actions only for the calculation of the TD-Target.", "The algorithm what we actually call the Q-Learning algorithm is a special case where the target policy \u03c0(a|s) is a greedy w.r.t. Q(s,a), which means that our strategy is taking actions which result in highest values of Q. That yields following target policy:", "In this case, the target policy is called the Greedy-Policy. Greedy-Policy means that we only pick actions that result in highest Q(s,a) values. This greedy target policy can be inserted into the equation for action-values Q(s,a) where we have followed a random policy \u03c0(a|s) before:", "The greedy policy provides us with optimal action-values Q*(s,a) because by definition Q*(s,a) is Q(s,a) that follows the policy that maximizes the action-values:", "The last line in Eq. 5 is nothing else than the Bellman Optimality Equation which we have derived in the last article. This equation is used as a recursive update rule to estimate the optimal action-value function Q*(s,a).", "However, TD-learning remains still the best way to find Q*(s,a). With greedy target policy the TD-learning update step for Q(s,a) in Eq. 3 becomes more simple and looks as follows:", "The TD-Learning algorithm for Q(s,a) with a greedy target policy be summarized in the following steps:", "Consider previous figure (Fig. 3), where the agent landed in state s\u2019 and knew the action values for the possible actions in that state. Following the greedy target policy, the agent would take the action with the highest action-value (blue path in Fig. 4). This policy gives us also a new value for Q(s, a_1) (Equation in the figure), which is by definition the TD-Target.", "We finally arrive at the point where the title of this article gets its preamble Deep \u2014 we finally make usage of deep learning. If you look on the update rule for Q(s,a) you may recognize that we don't get any updates if the TD-Target and Q(s,a) have the same values. In this case Q(s,a), converged to the true action-values and the goal is achieved.", "This means that our objective is minimizing the distance between the TD-Target and Q(s,a), which can be expressed by the squared error loss function (Eq. 10). Minimization of this loss function can be achieved by usual gradient descent algorithms.", "In Deep-Q Learning TD-Target y_i and Q(s,a) are estimated separately by two different neural networks, which are often called the Target-, and Q-Networks (Fig. 4). The parameters \u03b8(i-1) (weights, biases) of the Target-Network correspond to the parameter \u03b8(i) of the Q-Network at an earlier point in time. Meaning the Target-Network parameter are frozen in time. They get updated after n iterations with the parameters of the Q-Network.", "Remember: Given the current state s, the Q-Network calculates the action-values Q(s,a). At the same time the Target-Network uses the next state s\u2019 to calculate Q(s\u2019,a) for the TD-target.", "The research has shown that using two different neural networks for TD-Target and Q(s,a) calculation leads to a better stability of the models.", "While the target policy \u03c0(a|s) remains the greedy policy, the behavior policy \u00b5(a|s) determines the action a_i the AI agent takes, thus which Q(s,a_i) (calculated by the Q-Network) must be inserted into the squared error loss function.", "Behavior policy is usually chosen to be \u03b5-Greedy. With \u03b5-Greedy policy, the agent selects at each time step a random action with a fixed probability \u03b5. If \u03b5 has a higher value than a randomly generated number p, 0 \u2264 p \u2264 1, the AI agent picks a random action from the action space. Otherwise, the action is chosen greedily according to the leaned action-value Q(s,a):", "The choice of \u03b5-Greedy policy as the behavior policy \u00b5 solves the dilemma of Exploration/Exploitation trade off.", "Decision-making with regards to which action to take involves a fundamental choice:", "In terms of exploitation, the agent takes the best possible action according to the behavior policy \u00b5. But this may result in a problem. Maybe sometimes there is another (alternative) action that can be taken that results (long term) in a better path through the sequence of states, but this alternative action may be not taken if we follow the behavior policy. In this case, we exploit the current policy but do not explore other alternative actions.", "The \u03b5-Greedy policy solves this problem by allowing the AI agent taking random actions from the action-space with a certain probability \u03b5. This is called exploration. Usually, the value of \u03b5 gets decreased over time, according to Eq. 12. Here n is the number of iterations. Decreasing \u03b5 means that at the beginning of training we try to explore more alternative paths, while in the end, we let the policy make decisions on which action to take.", "In the past, it could be shown that the neural network approach to estimate the TD-Target and Q(s,a) becomes more stable if the Deep-Q Learning model implements experience replay. Experience replay is nothing more than the memory that stores <s, s\u2019, a\u2019, r> tuple, where", "While training the neural network we don\u2019t usually use the most recent <s, s\u2019, a\u2019, r> tuple. Rather we take random batches of <s, s\u2019, a\u2019, r> from the experience replay to calculate the TD-Target, Q(s,a) and apply gradient descent in the end.", "The following pseudo-algorithm implements the Deep-Q Learning with Experience Replay. All topics we have discussed previously are incorporated in this algorithm in the right order, exactly how it would be implemented in code.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb5ac60c3f47&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----b5ac60c3f47---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Part I: Markov Decision Processes"}, {"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "anchor_text": "Part III: Deep (Double) Q-Learning"}, {"url": "https://www.linkedin.com/in/artem-oppermann-929154199/?locale=en_US", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "To understand the following topics completely I would advice you to recap the first article."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b5ac60c3f47---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b5ac60c3f47---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b5ac60c3f47---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b5ac60c3f47---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b5ac60c3f47---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&user=Artem+Oppermann&userId=619319ac8220&source=-----b5ac60c3f47---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&user=Artem+Oppermann&userId=619319ac8220&source=-----b5ac60c3f47---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb5ac60c3f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b5ac60c3f47---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b5ac60c3f47--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}