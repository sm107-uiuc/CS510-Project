{"url": "https://towardsdatascience.com/reinforcement-learning-policy-approximation-f5606ad3d909", "time": 1683000542.0509791, "path": "towardsdatascience.com/reinforcement-learning-policy-approximation-f5606ad3d909/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Policy Approximation | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Policy Approximation", "description": "Till now, all algorithms being introduced are either value function or Q function based gradient algorithm, that is we assume there exists a true value V(or Q) for different state S(or [S, A]) , and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b", "anchor_text": "tile coding", "paragraph_index": 14}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/ShortCorridor/ShortCorridor.py", "anchor_text": "full implementation", "paragraph_index": 23}], "all_paragraphs": ["Till now, all algorithms being introduced are either value function or Q function based gradient algorithm, that is we assume there exists a true value V(or Q) for different state S(or [S, A]) , and to approach the true value we use gradient method that comes with either \u2207V or \u2207Q in the formula, and at the end of the learning process, a policy \u03c0(A | S) is generated by choosing the most rewarding action at each state based on V or Q function estimation. However, policy gradient method proposes a total different view on reinforcement learning problems, instead of learning a value function, one can directly learn or update a policy. So in this post, we will be:", "Remember in previous posts, the policy being used in the learning process is always \u03f5-greedy, which means the agent will take random action will a certain probability and take greedy action in the rest. However, in gradient policy method, the problem is formulated as, P(A|S, \u03b8) = \u03c0(A|S, \u03b8) , which is saying, for each state, the policy gives a probability of each action possible taken from that state, and in order to optimise the policy, it is parameterised with \u03b8 (similar to weight parameter w in value function we introduced before).", "Another thing that is worth mentioning is that a performance variable is defined in this case as:", "J(\u03b8) is the true value of V under the current parameterised policy \u03c0 , and the goal would be to maximise the performance J , so consequently we get a gradient update process of :", "And because of J is a representation of policy \u03c0 , we know that the update of \u03b8 will include the current policy, and after a series of deduction(for details, please refer to Sutton\u2019s book, chapter 13), we get the update process:", "and our first policy gradient method would be:", "G is still the cumulative discounted reward, and the parameter \u03b8 will be updated with current derivative of policy.", "Firstly, the obvious advantage of policy gradient method would be that the process is simplified, comparing with value function approximation, where we first learn a value function V(S, A) , and from which we infer the policy \u03c0(A|S) , in policy approximation, in each iteration, the policy is directly updated by updating parameter \u03b8 in \u03c0(A|S, \u03b8) .", "Secondly is that in value function update process, we generally use \u03f5-greedy method so that the agent will always has a probability of \u03f5 taking random actions. However, policy gradient method is able to approach deterministic policy by updating the probability of some actions to zero.", "Thirdly, if you still remember, in previous posts we always require the action space to be discrete, as the general value function approximation methods are a little bit tricky to deal with actions with strictly continuous space. But in policy approximation definition, the action can actually be continuous with respect to state, for example, one can define the policy as:", "in this case, the action a will have a probability of normal distribution with \u03bc and \u03c3 defined by state s .", "Last but not the least, it enables the selection of actions with arbitrary probabilities, in cases that the optimal policy is non-deterministic and always select action with optimal probabilities.", "Now let\u2019s get to an example to apply our knowledge.", "Consider the small corridor grid world shown inset in the graph below. The reward is -1 per step, as usual. In each of the three nonterminal states there are only two actions, right and left. These actions have their usual consequences in the first and third states (left causes no movement in the first state), but in the second state they are reversed, so that right moves to the left and left moves to the right. The problem is difficult because all the states appear identical under the function approximation. In particular, we define x(s, right) = [1, 0]) and x(s, left) = [0, 1]) for all s .", "In this scenario, the state, action representation is using the simplest format and treating all states as the same(the binary representation is same as it is in tile coding).", "The algorithm and components are listed below:", "So here, policy \u03c0(a|s, \u03b8) is defined using softmax function(other functions may be applied as well), which generates a probability between 0 and 1, and component h is defined as a simple linear function with parameter \u03b8 .", "Now let\u2019s get to the implementation.", "inside the init function, we have definition of self.x which we talked above and self.theta , which has the initial weights for self.x , and the starting state is 0.", "Function softmax takes in an vector and returns the probability of each component in the vector. Action is chosen by the probability generated by softmax , and the takeAction function takes in an action and returns the next state(note that the second state is reversed).", "As described in the problem, besides the final state, all states have a reward of -1.", "In each episode, the agent goes through the process current_state -> take_action -> next_state -> get_reward . When it reaches the end of the state(3), we calculated the accumulative reward G. Gradient grad is calculated for the specific action being taken in that step( j is the index for that action). The reward_sum only records the reward for each episode for displaying purpose.", "Let\u2019s run the game for 1000 episodes with alpha = 2e-4and gamma = 1 , got result:", "From the probability in increasing round, we see that the agent is updating the policy gradually, in fact, in this game setting, the optimal policy is probability of [0.4, 0.6] and our learning result is approaching the optimal value (full implementation).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff5606ad3d909&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----f5606ad3d909---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b", "anchor_text": "tile coding"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/ShortCorridor/ShortCorridor.py", "anchor_text": "full implementation"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f5606ad3d909---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f5606ad3d909---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f5606ad3d909---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/startup?source=post_page-----f5606ad3d909---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----f5606ad3d909---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----f5606ad3d909---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff5606ad3d909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f5606ad3d909---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f5606ad3d909--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f5606ad3d909--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f5606ad3d909--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-policy-approximation-f5606ad3d909&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}