{"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "time": 1683014438.138782, "path": "towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a/", "webpage": {"metadata": {"title": "Reinforcement Learning frameworks | by Jordi TORRES.AI | Towards Data Science", "h1": "Reinforcement Learning frameworks", "description": "This is the post number 20 in the \u201cDeep Reinforcement Learning Explained\u201d series devoted to Reinforcement Learning frameworks."}, "outgoing_paragraph_urls": [{"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "previous posts", "paragraph_index": 1}, {"url": "http://rllib.io/", "anchor_text": "RLlib", "paragraph_index": 2}, {"url": "https://ray.io/", "anchor_text": "Ray", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "previous post", "paragraph_index": 3}, {"url": "http://incompleteideas.net/williams-92.pdf", "anchor_text": "REINFORCE", "paragraph_index": 3}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "A2C, A3C, DDPG, TD3, SAC, PPO", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a", "anchor_text": "we have spoken extensively about both in this blog", "paragraph_index": 10}, {"url": "https://github.com/deepmind/acme", "anchor_text": "Acme", "paragraph_index": 14}, {"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "RLlib", "paragraph_index": 14}, {"url": "https://ray.io", "anchor_text": "Ray", "paragraph_index": 14}, {"url": "https://github.com/deepmind/acme", "anchor_text": "Acme", "paragraph_index": 15}, {"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "RLlib", "paragraph_index": 15}, {"url": "https://ray.io", "anchor_text": "Ray", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/supercomputing-the-heart-of-deep-learning-and-artificial-intelligence-49218c6bdee5", "anchor_text": "Supercomputing", "paragraph_index": 18}, {"url": "https://www.bsc.es", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 19}, {"url": "https://dl.acm.org/doi/book/10.5555/554797", "anchor_text": "Daniel Hillis", "paragraph_index": 24}, {"url": "https://docs.python.org/2/library/multiprocessing.html", "anchor_text": "multiprocessing module", "paragraph_index": 27}, {"url": "https://ray.io/", "anchor_text": "Ray", "paragraph_index": 27}, {"url": "https://docs.ray.io/en/latest/", "anchor_text": "official project page", "paragraph_index": 28}, {"url": "https://docs.ray.io/en/latest/rllib.html#rllib-index", "anchor_text": "RLlib", "paragraph_index": 29}, {"url": "https://docs.ray.io/en/latest/tune/index.html", "anchor_text": "Tune (Scalable Hyperparameter Tuning", "paragraph_index": 29}, {"url": "https://docs.ray.io/en/latest/raysgd/raysgd.html#sgd-index", "anchor_text": "RaySGD (Distributed Training Wrappers)", "paragraph_index": 29}, {"url": "https://docs.ray.io/en/latest/serve/index.html#rayserve", "anchor_text": "Ray Serve (Scalable and Programmable Serving", "paragraph_index": 29}, {"url": "https://docs.ray.io/en/latest/rllib.html#rllib-index", "anchor_text": "RLlib", "paragraph_index": 30}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html", "anchor_text": "API documentation", "paragraph_index": 31}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "built-in algorithms", "paragraph_index": 31}, {"url": "https://docs.ray.io/en/latest/rllib-concepts.html", "anchor_text": "custom algorithms", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow vs. PyTorch: The battle continues", "paragraph_index": 34}, {"url": "https://docs.ray.io/en/master/rllib-training.html#common-parameters", "anchor_text": "trainer config", "paragraph_index": 35}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_tf_policy.py", "anchor_text": "PPOTFPolicy", "paragraph_index": 35}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_torch_policy.py", "anchor_text": "PPOTorchPolicy", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole Environment", "paragraph_index": 36}, {"url": "https://arxiv.org/pdf/1707.06347.pdf", "anchor_text": "Proximal Policy Optimization Algorithms", "paragraph_index": 36}, {"url": "https://docs.ray.io/en/master/rllib-algorithms.html", "anchor_text": "algorithms already programmed", "paragraph_index": 36}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 37}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 37}, {"url": "https://github.com/ray-project/ray/tree/master/rllib/agents", "anchor_text": "Here", "paragraph_index": 39}, {"url": "https://docs.aws.amazon.com/sagemaker/latest/dg/reinforcement-learning.html", "anchor_text": "AWS", "paragraph_index": 50}, {"url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-reinforcement-learning", "anchor_text": "AzureML", "paragraph_index": 50}, {"url": "https://anyscale.com/", "anchor_text": "ANYSCALE", "paragraph_index": 50}, {"url": "https://anyscale.com/blog/page/3/", "anchor_text": "20 million", "paragraph_index": 50}, {"url": "https://events.linuxfoundation.org/ray-summit/?utm_source=ion&utm_medium=blog&utm_campaign=ray_summit", "anchor_text": "Ray Summit", "paragraph_index": 50}, {"url": "https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en", "anchor_text": "Oriol Vinyals", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1712.09381", "anchor_text": "involved great researchers from the University of California at Berkeley", "paragraph_index": 50}, {"url": "https://people.eecs.berkeley.edu/~istoica/", "anchor_text": "Ion Stoica", "paragraph_index": 50}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 52}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 52}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 53}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 54}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 56}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 56}, {"url": "https://arxiv.org/pdf/2002.03647.pdf", "anchor_text": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills", "paragraph_index": 56}, {"url": "https://icml.cc/", "anchor_text": "37th International Conference on Machine Learning (ICML2020)", "paragraph_index": 56}, {"url": "https://twitter.com/vcampos7", "anchor_text": "@vcampos7", "paragraph_index": 56}, {"url": "https://twitter.com/DocXavi", "anchor_text": "@DocXavi", "paragraph_index": 56}, {"url": "https://twitter.com/alexrtrott", "anchor_text": "@alexrtrott", "paragraph_index": 56}, {"url": "https://twitter.com/CaimingXiong", "anchor_text": "@CaimingXiong", "paragraph_index": 56}, {"url": "https://twitter.com/RichardSocher", "anchor_text": "@RichardSocher", "paragraph_index": 56}, {"url": "https://einstein.ai/", "anchor_text": "Salesforce Research", "paragraph_index": 56}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 58}], "all_paragraphs": ["Welcome to this 20th post that concludes the \u201cDeep Reinforcement Learning Explained\u201d series that presents a practical approach to getting started in the exciting world of Deep Reinforcement Learning.", "So far, in previous posts, we have been looking at a basic representation of the corpus of RL algorithms (although we have skipped several) that have been relatively easy to program. But from now on, we need to consider both the scale and complexity of the RL algorithms. In this scenario, programming a Reinforcement Learning implementation from scratch can become tedious work with a high risk of programming errors.", "To address this, the RL community began to build frameworks and libraries to simplify the development of RL algorithms, both by creating new pieces and especially by involving the combination of various algorithmic components. In this post, we will make a general presentation of those Reinforcement Learning frameworks and solving the previous problem of CartPole using RLlib, an open-source library in Python, based on Ray framework.", "But before continuing, as a motivational example, let\u2019s remember that in the previous post, we presented REINFORCE and its limitations. The research community created many training algorithms to solve it: A2C, A3C, DDPG, TD3, SAC, PPO, among many others. But programming these algorithms from scratch becomes more convoluted than that of REINFORCE. Also, the more involved you become in the field, the more often you will realise that you are writing the same code over and over again.", "The practical applications of Reinforcement Learning are relatively young compared to other domains as Deep Learning, where well-established frameworks as TensorFlow, PyTorch, or MXnet simplify DL practitioners\u2019 lives. However, the emergence of RL frameworks has already begun and right now we can choose from several projects that greatly facilitate the use of advanced RL methods.", "Before presenting these RL frameworks, let\u2019s see a bit of their context.", "In the last several years pattern-recognition side has been the focus of much of the work and much of the discussion in the community of Deep Learning. We are using powerful supercomputers that process large labeled data sets (with expert-provided outputs for the training set), and apply gradient-based methods that find patterns in those data sets that can be used to predict or to try to find structures inside the data.", "This contrasts with the fact that an important part of our knowledge of the world is acquired through interaction, without an external teacher telling us what the outcomes of every single action we take will be. Humans are able to discover solutions to new problems from interaction and experience, acquiring knowledge about the world by actively exploring it.", "For this reason, current approaches will study the problem of learning from interaction with simulated environments through the lens of Deep Reinforcement Learning (DRL), a computational approach to goal-directed learning from the interaction that does not rely on expert supervision. I.e., a Reinforcement Learning Agent must interact with an Environment to generate its own training data.", "This motivates interacting with multiple instances of an Environment in parallel to generate faster more experience to learn from. This has led to the widespread use of increasingly large-scale distributed and parallel systems in RL training. This introduces numerous engineering and algorithmic challenges that can be fixed by these frameworks we are talking about.", "In recent years, frameworks such as TensorFlow or PyTorch (we have spoken extensively about both in this blog) have arisen to help turn pattern recognition into a commodity, making deep learning easier to try and use for practitioners.", "A similar pattern is beginning to play out in the Reinforcement Learning arena. We are beginning to see the emergence of many open source libraries and tools to address this both by helping in creating new pieces (not writing from scratch), and above all, involving the combination of various prebuild algorithmic components. As a result, these Reinforcement Learning frameworks help engineers by creating higher-level abstractions of the core components of an RL algorithm. In summary, this makes code easier to develop, more comfortable to read, and improves efficiency.", "Next, I provide a list of the most popular RL frameworks available. I think the readers will benefit by using code from an already-established framework or library. At the time of writing this post, I could mention the most important ones (and I\u2019m sure I\u2019m leaving some of them out):", "Deciding which one of the RL frameworks listed here to use, depends on your preferences and what you want to do with it exactly. The reader can follow the links for more information.", "We are using Acme for doing research at our research center. But to describe one of these environments so that the reader can see the possibilities they offer, I have personally opted for RLlib based in Ray for several reasons that I will explain below.", "We are using Acme for doing research at our research center. But to describe one of these environments so that the reader can see the possibilities they offer, I have opted for RLlib based in Ray for several reasons that I will explain below.", "Deep Reinforcement Learning algorithms involve a large number of simulations adding another multiplicative factor to the computational complexity of Deep Learning in itself. Mostly this is required by the algorithms we have not yet seen in this series, such as the distributed actor-critic methods or multi-agents methods, among others.", "But even finding the best model often requires hyperparameter tuning and searching among various hyperparameter settings; it can be costly. All this entails the need for high computing power provided by supercomputers based on distributed systems of heterogeneous servers (with multi-core CPUs and hardware accelerators as GPUs or TPUs).", "Two years ago, when I debuted as an author on Medium, I already explained what this type of infrastructure is like in the article \u201cSupercomputing\u201d. In Barcelona, we now have a supercomputer, named Marenostrum 4, which has a computing power of 13 Petaflops.", "Barcelona Supercomputing Center will host a new supercomputer next year, Marenostrum 5, which will multiply the computational power by a factor of x17.", "The current supercomputer MareNostrum 4 is divided into two differentiated hardware blocks: a block of general-purpose and a block-based on an IBM system designed especially for Deep Learning and Artificial Intelligence applications.", "In terms of hardware, this part of the Marenostrum consists of a 54 node cluster based on IBM Power 9 and NVIDIA V100 with Linux operating system and interconnected by an Infiniband network at 100 Gigabits per second. Each node is equipped with 2 IBM POWER9 processors with 20 physical cores each and 512GB of memory. Each of these POWER9 processors is connected to two NVIDIA V100 (Volta) GPUs with 16GB of memory, a total of 4 GPUs per node.", "How can this hardware fabric be managed efficiently?", "Accelerating Reinforcement Learning with distributed and parallel systems introduce several challenges in managing the parallelization and distribution of the programs\u2019 execution. To address this growing complexity, new layers of software have begun to be proposed that we stack on existing ones in an attempt to maintain logically separate the different components of the layered software stack of the system", "Because of this key abstraction, we can focus on different software components that today supercomputers incorporate in order to perform complex tasks. I like to mention that Daniel Hillis, who co-founded Thinking Machines Corporation, a company that developed the parallel Connection Machine, says that the hierarchical structure of abstraction is our most important tool in understanding complex systems because it lets us focus on a single aspect of a problem at a time.", "And this is the case of RLlib, the framework for which I opted, that follows this divide and conquer philosophy with a layered design of the software stack.", "This hierarchical structure of abstraction that allows this functional abstraction is fundamental because it will let us manipulate information without worrying about its underlying representation. Daniel Hillis says that once we figure out how to accomplish a given function, we can put the mechanism inside a \u201dblack box\u201d of a \u201dbuilding block\u201d and stop thinking about it. The function embodied by the building block can be used over and over, without reference to the details of what\u2019s inside.", "In short, parallel and distributed computing is a staple of Reinforce Learning algorithms. We need to leverage multiple cores and accelerators (on multiple machines) to speed up RL applications, and Python\u2019s multiprocessing module is not the solution. Some of the RL frameworks, like Ray can handle this challenge excellently.", "On the official project page, Ray is defined as a fast and simple framework for building and running distributed applications:", "Ray Core provides simple primitives for application building. On top of Ray Core, beside RLlib, there are other libraries for solving problems in machine learning: Tune (Scalable Hyperparameter Tuning), RaySGD (Distributed Training Wrappers), and Ray Serve (Scalable and Programmable Serving).", "RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. RLlib natively supports TensorFlow, TensorFlow Eager, and PyTorch, but most of its internals are framework agnostic.", "At present, this library already has extensive documentation ( API documentation), offering a large number of built-in algorithms in addition to allowing the creation of custom algorithms.", "The key concepts in RLlib are Policies, Samples, and Trainers. In a nutshell, Policies are Python classes that define how an agent acts in an environment. All data interchange in RLlib is in the form of Sample batches that encode one or more fragments of a trajectory. Trainers are the boilerplate classes that put the above components together, managing algorithm configuration, optimizer, training metrics, the workflow of the execution parallel components, etc.", "Later in this series, when we have advanced more in distributed and multi-agent algorithms, we will present in more detail these key components of RLlib.", "In a previous post, TensorFlow vs. PyTorch: The battle continues, I showed that the battle between deep learning heavyweights TensorFlow and PyTorch is fully underway. And in this regard, the option taken by RLlib, allowing users to seamlessly switch between TensorFlow and PyTorch for their reinforcement learning work, also seems very appropriate.", "To allow users to easily switch between TensorFlow and PyTorch as a backend in RLlib, RLlib includes the \u201cframework\u201d trainer config. For example, to switch to the PyTorch version of an algorithm, we can specify {\"framework\":\"torch\"}. Internally, this tells RLlib to try to use the torch version of a policy for an algorithm (check out the examples of PPOTFPolicy vs. PPOTorchPolicy).", "Now, we will show a toy example to get you started and show you how to solve OpenAI Gym\u2019s Cartpole Environment with PPO algorithm using RLlib. PPO is one of the proposals that solves the limitations of REINFORCE, introduced in the paper \u201cProximal Policy Optimization Algorithms\u201d by John Schulman et al. (2017) at OpenAI. But the reader can use the code proposed in this section to test any of the algorithms already programmed in this framework.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "Warning: Given that we are executing our examples in Colab we need to restart the runtime after installing ray package and uninstall pyarrow.", "The various algorithms you can access are available through ray.rllib.agents. Here, you can find a long list of different implementations in both PyTorch and Tensorflow to begin playing with.", "If you want to use PPO you can run the following code:", "The ray.init() command starts all of the relevant Ray processes. This must be done before we instantiate any RL agents, for instance PPOTrainer object in our example:", "We can pass in a config object many hyperparameters that specify how the network and training procedure should be configured. Changing hyperparameters is as easy as passing them as a dictionary to the config argument. A quick way to see what\u2019s available is to call trainer.config to print out the options that are available for your chosen algorithm:", "Once we have specified our configuration, calling the train() method on our trainerobject will update and send the output to a new dictionary called results.", "All the algorithms follow the same basic construction alternating from lower case abbreviation to uppercase abbreviation followed by Trainer . For instance, if you want to try a DQN instead, you can call:", "The simplest way to programmatically compute actions from a trained agent is to use trainer.compute_action():", "This method preprocesses and filters the observation before passing it to the agent policy. Here is a simple example of how to watch the Agent that uses compute_action():", "Using watch_agent function, we can compare the behavior of the Agent before and after being trained running multiple updates calling the train() method for a given number:", "The last line of code shows how we can monitor the training loop printing information included in the return of the method train().", "This is a toy implementation of a simple algorithm to show this framework very briefly. The actual value of the RLlib framework lies in its use in large infrastructures executing inherently parallel and, at the same time, complex algorithms were writing the code from scratch is totally unfeasible.", "As I said, I opted for RLlib after taking a look at all the other frameworks mentioned above. The reasons are diverse; some are already presented in this post. Add that for me; it is relevant that it has already been included in major cloud providers such as AWS and AzureML. Or that there is a pushing company like ANYSCALE that has already raised 20 million and organizes the Ray Summit conference, which will be held online this week (September 30 through October 1) with great speakers (as our friend Oriol Vinyals ;-). Maybe add more context details, but for me, just as important as the above reasons is the fact that there are involved great researchers from the University of California at Berkeley, including the visionary Ion Stoica, whom I met about Spark, and they clearly got it right!", "I hope these posts have served to encourage readers to get introduced to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence. We haven\u2019t really seen much, just the bare minimum of basics that will allow you to continue on your own to discover and enjoy this fabulous world. The following references may be useful.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Our research group at UPC Barcelona Tech and Barcelona Supercomputing Center is doing research on this topic. Our latest paper in this area is \u201cExplore, Discover and Learn: Unsupervised Discovery of State-Covering Skills\u201d presented at the 37th International Conference on Machine Learning (ICML2020). The paper presents a novel paradigm for unsupervised skill discovery in Reinforcement Learning. It is the last contribution of @vcampos7, one of our Ph.D. students, co-advised with@DocXavi. This paper is co-authored with @alexrtrott, @CaimingXiong, @RichardSocher from Salesforce Research.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe349de4f645a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----e349de4f645a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 20"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "previous posts"}, {"url": "http://rllib.io/", "anchor_text": "RLlib"}, {"url": "https://ray.io/", "anchor_text": "Ray"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/11-frameworks-de-aprendizaje-por-refuerzo-ray-rllib-1329e93f14ee", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/11-frameworks-de-aprendizaje-por-refuerzo-ray-rllib-1329e93f14ee", "anchor_text": "11. Frameworks de aprendizaje por refuerzo: Ray+RLlibAcceso abierto al cap\u00edtulo 11 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "previous post"}, {"url": "http://incompleteideas.net/williams-92.pdf", "anchor_text": "REINFORCE"}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "A2C, A3C, DDPG, TD3, SAC, PPO"}, {"url": "https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a", "anchor_text": "we have spoken extensively about both in this blog"}, {"url": "https://github.com/keras-rl/keras-rl", "anchor_text": "Keras-RL"}, {"url": "https://github.com/NervanaSystems/coach", "anchor_text": "RL Coach"}, {"url": "https://github.com/facebookresearch/ReAgent", "anchor_text": "ReAgent"}, {"url": "https://ray.readthedocs.io/en/master/rllib.html", "anchor_text": "Ray+RLlib"}, {"url": "https://github.com/google/dopamine", "anchor_text": "Dopamine"}, {"url": "https://github.com/tensorforce/tensorforce", "anchor_text": "Tensorforce"}, {"url": "https://github.com/rlgraph/rlgraph", "anchor_text": "RLgraph"}, {"url": "https://github.com/rlworkgroup/garage", "anchor_text": "Garage"}, {"url": "https://github.com/VinF/deer", "anchor_text": "DeeR"}, {"url": "https://github.com/deepmind/acme", "anchor_text": "Acme"}, {"url": "https://github.com/openai/baselines", "anchor_text": "Baselines"}, {"url": "https://github.com/pfnet/pfrl", "anchor_text": "PFRL"}, {"url": "https://github.com/DLR-RM/stable-baselines3", "anchor_text": "StableBaselines"}, {"url": "https://github.com/deepmind/acme", "anchor_text": "Acme"}, {"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "RLlib"}, {"url": "https://ray.io", "anchor_text": "Ray"}, {"url": "https://github.com/deepmind/acme", "anchor_text": "Acme"}, {"url": "https://docs.ray.io/en/master/rllib.html", "anchor_text": "RLlib"}, {"url": "https://ray.io", "anchor_text": "Ray"}, {"url": "https://towardsdatascience.com/supercomputing-the-heart-of-deep-learning-and-artificial-intelligence-49218c6bdee5", "anchor_text": "Supercomputing"}, {"url": "https://www.bsc.es", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://dl.acm.org/doi/book/10.5555/554797", "anchor_text": "Daniel Hillis"}, {"url": "https://docs.ray.io/en/latest/rllib.html", "anchor_text": "docs.ray.io"}, {"url": "https://docs.python.org/2/library/multiprocessing.html", "anchor_text": "multiprocessing module"}, {"url": "https://ray.io/", "anchor_text": "Ray"}, {"url": "https://docs.ray.io/en/latest/", "anchor_text": "official project page"}, {"url": "https://docs.ray.io/en/latest/rllib.html#rllib-index", "anchor_text": "RLlib"}, {"url": "https://docs.ray.io/en/latest/tune/index.html", "anchor_text": "Tune (Scalable Hyperparameter Tuning"}, {"url": "https://docs.ray.io/en/latest/raysgd/raysgd.html#sgd-index", "anchor_text": "RaySGD (Distributed Training Wrappers)"}, {"url": "https://docs.ray.io/en/latest/serve/index.html#rayserve", "anchor_text": "Ray Serve (Scalable and Programmable Serving"}, {"url": "https://docs.ray.io/en/latest/rllib.html#rllib-index", "anchor_text": "RLlib"}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html", "anchor_text": "API documentation"}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "built-in algorithms"}, {"url": "https://docs.ray.io/en/latest/rllib-concepts.html", "anchor_text": "custom algorithms"}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow vs. PyTorch: The battle continues"}, {"url": "https://docs.ray.io/en/master/rllib-training.html#common-parameters", "anchor_text": "trainer config"}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_tf_policy.py", "anchor_text": "PPOTFPolicy"}, {"url": "https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_torch_policy.py", "anchor_text": "PPOTorchPolicy"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole Environment"}, {"url": "https://arxiv.org/pdf/1707.06347.pdf", "anchor_text": "Proximal Policy Optimization Algorithms"}, {"url": "https://docs.ray.io/en/master/rllib-algorithms.html", "anchor_text": "algorithms already programmed"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://github.com/ray-project/ray/tree/master/rllib/agents", "anchor_text": "Here"}, {"url": "https://docs.aws.amazon.com/sagemaker/latest/dg/reinforcement-learning.html", "anchor_text": "AWS"}, {"url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-reinforcement-learning", "anchor_text": "AzureML"}, {"url": "https://anyscale.com/", "anchor_text": "ANYSCALE"}, {"url": "https://anyscale.com/blog/page/3/", "anchor_text": "20 million"}, {"url": "https://events.linuxfoundation.org/ray-summit/?utm_source=ion&utm_medium=blog&utm_campaign=ray_summit", "anchor_text": "Ray Summit"}, {"url": "https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en", "anchor_text": "Oriol Vinyals"}, {"url": "https://arxiv.org/abs/1712.09381", "anchor_text": "involved great researchers from the University of California at Berkeley"}, {"url": "https://people.eecs.berkeley.edu/~istoica/", "anchor_text": "Ion Stoica"}, {"url": "https://mitpress.mit.edu/books/reinforcement-learning-second-edition", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "[on-line available from incompleteideas.net]"}, {"url": "https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks", "anchor_text": "repo of Python code"}, {"url": "https://www.manning.com/books/grokking-deep-reinforcement-learning", "anchor_text": "Grokking Deep Reinforcement Learning"}, {"url": "https://www.manning.com/books/deep-reinforcement-learning-in-action", "anchor_text": "Deep Reinforcement Learning in Action"}, {"url": "https://www.packtpub.com/data/deep-reinforcement-learning-hands-on-second-edition", "anchor_text": "Deep Reinforcement Learning Hands-on"}, {"url": "https://www.packtpub.com/product/hands-on-reinforcement-learning-for-games/9781839214936", "anchor_text": "Hands-on Reinforcement Learning for Games"}, {"url": "https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/", "anchor_text": "Reinforcement Learning, Industrial Applications and Intelligent Agents"}, {"url": "https://www.packtpub.com/product/deep-reinforcement-learning-with-python-second-edition/9781839210686", "anchor_text": "Deep Reinforcement Learning with Python"}, {"url": "https://www.davidsilver.uk/teaching/", "anchor_text": "UCL Course on RL by David Silver"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "UC Berkeley CS 285 by Sergey Levine"}, {"url": "https://www.youtube.com/watch?v=ISk80iLhdfU&list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb", "anchor_text": "DeepMind & UCL by Hado Van Hasselt"}, {"url": "http://web.stanford.edu/class/cs234/index.html", "anchor_text": "Stanford CS234 by Emma Brunskill"}, {"url": "https://www.cs.upc.edu/~mmartin/url-RL.html", "anchor_text": "UPC Barcelona Tech by Mario Martin"}, {"url": "https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/schedule.html", "anchor_text": "University of Waterloo CS 885 by Pascal Poupart"}, {"url": "https://sites.google.com/view/deep-rl-bootcamp/lectures", "anchor_text": "Berkeley DRL Bootcamp by Pieter Abbeel et all"}, {"url": "https://telecombcn-dl.github.io/drl-2020/", "anchor_text": "UPC Barcelona Tech by Xavier Gir\u00f3-i-Nieto et all"}, {"url": "https://spinningup.openai.com/en/latest/", "anchor_text": "OpenAI by Josh Achiam"}, {"url": "https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html", "anchor_text": "Lilian Weng: A (Long) Peek into Reinforcement Learning"}, {"url": "https://paperswithcode.com/methods/area/reinforcement-learning", "anchor_text": "Papers with code"}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "Alex Irpan: Sorta Insightful: Deep Reinforcement Learning Doesn\u2019t Work Yet"}, {"url": "https://karpathy.github.io/2016/05/31/rl/", "anchor_text": "Andrej Karpathy Blog: Deep Reinforcement Learning from Pixels"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://arxiv.org/pdf/2002.03647.pdf", "anchor_text": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"}, {"url": "https://icml.cc/", "anchor_text": "37th International Conference on Machine Learning (ICML2020)"}, {"url": "https://twitter.com/vcampos7", "anchor_text": "@vcampos7"}, {"url": "https://twitter.com/DocXavi", "anchor_text": "@DocXavi"}, {"url": "https://twitter.com/alexrtrott", "anchor_text": "@alexrtrott"}, {"url": "https://twitter.com/CaimingXiong", "anchor_text": "@CaimingXiong"}, {"url": "https://twitter.com/RichardSocher", "anchor_text": "@RichardSocher"}, {"url": "https://einstein.ai/", "anchor_text": "Salesforce Research"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e349de4f645a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e349de4f645a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----e349de4f645a---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e349de4f645a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e349de4f645a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e349de4f645a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e349de4f645a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe349de4f645a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e349de4f645a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e349de4f645a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e349de4f645a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e349de4f645a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e349de4f645a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e349de4f645a--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-frameworks-e349de4f645a&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}