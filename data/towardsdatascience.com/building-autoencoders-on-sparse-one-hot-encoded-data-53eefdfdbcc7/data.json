{"url": "https://towardsdatascience.com/building-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7", "time": 1683014481.3411121, "path": "towardsdatascience.com/building-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7/", "webpage": {"metadata": {"title": "Building Autoencoders on Sparse, One Hot Encoded Data | by Nick Hespe | Towards Data Science", "h1": "Building Autoencoders on Sparse, One Hot Encoded Data", "description": "Since their introduction in 1986 [1], general Autoencoder Neural Networks have permeated into research in most major divisions of modern Machine Learning over the past 3 decades. Having been shown to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "anchor_text": "Scikit-Learn OneHotEncoder module", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "Bootstrap Aggregating", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "features sampling", "paragraph_index": 4}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html", "anchor_text": "Torch.nn.CosineEmbeddingLoss", "paragraph_index": 11}, {"url": "https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b#:~:text=Dice%20loss%20originates%20from%20S%C3%B8rensen,between%20two%20samples%20%5BWikipedia%5D.&text=3%20from%20the%20perspective%20of,of%20overlap%20between%20two%20sets.", "anchor_text": "this medium post by Shuchen Du", "paragraph_index": 13}, {"url": "https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch", "anchor_text": "Loss Function Library \u2014 Keras & PyTorch", "paragraph_index": 14}], "all_paragraphs": ["Since their introduction in 1986 [1], general Autoencoder Neural Networks have permeated into research in most major divisions of modern Machine Learning over the past 3 decades. Having been shown to be exceptionally effective in embedding complex data, Autoencoders offer simple means to encode complex non-linear dependencies into trivial vector representations. But while their effectiveness has been proven in many aspects, they often fall short in being able to reproduce sparse data, especially when the columns are correlated like One Hot Encodings.", "In this article, I\u2019ll briefly discuss One Hot Encoding (OHE) data and general autoencoders. Then I\u2019ll cover the use cases that bring about the issues with Autoencoders trained on One Hot Encoded Data. Lastly, I\u2019ll discuss the issue of reconstructing sparse OHE data in-depth, then cover 3 loss functions that I found to work well under these conditions:", "\u2014 that solve for the aforementioned challenges, including code to implement them in PyTorch.", "One hot encoding data is one of the simplest, yet often misunderstood data preprocessing techniques in general machine learning scenarios. The process binarizes categorical data with \u2018N\u2019 distinct categories into N columns of binary 0\u2019s and 1\u2019s. Where the presence of a 1 in the \u2018N\u2019th category indicates that the observation belongs to that category. This process is simple in Python using the Scikit-Learn OneHotEncoder module:", "But while simple, this technique can sour fast if you are not careful. It can easily add superfluous complexity into your data, as well as change the effectiveness of certain classification methods on your data. For example, columns that are transformed into OHE vectors are now co-dependent, this interaction makes it difficult to represent aspects of the data effectively in certain types of classifiers. For example, if you had a column with 15 different categories, it would take an individual decision tree with a depth of 15 to handle the if-then patterns in that one hot encoded column. A great example of these issues can be found here if you\u2019re interested. Similarly, since the columns are co-dependent, if you use a classification strategy with bagging (Bootstrap Aggregating) and perform features sampling, you may miss the one-hot encoded column entirely, or consider only part of its component classes.", "Autoencoders are unsupervised neural networks that work to embed data into an efficient compressed format. It does this by utilizing an encoding and decoding process to encode the data down to a smaller format, then decoding that smaller format back into the original input representation. The model is trained by taking the loss between the model reconstruction (decoding), and the original data.", "Actually representing this network in code is also quite easy to do. We start with two functions: The Encoder Model, and the Decoder Model. Both \u2018models\u2019 are wrapped into a class called Network, which will encompass the entire system for our training and evaluation. Lastly, we define a function Forward, which is what PyTorch uses as the entryway into the Network that wraps both the encoding and the decoding of the data.", "As we can see above, we have an encoding function, which starts at the shape of the input data \u2014 then reduces its dimensionality as it propagates down to a shape of 50. From there, the decoding layer takes that embedding, then expands it back out to the original shape. In training, we take the reconstruction from the decoder and take the loss of the reconstruction vs the original input.", "So now we\u2019ve covered the Autoencoder Structure and the One Hot Encoding Process we can finally talk about the problems associated with using One Hot Encodings in Autoencoders, and how to solve for this issue. When an autoencoder compares the reconstruction to the original input data, there must be some valuation of the distance between the proposed reconstruction and the true value. Typically, in cases where the values output is considered disjoint from one another one would use a cross-entropy loss or MSE loss. But in the case of our One Hot Encodings, there are several issues that make the system more complex:", "These problems combine to lead the two aforementioned losses (MSE, Cross-Entropy) to be ineffective in reconstructing sparse OHE data. Below I\u2019ll cover three losses that offer a solution to one or both of the issues presented above, and code to implement them in PyTorch:", "Cosine Distance is a classic vector distance metric that is used commonly when comparing Bag of Words representations in NLP problems. The distance is calculated by finding the cosine angle between the two vectors calculated as:", "This method proves to be good at quantifying the error in the reconstruction of the sparse OHE embeddings because of its ability to evaluate the distance of the two vectors taking into account the deviations of the binary values in the individual columns. This loss is by far the easiest to implement in PyTorch as it has a pre-built solution in Torch.nn.CosineEmbeddingLoss", "Dice loss is an implementation of the S\u00f8rensen\u2013Dice coefficient [2], which is very popular in the field of computer vision in segmentation tasks. In simple terms, it is a measure of overlap between two sets, and is related to the Jaccard distance between two vectors. The dice coefficient is highly sensitive to differences in column values in the vectors and is popular in Image Segmentation as it utilizes this sensitivity to effectively differentiate between pixel edges in the image. Dice loss follows the following equation:", "For more information about the Sorensen Dice Coefficient \u2014 you can check out this medium post by Shuchen Du", "PyTorch does not have an in-house Implementation of Dice Coefficient. But a good implementation can be found on Kaggle in their Loss Function Library \u2014 Keras & PyTorch [3]:", "Lastly, you can treat each One Hot Encoded column as its own classification problem and take the loss for each of those classifications. This is a use case of a Multi-Task learning problem, where the autoencoder is solving for reconstructing the individual components of the input vector. This works best when you have several / all OHE columns in your input data. For example, if you had an encoded column with 7 categories as the first seven columns: you could treat that as a multi-class classification problem and take the loss as the cross-entropy loss of the sub-problem. You can then combine the losses of the sub-problems together and pass that backward as the loss of the batch as a whole.", "Below you\u2019ll see an example of this process with an example of three One Hot Encoded Columns, each with 50 categories.", "In the above code you can see how individual losses are taken on subsets of the reconstructed output, and then combined as a sum at the end. Here we use a negative log-likelihood loss (nll_loss) which is a good loss function for multiclass classification schemes and is related to Cross-Entropy Loss.", "In this article, we glanced over the concepts of One Hot Encoding categorical variables and the General Structure and Goal of Autoencoders. We discussed the downsides of One Hot Encoding Vectors, and the main issues when trying to train Autoencoder models on Sparse, One Hot Encoded Data. Lastly, we covered 3 loss functions that tackle the Sparse One Hot Encoding issue. There\u2019s no better or worse Loss for trying to train these networks, of the functions that I\u2019ve presented there's no way to tell which one is right for your use case until you try them out!", "Below I\u2019ve included a bunch of resources that go in-depth into the topics that I\u2019ve discussed above, as well as some resources for the loss functions that I\u2019ve presented.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Applied Scientist at JP Morgan Chase"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F53eefdfdbcc7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nahespe?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahespe?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Nick Hespe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2576a4a7dfae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&user=Nick+Hespe&userId=2576a4a7dfae&source=post_page-2576a4a7dfae----53eefdfdbcc7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@vivalaveronica?utm_source=medium&utm_medium=referral", "anchor_text": "Veronica Benavides"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "anchor_text": "Scikit-Learn OneHotEncoder module"}, {"url": "https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "Bootstrap Aggregating"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "features sampling"}, {"url": "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798", "anchor_text": "Applied Deep Learning \u2014 Part 3: Autoencoders"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html", "anchor_text": "Torch.nn.CosineEmbeddingLoss"}, {"url": "https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b#:~:text=Dice%20loss%20originates%20from%20S%C3%B8rensen,between%20two%20samples%20%5BWikipedia%5D.&text=3%20from%20the%20perspective%20of,of%20overlap%20between%20two%20sets.", "anchor_text": "this medium post by Shuchen Du"}, {"url": "https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch", "anchor_text": "Loss Function Library \u2014 Keras & PyTorch"}, {"url": "https://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab", "anchor_text": "Kongelige Danske Videnskabernes Selskab"}, {"url": "https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch", "anchor_text": "https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch"}, {"url": "https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769", "anchor_text": "https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205"}, {"url": "https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b", "anchor_text": "https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----53eefdfdbcc7---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/one-hot-encoding?source=post_page-----53eefdfdbcc7---------------one_hot_encoding-----------------", "anchor_text": "One Hot Encoding"}, {"url": "https://medium.com/tag/sparse-data?source=post_page-----53eefdfdbcc7---------------sparse_data-----------------", "anchor_text": "Sparse Data"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----53eefdfdbcc7---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----53eefdfdbcc7---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&user=Nick+Hespe&userId=2576a4a7dfae&source=-----53eefdfdbcc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&user=Nick+Hespe&userId=2576a4a7dfae&source=-----53eefdfdbcc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F53eefdfdbcc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----53eefdfdbcc7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----53eefdfdbcc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahespe?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahespe?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nick Hespe"}, {"url": "https://medium.com/@nahespe/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2576a4a7dfae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&user=Nick+Hespe&userId=2576a4a7dfae&source=post_page-2576a4a7dfae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1db56744a68c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7&newsletterV3=2576a4a7dfae&newsletterV3Id=1db56744a68c&user=Nick+Hespe&userId=2576a4a7dfae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}