{"url": "https://towardsdatascience.com/mapping-word-embeddings-with-word2vec-99a799dc9695", "time": 1682993638.220246, "path": "towardsdatascience.com/mapping-word-embeddings-with-word2vec-99a799dc9695/", "webpage": {"metadata": {"title": "Mapping Word Embeddings with Word2vec | by Sam Liebman | Towards Data Science", "h1": "Mapping Word Embeddings with Word2vec", "description": "Natural Language Processing (NLP) is an area of artificial intelligence focused on allowing computers to understand, process, and analyze human language. NLP is widely used in the tech industry\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1310.4546.pdf", "anchor_text": "Tomas Mikolov", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "TensorFlow tutorial", "paragraph_index": 5}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "t-SNE dimensionality reduction technique", "paragraph_index": 5}, {"url": "https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/examples/tutorials/word2vec/word2vec_basic.py", "anchor_text": "sample code", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1607.06520", "anchor_text": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "paragraph_index": 12}, {"url": "https://translate.google.com/#tr/en/o%20bir%20asker%0Ao%20bir%20%C3%B6%C4%9Fretmen%0Ao%20bir%20doktor%0Ao%20bir%20hem%C5%9Fire%0A%0Ao%20bir%20yazar%0Ao%20bir%20kopek%0Ao%20bir%20dad%C4%B1%0Ao%20bir%20kedi%0A%0Ao%20bir%20ba%C5%9Fkan%0Ao%20bir%20giri%C5%9Fimci%0Ao%20bir%20%C5%9Eark%C4%B1c%C4%B1%0Ao%20bir%20%C3%96%C4%9Frenci%0Ao%20bir%20Terc%C3%BCman%0A%0Ao%20%C3%A7al%C4%B1%C5%9Fkan%0Ao%20tembel%0A%0Ao%20bir%20ressam%0Ao%20bir%20kuaf%C3%B6r%0Ao%20bir%20garson%0Ao%20bir%20m%C3%BChendis%0Ao%20bir%20mimar%0Ao%20bir%20Sanat%C3%A7%C4%B1%0Ao%20bir%20sekreter%0Ao%20bir%20di%C5%9F%C3%A7i%0Ao%20bir%20%C3%A7i%C3%A7ek%C3%A7i%0Ao%20bir%20muhasebeci%0Ao%20bir%20f%C4%B1r%C4%B1nc%C4%B1%0Ao%20bir%20Avukat%0Ao%20bir%20dans%C3%B6z%0Ao%20bir%20polis%0A%0Ao%20g%C3%BCzel%0Ao%20%C3%A7ok%20g%C3%BCzel%0Ao%20%C3%A7irkin%0Ao%20k%C3%BC%C3%A7%C3%BCk%0Ao%20ya%C5%9Fl%C4%B1%0A%0Ao%20kuvvetli%0Ao%20zay%C4%B1f%0Ao%20k%C3%B6t%C3%BCmser%0Ao%20iyimser", "anchor_text": "More examples", "paragraph_index": 12}], "all_paragraphs": ["Natural Language Processing (NLP) is an area of artificial intelligence focused on allowing computers to understand, process, and analyze human language. NLP is widely used in the tech industry, serving as a backbone to search engines, spam filters, language translation and much more. NLP enables computers to transform human language into a form that it can read and understand, such as a vector or discrete symbol. For example, NLP can take in the sentence So hungry, need food and break it down into four arbitrary symbols: so represented as K45, hungry as J83, need as Q67, and food as P21, all of which can then be processed by the computer. Each unique word is represented by a different symbol; however, the downside is that there is no apparent relationship between the symbols designated to hungry and food. This hinders the NLP model from using what it learned about hungry and applying it to food, which are semantically related. Vector Space Models (VSM) help address this issue by embedding the words in a vector space where similarly defined words are mapped near each other. This space is called a Word Embedding.", "Word2vec, a brainchild of a team of researchers led by Google\u2019s Tomas Mikolov, is one of the most popular models used to create word embeddings. Word2vec has two primary methods of contextualizing words: the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model, which i will summarize in this post. Both models arrive at a similar conclusion, but take nearly inverse paths to get there.", "CBOW, which is the less popular of the two models, uses source words to predict the target words. For example, take the sentence I want to learn python. In this instance, the target word is python, while the source words are I want to learn. CBOW is primarily used in smaller datasets, since it treats the context of the sentence as a single observation towards predicting the target word. In practice, this becomes very inefficient when working with a large set of words.", "The Skip-Gram model works in the opposite fashion of the CBOW model, using target words to predict the source, or context, of the surrounding words. Consider the sentence the quick brown fox jumped over the lazy dog and suppose we use a simple definition for the context of a given word as the words immediately preceding and following it. The Skip-Gram model will break the sentence into (context, target) pairs, resulting in a set of pairs in the following format:", "These pairs are winnowed further into (input, output) pairs, representing each word (input) with the word either directly before or after it. This is necessary because the Skip-Gram model works by using the target words (inputs) to predict the context, or output. These pairs are represented as follows:", "Now that each word is able to be represented in context, the fun begins. I won\u2019t go into the math \u2014 this TensorFlow tutorial provides an in depth explanation \u2014 but the loss function for predicting each word given context can be optimized using stochastic gradient descent and iterating through every pair in the dataset. From there, the vectors can be reduced to two dimensions using the t-SNE dimensionality reduction technique.", "Once the word vectors are reduced to two-dimensions, it is possible to see relationships between certain words. Examples of a semantic relationship are male/female designations and Country/Capital relationships, while an example of syntactic relationship is past vs. present tense. The diagram below does an excellent job of visualizing these relationships:", "Words that share semantic or syntactic relationships will be represented by vectors of similar magnitude and be mapped in close proximity to each other in the word embredding. No longer is the case of king being represented with a arbitrary discrete symbol K93 and queen with S83. Instead, the relationship between king and queen is more apparent \u2014 in fact, it is exactly the same as the relationship between the vectors for man and woman, respectively. This allows for extremely cool and magically simple arithmetic to be performed on vectors. For example: if you subtract the vector for boy from the vector representation of brother, and then add the vector for girl, you will get sister.", "This opens up an entirely new dimension of possibilities for finding patterns or other insights in the data.", "Using a sample code showcased in TensorFlow\u2019s tutorial, I will demonstrate how word2vec works in practice. TensorFlow is a machine learning library developed by the Google Brain team for internal use, open-sourced to the public in 2015 in an effort to accelerate the growth of AI. TensorFlow is a powerful tool for deep learning, logistic regression, and reinforcement learning, and has grown popular due to its ability to optimize computational efficiency when training large datasets.", "The example code provided reads a large dataset of 50,000 words, and trains a skip-gram model to vectorize the words in a contextual manner. It then iterates through the data 100,000 times in order to optimize the loss function for a randomized batch of popular words in the dataset. At first, the nearest neighbors for the popular words don\u2019t show any syntactic or semantic relationship with each of the words. After 100,000 steps, a much clearer relationship is visible, and the loss function decreased by over 98%.", "As seen above, before the skip-gram model was trained, the eight nearest neighbors for the word three were: bissau, zeal, chong, salting, cooperate, quarterfinals, legislatures, ample. By the time the iterations are complete, the nearest neighbors for three are: five, four, seven, six, two, eight, nine, agouti. While not perfect \u2014 last I checked, agouti is a tropical American rodent, not a number \u2014 7 of the 8 nearest neighbors displayed a clear semantic relationship to the word three.", "While word2vec can create word embeddings that illustrate semantic and syntactic relationships between words, the model is not without some flaws. A 2016 study entitled Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi, Chang, Zou, Saligrama, Kalai) demonstrated how word embeddings used by Google reinforced gender stereotypes at an alarming rate, and identified potential fixes to the issue. Data scientist and entrepreneur Emre \u015earbak used Google Translate to further emphasize the gender biases exhibited by the word embedding algorithms. \u015earbak, who is fluent in Turkish, tested how Google would translate sentences from Turkish, which uses gender neutral pronouns, to English. The results were a mix between fascinating and disturbing. (More examples)", "For the most part, when a sentence contained descriptors stereotypically attributed to women (cook, teacher, nurse), the Turkish gender-neutral pronoun o was translated to she. Conversely, sentences with terms such as hard working, lawyer, and engineer saw the pronoun translated to male form. Google is not solely to blame for this \u2014 its algorithms are based off a corpus of human words containing billions of data points, so Google is merely reflecting already existing biases. Yet, Google is still determining what millions of people see when they use Translate (or Search, YouTube and any other popular Google platform).", "Ultimately, this is most likely an unintended negative consequence of a powerful tool, but it raises an important issue about how easily we let computers and artificial intelligence dictate what we think and see. NLP and word embeddings are essential tools and are arguably the future of artificial intelligence; however, an open dialogue about exactly how machine learning algorithms are making their decisions is important so that marginalized voices don\u2019t get shut out.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist and Machine Learning Engineer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F99a799dc9695&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----99a799dc9695--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@slieb74?source=post_page-----99a799dc9695--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slieb74?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Sam Liebman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1d6afd42433f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&user=Sam+Liebman&userId=1d6afd42433f&source=post_page-1d6afd42433f----99a799dc9695---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.adityathakker.com/introduction-to-word2vec-how-it-works/", "anchor_text": "https://www.adityathakker.com/introduction-to-word2vec-how-it-works/"}, {"url": "https://arxiv.org/pdf/1310.4546.pdf", "anchor_text": "Tomas Mikolov"}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "TensorFlow tutorial"}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "t-SNE dimensionality reduction technique"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "https://www.tensorflow.org/tutorials/representation/word2vec"}, {"url": "https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/examples/tutorials/word2vec/word2vec_basic.py", "anchor_text": "sample code"}, {"url": "https://arxiv.org/abs/1607.06520", "anchor_text": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"}, {"url": "https://translate.google.com/#tr/en/o%20bir%20asker%0Ao%20bir%20%C3%B6%C4%9Fretmen%0Ao%20bir%20doktor%0Ao%20bir%20hem%C5%9Fire%0A%0Ao%20bir%20yazar%0Ao%20bir%20kopek%0Ao%20bir%20dad%C4%B1%0Ao%20bir%20kedi%0A%0Ao%20bir%20ba%C5%9Fkan%0Ao%20bir%20giri%C5%9Fimci%0Ao%20bir%20%C5%9Eark%C4%B1c%C4%B1%0Ao%20bir%20%C3%96%C4%9Frenci%0Ao%20bir%20Terc%C3%BCman%0A%0Ao%20%C3%A7al%C4%B1%C5%9Fkan%0Ao%20tembel%0A%0Ao%20bir%20ressam%0Ao%20bir%20kuaf%C3%B6r%0Ao%20bir%20garson%0Ao%20bir%20m%C3%BChendis%0Ao%20bir%20mimar%0Ao%20bir%20Sanat%C3%A7%C4%B1%0Ao%20bir%20sekreter%0Ao%20bir%20di%C5%9F%C3%A7i%0Ao%20bir%20%C3%A7i%C3%A7ek%C3%A7i%0Ao%20bir%20muhasebeci%0Ao%20bir%20f%C4%B1r%C4%B1nc%C4%B1%0Ao%20bir%20Avukat%0Ao%20bir%20dans%C3%B6z%0Ao%20bir%20polis%0A%0Ao%20g%C3%BCzel%0Ao%20%C3%A7ok%20g%C3%BCzel%0Ao%20%C3%A7irkin%0Ao%20k%C3%BC%C3%A7%C3%BCk%0Ao%20ya%C5%9Fl%C4%B1%0A%0Ao%20kuvvetli%0Ao%20zay%C4%B1f%0Ao%20k%C3%B6t%C3%BCmser%0Ao%20iyimser", "anchor_text": "More examples"}, {"url": "https://www.facebook.com/photo.php?fbid=10154851496086949&set=a.10150241543551949&type=3&theater", "anchor_text": "https://www.facebook.com/photo.php?fbid=10154851496086949&set=a.10150241543551949&type=3&theater"}, {"url": "https://www.tensorflow.org/tutorials/representation/word2vec", "anchor_text": "TensorFlow Word2vec Tutorial"}, {"url": "https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b", "anchor_text": "Word2Vec (skip-gram model): PART 1 \u2014 Intuition"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "Word2Vec Tutorial \u2014 The Skip-Gram Model"}, {"url": "https://github.com/tensorflow/tensorflow/tree/r1.10", "anchor_text": "TensorFlow Github"}, {"url": "https://medium.com/tag/nlp?source=post_page-----99a799dc9695---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----99a799dc9695---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/data-science?source=post_page-----99a799dc9695---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----99a799dc9695---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----99a799dc9695---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&user=Sam+Liebman&userId=1d6afd42433f&source=-----99a799dc9695---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&user=Sam+Liebman&userId=1d6afd42433f&source=-----99a799dc9695---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F99a799dc9695&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----99a799dc9695---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----99a799dc9695--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----99a799dc9695--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----99a799dc9695--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----99a799dc9695--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----99a799dc9695--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slieb74?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slieb74?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sam Liebman"}, {"url": "https://medium.com/@slieb74/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "185 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1d6afd42433f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&user=Sam+Liebman&userId=1d6afd42433f&source=post_page-1d6afd42433f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1d6afd42433f%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmapping-word-embeddings-with-word2vec-99a799dc9695&user=Sam+Liebman&userId=1d6afd42433f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}