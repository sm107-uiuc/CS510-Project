{"url": "https://towardsdatascience.com/5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b", "time": 1683004273.623938, "path": "towardsdatascience.com/5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b/", "webpage": {"metadata": {"title": "5 Secrets About LSTM and GRU Everyone Else Knows | by Michel Kana, Ph.D | Towards Data Science", "h1": "5 Secrets About LSTM and GRU Everyone Else Knows", "description": "We explain how LSTM works and why it has been so effective and popular for processing sequence data for Apple, Google, Facebook, Amazon."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/recurrent-neural-networks-explained-ffb9f94c5e09", "anchor_text": "recurrent neural networks (RNNs)", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a", "anchor_text": "sentiment analysis", "paragraph_index": 2}, {"url": "https://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Long Short Term Memory (LSTM) networks", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1406.1078.pdf", "anchor_text": "2014 paper", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "this article", "paragraph_index": 52}, {"url": "http://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/", "anchor_text": "this article", "paragraph_index": 52}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "this blog", "paragraph_index": 54}, {"url": "https://github.com/brightmart/text_classification", "anchor_text": "this GitHub repository", "paragraph_index": 55}], "all_paragraphs": ["We explain why Long Short Term Memory (LSTM) has been so effective and popular for processing sequence data for Apple, Google, Facebook, Amazon.", "Secret 1 \u2014 LSTMs greatly improved speech recognition on over 4 billion Android phones (since mid 2015).", "Previously, we introduced recurrent neural networks (RNNs) and demonstrated how they can be used for sentiment analysis.", "The issue with RNNs is long range memory. For example, they are able to predict the next word \u201csky\u201d in the sentence \u201cthe clouds are in the \u2026\u201d But they come short in predicting the missing word in the following sentence:", "\u201cShe grew up in France. Now she has been in China for few months only. She speaks fluent \u2026\u201d", "As that gap grows, RNNs become unable to learn to connect the information. In this example, recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. In natural language text, it is entirely possible for the gap between the relevant information and the point where it is needed to be very large. This is also very common in the German language.", "Why do RNNs have huge problems with long sequences? By design, RNNs take two inputs at each time step: an input vector (e.g. one word from the input sentence), and a hidden state (e.g. a memory representation from previous words).", "The next RNN step takes the second input vector and first hidden state to create the output of that time step. Therefore, in order to capture semantic meanings in long sequences, we need to run RNNs over many time steps, turning the unrolled RNN into a very deep network.", "Long sequences are not the only troublemakers for RNNs. Just like any very deep neural network, RNNs suffers from the vanishing and exploding gradients problem, thus taking forever to train. Many techniques have been suggested to alleviate this problem, but they could not eliminate it:", "These workarounds have their limits, still. Additionally, besides the long training time, another problem faced by long-running RNNs is the fact that the memory of the first inputs gradually fades away.", "After a while, the RNN\u2019s state contains virtually no trace of the first inputs. For example, if we want to perform sentiment analysis on a long review that starts with \u201cI loved this product,\u201d but the rest of the review lists the many things that could have made the product even better, then, the RNN will gradually forget the first positive sentiment and will completely misinterpret the review as negative.", "In order to solve these RNNs problems, various types of cells with long term memory have been introduced in research. In practice, basic RNNs are not used anymore and most of work is done using the so-called Long Short Term Memory (LSTM) networks. They were invented by S. Hochreiter and J. Schmidhuber.", "Secret 2 \u2014 LSTMs greatly improved machine translation through Google Translate since Nov 2016.", "Each single LSTM cell governs what to remember, what to forget and how to update the memory using gates. By doing so, the LSTM network solves the problem of exploding or vanishing gradients, as well as all other problems mentioned previously!", "The architecture of a LSTM cell is depicted in the impressive diagram below.", "h is the hidden state, representing short term memory. C is the cell state, representing long term memory and x is the input.", "The gates perform only few matrices transformations, sigmoid and tanh activation in order to magically solve all the RNN problems.", "We will dive into how this happens in the next sections, by looking at how the cell forgets, remembers and updates its memory.", "Let\u2019s explore the diagram within a funny plot. Assume that you are the boss, and your employee asks for salary increase. Will you agree? Well, this will depend, let\u2019s say, on your state of mind.", "Below we consider your mind as a LSTM cell, with no mean to offense your lightning brain.", "Your long term state C will impact your decision. On average, 70% of time you are in good mood and you have 30% of total budget left. Therefore your cell state is C=[0.7, 0.3].", "Recently, things are really going well for you, boosting your good mood with probability 100% and you have operative budget left with high probability 100%. This turns your hidden state to h=[1, 1].", "Today, three things happened: your kids succeeded at school exams, although you got an ugly review from your boss, however you figured out that you still have plenty of time to complete the work. So, today\u2019s input is x=[1, -1, 1].", "Based on this evaluation, will you give a salary increase to your employee?", "Secret 3\u2014 Facebook performs over 4 billion LSTM based translations per day.", "In the situation described above, your first step will be probably to figure out how things which happened today (input x) and things which happened recently (hidden state h) will affect your long-term view of the situation (cell state C). Forget Gates control how much of the past memory is kept.", "After receiving your employee\u2019s request for salary increase, your forget gate will run the following calculation of f_t, whose value will ultimately affect your long-term memory.", "The weights shown in the picture below are chosen arbitrary for illustration purposes. Their values are normally calculated during training of the network. The result [0,0] indicates to erase (forget completely) your long term memory and not let it affect your decision today.", "Secret 4 \u2014 Siri is LSTM-based on almost 2 billion iPhones since 2016.", "Next, you need to decide which information about what happened recently (hidden state h) and what happened today (input x) you want to record in your long-term view of the situation (cell state C). LSTM decides what to remember by using Input Gates.", "First, you will calculate your input gate values i_t, which falls between 0 and 1 thanks to sigmoid activation.", "Next, you will scale your input between -1 and 1 using tanh activation.", "Finally, you will estimate your new cell state by adding both results.", "The result [1, 1] indicates that based on the recent and current information, you are 100% in good mood and very likely to have operative budget. This are looking promising for your employee.", "Secret 5 \u2014 The answers of Amazon\u2019s Alexa are based on LSTMs.", "Now, you know how things which recently happened would affect your state. Next, it is time to update your long-term view of the situation based on the new rationales.", "When new values come in, LSTM decides on how to update its memory, again by using gates. The gated new values are added to the current memory. This additive operation is what solves the exploding or vanishing gradients problem of simple RNNs.", "Instead of multiplying, LSTM adds things to compute the new state. The result C_t is stored as the new long-term view of the situation (cell state).", "The values [1,1] suggest that you are overall 100% of the time in a good mood and 100% likelihood to have money all the time! You are the perfect boss!", "Based on this information, you can update your short-term view of the situation h_t (next hidden state). The values [0.9, 0.9] indicate that there is 90% likelihood that you will increase your employee\u2019s salary in the next time step! Congratulations to him!", "A variant of the LSTM cell is called the Gated Recurrent Unit, or GRU. GRU was proposed by Kyunghyun Cho et al. in a 2014 paper.", "GRU is a simplified version of the LSTM cell, can be a bit faster than LSTM, and it seems to perform similarly, which explains its growing popularity.", "As shown above, both state vectors are merged into a single vector. A single gate controller controls both the forget gate and the input gate. If the gate controller outputs a 1, the input gate is open and the forget gate is closed. If it outputs a 0, the opposite happens. In other words, whenever a memory must be stored, the location where it will be stored is erased first.", "There is no output gate; the full state vector is output at every time step. However, there is a new gate controller that controls which part of the previous state will be shown to the main layer.", "By aligning multiple LSTM cells, we can process input of sequence data, for example, a 4-words sentence in the picture below.", "LSTM units are typically arranged in layers, so that each the output of each unit is the input to the other units. In the example, we have 2 layers, each having 4 cells. In this way, the network becomes richer and captures more dependencies.", "RNNs, LSTMs and GRUs are designed to analyze sequence of values. Sometimes it makes sense to analyze the sequence in a reverse order.", "For example in the sentence \u201che needs to work harder, the boss said about the employee.\u201d, although the \u201che\u201d appears at the very beginning, it refers to the employee, mentioned at the very end.", "Therefore the order has to be reversed or by combining forward and backward. This bidirectional architecture is depicted in the figure below.", "The following diagram further illustrates bidirectional LSTMs. The network in the bottom receives the sequence in the original order, while the network in the top takes receives the same input but in reverse order. Both networks are not necessarily identical. Important is, their outputs are combined for the final prediction.", "As we have just explained, a LSTM cell can learn to recognize an important input (that\u2019s the role of the input gate), store it in the long term state, learn to preserve it for as long as it is needed (that\u2019s the role of the forget gate), and learn to extract it whenever it is needed.", "LSTMs have transformed machine learning and are now available to billions of users through the world\u2019s most valuable public companies like Google, Amazon and Facebook.", "If you wish to know even more about LSTMs and GRUs, check this article with amazing animations by Michael Nguyen. For those who prefer to build their own LSTM from scratch, this article might work.", "Practical implementations of LSTM networks in Python are available in my article below.", "Attention-based sequence-to-sequence models and Transformers go beyond LSTMs and have amazed folks recently with their impressive results in machine translation at Google and text generation at OpenAI. You might want to check this blog or my article below to learn more.", "A comprehensive implementation of text classification using BERT, FastText, TextCNN, Transformer, Se2seq, etc. can be found on this GitHub repository or you can check my tutorial about BERT.", "Thanks to Anne Bonner from Towards Data Science for editorial notes.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F97446d89e35b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michel-kana.medium.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----97446d89e35b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/trees-forest-tribe-wood-dead-wood-3948376/", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-explained-ffb9f94c5e09", "anchor_text": "recurrent neural networks (RNNs)"}, {"url": "https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a", "anchor_text": "sentiment analysis"}, {"url": "https://engineering.fb.com/core-data/using-apache-spark-for-large-scale-language-model-training/", "anchor_text": "FB Engineering"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-explained-ffb9f94c5e09", "anchor_text": "Recurrent Neural Networks (RNNs) for DummiesAn entertaining and illustrated guide to understand the intuition.towardsdatascience.com"}, {"url": "https://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Long Short Term Memory (LSTM) networks"}, {"url": "https://arxiv.org/pdf/1406.1078.pdf", "anchor_text": "2014 paper"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "this article"}, {"url": "http://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a", "anchor_text": "Sentiment Analysis: a benchmarkRecurrent neural networks explained. Classifying Customer Reviews using FCNNs, CNNs, RNNs and Embeddings.towardsdatascience.com"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "this blog"}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "Practical guide to attention mechanism for NLU tasksTested hands-on strategies to tackle attention for improving sequence to sequence modelstowardsdatascience.com"}, {"url": "https://github.com/brightmart/text_classification", "anchor_text": "this GitHub repository"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----97446d89e35b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----97446d89e35b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----97446d89e35b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----97446d89e35b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----97446d89e35b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----97446d89e35b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----97446d89e35b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F97446d89e35b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----97446d89e35b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----97446d89e35b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----97446d89e35b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----97446d89e35b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----97446d89e35b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----97446d89e35b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-secrets-about-lstm-and-gru-everyone-else-know-97446d89e35b&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}