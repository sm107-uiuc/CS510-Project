{"url": "https://towardsdatascience.com/understanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700", "time": 1682993492.895433, "path": "towardsdatascience.com/understanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700/", "webpage": {"metadata": {"title": "Understanding basic machine learning with Python \u2014 Perceptrons and Artificial Neurons | by Ben Fraser | Towards Data Science", "h1": "Understanding basic machine learning with Python \u2014 Perceptrons and Artificial Neurons", "description": "If you\u2019re learning machine learning, it's likely your goal is to apply cutting-edge algorithms and \u2018Deep Learning\u2019 to create powerful applications. With the wide availability of intuitive API\u2019s, this\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["If you\u2019re learning machine learning, it's likely your goal is to apply cutting-edge algorithms and \u2018Deep Learning\u2019 to create powerful applications. With the wide availability of intuitive API\u2019s, this is achievable with minimal understanding of what\u2019s going on or how the deep underlying layers actually work.", "However, before doing this a basic knowledge of shallow architectures and insights into what's under the hood is beneficial. Without this, it will become difficult to build models that deviate from blindly followed tutorials (Monkey See Monkey Do Syndrome).", "In this post, we\u2019ll cover the starting points for Artificial Neurons and classification, including the Perceptron and Adaline models. For each model, we\u2019ll talk through the main components, followed by combining them into full Python class implementations.", "The most fundamental starting point for machine learning is the Artificial Neuron. The first model of a simplified brain cell was published in 1943 and is known as the McCullock-Pitts (MCP) neuron. Essentially, this is a basic logic gate with binary outputs (\u20180\u2019 or \u20181\u2019). A \u20181\u2019 is produced if the sum of inputs arriving at the neuron exceed a given threshold, where each input is multiplied by a corresponding weight coefficient to produce this sum. Thus, the firing of a neuron depends largely on the set of inputs and the corresponding weight coefficient values.", "Frank Rosenblatt took the basic MCP neuron concept soon after its publication and produced the Perceptron Rule Algorithm. This allows automatic learning of the optimal weight coefficients for a neuron model. Through such a model, we can make predictions on whether a given input belongs to one class or another (binary classification).", "There may be any number of inputs to the perceptron model, which are also known as features. Since we usually have multiple features, it helps to represent our inputs and weights as vectors and matrices. Don\u2019t confuse input samples with input features; Each sample of data has a number of features, n, which are fed through our model and processed one at a time (sequentially). For example, if we have 100 samples of car data, where each sample has m features (representing things like top-speed, bhp, cost, etc.), then each sample of data, x, will have a vector of m features. Each input feature has a corresponding weight, which after training will become optimised for the data we have trained it on.", "For each data sample, our input features and weights are taken in as a weighted sum at the perceptron, which is commonly known as the net input function, z. The benefit of using vectors for x and w above is that we can use linear algebra to compactly represent this , like so:", "We transpose the weight vector, w, followed by vector multiplication with our input vector, x, which forms the sum that we require. It should also be noted that we also use a bias term to allow a threshold in our perceptron model, which is equal to the first weight term in our vector. To facilitate this, the first x term in our input matrix is always set to 1. This means we need to add in the bias term to our input vector before firing it into the model.", "If you\u2019re completely new to this type of mathematical notation, I\u2019d highly recommend some introductory linear algebra. Almost everything you\u2019ll cover in further machine learning will be heavily linear algebra based.", "After the net input summation, our value is passed to the perceptron threshold, or decision function. In this case the function is a unit step function. This outputs \u20181\u2019 if its input is above a given value, and \u20180\u2019 if not.", "So how does our perceptron actually learn from a set of data? The overall perceptron process is as follows:", "The cost function for a Perceptron is simplistic and is known as the Perceptron Learning Rule. A small change is made to each weight in our weight vector after each training iteration like so:", "The := notation means that we simultaneously update all weights in the vector of weights at the same time, rather than iteratively. Thus, the output predictions are made, followed by updating all weight parameters as part of one training cycle. The alpha term above is also equal to the learning rate, which determines how quickly our model learns with each training cycle.", "Providing the data we are using for training is linearly separable, the perceptron will update the weights to the optimal values if subjected to enough training cycles (epochs).", "We\u2019ll start by coding each of the components of the Perceptron in Python, and then combine these into one larger Perceptron class at the end. For this basic model, the only Python package we need is numpy and matplotlib.", "For this we\u2019ll generate a simple dataset with two distinct output classes for our perceptron to train on. Note: this random training data is generated through random functions, and is not perfectly reproducible.", "To help appreciate what we\u2019re trying to do, its helpful to visualise our data. In this case, our input samples have only two features, so visualising our data is easy with a scatter plot. The data generation, followed by plotting on a scatter graph, is shown below.", "We want to initialise our weights to small, randomised numbers, rather than having them initialised to zero. We need to do this, otherwise our learning rate will have reduced effect on our classification outcome during training. The justification behind this won\u2019t be covered, however it is because the learning rate only affects the scale of the weight vector, not the direction, if the weights are initialised to zero.", "These weights may be generated in different ways. One such example using numpy.random is:", "For the bias term, all we need to do is add a column of 1\u2019s to our input data, X. In numpy this can be achieved using the ones and hstack numpy functions.", "As discussed above, since we are using vectors for our input features for each sample and their associated weights, we can calculate the Net Input sum very easily. Using vectorisation with numpy in Python, dependent on whether we added the bias column of 1\u2019s to our training data, the net input function becomes:", "We want to return 1 if our net input sum is greater than, or equal to, zero, and otherwise return 0.", "Using the learning rule defined previously in Figure 6, we can carry out a full training cycle. This comprises of looping through all training samples and updating the weights based on the perceptron learning rule, like so:", "With the basic components of a Perceptron formed using numpy, we can piece it all together and implement a full Perceptron binary classifier. Using the classifier we can then visualise our errors over training cycles, and (hopefully!) see the number of errors decrease over time as the weights are optimised.", "As can be visualised on the plot of errors against training cycles (epochs), the number of errors we get is reduced over time. After approximately 7 cycles, our model reduces the number of errors to zero. The decision boundary made by our perceptron classifier after training is shown on the right, which effectively separates Golden Eagles from Horned Owls. We could make predictions with new data using our trained model by calling the classifier predict function with a set of new inputs, X.", "A problem with Perceptrons is that they will only optimise the weights correctly if the data provided is perfectly linearly separable. For the data used above, it is linearly separable. However, if we use more realistic data, which is often not separable by a straight line, then our Perceptron classifier will never stop trying to optimise the weights and will continually have errors. This is a huge downside, and one reason why Perceptrons aren\u2019t used. They do however serve as a good introduction to basic neurons and more complex models.", "An improvement on the original perceptron model is Adaline, which adds a Linear Activation Function that is used to optimise weights. With this addition, a continuous Cost Function is used rather than the Unit Step. Adaline is important because it lays the foundations for much more advanced machine learning models.", "The unit step still occurs, as you can see above, however it is only used for output classification (\u20181\u2019 or \u20180\u2019) at the end of the model. The continuous output value of the linear activation model is used for the Cost (or Objective) Function. This indicates as to how poorly (or well) our algorithm is doing on our training data, and it can be minimised through well-known algorithms such as Gradient Descent.", "In this case, we\u2019ll use the Sum of Squared Errors as our cost function, which can be implemented like so:", "To apply Gradient Descent we need to determine the partial derivatives of the cost function with respect to each of our weights. Using differentiation we find that this partial derivative is equal to:", "To optimise our weights we want to take a step in the opposite direction of our gradient, at a rate chosen by our learning rate parameter. The logic behind this is to minimise the cost and eventually arrive at the global minima. So for each weight optimisation we need to apply:", "We can combine this weight optimisation step and our cost calculation step into one training cycle snippet of code like so:", "In addition to the activation function and gradient descent optimisation, we\u2019ll also enhance the performance of our model through Feature Standardisation. This is a common practice in machine learning that makes the optimisation process faster and more efficient for a given number of epochs. For this we shall subtract the mean value of each feature from each associated feature value in the training data, followed by dividing the feature values by their respective standard deviations.", "With the additional components of the Adaline model worked out, we can produce a final implementation in Python like we did with the Perceptron. This Adaline implementation is shown below:", "We can see how important setting our hyper-parameters for our model is from this basic Adaline example in the plots below; If we choose a learning rate of 0.1 the cost tends to infinity and does not converge, whereas if we choose a learning rate of 0.001 our model successfully converges to the global minimum of the cost function. The number of iterations is also important, and should be made high enough to sufficiently lower the cost. These adjustable aspects of machine learning models are known as Hyperparameters, and the optimisation of these are an important aspect for any model we create.", "There are lots of ways this model can be improved further, including Stochastic Gradient Descent, which optimises much more frequently, rather than using the entire batch of training data like the above model uses. We can also extend it to more than just binary classification, using One versus All Muticlass Classification.", "In addition, there are much better classifiers available than Adaline. Adaline is a starting point that you can use as a foundation for understanding more complex classification models, such as Logistic Regression and neural networks.", "Throughout this post we have studied the features of basic neurons, followed by building a basic Rosenblatt Perceptron model using Python and numpy. We then built a working Adaline classifier, which improved upon the Perceptron in many ways through its activation function and optimisation of a cost function. During this, we also applied some valuable feature processing, which including standardisation to make gradient descent faster and more efficient.", "Although these models are no substitute for professionally built and optimised ones available through Sci-kit Learn and other packages, they should have provided understanding into how basic classifiers work. I hope the process we followed has helped your understanding of basic machine learning classification, and provided you with insight into using Python to implement these models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Aerospace and electronic systems engineer. Ministry of Defence, United Kingdom."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdfae8fe61700&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@benjamindavidfraser?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Ben Fraser"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe64b86a0996&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&user=Ben+Fraser&userId=be64b86a0996&source=post_page-be64b86a0996----dfae8fe61700---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dfae8fe61700---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/perceptron?source=post_page-----dfae8fe61700---------------perceptron-----------------", "anchor_text": "Perceptron"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----dfae8fe61700---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----dfae8fe61700---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/coding?source=post_page-----dfae8fe61700---------------coding-----------------", "anchor_text": "Coding"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&user=Ben+Fraser&userId=be64b86a0996&source=-----dfae8fe61700---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&user=Ben+Fraser&userId=be64b86a0996&source=-----dfae8fe61700---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdfae8fe61700&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dfae8fe61700---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dfae8fe61700--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dfae8fe61700--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dfae8fe61700--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@benjamindavidfraser?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ben Fraser"}, {"url": "https://medium.com/@benjamindavidfraser/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "57 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe64b86a0996&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&user=Ben+Fraser&userId=be64b86a0996&source=post_page-be64b86a0996--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8eb7d91aff82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700&newsletterV3=be64b86a0996&newsletterV3Id=8eb7d91aff82&user=Ben+Fraser&userId=be64b86a0996&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}