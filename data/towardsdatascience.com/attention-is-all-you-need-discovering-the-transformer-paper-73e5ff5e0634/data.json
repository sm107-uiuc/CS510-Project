{"url": "https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634", "time": 1683015884.187516, "path": "towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634/", "webpage": {"metadata": {"title": "Attention is all you need: Discovering the Transformer paper | by Eduardo Mu\u00f1oz | Towards Data Science", "h1": "Attention is all you need: Discovering the Transformer paper", "description": "In this post we will describe and demystify the relevant artifacts in the paper \u201cAttention is all you need\u201d (Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "\u201cAttention is all you need\u201d", "paragraph_index": 0}, {"url": "https://medium.com/better-programming/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb", "anchor_text": "\u201cA Guide on the Encoder-Decoder Model and the Attention Mechanism\u201d", "paragraph_index": 1}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "Transformers from scratch by Peter Bloem", "paragraph_index": 11}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "\u201cTransformers from scratch\u201d", "paragraph_index": 21}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "\u201cThe Ilustrated Transformer\u201d", "paragraph_index": 35}, {"url": "https://github.com/edumunozsala/Transformer-NMT", "anchor_text": "Transformer-NMT", "paragraph_index": 49}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "\u201cAttention is all you need\u201d", "paragraph_index": 51}], "all_paragraphs": ["In this post we will describe and demystify the relevant artifacts in the paper \u201cAttention is all you need\u201d (Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017))[1]. This paper was a great advance in the use of the attention mechanism, being the main improvement for a model called Transformer. The most famous current models that are emerging in NLP tasks consist of dozens of transformers or some of their variants, for example, GPT-2 or BERT.", "We will describe the components of this model, analyze their operation and build a simple model that we will apply to a small-scale NMT problem (Neural Machine Translation). To read more about the problem that we will address and to know how the basic attention mechanism works, I recommend you to read my previous post \u201cA Guide on the Encoder-Decoder Model and the Attention Mechanism\u201d.", "In sequence-to-sequence problems such as the neural machine translation, the initial proposals were based on the use of RNNs in an encoder-decoder architecture. These architectures have a great limitation when working with long sequences, their ability to retain information from the first elements was lost when new elements were incorporated into the sequence. In the encoder, the hidden state in every step is associated with a certain word in the input sentence, usually one of the most recent. Therefore, if the decoder only accesses the last hidden state of the decoder, it will lose relevant information about the first elements of the sequence. Then to deal with this limitation, a new concept were introduced the attention mechanism.", "Instead of paying attention to the last state of the encoder as is usually done with RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what attention does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output. Learning in every step to focus in the right element of the input to predict the next output element.", "But this approach continues to have an important limitation, each sequence must be treated one element at a time. Both the encoder and the decoder have to wait till the completion of t-1 steps to process thet-th step. So when dealing with huge corpus it is very time consuming and computationally inefficient.", "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization \u2026 the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.", "\u201cAttention is all you need\u201d paper [1]", "The Transformer model extract features for each word using a self-attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. And no recurrent units are used to obtain this features, they are just weighted sums and activations, so they can be very parallelizable and efficient.", "But we will dive deeper into its architecture (next figure) to understand what all this pieces do [1].", "We can observe there is an encoder model on the left side and the decoder on the right one. Both contains a core block of \u201can attention and a feed-forward network\u201d repeated N times. But first we need to explore a core concept in depth: the self-attention mechanism.", "Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let\u2019s call the input vectors x1, x2,\u2026, xt and the corresponding output vectors y1, y2,\u2026, yt. The vectors all have dimension k. To produce output vector yi, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product.", "Transformers from scratch by Peter Bloem [2]", "In the self-attention mechanism of our model we need to introduce three elements: Queries, Values and Keys", "Every input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. In every role, it is compared to the other vectors to get its own output yi(Query), to get the j-th output yj(Key) and to compute each output vector once the weights have been established (Value).", "To obtain this roles, we need three weight matrices of dimensions k x k and compute three linear transformation for each xi:", "These three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a \u201cself-attention\u201d.", "The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot product of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.", "\u201cAttention is all you need\u201d paper [1]", "Then we use the Q, K and V matrices to calculate the attention scores. The scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position. That is, the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So, for position 1 we calculate the dot product (.) of q1and k1, then q1. k2, q1. k3 and so on,\u2026", "Next we apply the \u201cscaled\u201d factor to have more stable gradients. The softmax function can not work properly with large values, resulting in vanishing the gradients and slowing down the learning, [2]. After \u201csoftmaxing\u201d we multiply by the Value matrix to keep the values of the words we want to focus on and minimizing or removing the values for the irrelevant words (its value in V matrix should be very small).", "The formula for these operations is:", "In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, we would like to attend to different segments of the words. \u201cWe can give the self attention greater power of discrimination, by combining several self attention heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.\u201d, [2] Peter Bloem, \u201cTransformers from scratch\u201d. This produce h different output matrices of scores.", "But the next layer (the Feed-Forward layer) is expecting just one matrix, a vector for each word, so \u201cafter calculating the dot product of every head, we concatenate the output matrices and multiply them by an additional weights matrix Wo,\u201d[3]. This final matrix captures information from all the attention heads.", "We mentioned briefly that the order of the words in the sentence is an issue to solve in this model, because the network and the self-attention mechanism is permutation invariant. If we shuffle up the words in the input sentence, we get the same solutions. We need to create a representation of the position of the word in the sentence and add it to the word embedding.", "To this end, we add \u201cpositional encodings\u201d to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, so that the two can be summed. There are many choices of positional encodings.", "\u201cAttention is all you need\u201d paper", "So, we apply a function to map the position in the sentence to a real valued vector. The network will learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. \u201cIt would requiere sentences of all accepted positions during the training loop but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered during training\u201d, [2].", "In the paper a sinusoidal function is applied:", "Now that all the main pieces of the model have been described we can introduce the encoder components, [4]:", "Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.", "Peter Bloem, \u201cTransformers from scratch\u201d [2]", "First we implement the encoder layer, each one of the six blocks, contained in an encoder:", "The next figure will show the components detailed:", "Keep in mind that only the vector from the last layer (6-th) is sent to the decoder.", "The decoder share some components with the encoder but they are used in a different way to take into account the encoder output, [4]:", "At the end of the N stacked decoders, the linear layer, a fully-connected network, transforms the stacked outputs to a much larger vector, the logits. \u201cThe softmax layer then turns those scores (logits) into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step\u201d, [3] Jay Alammar, \u201cThe Ilustrated Transformer\u201d .", "Once we have defined our components and created the encoder, the decoder and the linear-softmax final layer, we join the pieces to form our model, the Transformer.", "It is worth mentioning that we create 3 masks, each of which will allow us:", "As you can see, then we call the encoder, the decoder and the final linear-softmax layer to get the predicted output from our Transformer model.", "Now that we have described in detail the components in the paper, we are ready to implement them and train a transformer model on a NMT problem. It is a toy problem for educational purposes.", "We won\u2019t deal with the data wrangling in this blog post. Follow the link I mentioned in the introduction for more information and the code provided to see how the data is loaded and prepared. In summary, create the vocabulary, tokenize (including a eos and sos token) and pad the sentences. Then we create a Dataset, a batch data generator, for training on batches.", "We need to create a custom loss function to mask the padding tokens.", "We use an Adam optimizer described in the paper, with beta1=0.9, beta2=0.98 and epsilon=10e-9 . And then we create a scheduler to vary the learning rate over the training process according to:", "The train function is similar to many other Tensorflow trainings, an usual training loop for sequence-to-sequence tasks:", "And that\u2019s all, we have all the necessary elements to train our model, we just need to create them and call the train function:", "When training a ML model we are not only interested in optimize losses or accuracies, we want our model to make good enough predictions and, in this case, see how the model works with new sentences. The predict function will input a tokenized sentence to the model and return the predicted new sentence, in our example, a translation from English to Spanish.", "These are the steps in that process:", "And finally our last function receives a sentence in English, calls the transformer to translate it to Spanish and shows the result.", "For this example, we just experiment with some values for the model dimension and the units of the feedforward network to train the model for an hour. If you want to optimize the model, you should probably train it for longer and with many different values for the hyperparameters.", "The code is available in my github repository \u201cTransformer-NMT\u201d. The code is partially extracted from an excellent course by SuperDataScience Team called \u201cModern Natural Language Processing in Python\u201d on Udemy. I highly recommend it.", "I hope you enjoy experimenting with the Transformer model. In future post we will deal with another NLP tasks.", "[1] Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia, \u201cAttention is all you need\u201d , 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An experienced software engineer, a machine learning practitioner and enthusiastic data scientist. Learning every day."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F73e5ff5e0634&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://edumunozsala.medium.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": ""}, {"url": "https://edumunozsala.medium.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Eduardo Mu\u00f1oz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F752e02c1dc91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&user=Eduardo+Mu%C3%B1oz&userId=752e02c1dc91&source=post_page-752e02c1dc91----73e5ff5e0634---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://medium.com/better-programming/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb", "anchor_text": "\u201cA Guide on the Encoder-Decoder Model and the Attention Mechanism\u201d"}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "Transformers from scratch by Peter Bloem"}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "\u201cTransformers from scratch\u201d"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "\u201cThe Ilustrated Transformer\u201d"}, {"url": "https://pixabay.com/es/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2167835", "anchor_text": "Gerd Altmann"}, {"url": "https://pixabay.com/es/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2167835", "anchor_text": "Pixabay"}, {"url": "https://unsplash.com/@korpa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Jr Korpa"}, {"url": "https://unsplash.com/s/photos/language-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/edumunozsala/Transformer-NMT", "anchor_text": "Transformer-NMT"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "\u201cTransformers from scratch\u201d"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "\u201cThe Ilustrated Transformer\u201d"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "\u201cAttention? Attention!!\u201d"}, {"url": "https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html", "anchor_text": "\u201cAttention is all you need\u2019s review\u201d"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "\u201cThe Annotated Transformer\u201d"}, {"url": "https://medium.com/tag/nlp?source=post_page-----73e5ff5e0634---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----73e5ff5e0634---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----73e5ff5e0634---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----73e5ff5e0634---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----73e5ff5e0634---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&user=Eduardo+Mu%C3%B1oz&userId=752e02c1dc91&source=-----73e5ff5e0634---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&user=Eduardo+Mu%C3%B1oz&userId=752e02c1dc91&source=-----73e5ff5e0634---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F73e5ff5e0634&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----73e5ff5e0634---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----73e5ff5e0634--------------------------------", "anchor_text": ""}, {"url": "https://edumunozsala.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://edumunozsala.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eduardo Mu\u00f1oz"}, {"url": "https://edumunozsala.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F752e02c1dc91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&user=Eduardo+Mu%C3%B1oz&userId=752e02c1dc91&source=post_page-752e02c1dc91--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6144dc7fd7a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&newsletterV3=752e02c1dc91&newsletterV3Id=6144dc7fd7a2&user=Eduardo+Mu%C3%B1oz&userId=752e02c1dc91&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}