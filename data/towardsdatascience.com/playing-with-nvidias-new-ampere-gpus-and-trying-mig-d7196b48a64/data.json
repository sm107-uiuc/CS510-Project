{"url": "https://towardsdatascience.com/playing-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64", "time": 1683017339.558814, "path": "towardsdatascience.com/playing-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64/", "webpage": {"metadata": {"title": "Playing with Nvidia\u2019s New Ampere GPUs and Trying MIG | by Alexander Veysov | Towards Data Science", "h1": "Playing with Nvidia\u2019s New Ampere GPUs and Trying MIG", "description": "Every time when the essential question arises, whether to upgrade the cards in the server room or not, I look through similar articles and watch such videos. Channel with the aforementioned video is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/", "anchor_text": "similar", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=deqljDr618c&t=1100s", "anchor_text": "videos", "paragraph_index": 0}, {"url": "https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices", "anchor_text": "TF32", "paragraph_index": 3}, {"url": "https://t.me/snakers4/2590", "anchor_text": "https://t.me/snakers4/2590", "paragraph_index": 18}, {"url": "https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf", "anchor_text": "MPS", "paragraph_index": 19}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#lgi", "anchor_text": "MIG technology", "paragraph_index": 24}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts", "anchor_text": "documentation", "paragraph_index": 27}, {"url": "https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/", "anchor_text": "post", "paragraph_index": 27}], "all_paragraphs": ["Every time when the essential question arises, whether to upgrade the cards in the server room or not, I look through similar articles and watch such videos.", "Channel with the aforementioned video is very underestimated, but the author does not deal with ML. In general, when analyzing comparisons of accelerators for ML, several things usually catch your eye:", "The answer to the question \u201cwhich card is better?\u201d is not rocket science: Cards of the 20* series didn\u2019t get much popularity, while the 1080 Ti from Avito (Russian craigslist) still are very attractive (and, oddly enough, don\u2019t get cheaper, probably for this reason).", "All this is fine and dandy and the standard benchmarks are unlikely to lie too much, but recently I learned about the existence of Multi-Instance-GPU technology for A100 video cards and native support for TF32 for Ampere devices and I got the idea to share my experience of the real testing cards on the Ampere architecture (3090 and A100). In this short note, I will try to answer the questions:", "Let\u2019s immediately address the elephant in the room. At the time of this writing:", "For obvious reasons, the holy war \u2014 to bow to the AWS religion \u2014 is omitted here.", "According to the utilities from Nvidia, the 3090 and A100 are 15\u201320 degrees cooler than Maxwell and Pascal. I did not take accurate measurements, but on average the situation is like this:", "There are 3 hypotheses why this is the case:", "A clear illustration of the differences between the cards, could someone from the comments tell the diameter of the fan?", "First, to make sure the drivers work correctly (and when they did not work correctly, the numbers were completely different), let\u2019s test all the available cards with gpu-burn. The result is on the picture and correlates very strongly with what is reported in the reviews.", "MIG wasn\u2019t tested here, you will see why further in the article.", "It is important to note here that we bought the 1080 Tis and Titan Xs from the second hand market almost \u201cnew\u201d (less than a year of use). We will not dwell once again on the holy wars about miners and Nvidia\u2019s pricing policy, but if you use even secondhand gaming cards carefully, their service life is about 3\u20134 years. Prices and characteristics are approximate. According to the information from Nvidia partners in Russia, only one A100 is on sale until the new year. When new 1080 Ti\u044b were available, prices ranged from about 50k to 100k rubles.", "And now let\u2019s move on to the most interesting thing \u2014 to real down-to-earth tests. In theory, it seems that if the memory and computing capabilities of the 3090 or A100 are 2\u20133 times higher than the 1080 Ti, then 1 such card can replace 2\u20133 1080 Ti and a standard server with 4 proper PCIE ports can replace a server with 12 cards? Or is it possible to take, let\u2019s say, 3\u20134 PCIE versions of A100 and get a very powerful server, dividing each of them into several compute instances using MIG?", "The short answer is no, the longer answer is also no, but with many caveats.", "Why, would you ask? Well, server rack platforms that fully support 8\u201316 video cards even in the smallest reasonable configuration cost 4\u20135 times more expensive than standard ATX professional solutions. And DGX Workstation or DGX are sold with about a 50% premium to similar configurations assembled on Mikrotik or Gigabyte platforms.", "Card manufacturers are in no hurry to release fully-fledged single-slot GPUs (except for PNY with the Quadro series, but this is a separate story and is more likely for design or inference). Of course, you can assemble a custom water circuit for 7 cards (there were several motherboard models with 7 proper PCIE ports), but it\u2019s \u201cdifficult\u201d and it\u2019s not clear where to host it (and the game is not worth the trouble). With the advent of PCIE 4.0, the attractiveness of such solutions, in theory, should grow, but I haven\u2019t seen anything interesting on the market yet.", "A couple of remarks about the task on which we tested:", "Contrary to the trend of making more and more gigantic networks, we are miniaturizing our algorithms and are trying to make our networks more and more efficient. Therefore, it is more interesting to increase the worker count, not the networks size or batch size.", "And here we come across the first pitfall ( https://t.me/snakers4/2590) \u2014 Distributed Data Parallel from PyTorch (DDP, the optimal way of scaling networks to \u201cmany\u201d video cards) out of the box is essentially configured only for 1 process on 1 card. That is, 1 process can use 1+ card. 2 processes cannot use 1 card, even if there is more than enough IO / compute / RAM. In older driver versions, there is no explicit limitation, and on 1080 Ti 2 processes per 1 card could be launched (but the speed increase is only 5\u201310% instead of 40\u201350%). With the new cards, an exception has already been cut in there.", "But not everything is so sad and bad. Maybe because of some low-level magic in the drivers, maybe because of TF32 (I hope experts will prompt here), maybe because of the developments in MPS 3090s behave a little differently in our benchmark:", "When we try to run 2 DDP workers on 1 card, we just get an error, when we try to train 2 networks \u201cat the same time\u201d we get a proportional slowdown, when the batch increases, the speed gain is insignificant. The timings for 2 * 3090 are like this:", "For the sake of completeness, it is also important to note that Nvidia has an MPS which supposedly allows you to spin 2 processes on the cards without switching the context, and PyTorch has a built-in RPC framework. But I simply could not adequately use the former without getting incomprehensible low-level errors, and the latter requires a radical rewriting of the code and drastically complicates the code for training models (although it is very interesting in the long term).", "So, with 3090 everything is clear. It will not replace two cards, of course, but by itself, even with \u201cextra\u201d memory (I remind, we train small networks), it works 2\u20133 times faster. Whether this is equivalent to having 2\u20133 cards depends on the task.", "Having looked at the metrics, availability, and price of cards, the A100 at first glance does not seem to be an interesting option at all, except for perhaps to train for 3 days 1 large network on 16 A100s on a small, not very private dataset in the cloud. Also, if AMP / FP16 helps your algorithms a lot, then A100 can significantly add speed.", "But the A100 has an interesting MIG technology (Multi-Instance GPU). In fact, it allows you to break one \u201clarge and powerful\u201d card into a set of small \u201csubcards\u201d and then create virtual Compute Instances, which can be accessed as separate cards.", "There are quite a lot of details, check the documentation for them, but the following presets are available there:", "The question arises, what if our network is small, and A100 in theory (at least on FP16) should be 2 times more powerful than 3090? Is it possible to take 4 A100 and make 12 GPUs similar to 1080 Ti? Is it possible to train neural networks on these numerous \u201cmicro-cards\u201d in the same way as on several conventional ones?", "We will answer the questions one by one. Here are both the documentation itself and a very recent blog post from Nvidia.", "There is a paragraph in the documentation:", "At first, when I read it, it seemed to me that it just meant that you cannot divide 2 cards at the same time. After I tried to play around with a real card, it turned out that the framework inside the container sees only 1 \u201ccard\u201d (and apparently it only selects the \u201cfirst\u201d one). Moreover, if we carefully read the examples that Nvidia gives in its blog, they essentially all refer to the scenario \u201c1 container \u2014 1 piece of the card\u201d or \u201ctuning 7 small models in parallel\u201d.", "There is also a passage like this:", "If you use MIG for its intended purpose, that is, divide the card into physical pieces (slices), assign them Compute Instances, and drop them into isolated containers \u2014 then everything works as it should. It just works. Otherwise \u2014 it does not.", "Here are not really ideal comparisons (on Titan I had DP and not DDP), and on the A100, in the end, I did not run experiments for 10, 20, 30 hours (why pollute the atmosphere), but I measured the time for 1 epoch.", "When you launch 1 network on the A100, the utilization does not even reach half \u2014 well, that is, if it could be cut into 2\u20133 cards, everything would be fine", "On 1080 Ti, resources were only to run 1 epoch.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd7196b48a64&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://aveysov.medium.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": ""}, {"url": "https://aveysov.medium.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Alexander Veysov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff29885e9bef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&user=Alexander+Veysov&userId=f29885e9bef3&source=post_page-f29885e9bef3----d7196b48a64---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/", "anchor_text": "similar"}, {"url": "https://www.youtube.com/watch?v=deqljDr618c&t=1100s", "anchor_text": "videos"}, {"url": "https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices", "anchor_text": "TF32"}, {"url": "https://github.com/egorsmkv/speech-recognition-uk", "anchor_text": "dataset"}, {"url": "https://t.me/snakers4/2590", "anchor_text": "https://t.me/snakers4/2590"}, {"url": "https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf", "anchor_text": "MPS"}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#lgi", "anchor_text": "MIG technology"}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts", "anchor_text": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html"}, {"url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts", "anchor_text": "documentation"}, {"url": "https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/", "anchor_text": "post"}, {"url": "https://habr.com/ru/post/531436/", "anchor_text": "https://habr.com"}, {"url": "https://medium.com/tag/hardware?source=post_page-----d7196b48a64---------------hardware-----------------", "anchor_text": "Hardware"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d7196b48a64---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&user=Alexander+Veysov&userId=f29885e9bef3&source=-----d7196b48a64---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&user=Alexander+Veysov&userId=f29885e9bef3&source=-----d7196b48a64---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd7196b48a64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d7196b48a64---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d7196b48a64--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d7196b48a64--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d7196b48a64--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d7196b48a64--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d7196b48a64--------------------------------", "anchor_text": ""}, {"url": "https://aveysov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://aveysov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexander Veysov"}, {"url": "https://aveysov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "87 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff29885e9bef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&user=Alexander+Veysov&userId=f29885e9bef3&source=post_page-f29885e9bef3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Faf8197dc5186&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-with-nvidias-new-ampere-gpus-and-trying-mig-d7196b48a64&newsletterV3=f29885e9bef3&newsletterV3Id=af8197dc5186&user=Alexander+Veysov&userId=f29885e9bef3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}