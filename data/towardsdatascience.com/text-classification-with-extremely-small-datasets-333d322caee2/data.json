{"url": "https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2", "time": 1683001491.8240771, "path": "towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2/", "webpage": {"metadata": {"title": "Text Classification with Extremely Small Datasets | by Anirudh Shenoy | Towards Data Science", "h1": "Text Classification with Extremely Small Datasets", "description": "As the saying goes, in this era of deep learning \u201cdata is the new oil\u201d. However, unless you work for a Google, a Facebook or some other tech giant, getting access to adequate data can be a tough\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait", "anchor_text": "Why BuzzFeed Doesn\u2019t Do Clickbait", "paragraph_index": 8}, {"url": "https://people.mpi-sws.org/~achakrab/papers/chakraborty_clickbait_asonam16.pdf", "anchor_text": "Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media", "paragraph_index": 9}, {"url": "https://github.com/bhargaviparanjape/clickbait", "anchor_text": "repo", "paragraph_index": 9}, {"url": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#The_curse_of_dimensionality_and_overfitting", "anchor_text": "blog", "paragraph_index": 21}, {"url": "https://github.com/anirudhshenoy/text-classification-small-datasets", "anchor_text": "GitHub repo", "paragraph_index": 26}, {"url": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html", "anchor_text": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html", "paragraph_index": 29}, {"url": "https://webis.de/downloads/publications/papers/stein_2016b.pdf", "anchor_text": "Potthast et al", "paragraph_index": 61}, {"url": "http://linkedin.com/in/anirudhshenoy/", "anchor_text": "linkedin.com/in/anirudhshenoy/", "paragraph_index": 134}], "all_paragraphs": ["As the saying goes, in this era of deep learning \u201cdata is the new oil\u201d. However, unless you work for a Google, a Facebook or some other tech giant, getting access to adequate data can be a tough task. This is especially true for small companies operating in niche domains or personal projects that you or I might have.", "In this blog, we\u2019ll simulate a scenario where we only have access to a very small dataset and explore this concept at length. In particular, we\u2019ll build a text classifier that can detect clickbait titles and experiment with different techniques and models to deal with small datasets.", "Often, you might have come across titles like these:", "\u201cWe tried building a classifier with a small dataset. You Wont Believe What Happens Next!\u201d", "\u201cWe love these 11 techniques to build a text classifier. # 7 will SHOCK you.\u201d", "\u201cSmart Data Scientists use these techniques to work with small datasets. Click to know what they are\u201d", "These types of catchy titles are all over the internet. But what makes a title \u201cClickbait-y\u201d? Wikipedia defines it as :", "Clickbait is a form of false advertisement which uses hyperlink text or a thumbnail link that is designed to attract attention and entice users to follow that link and read, view, or listen to the linked piece of online content, with a defining characteristic of being deceptive, typically sensationalized or misleading.", "In general, the question of whether a post is clickbait or not seems to be rather subjective. (Check out: \u201cWhy BuzzFeed Doesn\u2019t Do Clickbait\u201d [1]). This means that while finding a dataset, it would be best to look for one that is manually reviewed by multiple people.", "After some searching, I found: Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media by Chakraborty et al (2016)[2] and their accompanying Github repo", "The dataset contains 15,000+ article titles that have been labeled as clickbait and Non-clickbait. The non-clickbait titles come from Wikinews and have been curated by the Wikinews community while the clickbait titles come from \u2018BuzzFeed\u2019, \u2018Upworthy\u2019 etc.", "To ensure there aren\u2019t any false positives, the titles labeled as clickbait were verified by six volunteers and each title was further labeled by at least three volunteers. Section 2 of the paper contains more details.", "Baseline performance: The authors used 10-fold CV on a randomly sampled 15k dataset (balanced). The best results they achieved were with RBF-SVM achieving an accuracy of 93%, Precision 0.95, Recall 0.9, F1 of 0.93, ROC-AUC of 0.97", "We\u2019ll work with 50 data points for our train set and 10000 data points for our test set. This means the train set is just 0.5% of the test set. We will not use any part of our test set in training and it will merely serve the purpose as a leave-out validation set.", "Keeping track of performance metrics will be critical in understanding how well our classifier is doing as we progress through different experiments. F1-Score will be our main performance metric but we\u2019ll also keep track of Precision, Recall, ROC-AUC and Accuracy.", "Before we dive in, it\u2019s important to understand why small datasets are difficult to work with:", "Notice how the decision boundary changes wildly. This is because the classifier struggles to generalize with the small amount of data. Mathematically, this means our prediction will have high variance.", "I. Regularization: We\u2019ll have to use large amounts of L1, L2 and other forms of regularization.", "Ii. Simpler models: Low complexity linear models like Logistic Regression and SVMs will tend to perform better as they have smaller degrees of freedom.", "Outliers have dramatic effects on small datasets as they can skew the decision boundary significantly. In the plots below I added some noise and changed the label of one of the data points making it an outlier \u2014 notice the effect this has on the decision boundary.", "Outlier detection and Removal: We can use clustering algorithms like DBSCAN or ensemble methods like Isolation Forests", "As more features are added, the classifier has a higher chance to find a hyperplane to split the data. However, if we increase the dimensionality without increasing the number of training samples, the feature space becomes more sparse and the classifier overfits easily. This is a direct result of the curse of dimensionality \u2014 best explained in this blog", "I. Decomposition Techniques: PCA/SVD to reduce the dimensionality of the feature space", "II. Feature Selection: To remove features that aren\u2019t useful in prediction.", "We\u2019ll dive into these solutions in this blog.", "Let\u2019s begin by splitting our data into train and test sets. As mentioned earlier, we\u2019ll use 50 data points for train and 10000 data points for test.", "(To keep things clean here I\u2019ve removed some trivial code: You can check the GitHub repo for the complete code)", "An important step here is to ensure that our train and test sets come from the same distribution so that any improvements on the train set is reflected in the test set.", "A common technique used by Kagglers is to use \u201cAdversarial Validation\u201d between the different datasets. (I\u2019ve seen it go by many names, but I think this one is the most common)", "The idea is very simple, we mix both datasets and train a classifier to try and distinguish between them. If the classifier fails to do so \u2014 we can conclude that the distributions are similar. You can read more here: https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html", "ROC AUC is the preferred metric \u2014 a value of ~ 0.5 or lower means the classifier is as good as a random model and the distributions are the same.", "Let\u2019s use Bag-Of-Words to encode the titles before doing adversarial validation", "The low AUC value suggests that the distributions are similar.", "Just to see what would happen if the distributions were different, I ran a web crawler on breitbart.com, a news source that is not used in the dataset, and collected some article titles.", "The AUC values are much higher indicating that the distributions are different.", "Now let\u2019s move ahead and do some basic EDA on the train dataset.", "Let\u2019s start by checking if the datasets are balanced:", "Next, let\u2019s check the effect of number of words.", "Looks like Clickbait titles have more words in them. What about mean word length?", "Clickbait titles use shorter words as compared to non-clickbait titles. Since clickbait titles generally have simpler words, we can check what % of the words in the titles are stop-words", "Strange, the clickbait titles seem to have no stopwords that are in the NLTK stopwords list. This is probably a coincidence because of the train-test split or we need to expand our stop word list. Something to explore during feature engineering for sure. Also, stop word removal as a preprocessing step is not a good idea here.", "A word cloud can help us identify words that are more prominent in each class. Let\u2019s take a look:", "The distribution of words is quite different between clickbait and non-clickbait titles. For eg: Non-clickbait titles have states/countries like \u201cNigeria\u201d, \u201cChina\u201d, \u201cCalifornia\u201d etc and words more associated with the news like \u201cRiots\u201d, \u201cGovernment\u201d and \u201cbankruptcy\u201d. Non-clickbait titles seem to have more generic words like \u201cFavorite\u201d, \u201crelationships\u201d, \u201cthing\u201d etc", "Using Bag-Of-Words, TF-IDF or word embeddings like GloVe/W2V as features should help here. At the same time, we might also be able to get a lot of performance improvements with simple text features like lengths, word-ratios, etc.", "Let\u2019s try TSNE on Bag-of-Words encoding for the titles:", "Both the classes seem to be clustered together with BoW encoding. In the next section, we\u2019ll explore different embedding techniques.", "Before we start exploring embeddings lets write a couple of helper functions to run Logistic Regression and calculate evaluation metrics.", "Since we want to optimize our model for F1-Scores, for all models we\u2019ll first predict the probability of the positive class. We\u2019ll then use these probabilities to get the Precision-Recall curve and from here we can select a threshold value that has the highest F1-score. To predict the labels we can simply use this threshold value.", "In this section, we\u2019ll encode the titles with BoW, TF-IDF and Word Embeddings and use these as features without adding any other hand-made features.", "TFIDF performs slightly better than BoW. An interesting fact is that we\u2019re getting an F1 score of 0.837 with just 50 data points. This is why Log Reg + TFIDF is a great baseline for NLP classification tasks.", "Next, let\u2019s try 100-D GloVe vectors. We\u2019ll use the PyMagnitude library:(PyMagnitude is a fantastic library that includes great features like smart out-of-vocab representations. Highly recommended!)", "Since titles can have varying lengths, we\u2019ll find the GloVe representation for each word and average all of them together giving a single 100-D vector representation for each title.", "Woah! That\u2019s a huge increase in F1 score with just a small change in title encoding. The improved performance is justified since W2V are pre-trained embeddings that contain a lot of contextual information. This would contribute to the performance of the classifier, especially when we have a very limited dataset.", "Instead of just taking the average of each word, what if we did a weighted average \u2014 in particular, IDF-Weighted average?", "Our F1 increased by ~0.02 points. The increased performance makes sense \u2014 commonly occurring words get less weightage while less frequent (and perhaps more important) words have more say in the vector representation for the titles.", "Since GloVe worked so well, let\u2019s try one last embedding technique \u2014 Facebook\u2019s InferSent model. This model converts the entire sentence into a vector representation. However, a potential problem is that the vector representations are 4096 dimensional which might cause our model to overfit easily. Let\u2019s give it a shot anyway:", "As expected the performance drops \u2014 most likely due to overfitting from the 4096-dimensional features.", "Before we end this section, let\u2019s try TSNE again this time on IDF-Weighted Glove vectors", "This time we see some separation between the 2 classes in the 2D projection. To some extent, this explains the high accuracy we achieved with simple Log Reg.", "To increase performance further, we can add some hand made features. Let\u2019s try this in the next section.", "Creating new features can be tricky. The best way to get a headstart on this is to dive into the domain and look for research papers, blogs, articles, etc. Kaggle Kernels in related domains are also a good way to find information on interesting features.", "For clickbait detection, the paper we used for the dataset (Chakraborthy et al) mentioned a few features they used. I also found Potthast et al (2016) [3] in which they documented over 200 features.", "We can implement some of the easy ones along with the Glove embeddings from the previous section and check for any performance improvements. Here\u2019s a quick summary of the features:", "After implementing these we can choose to expand the feature space with polynomial (eg X\u00b2) or interaction features (eg XY) by using sklearn\u2019s PolynomialFeatures()", "Note: The choice of feature scaling technique made quite a big difference to the performance of the classifier, I tried RobustScaler, StandardScaler, Normalizer and MinMaxScaler and found that MinMaxScaler worked the best.", "Nice! We went from an F1 score of 0.957 to 0.964 on simple logistic regression. We might be able to squeeze out some more performance improvements when we try out different models and do hyperparameter tuning later.", "For now, let\u2019s take a short detour into model interpretability to check how our model is making these predictions. We\u2019ll use the SHAP and ELI5 libraries to understand the importance of the features.", "Let\u2019s start with feature importance. This is pretty straightforward with the ELI5 library.", "Apart from the glove dimensions, we can see a lot of the hand made features have large weights. The greener a feature is the more important it is to classify the sample as \u2018clickbait\u2019.", "For example, the starts_with_number feature is very important to classify a title is clickbait. This makes sense because, in the dataset, titles like \"12 reasons why you should XYZ\u201d are often clickbait.", "Let\u2019s take a look at the dale_chall_readability_score feature which has a weight of -0.280. If the Dale Chall Readability score is high, it means that the title is difficult to read. Here, our model has learned that if a title is more difficult to read, it is probably a News title and not clickbait. Pretty cool!", "In addition, there are some features that have a weight very close to 0. Removing these features might help in reducing overfitting, we\u2019ll explore this in the Feature Selection section.", "Now let\u2019s move onto the SHAP Force Plot", "A force plot is like a \u2018tug-of-war\u2019 game between features. Each feature pushes the output of the model to the left or right of the base value. The base value is the average output of the model over the entire Test dataset. Keep in mind this is not a probability value.", "Features in pink help the model detect the positive class i.e. \u2018Clickbait\u2019 titles while features in blue detect the negative class. The width of each feature is directly proportional to its weightage in the prediction.", "In the example above, the starts_with_number feature is 1 and has a lot of importance and hence pushes the model's output to the right. On the other hand, clickbait_subs_ratio and easy_words_ratio (high values in these features usually indicate clickbait, but in this case, the values are low) are both pushing the model to the left.", "We can verify that in this particular example, the model ends up predicting \u2018Clickbait\u2019", "As expected, the model correctly labels the title as clickbait.", "Let\u2019s take a look at another example:", "In this case, the model gets pushed to the left since features like sentiment_pos (clickbait titles usually have a positive sentiment) have a low value.", "Force plots are a wonderful way to take a look at how models do prediction on a sample-by-sample basis. In the next section, we\u2019ll try different models including ensembles along with hyperparameter tuning.", "In this section, we\u2019ll use the features we created in the previous section, along with IDF-weighted embeddings and try them on different models.", "As mentioned earlier, when dealing with small datasets, low-complexity models like Logistic Regression, SVMs, and Naive Bayes will generalize the best. We\u2019ll try these models along with non-parameteric models like KNN and non-linear models like Random Forest, XGBoost, etc.", "We\u2019ll also try bootstrap-aggregating or bagging with the best-performing classifier as well as model stacking. Let\u2019s get started!", "For hyperparameter tuning GridSearchCV is a good choice for our case since we have a small dataset (allowing it to run quickly) and it's an exhaustive search. We\u2019ll need to do a few hacks to make it (a) use our predefined test set instead of Cross-Validation (b) use our F1 evaluation metric which uses PR curves to select the threshold.", "Notice that the tuned parameters use both \u2014 high values of alpha (indicating large amounts of regularization) as well as elasticnet. These parameter choices are because the small dataset overfits easily.", "We can do the same tuning procedure for SVM, Naive Bayes, KNN, RandomForest, and XGBoost. The table below summarizes the results for these (You can refer the GitHub repo for the complete code)", "In the fast.ai course, Jeremy Howard mentions that deep learning has been applied to tabular data quite successfully in many cases. Let\u2019s see how well it performs for our use case:", "The 2-layer MLP model works surprisingly well, given the small dataset.", "Since SVM worked so well, we can try a bagging classifier by using SVM as a base estimator. This should improve the variance of the base model and reduce overfitting.", "The performance increase is almost insignificant.", "Finally, one last thing we can try is the Stacking Classifier (a.k.a Voting classifier)", "This is a weighted average of the predictions of different models. Since we are also using the Keras model we won\u2019t be able to use Sklearn\u2019s VotingClassifier instead we'll just run a simple loop that gets the predictions of each model and runs a weighted average. We\u2019ll use the tuned hyperparameters for each model.", "Now we need a way to select the best weights for each model. The best option is to use an optimization library like Hyperopt that can search for the best combination of weights that maximizes F1-score.", "Hyperopt finds a set of weights that gives an F1 ~ 0.971. Let\u2019s inspect the optimized weights:", "The low complexity models like Logistic Regression, Naive Bayes and SVM have high weights while non-linear models like Random Forest, XGBoost and the 2 \u2014 Layer MLP have much lower weights. This in line with what we had expected i.e. Low complexity and simple models will generalize the best with smaller datasets.", "Finally, running the stacking classifier with the optimized weights gives:", "In the next section, we\u2019ll address another concern with small datasets \u2014 high dimensional feature spaces.", "As we discussed in the intro, the feature space becomes sparse as we increase the dimensionality of small datasets causing the classifier to easily overfit.", "The solution is simply to reduce the dimensionality. Two broad ways to do this are Feature selection and Decomposition.", "These are techniques in which features are selected based on how relevant they are in prediction.", "We\u2019ll start with SelectKBest which, as the name suggests, simply selects the k-best features based on the chosen statistic (by default ANOVA F-Scores)", "A small problem with SelectKBest is that we need manually specify the number of features we want to keep. An easy way around this is to run a loop that checks the F1 score for each value of K. Here\u2019s a plot of the number of features vs F1 Score:", "Approximately 45 features give the best F1 value. Let\u2019s re-run SelectKBest with K = 45 :", "Another option is to use SelectPercentile which uses the percentage of features we want to keep.", "Doing the same procedure as above we get percentile = 37 for the best F1 Score. Now using SelectPercentile:", "Simple feature selection increased the F1 score from 0.966 (previous tuned Log Reg model) to 0.972. As mentioned earlier, this is because the lower-dimensional feature space reduces the chances of the model overfitting.", "For both techniques, we can also use selector.get_support() to retrieve the names of the features that were selected.", "RFE is a backward feature selection technique that uses an estimator to calculate the feature importance at each stage. The word recursive in the name implies that the technique recursively removes features that are not important for classification.", "We\u2019ll use the CV variant which uses cross-validation inside each loop to determine how many features to remove in each loop. RFECV needs an estimator which has the feature_importances_ attribute so we'll use SGDClassifier with log loss.", "We also need to specify the type of cross-validation technique required. We\u2019ll use the same PredefinedSplit that we used during hyperparameter optimization.", "Let\u2019s check the features that were selected:", "This time some additional features were selected that gives a slight boost in performance. Since an estimator and CV set is passed, the algorithm has a better way of judging which features to keep.", "The other advantage here is that we did not have to mention how many features to keep, RFECV automatically finds that out for us. However, we can mention the minimum number of features we'd like to have which by default is 1.", "Finally, let\u2019s try SFS - which does the same thing as RFE but instead adds features sequentially. SFS starts with 0 features and adds features 1-by-1 in each loop in a greedy manner. One small difference is that SFS solely uses the feature sets performance on the CV set as a metric for selecting the best features, unlike RFE which used model weights (feature_importances_).", "We can also check the selected features:", "Forward and backward selection quite often gives the same results. Now let\u2019s take a look at Decomposition techniques.", "Unlike feature selection which picks the best features, decomposition techniques factorize the feature matrix to reduce the dimensionality. Since these techniques change the feature space itself, one disadvantage is that we lose model/feature interpretability. We no longer know what each dimension of the decomposed feature space represents.", "Let\u2019s try TruncatedSVD on our feature matrix. The first thing we\u2019ll have to do is find out how the explained variance changes with the number of components.", "Looks like just 50 components are enough to explain 100% of the variance in the training set features. This means we have a lot of dependent features (i.e. some features are just linear combinations of other features).", "This is in line with what we saw in the feature selection section \u2014 even though we have 119 features, most techniques selected between 40\u201370 features (the remaining features might not be important since they are merely linear combinations of other features).", "Now we can reduce the feature matrix to 50 components.", "The performance is not as good as the feature selection techniques \u2014 Why?", "The main job of decomposition techniques, like TruncatedSVD, is to explain the variance in the dataset with a fewer number of components. While doing this, it never considers the importance each feature had in predicting the target (\u2018clickbait\u2019 or \u2018not-clickbait\u2019). However, in the feature selection techniques, the feature importance or model weights are used each time a feature is removed or added. RFE and SFS in particular select features to optimize for model performance. (You might have noticed we pass \u2018y\u2019 in every fit() call in feature selection techniques.)", "Finally, we can use any of the techniques above with the best performing model \u2014 Stacking Classifier. We\u2019ll have to retune each model to the reduced feature matrix and run hyperopt again to find the best weights for the stacking classifier.", "Now, after using the RFECV selected features and re-tuning:", "Here\u2019s a summary of all the models and experiments we\u2019ve run so far:", "Let\u2019s take a look at the Stacking Classifier\u2019s confusion matrix:", "And here are the top 10 high-confidence misclassified titles:", "All of the high-confidence misclassified titles are \u2018not-clickbait\u2019 and this is reflected in the confusion matrix.", "At first glance, these titles seem to be quite different from the conventional news titles. Here\u2019s a randomly chosen sample of \u2018not-clickbait\u2019 titles from the test set:", "We can try some techniques like Semi-Supervised Pseudo labeling, back-translation, etc to minimize these False Positives but in the interest of blog length, I\u2019ll keep it for another time.", "To conclude, by understanding how overfitting works in small datasets along with techniques like feature selection, stacking, tuning, etc we were able to improve performance from F1 = 0.801 to F1 = 0.98 with a mere 50 samples. Not bad!", "Feel free to connect with me if you have any questions. I hope you enjoyed!", "Product @ Yellow Messenger | Breaking down Machine Learning & Data Science topics into simple concepts | linkedin.com/in/anirudhshenoy/ | Views are personal |"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F333d322caee2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://anirudhshenoy.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Anirudh Shenoy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc577fea1786&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=post_page-c577fea1786----333d322caee2---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F333d322caee2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=-----333d322caee2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F333d322caee2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=-----333d322caee2---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait", "anchor_text": "Why BuzzFeed Doesn\u2019t Do Clickbait"}, {"url": "https://people.mpi-sws.org/~achakrab/papers/chakraborty_clickbait_asonam16.pdf", "anchor_text": "Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media"}, {"url": "https://github.com/bhargaviparanjape/clickbait", "anchor_text": "repo"}, {"url": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#The_curse_of_dimensionality_and_overfitting", "anchor_text": "blog"}, {"url": "https://github.com/anirudhshenoy/text-classification-small-datasets", "anchor_text": "GitHub repo"}, {"url": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html", "anchor_text": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html"}, {"url": "https://webis.de/downloads/publications/papers/stein_2016b.pdf", "anchor_text": "Potthast et al"}, {"url": "https://github.com/anirudhshenoy/text-classification-small-datasets", "anchor_text": "https://github.com/anirudhshenoy/text-classification-small-datasets"}, {"url": "https://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait", "anchor_text": "https://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait"}, {"url": "https://github.com/bhargaviparanjape/clickbait", "anchor_text": "https://github.com/bhargaviparanjape/clickbait"}, {"url": "https://webis.de/downloads/publications/papers/stein_2016b.pdf", "anchor_text": "https://webis.de/downloads/publications/papers/stein_2016b.pdf"}, {"url": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html", "anchor_text": "https://www.kdnuggets.com/2016/10/adversarial-validation-explained.html"}, {"url": "https://github.com/snipe/downworthy", "anchor_text": "https://github.com/snipe/downworthy"}, {"url": "http://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php", "anchor_text": "ttp://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php"}, {"url": "https://github.com/terrier-org/terrier-desktop/blob/master/share/stopword-list.txt", "anchor_text": "https://github.com/terrier-org/terrier-desktop/blob/master/share/stopword-list.txt"}, {"url": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#The_curse_of_dimensionality_and_overfitting", "anchor_text": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#The_curse_of_dimensionality_and_overfitting"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----333d322caee2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----333d322caee2---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/nlp?source=post_page-----333d322caee2---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----333d322caee2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----333d322caee2---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F333d322caee2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=-----333d322caee2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F333d322caee2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=-----333d322caee2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F333d322caee2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc577fea1786&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=post_page-c577fea1786----333d322caee2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa7b82d481cff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&newsletterV3=c577fea1786&newsletterV3Id=a7b82d481cff&user=Anirudh+Shenoy&userId=c577fea1786&source=-----333d322caee2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Written by Anirudh Shenoy"}, {"url": "https://anirudhshenoy.medium.com/followers?source=post_page-----333d322caee2--------------------------------", "anchor_text": "163 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://linkedin.com/in/anirudhshenoy/", "anchor_text": "linkedin.com/in/anirudhshenoy/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc577fea1786&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&user=Anirudh+Shenoy&userId=c577fea1786&source=post_page-c577fea1786----333d322caee2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa7b82d481cff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-extremely-small-datasets-333d322caee2&newsletterV3=c577fea1786&newsletterV3Id=a7b82d481cff&user=Anirudh+Shenoy&userId=c577fea1786&source=-----333d322caee2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Anirudh Shenoy"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Pseudo-Labeling to deal with small datasets \u2014 What, Why & How?A guide to using your model's output to improve your model's output!"}, {"url": "https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "\u00b716 min read\u00b7Dec 3, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd6f903213af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af&user=Anirudh+Shenoy&userId=c577fea1786&source=-----fd6f903213af----0-----------------clap_footer----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=author_recirc-----333d322caee2----0---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd6f903213af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af&source=-----333d322caee2----0-----------------bookmark_preview----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----333d322caee2----1---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----333d322caee2----1-----------------bookmark_preview----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----333d322caee2----2---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----333d322caee2----2-----------------bookmark_preview----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Anirudh Shenoy"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "How Are Convolutions Actually Performed Under the Hood?2 simple tricks that PyTorch & TensorFlow use to speed up convolutions."}, {"url": "https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": "\u00b79 min read\u00b7Dec 13, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F226523ce7fbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-convolutions-actually-performed-under-the-hood-226523ce7fbf&user=Anirudh+Shenoy&userId=c577fea1786&source=-----226523ce7fbf----3-----------------clap_footer----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=author_recirc-----333d322caee2----3---------------------c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F226523ce7fbf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-are-convolutions-actually-performed-under-the-hood-226523ce7fbf&source=-----333d322caee2----3-----------------bookmark_preview----c6cc3c10_d32c_4890_9a8a_f3a8f983abba-------", "anchor_text": ""}, {"url": "https://anirudhshenoy.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "See all from Anirudh Shenoy"}, {"url": "https://towardsdatascience.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Saupin Guillaume"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "How Does XGBoost Handle Multiclass Classification?It\u2019s crucial to understand the underlying workings of classification using this kind of model, as it impacts performance."}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b77 min read\u00b7Jan 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&user=Saupin+Guillaume&userId=891e27328f3a&source=-----6c76ba71f6f0----0-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&source=-----333d322caee2----0-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----333d322caee2----1-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://soumenatta.medium.com/?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://soumenatta.medium.com/?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Dr. Soumen Atta, Ph.D."}, {"url": "https://blog.devgenius.io/?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Dev Genius"}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "NLTK vs SpaCy: Which Library Is Best for Named Entity Recognition?Named Entity Recognition (NER) is the process of extracting and categorizing named entities such as people, organizations, locations, and\u2026"}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b74 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdev-genius%2F2c8d8faf85ac&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fnltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac&user=Dr.+Soumen+Atta%2C+Ph.D.&userId=f044b1850c1e&source=-----2c8d8faf85ac----0-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/nltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac?source=read_next_recirc-----333d322caee2----0---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c8d8faf85ac&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fnltk-vs-spacy-which-library-is-best-for-named-entity-recognition-2c8d8faf85ac&source=-----333d322caee2----0-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----1-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----333d322caee2----1---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----333d322caee2----1-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/unsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://clementdelteil.medium.com/?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://clementdelteil.medium.com/?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Cl\u00e9ment Delteil"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/unsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Unsupervised Sentiment Analysis With Real-World Data: 500,000 Tweets on Elon MuskGuided walkthrough in a real-world Natural Language Processing project."}, {"url": "https://pub.towardsai.net/unsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b715 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F3f0653135558&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Funsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558&user=Cl%C3%A9ment+Delteil&userId=2ab6c2c2e603&source=-----3f0653135558----2-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/unsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558?source=read_next_recirc-----333d322caee2----2---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f0653135558&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Funsupervised-sentiment-analysis-with-real-world-data-500-000-tweets-on-elon-musk-3f0653135558&source=-----333d322caee2----2-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/imbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Amy @GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo/imbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "Imbalanced Multi-Label Classification: Balanced Weights May Not Improve Your Model PerformanceCompare the random forest model and logistic regression model with and without balanced weights on imbalanced multi-class classification"}, {"url": "https://medium.com/grabngoinfo/imbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": "\u00b711 min read\u00b7Feb 1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgrabngoinfo%2Fcf71c6df030c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fimbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c&user=Amy+%40GrabNGoInfo&userId=ef6171ffb4ed&source=-----cf71c6df030c----3-----------------clap_footer----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/imbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c?source=read_next_recirc-----333d322caee2----3---------------------66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf71c6df030c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fimbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance-cf71c6df030c&source=-----333d322caee2----3-----------------bookmark_preview----66b5eed9_a999_4b2c_8908_3c3d3cdffd54-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----333d322caee2--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----333d322caee2--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}